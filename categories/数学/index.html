<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>数学 | s-ai-unix's Blog</title><meta name=keywords content><meta name=description content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/categories/%E6%95%B0%E5%AD%A6/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://s-ai-unix.github.io/categories/%E6%95%B0%E5%AD%A6/index.xml title=rss><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/categories/%E6%95%B0%E5%AD%A6/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/categories/%E6%95%B0%E5%AD%A6/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="数学"><meta property="og:description" content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="数学"><meta name=twitter:description content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body class=list id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/categories/>Categories</a></div><h1>数学</h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/2026-01-26-connection-differential-geometry.jpg alt=微分几何中的联络></figure><header class=entry-header><h2 class=entry-hint-parent>微分几何中的联络：一场从直观到严格的数学之旅</h2></header><div class=entry-content><p>引言：一个根本的数学困境 想象你站在地球表面的赤道上，手里拿着一根箭，箭头指向正北方。现在，你带着这根箭沿着赤道向东行走，始终保持箭头指向"正北方"（相对于你当前的地理位置）。当你绕地球一周回到起点时，会发生什么？
这个看似简单的问题揭示了微分几何中一个深刻的困境：如何比较流形上不同点的切向量？
图1：球面上的平行移动（交互式）。红色箭头表示向量在赤道上平行移动。你可以拖动图形来从不同角度观察，注意绕赤道一周后向量方向的变化。
在欧几里得空间中，我们从来不需要担心这个问题。如果在 $\mathbb{R}^n$ 的两个不同点 $p$ 和 $q$ 各有一个向量 $v_p$ 和 $v_q$，我们可以直接平移 $v_p$ 到 $q$ 点，然后和 $v_q$ 比较。这是因为欧氏空间有一个自然的平行性——所有点的切空间都可以自然地等同起来。
图3：联络的必要性（交互式）。左侧展示欧氏空间中不同点的切向量可以直接比较（因为切空间自然等同）；右侧展示弯曲流形上需要"联络"来定义不同点切空间之间的对应关系（橙色虚线表示联络）。你可以拖动图形从不同角度观察。
图2：在平面上，不同点的切向量可以直接平移比较。每个点上的红色箭头代表同一个向量平移后的结果。
但在一般的流形上，比如球面上，没有这种自然的等同。每一点的切空间都是一个独立的向量空间，点与点之间的切空间之间没有任何天然的联系。这就是联络概念要解决的根本问题：如何在流形上建立不同点切空间之间的"联络"，从而能够定义方向导数、平行移动，并最终定义曲率。
联络的概念是现代微分几何的基石，它的历史可以追溯到19世纪中叶。Riemann 在1854年的著名演讲《论几何基础的假设》中已经隐含了联络的思想，但严格的数学表述则是由Levi-Civita、Christoffel、Ricci、Cartan等人在后续几十年中逐步完善的。本文将带你踏上一段从直观到严格的数学之旅，深入理解这个优美而深刻的数学概念。
第一章：预备知识——流形与切丛 在深入联络的概念之前，我们需要一些基本的几何语言。如果你已经熟悉流形和切丛的概念，可以快速浏览这一章。
1.1 什么是流形？ 直观地说，流形是一个局部看起来像欧氏空间，但整体可能有复杂弯曲结构的几何对象。
一维流形：曲线，如圆、线段 二维流形：曲面，如球面、环面、甜甜圈表面 高维流形：难以直接可视化，但数学定义同样适用 形式化定义：一个 $n$ 维拓扑流形是一个豪斯多夫空间 $M$，使得对于任意 $p \in M$，存在一个开邻域 $U \subset M$ 和同胚映射 $\phi: U \to V$，其中 $V$ 是 $\mathbb{R}^n$ 的开子集。$(U, \phi)$ 称为一个坐标卡或坐标图。
1.2 切空间与切向量 在 $\mathbb{R}^n$ 中，切向量的概念很直观：它是一个指向某个方向的箭头。但在流形上，我们需要更仔细地定义切向量。
有几种等价的定义方式：
定义1（方向导数视角）：$p$ 点的切向量是作用在函数上的方向导算子。如果 $v$ 是一个切向量，$\gamma: (-\varepsilon, \varepsilon) \to M$ 是一条满足 $\gamma(0) = p$ 的曲线，那么： $$ v[f] = \left.\frac{d}{dt}\right|_{t=0} f(\gamma(t)) $$
...</p></div><footer class=entry-footer><span title='2026-01-26 19:30:00 +0800 CST'>January 26, 2026</span>&nbsp;·&nbsp;<span>17 min</span>&nbsp;·&nbsp;<span>3558 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 微分几何中的联络：一场从直观到严格的数学之旅" href=https://s-ai-unix.github.io/posts/2026-01-26-connection-differential-geometry/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/calculus-ml-journey.jpg alt=微积分的几何美感></figure><header class=entry-header><h2 class=entry-hint-parent>微积分与机器学习：从变化率到神经网络梯度的完整旅程</h2></header><div class=entry-content><p>引言：为什么需要微积分？ 想象你在山上，想找到最低点。你会怎么做？你会观察脚下的坡度，选择最陡峭的方向迈出一步，然后重复这个过程。这个简单的直觉——沿着负梯度方向走——正是现代人工智能的核心算法。
从ChatGPT的语言模型到AlphaGo的围棋策略，从图像识别到语音合成，所有这些技术背后都有一个共同的数学基础：微积分。
微积分研究的是变化。而机器学习本质上是关于优化——通过不断调整参数来减少错误。当我们在高维空间中优化复杂的神经网络时，微积分提供了描述和计算这种变化的精确语言。
这篇文章将带你深入理解微积分如何驱动现代人工智能。我们不会停留在表面，而是会深入到数学推导的核心，揭示梯度下降、反向传播等算法的数学本质。这是一次从17世纪牛顿和莱布尼茨的发明，到21世纪深度学习革命的完整旅程。
第一部分：微积分基础理论 1. 导数的本质：从变化率到瞬时变化率 1.1 变化率的直观理解 变化率是人类最早思考的数学问题之一。如果一辆车2小时行驶100公里，平均速度是50公里/小时。但它某一时刻的瞬时速度是多少？
微积分的答案是：用极限。考虑函数 $f(x)$ 在 $x_0$ 附近的平均变化率： $$ \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x} $$
当 $\Delta x \to 0$ 时，这个平均变化率的极限就是导数： $$ f^{\prime}(x_0) = \lim_{\Delta x \to 0} \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x} $$
1.2 导数的几何意义 几何直观：导数是切线的斜率。在 $x_0$ 处，曲线 $f(x)$ 可以用直线（切线）逼近： $$ f(x) \approx f(x_0) + f^{\prime}(x_0)(x - x_0) $$
这就是一阶泰勒公式，也是线性化的思想：局部用简单的线性函数逼近复杂的非线性函数。
严格定义（$\epsilon-\delta$ 语言）： $$ \forall \epsilon > 0, \exists \delta > 0 \text{ s.t. } |\Delta x| &lt; \delta \implies \left|\frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x} - f^{\prime}(x_0)\right| &lt; \epsilon $$
...</p></div><footer class=entry-footer><span title='2026-01-25 19:00:00 +0800 CST'>January 25, 2026</span>&nbsp;·&nbsp;<span>9 min</span>&nbsp;·&nbsp;<span>1716 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 微积分与机器学习：从变化率到神经网络梯度的完整旅程" href=https://s-ai-unix.github.io/posts/2026-01-25-calculus-ml-systematic-review/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/spectral-theorem.jpg alt=谱定理可视化></figure><header class=entry-header><h2 class=entry-hint-parent>谱定理：线性代数的优雅与机器学习的基石</h2></header><div class=entry-content><p>引言：对称性的数学之美 在数学的众多分支中，有一个深刻的原理反复出现：对称性带来简化。在物理学中，空间的对称性意味着守恒量；在群论中，对称结构导致简单的表示；在线性代数中，对称矩阵拥有最优雅的对角化理论——这就是谱定理。
想象你站在一个椭圆中心。如果你沿任意方向看出去，椭圆的"宽度"各不相同。但有两个特殊的方向——椭圆的长轴和短轴——沿这些方向，椭圆的形状最简单，只是一个被拉伸的圆。这两个正交的方向，就是椭圆的"主轴"，它们对应的拉伸倍数，就是"特征值"。
这个直观的几何图像，正是谱定理的核心。谱定理告诉我们：任何实对称矩阵都可以通过正交变换对角化。换句话说，在适当的坐标系下，对称矩阵描述的线性变换只是沿某些正交方向的简单拉伸。
在机器学习和深度学习中，谱定理无处不在。从主成分分析（PCA）到奇异值分解（SVD），从谱聚类到图神经网络，谱定理提供了理解数据和算法的理论基础。
在这篇文章中，我们将系统性地介绍谱定理的核心理论，从实对称矩阵的正交对角化到一般的奇异值分解，从PCA到谱聚类，深入浅出地推导每一个公式，并通过可视化图形直观理解这些概念。
第一章：谱定理的基础理论 1.1 特征值与特征向量：不变的方向 给定一个 $n \times n$ 矩阵 $A$，如果存在非零向量 $v \in \mathbb{R}^n$ 和标量 $\lambda \in \mathbb{R}$，使得
$$ Av = \lambda v $$
则称 $\lambda$ 是 $A$ 的特征值，$v$ 是对应的特征向量。
几何意义：特征向量 $v$ 是线性变换 $A$ 下的"不变方向"——变换后，这个向量只是被拉伸或压缩了 $\lambda$ 倍，方向保持不变。
特征多项式：特征值是特征方程的根
$$ \det(A - \lambda I) = 0 $$
对于 $n \times n$ 矩阵，这是一个 $n$ 次多项式，在复数域上有 $n$ 个根（计入重数）。
1.2 对称矩阵的特殊性质 实对称矩阵 $A \in \mathbb{R}^{n \times n}$（即 $A^\top = A$）拥有三个重要性质：
...</p></div><footer class=entry-footer><span title='2026-01-25 18:00:00 +0800 CST'>January 25, 2026</span>&nbsp;·&nbsp;<span>7 min</span>&nbsp;·&nbsp;<span>1458 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 谱定理：线性代数的优雅与机器学习的基石" href=https://s-ai-unix.github.io/posts/2026-01-25-spectral-theorem/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/riemann-geometry.jpg alt=黎曼几何可视化></figure><header class=entry-header><h2 class=entry-hint-parent>黎曼几何：弯曲空间的优雅语言</h2></header><div class=entry-content><p>引言：从平行公设到弯曲空间 在人类思想的漫长历程中，欧几里得几何曾被视为绝对真理的典范。两千多年来，人们相信平行公设——“给定一条直线和一个点，通过该点有且仅有一条平行线”——是放之四海而皆准的真理。
然而，数学的进步往往源于对"显而易见"的质疑。19世纪，几位大胆的数学家独立发现：如果改变平行公设，可以得到完全自洽的几何体系。高斯、波尔约、罗巴切夫斯基发现了双曲几何（负曲率几何），而黎曼则走得更远——他设想了一种全新的几何，其中空间的性质可以逐点变化。
1854年，黎曼在哥廷根大学的著名演讲《论几何基础的假设》中，提出了一个革命性的概念：空间本身可以是弯曲的，而且这种弯曲可以因位置而异。这一思想后来成为爱因斯坦广义相对论的数学基础。
在黎曼几何中，距离不再由简单的勾股定理给出，而是由一个依赖于位置的"度量张量"决定。直线被"测地线"取代，平行移动会导致向量旋转，曲率不再是单一数值而是一个复杂的张量。
在这篇文章中，我们将系统性地介绍黎曼几何的核心概念，从度量张量到曲率张量，从测地线到指数映射，从Ricci流到庞加莱猜想。我们不仅要理解这些概念的数学形式，更要感受它们所蕴含的深刻几何直觉。
第一章：黎曼流形的基础概念 1.1 从欧氏空间到流形 欧几里得空间 $\mathbb{R}^n$ 是最简单的几何空间。在 $\mathbb{R}^n$ 中，距离由勾股定理给出：两点 $x = (x_1, \ldots, x_n)$ 和 $y = (y_1, \ldots, y_n)$ 之间的距离是
$$ d(x, y) = \sqrt{\sum_{i=1}^n (y_i - x_i)^2} $$
这个公式隐含了一个假设：空间在任何地方、任何方向上的"测量标准"都是一样的。但如果我们放松这个假设呢？
黎曼流形的直觉：想象一张可以任意弯曲但不能拉伸的橡皮膜。膜上每一点的"拉伸程度"不同，导致距离的测量方式也不同。这就是黎曼流形的直观图像。
定义：黎曼流形 $(M, g)$ 是一个光滑流形 $M$ 配备一个黎曼度量 $g$。黎曼度量 $g$ 是一个对称、正定的 $(0, 2)$ 型张量场，即在每一点 $p \in M$，$g_p$ 是切空间 $T_pM$ 上的内积。
1.2 局部坐标与度量张量 在局部坐标系 $(x^1, \ldots, x^n)$ 下，黎曼度量可以表示为
$$ g = \sum_{i,j=1}^n g_{ij} dx^i \otimes dx^j $$
...</p></div><footer class=entry-footer><span title='2026-01-25 17:00:00 +0800 CST'>January 25, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>925 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 黎曼几何：弯曲空间的优雅语言" href=https://s-ai-unix.github.io/posts/2026-01-25-riemann-geometry/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/intrinsic-extrinsic.jpg alt=内蕴与外蕴几何></figure><header class=entry-header><h2 class=entry-hint-parent>内蕴与外蕴：几何学的两种视角</h2></header><div class=entry-content><p>引言：蚂蚁与上帝 想象一只生活在一个曲面上的蚂蚁。这只蚂蚁不知道它生活在一个二维曲面上，它只知道在自己的"世界"里移动。如果蚂蚁沿着某个方向走了一圈，回到起点，它会发现走过的角度不等于 360 度——这在圆柱面上是 720 度（转了两圈），但在球面上可能大于 360 度。这只蚂蚁能感知到的几何性质，就是我们所说的内蕴几何。
现在想象一个悬浮在曲面之上的观察者——我们称之为"上帝视角"。这个观察者能看到曲面在三维空间中的具体形状，知道曲面是弯的、扭的、有孔的。这个观察者能看到的几何性质，就是我们所说的外蕴几何。
内蕴几何与外蕴几何的区别，是微分几何中最核心、最美妙的概念之一。理解了这两个概念，你就掌握了理解黎曼几何的钥匙。
在本篇文章中，我们将从直观的例子出发，系统性地介绍内蕴几何与外蕴几何的核心内容，探讨它们的区别与联系，并解释 Gauss 的绝妙定理——高斯曲率是内蕴的这一革命性发现。
第一章：内蕴几何——曲面本身的语言 1.1 蚂蚁的视角：什么是内蕴几何 内蕴几何研究的是不依赖于曲面如何嵌入外部空间的几何性质。简单来说，就是"生活在曲面上的生物"所能感知到的几何性质。
假设一只蚂蚁生活在一个曲面上。这只蚂蚁可以：
在曲面上爬行，测量两点之间的路径长度 测量区域的面积 画三角形，计算角度 沿着某个方向走一圈，测量角度的"亏空"或"过剩" 所有这些测量都不需要蚂蚁知道"曲面是在三维空间中的"。
1.2 第一基本形式：内蕴几何的度量工具 为了描述曲面的内蕴几何，我们需要一个数学工具来测量长度和角度。这个工具就是第一基本形式。
设曲面由参数方程 $\mathbf{r}(u, v) = (x(u, v), y(u, v), z(u, v))$ 给出。
定义三个基本量：
$$ \begin{aligned} E &= \frac{\partial \mathbf{r}}{\partial u} \cdot \frac{\partial \mathbf{r}}{\partial u} = x_u^2 + y_u^2 + z_u^2 \ F &= \frac{\partial \mathbf{r}}{\partial u} \cdot \frac{\partial \mathbf{r}}{\partial v} = x_u x_v + y_u y_v + z_u z_v \ G &= \frac{\partial \mathbf{r}}{\partial v} \cdot \frac{\partial \mathbf{r}}{\partial v} = x_v^2 + y_v^2 + z_v^2 \end{aligned} $$
...</p></div><footer class=entry-footer><span title='2026-01-25 16:00:00 +0800 CST'>January 25, 2026</span>&nbsp;·&nbsp;<span>3 min</span>&nbsp;·&nbsp;<span>511 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 内蕴与外蕴：几何学的两种视角" href=https://s-ai-unix.github.io/posts/2026-01-25-intrinsic-extrinsic-geometry/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/ig-overview.jpg alt=信息几何可视化></figure><header class=entry-header><h2 class=entry-hint-parent>信息几何：在概率空间中寻找最短路径</h2></header><div class=entry-content><p>引言：当概率成为空间上的点 想象一下，你站在一个巨大的画廊里。墙上挂着无数幅画，每一幅画都是一张概率分布的直方图。如果你要量化两幅画之间的"距离"，你会怎么做？直接比较每个柱子的高度差异？还是考虑某种更本质的、统计学意义上的距离？
这个问题触及了统计学的核心：如何量化两个概率分布之间的差异。传统的做法是使用 KL 散度或互信息，但这些度量缺乏几何直观——它们不是真正的"距离"，也不满足三角不等式。
信息几何给出了一种全新的视角：将所有概率分布看作一个黎曼流形，每个分布是流形上的一个点，Fisher 信息矩阵定义了这个流形上的度量张量。在这个框架下，我们可以谈论"两点之间的最短路径"（测地线），可以计算"梯度"（自然梯度），可以定义"曲率"（统计流形的曲率）。
这个领域的诞生可以追溯到 1945 年，印度统计学家 C. R. Rao 提出了 Fisher 信息度量可以作为微分几何的度量张量。此后，法国数学家 Amari 系统性地发展了信息几何的理论，并将其与神经网络、优化算法相结合。
在这篇文章中，我们将从基础概念开始，系统性地介绍信息几何的核心理论，探讨其在深度学习中的应用，并对未来的发展方向做出展望。
第一章：几何概率空间 1.1 概率分布作为流形 考虑一个简单的例子：所有零均值、单位方差的一维高斯分布 $\mathcal{N}(0, \sigma^2)$ 可以用一个参数 $\sigma$ 来表示。但如果我们考虑所有可能的高斯分布 $\mathcal{N}(\mu, \sigma^2)$，这就变成了一个二维的空间。
更一般地，考虑一个参数族 $\mathcal{P} = {p(x \mid \theta) : \theta \in \Theta}$，其中 $\theta \in \mathbb{R}^n$ 是参数。这个参数族可以看作一个 $n$ 维的流形——这就是统计流形。
关键洞察：每个概率分布不是孤立的对象，而是镶嵌在无穷维分布空间中的一个点。信息几何的任务就是给这个流形装备一个自然的几何结构。
1.2 Fisher 信息度量 1945 年，C. R. Rao 发现了一个重要的事实：Fisher 信息矩阵可以定义一个黎曼度量。
定义：对于参数族 $p(x \mid \theta)$，Fisher 信息矩阵定义为：
$$ I(\theta){ij} = \mathbb{E}{p(x \mid \theta)}\left[\frac{\partial \log p(x \mid \theta)}{\partial \theta_i} \frac{\partial \log p(x \mid \theta)}{\partial \theta_j}\right] $$
...</p></div><footer class=entry-footer><span title='2026-01-25 15:00:00 +0800 CST'>January 25, 2026</span>&nbsp;·&nbsp;<span>3 min</span>&nbsp;·&nbsp;<span>483 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 信息几何：在概率空间中寻找最短路径" href=https://s-ai-unix.github.io/posts/2026-01-25-information-geometry/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/pde-overview.jpg alt=偏微分方程可视化></figure><header class=entry-header><h2 class=entry-hint-parent>偏微分方程：描述物理世界的数学语言</h2></header><div class=entry-content><p>引言：方程背后的宇宙图景 想象一下，你向平静的湖面扔下一颗石子。涟漪一圈圈向外扩散，逐渐消失。如果有人问你：用什么数学方程来描述这个现象？你可能会想到一个关于时间和空间的方程——这就是偏微分方程的雏形。
偏微分方程（Partial Differential Equation, PDE）是描述物理世界的终极语言。它将复杂的时空演化浓缩进几个偏导数的关系中，从热量的扩散到波的传播，从流体的流动到量子的跃迁，无不遵循着偏微分方程的规律。
PDE 的历史可以追溯到 18 世纪。达朗贝尔、欧拉、伯努利等数学家在研究振动问题时，首次系统性地使用了偏微分方程。到了 19 世纪，傅里叶的热传导理论和纳维-斯托克斯方程的提出，进一步丰富了 PDE 的理论体系。20 世纪，希尔伯特、索伯列夫、施瓦茨等数学家为 PDE 建立了严格的泛函分析基础。
在这篇文章中，我们将系统地介绍偏微分方程的经典理论。从三大基本方程开始，逐步深入到达朗贝尔公式、极值原理、格林函数，最后探讨薛定谔方程和纳维-斯托克斯方程。我们不仅要理解这些方程的数学形式，更要感受它们所蕴含的物理直觉和美学价值。
第一章：三大基本方程 偏微分方程的分类源于它们所描述的不同物理现象。椭圆型方程描述平衡状态，抛物型方程描述扩散过程，双曲型方程描述波动传播。这三类方程构成了 PDE 理论的基石。
1.1 拉普拉斯方程：平衡的语言 拉普拉斯方程是最简单的椭圆型偏微分方程：
$$ \Delta u = \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} + \frac{\partial^2 u}{\partial z^2} = 0 $$
在二维情况下，它简化为：
$$ \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} = 0 $$
这个方程描述了什么？它描述的是一种平衡状态——没有源头，没有汇，函数值在任何点的"净流出"为零。
物理意义：稳态温度分布、静电场、引力势、无源流体流动等都满足拉普拉斯方程。
调和函数的美学：拉普拉斯方程的解被称为调和函数。它们有一个极其优雅的性质——均值定理：函数在任何点的值等于其周围邻域的平均值。
图1：调和函数 $u = x^2 - y^2$ 的等值线。注意等值线呈现完美的双曲线形状，体现了拉普拉斯方程描述的对称与平衡。
1.2 热传导方程：熵增的数学表达 热传导方程是抛物型偏微分方程的代表：
$$ \frac{\partial u}{\partial t} = \alpha \Delta u = \alpha \left(\frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} + \frac{\partial^2 u}{\partial z^2}\right) $$
...</p></div><footer class=entry-footer><span title='2026-01-25 14:00:00 +0800 CST'>January 25, 2026</span>&nbsp;·&nbsp;<span>3 min</span>&nbsp;·&nbsp;<span>631 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 偏微分方程：描述物理世界的数学语言" href=https://s-ai-unix.github.io/posts/2026-01-25-pde-overview/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/probability-statistics-ml-guide.jpg alt=概率分布可视化></figure><header class=entry-header><h2 class=entry-hint-parent>概率论与数理统计：机器学习的概率基石</h2></header><div class=entry-content><p>引言：在不确定的世界中寻找确定性 想象一下，你站在一个赌场的轮盘赌桌前。小球在旋转的轮盘上跳跃，最终停在一个数字上。你知道这个结果是完全随机的吗？还是说，如果你能足够精确地测量小球的初始位置、速度、轮盘的摩擦系数等所有参数，你就能预测出最终的结果？
这个思想实验引发了人类对概率本质的深刻思考。17世纪，法国数学家帕斯卡和费马在通信中讨论赌博问题，标志着概率论作为一门数学学科的诞生。随后的几个世纪里，伯努利、拉普拉斯、高斯等数学大师们为概率论的发展做出了巨大贡献。
到了20世纪初，俄罗斯数学家柯尔莫哥洛夫给出了概率论的严格公理化定义，将概率论建立在坚实的数学基础之上。几乎同时，贝叶斯的理论开始重新受到重视，为我们提供了一种全新的思考不确定性的方式。
那么，概率论和机器学习有什么关系呢？
假设你是一名医生，你需要根据患者的症状来诊断疾病。你有体温、血压、血常规等数据，以及过去的诊断记录。你会怎么做？你会综合考虑所有因素，得出一个诊断结论。这个过程本质上就是一个概率推断过程——根据观测到的数据（症状），推断最可能的原因（疾病）。
机器学习也是如此。给定一堆数据，模型需要学习数据背后的规律，然后对新的数据进行预测。在这个过程中，不确定性无处不在：数据可能有噪声，模型可能不完美，预测结果也可能有偏差。概率论为我们提供了处理这些不确定性的数学工具。
在这篇文章中，我们将系统地介绍概率论与数理统计在机器学习中的应用。从基础的概率公理开始，逐步深入到极限定理、统计推断、信息论基础，最后探讨这些理论如何在现代机器学习和深度学习算法中发挥作用。
第一章：概率基础 1.1 概率的公理化定义 1933年，柯尔莫哥洛夫建立了现代概率论的基础。他提出了三条基本公理：
公理1（非负性）：对于任何事件 $A$，都有 $P(A) \geq 0$。
公理2（规范性）：样本空间 $\Omega$ 的概率为 $1$，即 $P(\Omega) = 1$。
公理3（可加性）：对于任意可数个互斥事件 $A_1, A_2, \ldots$，有
$$ P\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i) $$
这三条公理看起来很简单，但它们是整个概率论大厦的基石。从这些公理出发，我们可以推导出概率论的所有重要结果。
例如，对于两个事件 $A$ 和 $B$，我们可以推导出并集的概率公式：
$$ P(A \cup B) = P(A) + P(B) - P(A \cap B) $$
这个公式的直观理解是：将 $A$ 的概率和 $B$ 的概率相加时，$A$ 和 $B$ 的交集部分被计算了两次，所以需要减去一次。
1.2 条件概率和贝叶斯公式 条件概率是概率论中最重要的概念之一。直观地说，条件概率 $P(A \mid B)$ 表示"在事件 $B$ 已经发生的条件下，事件 $A$ 发生的概率"。
...</p></div><footer class=entry-footer><span title='2026-01-25 12:00:00 +0800 CST'>January 25, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>1058 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 概率论与数理统计：机器学习的概率基石" href=https://s-ai-unix.github.io/posts/2026-01-25-probability-statistics-ml-guide/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/linear-algebra-journey.jpg alt=线性代数的几何美感></figure><header class=entry-header><h2 class=entry-hint-parent>线性代数：从理论到 AI 应用的完整旅程</h2></header><div class=entry-content><p>引言：为什么线性代数如此重要？ 想象你站在一个开阔的平原上,手中拿着一支箭。这支箭可以指向任何方向,可以伸长或缩短,可以与另一支箭相加。这就是向量的原始概念——一个既有方向又有大小的量。从这样简单的直观出发,人类发展出了一整套描述空间、变换和数据结构的数学语言:线性代数。
线性代数的美妙之处在于它的简洁性和普遍性。在二维平面上,一个点可以用两个坐标 $(x, y)$ 表示;在三维空间中,需要三个坐标 $(x, y, z)$;而在机器学习中处理的数据可能有一千维、一万维,甚至更高。线性代数提供了一套统一的工具来处理这些高维空间,而且它的规律在任意维数下都保持不变。
更令人惊讶的是,当你使用 ChatGPT、看 Netflix 推荐、或在 Google 搜索时,背后都有线性代数的身影。深度学习的神经网络本质上就是一系列线性变换和非线性激活的交替组合;推荐系统中的矩阵分解技术直接源自奇异值分解;而搜索引擎的 PageRank 算法则是特征值问题的经典应用。
在这篇文章中,我们将踏上一段从理论到应用的完整旅程。我们会从向量空间的几何直观出发,理解线性变换的本质,然后逐步深入到机器学习和深度学习的核心算法中。我们不仅会学习"怎么做",更重要的是理解"为什么"——为什么奇异值分解如此强大?为什么梯度下降会收敛?为什么注意力机制能够工作?
让我们开始这段旅程。
第一部分:线性代数基础理论 1. 向量空间的本质 1.1 从几何到抽象 在二维平面上,我们习惯用坐标表示向量。向量 $\mathbf{v} = (3, 2)$ 表示从原点出发,沿 $x$ 轴移动 3 个单位,再沿 $y$ 轴移动 2 个单位。但向量的概念远不止于此。
向量空间的抽象定义只需要 8 条公理:
加法封闭性: $\mathbf{u} + \mathbf{v}$ 仍在空间中 加法交换律: $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$ 加法结合律: $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$ 零向量存在: $\mathbf{0} + \mathbf{v} = \mathbf{v}$ 负向量存在: $\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$ 数乘封闭性: $c\mathbf{v}$ 仍在空间中 数乘分配律: $c(\mathbf{u} + \mathbf{v}) = c\mathbf{u} + c\mathbf{v}$ 数乘结合律: $c(d\mathbf{v}) = (cd)\mathbf{v}$ 这个定义看似抽象,但它统一了各种不同的对象:
...</p></div><footer class=entry-footer><span title='2026-01-25 08:45:00 +0800 CST'>January 25, 2026</span>&nbsp;·&nbsp;<span>14 min</span>&nbsp;·&nbsp;<span>2816 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 线性代数：从理论到 AI 应用的完整旅程" href=https://s-ai-unix.github.io/posts/2026-01-25-linear-algebra-complete-guide/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/probability-distributions-cover.jpg alt=抽象的概率分布可视化></figure><header class=entry-header><h2 class=entry-hint-parent>概率统计中的常见分布：从二项分布到正态分布的深层之旅</h2></header><div class=entry-content><p>引言：从掷骰子到高尔顿板 想象一下，你站在 19 世纪的英国街头，看着弗朗西斯·高尔顿展示他的发明——高尔顿板。成千上万的小珠子从上方落下，穿过钉子的阵列，最终在底部堆积成一条平滑的曲线。这条曲线就是我们熟知的钟形曲线，也就是正态分布的直观体现。高尔顿站在那里，向观众解释一个深刻的真理：看似混乱的随机现象背后，隐藏着惊人的秩序。
但在理解正态分布之前，我们需要回到更基础的问题。当你掷一枚硬币，正面朝上的概率是多少？如果你掷十次，恰好五次正面的概率又是多少？这些看似简单的问题，引导我们进入概率论的核心领域——概率分布。
概率分布是描述随机变量取值规律的数学工具。就像地图告诉我们哪里有山、哪里有河一样，概率分布告诉我们一个随机变量取不同值的可能性大小。在本文中，我们将踏上一段穿越时间和数学的旅程，探索概率统计中最重要的几个分布：二项分布、泊松分布、正态分布和指数分布。
这不是一本枯燥的教科书，而是一次探索。我们将从简单的硬币投掷开始，逐渐走向描述稀有事件的泊松分布，最终抵达连接万物的正态分布。准备好了吗？让我们开始这段旅程。
二项分布：从伯努利到组合数学 历史的种子 二项分布的起源可以追溯到 17 世纪的欧洲，那是一个赌博和数学碰撞的时代。当时，一位名叫布莱兹·帕斯卡的年轻法国数学家收到了朋友的来信。朋友是一位赌博爱好者，遇到了一个困扰他的问题：两个玩家在赌博中断后，应该如何公平地分配赌注？
这个问题现在被称为"点数问题"，它点燃了概率论的火花。帕斯卡与另一位数学天才皮埃尔·德·费马通信讨论，他们的信件往来奠定了现代概率论的基础。
但二项分布的真正数学形式要归功于雅各布·伯努利（Jacob Bernoulli）。这位瑞士数学家在他去世后于 1713 年出版的巨著《猜度术》（Ars Conjectandi）中，系统性地研究了独立重复试验的问题。伯努利提出的问题很简单：如果你重复做 $n$ 次独立的伯努利试验（每次只有成功或失败两种结果），恰好得到 $k$ 次成功的概率是多少？
数学定义与推导 让我们从最基本的概念开始。一个伯努利试验是指只有两个可能结果的随机试验：成功（用 $1$ 表示）或失败（用 $0$ 表示）。假设成功的概率是 $p$，失败的概率就是 $1-p$。
现在，我们重复进行 $n$ 次独立的伯努利试验，设 $X$ 为成功的次数。我们要求的是 $P(X = k)$，即恰好 $k$ 次成功的概率。
为了理解这个概率，让我们考虑一个具体的例子：$n = 3$ 次试验，恰好 $k = 2$ 次成功。所有可能的结果有：
成功、成功、失败（SSF） 成功、失败、成功（SFS） 失败、成功、成功（FSS） 每种结果的概率是相同的：$p \cdot p \cdot (1-p) = p^2(1-p)$。因为有 $3$ 种不同的排列方式，所以总概率是 $3 \cdot p^2(1-p)$。
这个数字 $3$ 是什么？它是从 $3$ 个位置中选择 $2$ 个位置放成功的组合数。一般地，从 $n$ 个位置中选择 $k$ 个位置放成功的组合数是：
...</p></div><footer class=entry-footer><span title='2026-01-24 11:42:07 +0800 CST'>January 24, 2026</span>&nbsp;·&nbsp;<span>6 min</span>&nbsp;·&nbsp;<span>1181 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 概率统计中的常见分布：从二项分布到正态分布的深层之旅" href=https://s-ai-unix.github.io/posts/2026-01-24-probability-distributions-guide/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://s-ai-unix.github.io/categories/%E6%95%B0%E5%AD%A6/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var n,t=e.innerHTML,s=!1;for(t.indexOf("<em>")!==-1&&(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("</em>")!==-1&&(n=t.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),n=n.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("<em>$")!==-1&&(n=t.replace(/<em>\$/g,"$"),n!==t&&(t=n,s=!0)),t.indexOf("</em>$")!==-1&&(n=t.replace(/<\/em>\$/g,"$"),n!==t&&(t=n,s=!0));t.indexOf("<em>")!==-1&&t.indexOf("</em>")!==-1;){if(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n===t)break;t=n,s=!0}t.indexOf("_{")!==-1&&(n=t.replace(/_{/g,"_{"),n!==t&&(t=n,s=!0)),t.indexOf("}_")!==-1&&(n=t.replace(/}_/g,"}"),n!==t&&(t=n,s=!0)),t.indexOf("\\\\")!==-1&&(n=t.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),n!==t&&(t=n,s=!0)),s&&(e.innerHTML=t)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var n,t=e.innerHTML,s=!1;for(t.indexOf("<em>")!==-1&&(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("</em>")!==-1&&(n=t.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),n=n.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("<em>$")!==-1&&(n=t.replace(/<em>\$/g,"$"),n!==t&&(t=n,s=!0)),t.indexOf("</em>$")!==-1&&(n=t.replace(/<\/em>\$/g,"$"),n!==t&&(t=n,s=!0));t.indexOf("<em>")!==-1&&t.indexOf("</em>")!==-1;){if(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n===t)break;t=n,s=!0}t.indexOf("_{")!==-1&&(n=t.replace(/_{/g,"_{"),n!==t&&(t=n,s=!0)),t.indexOf("}_")!==-1&&(n=t.replace(/}_/g,"}"),n!==t&&(t=n,s=!0)),t.indexOf("\\\\")!==-1&&(n=t.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),n!==t&&(t=n,s=!0)),s&&(e.innerHTML=t)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>