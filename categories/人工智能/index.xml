<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>人工智能 on s-ai-unix's Blog</title><link>https://s-ai-unix.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/</link><description>Recent content in 人工智能 on s-ai-unix's Blog</description><generator>Hugo -- 0.154.5</generator><language>zh-cn</language><lastBuildDate>Sat, 31 Jan 2026 09:30:00 +0800</lastBuildDate><atom:link href="https://s-ai-unix.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/index.xml" rel="self" type="application/rss+xml"/><item><title>AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作</title><link>https://s-ai-unix.github.io/posts/2026-01-31-ai-paper-llama3-herd-of-models/</link><pubDate>Sat, 31 Jan 2026 09:30:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-31-ai-paper-llama3-herd-of-models/</guid><description>深入解读 Meta AI 的 Llama 3 论文，从 Scaling Laws、模型架构到多模态扩展，全面剖析这个拥有 405B 参数的开源大模型集群的设计理念与技术细节。</description></item><item><title>AI 论文解读系列：AlphaZero - 从零开始的自我博弈通用算法</title><link>https://s-ai-unix.github.io/posts/2026-01-30-alphazero-paper-interpretation/</link><pubDate>Fri, 30 Jan 2026 13:00:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-30-alphazero-paper-interpretation/</guid><description>深入解读 DeepMind 发表于 Science 2018 的里程碑论文，剖析 AlphaZero 如何从零开始，通过纯自我博弈掌握国际象棋、将棋和围棋</description></item><item><title>AI 论文解读系列：AlphaGo - 深度学习与树搜索征服围棋</title><link>https://s-ai-unix.github.io/posts/2026-01-30-alphago-paper-interpretation/</link><pubDate>Fri, 30 Jan 2026 12:30:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-30-alphago-paper-interpretation/</guid><description>深入解读 DeepMind 发表于 Nature 的里程碑论文，剖析 AlphaGo 如何结合深度神经网络与蒙特卡洛树搜索，首次在围棋领域击败人类职业棋手</description></item><item><title>AI 论文解读系列：Inception-v4 - Going Deeper with Convolutions</title><link>https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97-inception-v4-going-deeper-with-convolutions/</link><pubDate>Fri, 30 Jan 2026 12:30:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97-inception-v4-going-deeper-with-convolutions/</guid><description>深入解读 Google 的 Inception-v4 论文，从 Inception 系列的演进历程出发，剖析 Inception-v4 的架构设计思想、多尺度特征提取原理，以及 Inception-ResNet 如何将残差连接与 Inception 模块融合，创造当时最强图像分类网络。</description></item><item><title>AI 论文解读系列：BERT - 预训练深度双向 Transformer 的革命</title><link>https://s-ai-unix.github.io/posts/2026-01-30-bert-paper-interpretation/</link><pubDate>Fri, 30 Jan 2026 12:00:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-30-bert-paper-interpretation/</guid><description>深入解读 Google 发表于 NAACL 2019 的里程碑论文，剖析 BERT 如何通过双向预训练革命性地提升自然语言理解能力</description></item><item><title>AI 论文解读系列：Word2Vec - 词向量的革命</title><link>https://s-ai-unix.github.io/posts/2026-01-30-word2vec-paper-explained/</link><pubDate>Fri, 30 Jan 2026 09:00:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-30-word2vec-paper-explained/</guid><description>深入浅出解读 Mikolov 等人的 Word2Vec 论文，从词袋模型到神经语言模型，完整推导 CBOW 和 Skip-gram 的数学原理与应用。</description></item><item><title>AI 论文解读系列：GPT-3——当语言模型学会举一反三</title><link>https://s-ai-unix.github.io/posts/2026-01-30-gpt3-few-shot-learners-paper/</link><pubDate>Fri, 30 Jan 2026 08:50:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-30-gpt3-few-shot-learners-paper/</guid><description>深入解读 OpenAI 里程碑式论文 GPT-3: Language Models are Few-Shot Learners，从 Transformer 架构到少样本学习的范式转变，探讨大规模语言模型的涌现能力与未来前景。</description></item><item><title>AI 论文解读系列：Vision Transformer 视觉Transformer</title><link>https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97vision-transformer-%E8%A7%86%E8%A7%89transformer/</link><pubDate>Fri, 30 Jan 2026 08:46:42 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97vision-transformer-%E8%A7%86%E8%A7%89transformer/</guid><description>深入解读 Google Research 的 Vision Transformer 论文，从注意力机制的原理出发，剖析图像块嵌入、位置编码、Transformer Encoder 的完整架构，揭示 Transformer 如何在计算机视觉领域挑战 CNN 的统治地位。</description></item><item><title>AI 论文解读系列：ResNet 深度残差学习</title><link>https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97%E4%B9%8Bresnet-%E6%B7%B1%E5%BA%A6%E6%AE%8B%E5%B7%AE%E5%AD%A6%E4%B9%A0/</link><pubDate>Fri, 30 Jan 2026 08:38:11 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97%E4%B9%8Bresnet-%E6%B7%B1%E5%BA%A6%E6%AE%8B%E5%B7%AE%E5%AD%A6%E4%B9%A0/</guid><description>深入解读何恺明等人的 ResNet 论文，从深度网络的退化问题出发，剖析残差学习的核心思想、数学原理和架构设计，揭示为何简单的跳跃连接能够训练出超深层神经网络。</description></item><item><title>ISO/PAS 8800:2024 道路车辆人工智能安全工程——从确定性到概率性的范式转移</title><link>https://s-ai-unix.github.io/posts/2026-01-20-iso-8800-comprehensive-guide/</link><pubDate>Tue, 20 Jan 2026 21:10:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-20-iso-8800-comprehensive-guide/</guid><description>深入解析首个汽车 AI 安全国际标准，系统掌握数据定义安全的新方法论</description></item><item><title>大语言模型：为什么AI能这么快、这么聪明地回答问题</title><link>https://s-ai-unix.github.io/posts/2026-01-14-llm-principle-for-students/</link><pubDate>Wed, 14 Jan 2026 08:50:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-14-llm-principle-for-students/</guid><description>从预测下一个词的简单想法出发，解释大语言模型的工作原理，面向初中生和高中生的通俗易懂指南。</description></item><item><title>强化学习：从试错到智能的数学之旅</title><link>https://s-ai-unix.github.io/posts/2026-01-14-reinforcement-learning-comprehensive-guide/</link><pubDate>Wed, 14 Jan 2026 08:30:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-14-reinforcement-learning-comprehensive-guide/</guid><description>从马尔可夫决策过程到深度强化学习，探索人工智能如何通过试错学习最优策略，包括Q-learning、Policy Gradient和Actor-Critic等经典算法的完整数学推导。</description></item></channel></rss>