<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>机器学习 | s-ai-unix's Blog</title><meta name=keywords content><meta name=description content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://s-ai-unix.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml title=rss><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="机器学习"><meta property="og:description" content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="机器学习"><meta name=twitter:description content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body class=list id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/categories/>Categories</a></div><h1>机器学习</h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/llama3-herd-cover.jpg alt="Llama 3 模型集群架构示意图"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作</h2></header><div class=entry-content><p>引言：开源 AI 的黎明 2024 年 7 月 23 日，Meta AI 发布了一篇重磅论文——《The Llama 3 Herd of Models》。这篇论文不仅介绍了一个拥有 4050 亿参数的巨型语言模型，更标志着开源人工智能正式迈入了与闭源巨头分庭抗礼的新纪元。
回想 2022 年底，ChatGPT 的横空出世让整个 AI 领域为之震动。然而，最强大的模型始终被封闭在 OpenAI、Google 等公司的围墙之内。研究者无法探究其内部机理，开发者无法自由定制，这种"黑箱"状态严重阻碍了 AI 技术的普惠发展。
Llama 3 的出现改变了这一切。Meta 不仅开源了完整的模型权重，还详细披露了从数据筛选到训练优化的每一个技术细节。这意味着，任何研究者和开发者都可以在自己的硬件上运行这个媲美 GPT-4 的模型，深入理解它的工作原理，甚至在此基础上进行创新。
本文将带领读者深入这篇 92 页的论文，从数据、规模、复杂性管理三个核心维度，层层剥开 Llama 3 的技术奥秘。
第一章：模型概览 —— “模型群"的设计理念 1.1 为什么叫 “Herd”（群）？ 论文标题中的 “Herd of Models” 并非随意命名。Meta 同时发布了三个不同规模的模型：
模型 参数量 上下文长度 目标场景 Llama 3 8B $8 \times 10^9$ 128K tokens 边缘设备、低延迟推理 Llama 3 70B $70 \times 10^9$ 128K tokens 平衡性能与效率 Llama 3 405B $405 \times 10^9$ 128K tokens 顶级性能、复杂推理 这种"群"策略的核心思想是：用一个旗舰模型（405B）指导整个家族的优化方向，同时让每个成员在特定场景下发挥最大价值。
...</p></div><footer class=entry-footer><span title='2026-01-31 09:30:00 +0800 CST'>January 31, 2026</span>&nbsp;·&nbsp;<span>6 min</span>&nbsp;·&nbsp;<span>1184 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作" href=https://s-ai-unix.github.io/posts/2026-01-31-ai-paper-llama3-herd-of-models/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/seq2seq-cover.jpg alt="Seq2Seq 神经网络抽象图"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：Seq2Seq--从序列到序列的革命</h2></header><div class=entry-content><p>引言：翻译的困境 想象一下，你正在学习一门外语。当你听到一句法语 “Bonjour le monde” 时，你的大脑是如何将其转化为英语 “Hello world” 的？
这不是简单的逐词替换。“Bonjour” 对应 “Hello”，但 “le monde” 是 “the world” 的倒序。词序不同，语法结构不同，甚至可能一个词对应多个词。传统的机器翻译系统使用基于规则的方法或统计模型，需要大量的人工特征工程和复杂的对齐算法。
2014年，Ilya Sutskever、Oriol Vinyals 和 Quoc Le 在 Google 发表了一篇改变游戏规则的论文：“Sequence to Sequence Learning with Neural Networks”。他们提出的 Seq2Seq 架构，用一个统一的神经网络模型取代了复杂的流水线，让机器翻译的准确率跃升到了新的高度。
但这篇论文的意义远不止于翻译。它开创了序列转导（Sequence Transduction）这一全新的学习范式，为后来的注意力机制、Transformer 乃至大语言模型奠定了基础。
第一章：序列转导问题 1.1 什么让序列数据特殊 在深入 Seq2Seq 之前，让我们先理解序列数据的本质。
传统的机器学习任务，比如图像分类或房价预测，输入和输出的维度是固定的。一张图片永远是 $224 \times 224 \times 3$ 的像素矩阵，一套房子的特征永远是卧室数、面积、位置等固定字段。
但序列数据不同：
一句话可能有 5 个词，也可能有 50 个词 源语言和目标语言的词序可能不同 一个概念可能用一个词表达，也可能用多个词 上图展示了一个典型的机器翻译场景。输入序列 “Hello world this is a test” 需要被转换为 “Bonjour monde ceci est un test”。注意两个关键挑战：
...</p></div><footer class=entry-footer><span title='2026-01-30 09:00:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>763 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：Seq2Seq--从序列到序列的革命" href=https://s-ai-unix.github.io/posts/2026-01-30-seq2seq-paper-explained/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/decision-tree-cover.jpg alt=决策树></figure><header class=entry-header><h2 class=entry-hint-parent>决策树及其衍生算法：从ID3到现代梯度提升</h2></header><div class=entry-content><p>引言：从二十个问题到机器学习 想象你在玩一个经典游戏——“二十个问题”。你需要通过最多二十个 yes/no 问题，猜出对手心中想的一个物体。聪明的玩家会问这样的问题：
“它是活的吗？” “如果活着，它是动物吗？” “如果是动物，它会飞吗？” 每一个问题都将可能的答案空间一分为二，逐步缩小范围，直到锁定目标。这种分而治之的策略，正是决策树算法的核心思想。
决策树是机器学习中最直观、最易于解释的算法之一。从医学诊断到信用评估，从游戏 AI 到推荐系统，决策树及其衍生算法无处不在。它的魅力在于：
可解释性强：决策路径清晰，非技术人员也能理解 非参数化：不需要假设数据的分布形式 处理混合数据：能同时处理数值和类别特征 捕捉非线性关系：通过分层划分，自动学习复杂的决策边界 从1986年 Ross Quinlan 提出 ID3 算法，到今天 XGBoost、LightGBM 在 Kaggle 竞赛中称霸，决策树算法已经走过了近四十年的演进历程。本文将带你从最基本的树结构出发，逐步深入到现代梯度提升框架的数学原理，揭示这一算法的优雅与力量。
第一章：决策树基础 1.1 什么是决策树？ 决策树（Decision Tree）是一种树形结构的预测模型，其中：
内部节点表示对某个特征的测试或判断 分支表示测试的结果 叶节点表示最终的预测结果（类别或数值） 图 1：决策树的基本结构。从根节点开始，根据特征值进行判断，沿着分支走到叶节点得到预测结果。
决策树既可以用于分类（预测离散类别），也可以用于回归（预测连续数值）。前者的代表是 ID3、C4.5、CART（分类树），后者的代表是 CART（回归树）。
1.2 决策树的学习过程 构建决策树的核心问题是：*如何选择每个节点的分裂特征和分裂点？
这涉及三个关键决策：
*1. 特征选择准则
我们需要一个指标来度量分裂的"好坏"。常用的准则包括：
信息增益（Information Gain）：基于信息熵的减少 基尼指数（Gini Index）：基于概率分布的纯度 均方误差（MSE）：用于回归问题 *2. 分裂点选择
对于数值特征，需要确定最优的分裂阈值。通常采用贪婪搜索：遍历所有可能的分裂点，选择使准则最优化的那个。
*3. 停止条件
递归分裂何时停止？常见的停止条件：
节点中样本数少于阈值 节点纯度达到阈值 树深度达到上限 分裂带来的增益小于阈值 1.3 决策树的预测过程 预测一个新样本时，从根节点开始：
检查当前节点的分裂特征 根据样本在该特征上的取值，选择对应的分支 移动到子节点 重复直到到达叶节点 叶节点的标签（分类）或平均值（回归）即为预测结果 时间复杂度为 $O(\log n)$，其中 $n$ 是树的高度。这意味着即使对于大规模数据集，预测速度也非常快。
...</p></div><footer class=entry-footer><span title='2026-01-29 08:11:01 +0800 CST'>January 29, 2026</span>&nbsp;·&nbsp;<span>6 min</span>&nbsp;·&nbsp;<span>1197 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 决策树及其衍生算法：从ID3到现代梯度提升" href=https://s-ai-unix.github.io/posts/2026-01-29-decision-trees-and-beyond-from-id3-to-modern-gradient-boosting/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/calculus-ml-journey.jpg alt=微积分的几何美感></figure><header class=entry-header><h2 class=entry-hint-parent>微积分与机器学习：从变化率到神经网络梯度的完整旅程</h2></header><div class=entry-content><p>引言：为什么需要微积分？ 想象你在山上，想找到最低点。你会怎么做？你会观察脚下的坡度，选择最陡峭的方向迈出一步，然后重复这个过程。这个简单的直觉——沿着负梯度方向走——正是现代人工智能的核心算法。
从ChatGPT的语言模型到AlphaGo的围棋策略，从图像识别到语音合成，所有这些技术背后都有一个共同的数学基础：微积分。
微积分研究的是变化。而机器学习本质上是关于优化——通过不断调整参数来减少错误。当我们在高维空间中优化复杂的神经网络时，微积分提供了描述和计算这种变化的精确语言。
这篇文章将带你深入理解微积分如何驱动现代人工智能。我们不会停留在表面，而是会深入到数学推导的核心，揭示梯度下降、反向传播等算法的数学本质。这是一次从17世纪牛顿和莱布尼茨的发明，到21世纪深度学习革命的完整旅程。
第一部分：微积分基础理论 1. 导数的本质：从变化率到瞬时变化率 1.1 变化率的直观理解 变化率是人类最早思考的数学问题之一。如果一辆车2小时行驶100公里，平均速度是50公里/小时。但它某一时刻的瞬时速度是多少？
微积分的答案是：用极限。考虑函数 $f(x)$ 在 $x_0$ 附近的平均变化率： $$ \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x} $$
当 $\Delta x \to 0$ 时，这个平均变化率的极限就是导数： $$ f^{\prime}(x_0) = \lim_{\Delta x \to 0} \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x} $$
1.2 导数的几何意义 几何直观：导数是切线的斜率。在 $x_0$ 处，曲线 $f(x)$ 可以用直线（切线）逼近： $$ f(x) \approx f(x_0) + f^{\prime}(x_0)(x - x_0) $$
这就是一阶泰勒公式，也是线性化的思想：局部用简单的线性函数逼近复杂的非线性函数。
严格定义（$\epsilon-\delta$ 语言）： $$ \forall \epsilon > 0, \exists \delta > 0 \text{ s.t. } |\Delta x| &lt; \delta \implies \left|\frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x} - f^{\prime}(x_0)\right| &lt; \epsilon $$
...</p></div><footer class=entry-footer><span title='2026-01-25 19:00:00 +0800 CST'>January 25, 2026</span>&nbsp;·&nbsp;<span>9 min</span>&nbsp;·&nbsp;<span>1716 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 微积分与机器学习：从变化率到神经网络梯度的完整旅程" href=https://s-ai-unix.github.io/posts/2026-01-25-calculus-ml-systematic-review/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/spectral-theorem.jpg alt=谱定理可视化></figure><header class=entry-header><h2 class=entry-hint-parent>谱定理：线性代数的优雅与机器学习的基石</h2></header><div class=entry-content><p>引言：对称性的数学之美 在数学的众多分支中，有一个深刻的原理反复出现：对称性带来简化。在物理学中，空间的对称性意味着守恒量；在群论中，对称结构导致简单的表示；在线性代数中，对称矩阵拥有最优雅的对角化理论——这就是谱定理。
想象你站在一个椭圆中心。如果你沿任意方向看出去，椭圆的"宽度"各不相同。但有两个特殊的方向——椭圆的长轴和短轴——沿这些方向，椭圆的形状最简单，只是一个被拉伸的圆。这两个正交的方向，就是椭圆的"主轴"，它们对应的拉伸倍数，就是"特征值"。
这个直观的几何图像，正是谱定理的核心。谱定理告诉我们：任何实对称矩阵都可以通过正交变换对角化。换句话说，在适当的坐标系下，对称矩阵描述的线性变换只是沿某些正交方向的简单拉伸。
在机器学习和深度学习中，谱定理无处不在。从主成分分析（PCA）到奇异值分解（SVD），从谱聚类到图神经网络，谱定理提供了理解数据和算法的理论基础。
在这篇文章中，我们将系统性地介绍谱定理的核心理论，从实对称矩阵的正交对角化到一般的奇异值分解，从PCA到谱聚类，深入浅出地推导每一个公式，并通过可视化图形直观理解这些概念。
第一章：谱定理的基础理论 1.1 特征值与特征向量：不变的方向 给定一个 $n \times n$ 矩阵 $A$，如果存在非零向量 $v \in \mathbb{R}^n$ 和标量 $\lambda \in \mathbb{R}$，使得
$$ Av = \lambda v $$
则称 $\lambda$ 是 $A$ 的特征值，$v$ 是对应的特征向量。
几何意义：特征向量 $v$ 是线性变换 $A$ 下的"不变方向"——变换后，这个向量只是被拉伸或压缩了 $\lambda$ 倍，方向保持不变。
特征多项式：特征值是特征方程的根
$$ \det(A - \lambda I) = 0 $$
对于 $n \times n$ 矩阵，这是一个 $n$ 次多项式，在复数域上有 $n$ 个根（计入重数）。
1.2 对称矩阵的特殊性质 实对称矩阵 $A \in \mathbb{R}^{n \times n}$（即 $A^\top = A$）拥有三个重要性质：
...</p></div><footer class=entry-footer><span title='2026-01-25 18:00:00 +0800 CST'>January 25, 2026</span>&nbsp;·&nbsp;<span>7 min</span>&nbsp;·&nbsp;<span>1458 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 谱定理：线性代数的优雅与机器学习的基石" href=https://s-ai-unix.github.io/posts/2026-01-25-spectral-theorem/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/probability-statistics-ml-guide.jpg alt=概率分布可视化></figure><header class=entry-header><h2 class=entry-hint-parent>概率论与数理统计：机器学习的概率基石</h2></header><div class=entry-content><p>引言：在不确定的世界中寻找确定性 想象一下，你站在一个赌场的轮盘赌桌前。小球在旋转的轮盘上跳跃，最终停在一个数字上。你知道这个结果是完全随机的吗？还是说，如果你能足够精确地测量小球的初始位置、速度、轮盘的摩擦系数等所有参数，你就能预测出最终的结果？
这个思想实验引发了人类对概率本质的深刻思考。17世纪，法国数学家帕斯卡和费马在通信中讨论赌博问题，标志着概率论作为一门数学学科的诞生。随后的几个世纪里，伯努利、拉普拉斯、高斯等数学大师们为概率论的发展做出了巨大贡献。
到了20世纪初，俄罗斯数学家柯尔莫哥洛夫给出了概率论的严格公理化定义，将概率论建立在坚实的数学基础之上。几乎同时，贝叶斯的理论开始重新受到重视，为我们提供了一种全新的思考不确定性的方式。
那么，概率论和机器学习有什么关系呢？
假设你是一名医生，你需要根据患者的症状来诊断疾病。你有体温、血压、血常规等数据，以及过去的诊断记录。你会怎么做？你会综合考虑所有因素，得出一个诊断结论。这个过程本质上就是一个概率推断过程——根据观测到的数据（症状），推断最可能的原因（疾病）。
机器学习也是如此。给定一堆数据，模型需要学习数据背后的规律，然后对新的数据进行预测。在这个过程中，不确定性无处不在：数据可能有噪声，模型可能不完美，预测结果也可能有偏差。概率论为我们提供了处理这些不确定性的数学工具。
在这篇文章中，我们将系统地介绍概率论与数理统计在机器学习中的应用。从基础的概率公理开始，逐步深入到极限定理、统计推断、信息论基础，最后探讨这些理论如何在现代机器学习和深度学习算法中发挥作用。
第一章：概率基础 1.1 概率的公理化定义 1933年，柯尔莫哥洛夫建立了现代概率论的基础。他提出了三条基本公理：
公理1（非负性）：对于任何事件 $A$，都有 $P(A) \geq 0$。
公理2（规范性）：样本空间 $\Omega$ 的概率为 $1$，即 $P(\Omega) = 1$。
公理3（可加性）：对于任意可数个互斥事件 $A_1, A_2, \ldots$，有
$$ P\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i) $$
这三条公理看起来很简单，但它们是整个概率论大厦的基石。从这些公理出发，我们可以推导出概率论的所有重要结果。
例如，对于两个事件 $A$ 和 $B$，我们可以推导出并集的概率公式：
$$ P(A \cup B) = P(A) + P(B) - P(A \cap B) $$
这个公式的直观理解是：将 $A$ 的概率和 $B$ 的概率相加时，$A$ 和 $B$ 的交集部分被计算了两次，所以需要减去一次。
1.2 条件概率和贝叶斯公式 条件概率是概率论中最重要的概念之一。直观地说，条件概率 $P(A \mid B)$ 表示"在事件 $B$ 已经发生的条件下，事件 $A$ 发生的概率"。
...</p></div><footer class=entry-footer><span title='2026-01-25 12:00:00 +0800 CST'>January 25, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>1058 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 概率论与数理统计：机器学习的概率基石" href=https://s-ai-unix.github.io/posts/2026-01-25-probability-statistics-ml-guide/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/linear-algebra-journey.jpg alt=线性代数的几何美感></figure><header class=entry-header><h2 class=entry-hint-parent>线性代数：从理论到 AI 应用的完整旅程</h2></header><div class=entry-content><p>引言：为什么线性代数如此重要？ 想象你站在一个开阔的平原上,手中拿着一支箭。这支箭可以指向任何方向,可以伸长或缩短,可以与另一支箭相加。这就是向量的原始概念——一个既有方向又有大小的量。从这样简单的直观出发,人类发展出了一整套描述空间、变换和数据结构的数学语言:线性代数。
线性代数的美妙之处在于它的简洁性和普遍性。在二维平面上,一个点可以用两个坐标 $(x, y)$ 表示;在三维空间中,需要三个坐标 $(x, y, z)$;而在机器学习中处理的数据可能有一千维、一万维,甚至更高。线性代数提供了一套统一的工具来处理这些高维空间,而且它的规律在任意维数下都保持不变。
更令人惊讶的是,当你使用 ChatGPT、看 Netflix 推荐、或在 Google 搜索时,背后都有线性代数的身影。深度学习的神经网络本质上就是一系列线性变换和非线性激活的交替组合;推荐系统中的矩阵分解技术直接源自奇异值分解;而搜索引擎的 PageRank 算法则是特征值问题的经典应用。
在这篇文章中,我们将踏上一段从理论到应用的完整旅程。我们会从向量空间的几何直观出发,理解线性变换的本质,然后逐步深入到机器学习和深度学习的核心算法中。我们不仅会学习"怎么做",更重要的是理解"为什么"——为什么奇异值分解如此强大?为什么梯度下降会收敛?为什么注意力机制能够工作?
让我们开始这段旅程。
第一部分:线性代数基础理论 1. 向量空间的本质 1.1 从几何到抽象 在二维平面上,我们习惯用坐标表示向量。向量 $\mathbf{v} = (3, 2)$ 表示从原点出发,沿 $x$ 轴移动 3 个单位,再沿 $y$ 轴移动 2 个单位。但向量的概念远不止于此。
向量空间的抽象定义只需要 8 条公理:
加法封闭性: $\mathbf{u} + \mathbf{v}$ 仍在空间中 加法交换律: $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$ 加法结合律: $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$ 零向量存在: $\mathbf{0} + \mathbf{v} = \mathbf{v}$ 负向量存在: $\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$ 数乘封闭性: $c\mathbf{v}$ 仍在空间中 数乘分配律: $c(\mathbf{u} + \mathbf{v}) = c\mathbf{u} + c\mathbf{v}$ 数乘结合律: $c(d\mathbf{v}) = (cd)\mathbf{v}$ 这个定义看似抽象,但它统一了各种不同的对象:
...</p></div><footer class=entry-footer><span title='2026-01-25 08:45:00 +0800 CST'>January 25, 2026</span>&nbsp;·&nbsp;<span>14 min</span>&nbsp;·&nbsp;<span>2816 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 线性代数：从理论到 AI 应用的完整旅程" href=https://s-ai-unix.github.io/posts/2026-01-25-linear-algebra-complete-guide/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/gmm-probability.jpg alt=高斯混合模型可视化></figure><header class=entry-header><h2 class=entry-hint-parent>高斯混合模型：从数据中解构隐藏结构的艺术</h2></header><div class=entry-content><p>引言：从混沌中发现结构 想象你是一个天文学家，正在观测夜空中的恒星。这些恒星并非均匀分布，而是呈现出明显的"聚集"现象：有些恒星形成了紧密的星团，有些则稀疏地散布在广阔的空间中。你的任务是理解这些恒星是如何分布的——它们属于哪些星团，每个星团的形状和位置是什么。
这就是一个典型的聚类问题：将数据点分组成若干个有意义的组。
最直观的聚类方法是 K-means：将每个数据点分配到最近的簇中心，然后更新簇中心，迭代直至收敛。但 K-means 有一个致命的限制：它假设每个簇是"圆形"的（在二维）或"球形"的（在高维）。这意味着它只能捕捉硬边界的簇，无法处理更复杂的形状，也无法表示一个数据点可能"部分地"属于多个簇。
这时，一个更强大的工具出现了：高斯混合模型（Gaussian Mixture Model, GMM）。GMM 不再做非此即彼的硬分类，而是给每个数据点一个"软"的归属概率——它有多大可能性属于每个簇。这种软聚类的方法不仅更灵活，而且能捕捉更复杂的数据分布。
更重要的是，GMM 引入了机器学习中最深刻的算法之一：EM 算法（Expectation-Maximization，期望最大化）。EM 算法是一种优雅的迭代算法，用于解决含有隐变量的概率模型的参数估计问题。
本文将带你深入 GMM 的世界。我们将从高斯分布的复习开始，理解从 K-means 到 GMM 的自然演进，推导 EM 算法的每一步，探索几何直观，最后了解它在现实世界的应用。准备好了吗？让我们开始这场从数据中发现隐藏结构的旅程。
高斯分布的回顾：多元正态分布 在深入 GMM 之前，我们需要先熟悉多元高斯分布（Multivariate Gaussian Distribution）的数学形式。
一元高斯分布 回忆一下，一元高斯分布的概率密度函数是：
$$ f(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) $$
其中：
$\mu$ 是均值（期望） $\sigma^2$ 是方差 $\sigma > 0$ 是标准差 这个分布的形状是经典的"钟形曲线"：在 $\mu$ 处达到峰值，向两侧对称衰减。
多元高斯分布 多元高斯分布是上述概念的推广。设 $\mathbf{x} \in \mathbb{R}^d$ 是一个 $d$ 维随机向量，$\mathbf{\mu} \in \mathbb{R}^d$ 是均值向量，$\mathbf{\Sigma} \in \mathbb{R}^{d \times d}$ 是协方差矩阵（对称正定）。
...</p></div><footer class=entry-footer><span title='2026-01-24 18:00:00 +0800 CST'>January 24, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>913 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 高斯混合模型：从数据中解构隐藏结构的艺术" href=https://s-ai-unix.github.io/posts/2026-01-24-gmm-comprehensive-guide/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/bayesian-classifier.jpg alt=贝叶斯网络结构示意图></figure><header class=entry-header><h2 class=entry-hint-parent>贝叶斯分类器：从条件概率到智能决策的优雅之旅</h2></header><div class=entry-content><p>引言：不确定世界中的决策智慧 想象你在一家医院工作，面对一位病人。医生告诉你，这位病人有两种可能的疾病：疾病 A 和疾病 B。通过检查，你发现病人出现了某种症状 S。现在的关键问题是：这种症状的出现，是更倾向于指向疾病 A，还是疾病 B？
这就是分类问题的本质——根据观察到的特征，将样本划分到不同的类别中。而在众多分类算法中，贝叶斯分类器以其优美的数学形式和深刻的思想基础，始终占据着不可替代的位置。
它不依赖于复杂的神经网络或深度学习结构，仅仅基于概率论的基本原理，就能在许多实际应用中展现出令人惊讶的效果。更重要的是，它给了我们一种"在不确定情况下进行理性决策"的思维方式。
第一章：概率论的基石 在进入贝叶斯分类器的核心之前，让我们先回顾一些基础的概率概念。这些概念看似简单，却构成了整个贝叶斯理论的数学大厦。
1.1 条件概率 条件概率是贝叶斯理论的起点。它的直观含义是：在事件 B 发生的条件下，事件 A 发生的概率是多少？数学记为：
$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$
其中 $P(A \cap B)$ 表示 A 和 B 同时发生的概率，$P(B)$ 是事件 B 发生的概率。这个公式的直观理解是：如果我们把所有可能的情况看作一个空间，条件概率就是在"给定 B 发生"这个子空间内，A 所占的比重。
1.2 全概率公式 当我们面对一个复杂事件时，常常需要将其分解为若干互不相容的简单事件。这就是全概率公式的思想：
$$P(A) = \sum_{i=1}^{n} P(A|B_i) P(B_i)$$
其中 $B_1, B_2, \ldots, B_n$ 构成一个完备事件组（即它们互不相容且并集为整个样本空间）。全概率公式的几何直观是：将事件 A 的"面积"按照不同条件 $B_i$ 进行"切片"，然后将这些切片的面积加起来。
1.3 贝叶斯公式的诞生 将条件概率公式"反过来"使用，就得到了著名的贝叶斯公式：
$$P(B|A) = \frac{P(A|B) P(B)}{P(A)}$$
这个公式看似简单，却蕴含着深刻的哲学意义。它告诉我们：如果我们知道"在 B 发生的条件下 A 的概率"（$P(A|B)$），以及"先验概率" $P(B)$，就可以推导出"观察到 A 后，B 的概率"（$P(B|A)$）。
...</p></div><footer class=entry-footer><span title='2026-01-24 17:58:30 +0800 CST'>January 24, 2026</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>736 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 贝叶斯分类器：从条件概率到智能决策的优雅之旅" href=https://s-ai-unix.github.io/posts/2026-01-24-bayesian-classifier/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/pca-visualization.jpg alt="PCA 可视化"></figure><header class=entry-header><h2 class=entry-hint-parent>PCA 主成分分析：从数据降维的优雅艺术</h2></header><div class=entry-content><p>引言：从混沌中寻找秩序 想象你是一个天文学家，正在观测一群恒星的位置。这些恒星在三维空间中分布，你记录了每颗恒星到地球的距离、赤经和赤纬——这就是一个典型的三维数据集。但是，你想理解这些恒星的分布规律，三维空间太复杂了。你突然意识到：这些恒星实际上分布在一个接近平面的薄层上！如果能找到这个平面，你就可以用二维坐标来描述每颗恒星的位置，大大简化问题。
这个看似简单的思想——在高维数据中找到最能代表数据的低维子空间——就是主成分分析（Principal Component Analysis, PCA）的核心。
在机器学习、数据科学和统计学中，我们经常面临"维度灾难"：数据维度越高，计算越复杂，噪声越多，模型越容易过拟合。PCA 提供了一种优雅的解决方案：它不丢弃任何原始特征的信息，而是将数据投影到新的坐标系中，在这个新坐标系中，前几个坐标轴（主成分）包含了数据的大部分信息。
本文将带你深入 PCA 的世界。我们从直观的几何理解开始，穿越历史的长河，探索两种等价的数学推导视角，最终抵达实际应用的海岸。准备好了吗？让我们开始这场降维之旅。
PCA 的直观理解：投影的智慧 为什么需要降维？ 在深入数学之前，让我们先理解为什么降维如此重要。
假设你有一个包含 $1000$ 个人的数据集，每个人有 $100$ 个特征（身高、体重、血压、血糖、血细胞计数等）。这些特征之间往往存在相关性：身高和体重相关，血压和血糖相关。如果我们直接用 $100$ 个特征来分析，会遇到以下问题：
计算复杂度：随着维度增加，算法的运行时间呈指数级增长。 过拟合风险：特征越多，模型越容易记住训练数据，泛化能力下降。 存储压力：$1000$ 个人 $\times$ $100$ 个特征 $= 100,000$ 个数据点，存储和传输成本高。 可视化困难：我们只能在三维空间中直接观察数据，超过三维就无法直观理解。 PCA 的目标是找到一个低维表示，保留数据的大部分信息。关键问题是：如何衡量"信息保留"？答案是方差。
方差作为信息度量 在一个数据集中，方差大的方向包含更多的信息。考虑一个简单的例子：假设我们有一个二维数据集，点的分布如图所示。
图 1：PCA 的核心思想：将数据投影到方差最大的方向
如果我们把这些点投影到不同的直线上，哪种投影方式能最好地保留原始数据的信息？
直觉告诉我们：应该投影到数据"伸展"最厉害的方向上。在这个方向上，投影点的分布范围最广，方差最大，这意味着投影后保留了更多的原始信息。
让我们用数学语言来表述这个直觉。设 $n$ 个 $d$ 维数据点 $\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n \in \mathbb{R}^d$，我们想找到一个单位向量 $\mathbf{w} \in \mathbb{R}^d$（$|\mathbf{w}| = 1$），使得数据投影到 $\mathbf{w}$ 上的方差最大。
数据点 $\mathbf{x}_i$ 投影到 $\mathbf{w}$ 上的值是：
$$ z_i = \mathbf{w}^{\top} \mathbf{x}_i $$
...</p></div><footer class=entry-footer><span title='2026-01-24 12:00:00 +0800 CST'>January 24, 2026</span>&nbsp;·&nbsp;<span>6 min</span>&nbsp;·&nbsp;<span>1141 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to PCA 主成分分析：从数据降维的优雅艺术" href=https://s-ai-unix.github.io/posts/2026-01-24-pca-comprehensive-guide/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://s-ai-unix.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>