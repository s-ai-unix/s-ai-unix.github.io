<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>机器学习 | s-ai-unix's Blog</title><meta name=keywords content><meta name=description content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml title=rss><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="机器学习"><meta property="og:description" content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="机器学习"><meta name=twitter:description content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body class=list id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/tags/>Tags</a></div><h1>机器学习</h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/inception-v4-cover.jpg alt="AI 论文解读系列 Inception-v4 Going Deeper with Convolutions"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：Inception-v4 - Going Deeper with Convolutions</h2></header><div class=entry-content><p>AI 论文解读系列：Inception-v4 - Going Deeper with Convolutions 引言 2016年2月，Google 的 Christian Szegedy 等人在 arXiv 上发表了一篇名为《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》的论文。这篇论文不仅是 Inception 系列发展的重要里程碑，更提出了一种革命性的思路：将 Inception 的多尺度特征提取能力与 ResNet 的残差连接相结合。
让我们先回顾一下当时的背景。2015年，ResNet 横空出世，用简单的跳跃连接解决了深层网络的退化问题，将网络深度推向了一百层甚至上千层。与此同时，Inception-v3 以其独特的多分支结构，在计算效率和准确率之间取得了优异的平衡。一个自然的问题浮现出来：**这两种看似迥异的设计哲学能否融合？**如果能将 Inception 的高效特征提取与残差连接的优化优势结合起来，会发生什么？
本文将系统性地解读这篇经典论文，从 Inception 系列的演进脉络出发，深入剖析 Inception-v4 的架构设计原理，探讨 Inception-ResNet 的创新之处，以及残差缩放这一关键技术的数学本质。
图：Inception 系列演进历程与 ImageNet 竞赛 Top-5 错误率变化趋势
第一章：Inception 的演进之路 1.1 Inception-v1：多尺度特征提取的开创 要理解 Inception-v4，我们需要先回到2014年的 Inception-v1（GoogLeNet）。当时，深度学习领域的主流思路是"越深越好"——AlexNet 有8层，VGGNet 堆到了19层。但 Google 的研究者们提出了一个不同的观点：与其简单地堆叠相同的层，不如让网络自己选择如何组合不同尺度的特征。
Inception 模块的核心思想可以用一个简单的问题来概括：当我们观察一张图像时，我们究竟需要多大的感受野？
识别一只猫的脸，可能只需要一个 $3 \times 3$ 的区域就能看清它的眼睛和鼻子 但要判断这是一只完整卧着的猫，可能需要一个 $5 \times 5$ 的区域来捕捉整体轮廓 而对于更宏观的场景理解，甚至需要更大的视野 Inception 模块的解决方案是并行使用不同大小的卷积核，让网络自己学习每种尺度的权重。一个典型的 Inception 模块包含四个分支：
...</p></div><footer class=entry-footer><span title='2026-01-30 12:30:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>7 min</span>&nbsp;·&nbsp;<span>1455 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：Inception-v4 - Going Deeper with Convolutions" href=https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97-inception-v4-going-deeper-with-convolutions/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/word2vec-cover.jpg alt="Word2Vec 词向量可视化"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：Word2Vec - 词向量的革命</h2></header><div class=entry-content><p>“You shall know a word by the company it keeps.” — John Rupert Firth
引言：从符号到语义 想象一下，你正在阅读一篇关于"苹果"的文章。在"乔布斯推出了划时代的苹果产品"这句话中，“苹果"显然指的是一家公司；而在"我喜欢吃新鲜的苹果"中，它则是一种水果。人类能够毫不费力地根据上下文理解这种歧义，但对于计算机而言，这曾是一个巨大的挑战。
在 Word2Vec 出现之前，自然语言处理主要依赖独热编码（One-Hot Encoding）：每个词都被表示为一个高维稀疏向量，向量中只有对应位置为 $1$，其余全为 $0$。“苹果"可能是 $[0, 0, 1, 0, \ldots, 0]$，“香蕉"是 $[0, 0, 0, 1, \ldots, 0]$。这种方法的问题显而易见：任意两个词之间的余弦相似度都是 $0$，模型完全无法捕捉"苹果"和"香蕉"都是水果这一语义关系。
2013 年，Tomas Mikolov 等人在 Google 提出了 Word2Vec，这是一种能够从大规模语料库中学习词向量表示的浅层神经网络。其核心思想简单却深刻：语义相近的词，其上下文也相似。这一方法不仅在多项语义和语法任务上取得了当时最先进的性能，更开启了深度学习在自然语言处理领域的广泛应用。
本文将带你深入理解 Word2Vec 的数学原理，从神经概率语言模型出发，完整推导 CBOW 和 Skip-gram 两种架构，并探讨其在现代 NLP 中的深远影响。
第一章：从词袋到神经语言模型 1.1 统计语言模型的演进 语言模型的核心任务是计算一个句子出现的概率。对于包含 $n$ 个词的句子
$$w_1, w_2, \ldots, w_n$$ 其联合概率可以分解为：
$$P(w_1, w_2, \ldots, w_n) = \prod_{i=1}^{n} P(w_i \mid w_1, \ldots, w_{i-1})$$ 这个分解基于链式法则，但直接估计这些条件概率面临维度灾难——历史词的组合数是指数级的。
...</p></div><footer class=entry-footer><span title='2026-01-30 09:00:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>7 min</span>&nbsp;·&nbsp;<span>1442 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：Word2Vec - 词向量的革命" href=https://s-ai-unix.github.io/posts/2026-01-30-word2vec-paper-explained/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/gpt3-paper-cover.jpg alt="GPT-3 论文解读封面"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：GPT-3——当语言模型学会举一反三</h2></header><div class=entry-content><p>引言：从海量数据中学习 2020 年 6 月，OpenAI 发表了一篇注定载入人工智能史册的论文：《Language Models are Few-Shot Learners》。这篇论文介绍了 GPT-3——一个拥有 1750 亿参数的巨型语言模型。这个数字意味着什么？如果将 GPT-3 的参数全部打印出来，使用标准字体，这些纸张可以从地球堆到月球——再返回地球好几个来回。
但 GPT-3 的真正革命性之处不在于它的规模，而在于它展现出的少样本学习能力（Few-Shot Learning）。在此之前，如果我们想让一个 AI 模型完成翻译任务，需要用成千上万对双语句子"教"它；而 GPT-3 只需要看几个例子，就能理解任务并给出合理的输出。
这篇文章将带你走进 GPT-3 的世界，理解它背后的数学原理、技术架构，以及它如何改变了我们对人工智能的认知。
第一章：从 GPT-1 到 GPT-3 的演进之路 1.1 语言的统计本质 在深入 GPT-3 之前，让我们先思考一个基本问题：什么是语言模型？
从数学角度看，语言模型试图回答这样一个问题：给定一段已出现的词序列
$$\mathbf{x}_{...</p></div><footer class=entry-footer><span title='2026-01-30 08:50:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>1 min</span>&nbsp;·&nbsp;<span>38 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：GPT-3——当语言模型学会举一反三" href=https://s-ai-unix.github.io/posts/2026-01-30-gpt3-few-shot-learners-paper/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/vit-cover.jpg alt="AI 论文解读系列 Vision Transformer cover image"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：Vision Transformer 视觉Transformer</h2></header><div class=entry-content><p>AI 论文解读系列：Vision Transformer 视觉 Transformer 引言 2020 年，Google Research 发表了一篇极具颠覆性的论文《An Image is Worth 16$\times$16 Words: Transformers for Image Recognition at Scale》。这篇论文提出了 Vision Transformer（ViT），一个纯粹基于 Transformer 架构的视觉模型，在 ImageNet 分类任务上取得了与最先进的卷积神经网络（CNN）相媲美甚至超越的成绩。
这个成果的震撼之处在于：在计算机视觉领域统治了整整十年的卷积神经网络，终于遇到了真正的挑战者。CNN 凭借其归纳偏置（局部性、平移等变性）在视觉任务中表现出色，而 Transformer 原本是为自然语言处理设计的序列模型。ViT 的成功证明，只要有足够的数据和计算资源，纯粹的注意力机制同样可以在视觉任务中大放异彩。
本文将从注意力机制的基础出发，循序渐进地剖析 ViT 的架构设计、数学原理和训练策略，揭示为何"一张图片相当于 16$\times$16 个单词"这一简单想法能够改变计算机视觉的格局。
第一章：从 CNN 到 Transformer 的范式转移 1.1 卷积神经网络的统治时代 自 2012 年 AlexNet 在 ImageNet 竞赛中取得突破性成果以来，卷积神经网络（CNN）一直是计算机视觉领域的主流架构。CNN 的成功建立在几个关键设计之上：
局部感受野（Local Receptive Fields）：每个神经元只与输入的局部区域连接，捕捉局部特征如边缘、纹理。
权重共享（Weight Sharing）：同一个卷积核在整个输入上滑动，检测相同特征的不同位置。
平移等变性（Translation Equivariance）：输入图像平移，特征图也相应平移，保持空间关系。
这些归纳偏置（Inductive Bias）使 CNN 非常适合处理图像数据，但也带来了一些限制：
感受野有限，需要堆叠多层才能获取全局信息 对长距离依赖的建模能力较弱 难以直接捕捉空间上相距较远的像素之间的关系 1.2 Transformer 在自然语言处理中的成功 2017 年，Google 在论文《Attention Is All You Need》中提出了 Transformer 架构，彻底改变了自然语言处理（NLP）领域。Transformer 完全基于自注意力机制（Self-Attention），摒弃了循环和卷积结构。
...</p></div><footer class=entry-footer><span title='2026-01-30 08:46:42 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>986 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：Vision Transformer 视觉Transformer" href=https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97vision-transformer-%E8%A7%86%E8%A7%89transformer/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/resnet-cover.jpg alt="AI 论文解读系列 ResNet 深度残差学习 cover image"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：ResNet 深度残差学习</h2></header><div class=entry-content><p>AI 论文解读系列：ResNet 深度残差学习 引言 2015 年，微软研究院的何恺明等人在 ImageNet 竞赛中提出了一个看似简单却极具革命性的想法：如果神经网络学习的是残差而非直接的映射，会发生什么？这个想法催生了 ResNet（Residual Network），一个拥有 152 层甚至 1000 多层的深度网络，不仅赢得了 ImageNet 2015 的冠军，更重要的是，它解决了困扰深度学习领域多年的一个核心问题——深层网络的退化。
在 ResNet 出现之前，人们普遍认为更深的网络应该具有更强的表达能力。然而实践却给出了反直觉的结果：当网络层数增加到一定程度后，训练准确率反而下降。这不是过拟合，因为在训练集上的表现同样变差了。ResNet 的巧妙之处在于，它通过一个极其简单的跳跃连接（skip connection），让网络可以选择学习残差映射 $\mathcal{F}(\mathbf{x}) = \mathcal{H}(\mathbf{x}) - \mathbf{x}$，而非直接学习 $\mathcal{H}(\mathbf{x})$。
本文将系统性地解读这篇经典论文，从问题背景、核心思想、数学推导、架构设计到实验验证，循序渐进地揭示 ResNet 为何如此有效。
第一章：深层网络的困境 1.1 从浅层到深层：一个自然的假设 深度学习的成功在很大程度上归功于深层神经网络强大的表示能力。从 LeNet-5 的 5 层，到 AlexNet 的 8 层，再到 VGGNet 的 16-19 层，网络深度的增加似乎与性能提升正相关。这种趋势背后的直觉很简单：更深的网络可以学习更复杂的特征层次结构。
让我们形式化地思考这个问题。假设我们有一个浅层网络，它能够学习某个映射 $\mathcal{H}(\mathbf{x})$。如果我们在其后面添加更多层，直觉上，这些额外的层可以学习恒等映射（identity mapping），即直接输出输入：$\mathbf{y} = \mathbf{x}$。这样，深层网络至少应该和浅层网络表现一样好。
然而，实践观察到的却是另一番景象。
1.2 退化问题：理论与现实的鸿沟 2015 年之前的研究者发现，当网络层数超过 20 层后，出现了一个令人困惑的现象：随着网络加深，训练误差不降反升。
上图展示了在 CIFAR-10 数据集上的典型实验结果。20 层网络的训练误差约为 8%，而 56 层网络的训练误差却上升到了 20%。请注意，这是在训练集上的表现，因此这不是过拟合问题，而是优化问题。
这个现象被称为退化问题（Degradation Problem）。它的存在表明：
...</p></div><footer class=entry-footer><span title='2026-01-30 08:38:11 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>1008 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：ResNet 深度残差学习" href=https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97%E4%B9%8Bresnet-%E6%B7%B1%E5%BA%A6%E6%AE%8B%E5%B7%AE%E5%AD%A6%E4%B9%A0/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/decision-tree-cover.jpg alt=决策树></figure><header class=entry-header><h2 class=entry-hint-parent>决策树及其衍生算法：从ID3到现代梯度提升</h2></header><div class=entry-content><p>引言：从二十个问题到机器学习 想象你在玩一个经典游戏——“二十个问题”。你需要通过最多二十个 yes/no 问题，猜出对手心中想的一个物体。聪明的玩家会问这样的问题：
“它是活的吗？” “如果活着，它是动物吗？” “如果是动物，它会飞吗？” 每一个问题都将可能的答案空间一分为二，逐步缩小范围，直到锁定目标。这种分而治之的策略，正是决策树算法的核心思想。
决策树是机器学习中最直观、最易于解释的算法之一。从医学诊断到信用评估，从游戏 AI 到推荐系统，决策树及其衍生算法无处不在。它的魅力在于：
可解释性强：决策路径清晰，非技术人员也能理解 非参数化：不需要假设数据的分布形式 处理混合数据：能同时处理数值和类别特征 捕捉非线性关系：通过分层划分，自动学习复杂的决策边界 从1986年 Ross Quinlan 提出 ID3 算法，到今天 XGBoost、LightGBM 在 Kaggle 竞赛中称霸，决策树算法已经走过了近四十年的演进历程。本文将带你从最基本的树结构出发，逐步深入到现代梯度提升框架的数学原理，揭示这一算法的优雅与力量。
第一章：决策树基础 1.1 什么是决策树？ 决策树（Decision Tree）是一种树形结构的预测模型，其中：
内部节点表示对某个特征的测试或判断 分支表示测试的结果 叶节点表示最终的预测结果（类别或数值） 图 1：决策树的基本结构。从根节点开始，根据特征值进行判断，沿着分支走到叶节点得到预测结果。
决策树既可以用于分类（预测离散类别），也可以用于回归（预测连续数值）。前者的代表是 ID3、C4.5、CART（分类树），后者的代表是 CART（回归树）。
1.2 决策树的学习过程 构建决策树的核心问题是：*如何选择每个节点的分裂特征和分裂点？
这涉及三个关键决策：
*1. 特征选择准则
我们需要一个指标来度量分裂的"好坏"。常用的准则包括：
信息增益（Information Gain）：基于信息熵的减少 基尼指数（Gini Index）：基于概率分布的纯度 均方误差（MSE）：用于回归问题 *2. 分裂点选择
对于数值特征，需要确定最优的分裂阈值。通常采用贪婪搜索：遍历所有可能的分裂点，选择使准则最优化的那个。
*3. 停止条件
递归分裂何时停止？常见的停止条件：
节点中样本数少于阈值 节点纯度达到阈值 树深度达到上限 分裂带来的增益小于阈值 1.3 决策树的预测过程 预测一个新样本时，从根节点开始：
检查当前节点的分裂特征 根据样本在该特征上的取值，选择对应的分支 移动到子节点 重复直到到达叶节点 叶节点的标签（分类）或平均值（回归）即为预测结果 时间复杂度为 $O(\log n)$，其中 $n$ 是树的高度。这意味着即使对于大规模数据集，预测速度也非常快。
...</p></div><footer class=entry-footer><span title='2026-01-29 08:11:01 +0800 CST'>January 29, 2026</span>&nbsp;·&nbsp;<span>6 min</span>&nbsp;·&nbsp;<span>1197 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 决策树及其衍生算法：从ID3到现代梯度提升" href=https://s-ai-unix.github.io/posts/2026-01-29-decision-trees-and-beyond-from-id3-to-modern-gradient-boosting/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/differential-geometry-deep-learning.jpg alt=微分几何与深度学习></figure><header class=entry-header><h2 class=entry-hint-parent>微分几何与深度学习：从流形假设到几何深度学习</h2></header><div class=entry-content><p>引言：当深度学习遇见弯曲的空间 2012年，AlexNet 在 ImageNet 竞赛中以压倒性优势获胜，深度学习正式进入大众视野。此后，神经网络在各种任务上展现出惊人能力：图像识别、语音识别、机器翻译、游戏对战……但有一个问题始终困扰着研究者：为什么神经网络能够如此有效地学习？
答案或许藏在数据的本质结构中。想象你正在看一张人脸照片——1000 $\times$ 1000 像素的图像意味着这是一个百万维的空间中的点。但所有人脸照片都分布在这个百万维空间的一个极小子集上。为什么？因为真实的人脸受到物理规律的约束：两只眼睛在鼻子两侧，嘴巴在鼻子下方，等等。
这个子集不是随机的散点集合，而是一个流形（manifold）——一个局部看起来像欧几里得空间，但整体上可能弯曲、扭转的几何对象。
流形假设（Manifold Hypothesis）是连接微分几何与深度学习的桥梁：
真实世界的高维数据往往分布在一个低维流形上。
这个假设解释了为什么深度学习能够成功，也指明了改进的方向。从流形学习的早期算法，到现代的几何深度学习，微分几何正在成为理解神经网络本质的重要语言。
让我们从最基本的流形概念开始，逐步揭开这层神秘的面纱。
第一章：流形假设——数据的几何本质 1.1 什么是流形？ 在正式定义之前，让我们从一个直观的例子开始。
想象一只蚂蚁生活在地球表面。对于这只蚂蚁来说，地面看起来是平的——它可以向前、向后、向左、向右移动。只有当它旅行了很长距离后，才会意识到这个世界是弯曲的（比如绕地球一圈回到原点）。
流形正是这种"局部平坦，整体弯曲"的空间。数学上，一个 $n$ 维流形 $\mathcal{M}$ 是一个拓扑空间，其中每一点 $p \in \mathcal{M}$ 都有一个邻域，同胚于 $\mathbb{R}^n$。
关键特性：
局部坐标：在任何小区域内，我们可以用 $n$ 个坐标 $(x^1, x^2, \ldots, x^n)$ 描述位置 过渡函数：不同坐标系统之间的变换必须是光滑的 全局结构：局部坐标片可以"缝合"成复杂的整体结构 图1：流形学习的核心思想——高维数据（如瑞士卷）实际上分布在一个低维流形上，学习的目标就是"展开"这个流形，发现其内在的低维结构。
1.2 数据流形：从高维到低维 现在回到深度学习。考虑以下例子：
MNIST 手写数字：每个图像是 $28 \times 28 = 784$ 维的向量。但所有"3"的图像并不随机分布在 784 维空间中——它们形成了一个高度结构化的集合。写下"3"的方式虽然变化多端，但受到人体解剖学和书写习惯的约束。
人脸图像：如引言所述，人脸图像分布在由身份、表情、光照、角度等参数控制的低维流形上。这些参数可能有几十个，但远小于百万级的像素维度。
词向量：自然语言处理中的词嵌入将词汇映射到连续向量空间。语义相近的词在向量空间中也相近，形成某种几何结构。
流形维数的估计：如何确定数据流形的维数？这是一个活跃的研究领域。常用方法包括：
主成分分析（PCA）：线性估计 本征维数估计：基于最近邻距离的统计方法 分形维数：对于复杂结构的数据 1.3 为什么流形结构重要？ 理解数据的流形结构对深度学习有多方面的意义：
1. 维度灾难的缓解
在 $d$ 维欧几里得空间中，要覆盖单位立方体到精度 $\epsilon$，需要 $O(\epsilon^{-d})$ 个样本。这就是维度灾难。
...</p></div><footer class=entry-footer><span title='2026-01-28 23:54:26 +0800 CST'>January 28, 2026</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>738 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 微分几何与深度学习：从流形假设到几何深度学习" href=https://s-ai-unix.github.io/posts/2026-01-28-differential-geometry-deep-learning/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/differential-geometry-autonomous-driving.jpg alt="微分几何与自动驾驶 cover image"></figure><header class=entry-header><h2 class=entry-hint-parent>弯曲的道路，智能的决策：微分几何如何赋能自动驾驶</h2></header><div class=entry-content><p>引言：当数学遇见自动驾驶 想象你正在驾驶一辆汽车行驶在蜿蜒的山路上。前方是一个急转弯，你需要减速、打方向、保持车道——这一系列看似简单的动作，实际上涉及复杂的几何判断：道路的曲率如何？转弯半径是多少？轮胎与地面的摩擦力能否提供足够的向心力？
现在，把驾驶员换成自动驾驶系统。它没有了人类的直觉和经验，必须依靠数学模型来理解这个世界。微分几何——这门研究曲线、曲面和弯曲空间的数学分支，正是自动驾驶系统的"眼睛"和"大脑"背后的理论基础。
从古希腊欧几里得研究直线和平面，到高斯发现曲面可以"内蕴地"研究，再到黎曼建立起 $n$ 维弯曲空间的一般理论，微分几何经历了两千多年的发展。而今天，这门古老的数学正以全新的方式赋能现代科技：它帮助自动驾驶汽车理解道路的几何结构，规划平滑的行驶轨迹，感知周围环境的三维形态。
本文将带你走进微分几何与自动驾驶的交汇点，看看抽象的数学概念如何在现实世界中大放异彩。
第一章：微分几何的核心概念回顾 1.1 曲线：道路的一维模型 一条道路可以抽象为三维空间中的一条参数曲线：
$$ \mathbf{r}(t) = (x(t), y(t), z(t)) $$
其中 $t$ 是参数，可以是时间，也可以是弧长。对于自动驾驶而言，我们最关心的是曲线的两个几何量：切向量和曲率。
切向量告诉我们道路在每一点的"方向"：
$$ \mathbf{T}(t) = \frac{d\mathbf{r}/dt}{\lVert d\mathbf{r}/dt \rVert} $$
汽车的前进方向应该与切向量对齐，这是最基本的控制要求。
曲率则告诉我们道路弯曲的程度。对于以弧长 $s$ 参数化的曲线，曲率定义为：
$$ \kappa(s) = \left\lVert \frac{d\mathbf{T}}{ds} \right\rVert = \left\lVert \frac{d^2\mathbf{r}}{ds^2} \right\rVert $$
曲率的倒数 $\rho = 1/\kappa$ 称为曲率半径。当汽车以速度 $v$ 通过曲率为 $\kappa$ 的路段时，所需的向心加速度为 $a = v^2 \kappa$。这就是为什么急转弯需要减速——曲率越大，所需的向心力越大。
1.2 曲面：路面的二维模型 实际的道路不是一个简单的曲线，而是一个曲面。我们可以用参数方程描述：
$$ \mathbf{r}(u, v) = (x(u, v), y(u, v), z(u, v)) $$
...</p></div><footer class=entry-footer><span title='2026-01-28 23:42:32 +0800 CST'>January 28, 2026</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>831 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 弯曲的道路，智能的决策：微分几何如何赋能自动驾驶" href=https://s-ai-unix.github.io/posts/2026-01-28-differential-geometry-autonomous-driving/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/jacobian-hessian-cover.jpg alt="Jacobian and Hessian Matrices cover image"></figure><header class=entry-header><h2 class=entry-hint-parent>雅可比矩阵与黑塞矩阵：多变量微积分的双璧</h2></header><div class=entry-content><p>引言 当我们从单变量微积分迈向多变量微积分时，一个核心问题浮现出来：如何描述多元函数的变化？在单变量情形中，导数 $f’(x)$ 告诉我们函数在某点的瞬时变化率。但当函数 $f: \mathbb{R}^n \to \mathbb{R}^m$ 拥有多个输入和输出时，情况变得复杂起来。
想象一下，你正在攀登一座山峰。在任何一个位置，你都想知道：
哪个方向最陡峭？（梯度的方向） 这个陡峭程度在各个方向如何变化？（曲率的描述） 雅可比矩阵和黑塞矩阵正是回答这些问题的数学工具。它们是多变量微积分中的"双璧"——一个描述一阶变化（线性近似），一个描述二阶变化（曲率特性）。从牛顿法到神经网络训练，从机器人运动学到广义相对论，这对"双璧"无处不在。
第一章：从一维到多维 1.1 单变量函数的局限性 回顾单变量微积分，函数 $f: \mathbb{R} \to \mathbb{R}$ 在点 $x$ 处的导数定义为：
$$ f’(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h} $$
这个定义告诉我们函数在 $x$ 处的瞬时变化率。几何上，它表示函数曲线在该点切线的斜率。
但当函数有多个输入时，例如 $f(x, y) = x^2 + y^2$，我们可以问：
沿 $x$ 方向的变化率是多少？ 沿 $y$ 方向的变化率是多少？ 沿任意方向的变化率是多少？ 这就引出了偏导数的概念。
1.2 偏导数与方向导数 函数 $f(x_1, x_2, \ldots, x_n)$ 关于 $x_i$ 的偏导数定义为：
$$ \frac{\partial f}{\partial x_i} = \lim_{h \to 0} \frac{f(x_1, \ldots, x_i+h, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h} $$
...</p></div><footer class=entry-footer><span title='2026-01-28 21:54:27 +0800 CST'>January 28, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>879 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 雅可比矩阵与黑塞矩阵：多变量微积分的双璧" href=https://s-ai-unix.github.io/posts/2026-01-28-jacobian-hessian-matrices/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/calculus-ml-journey.jpg alt=微积分的几何美感></figure><header class=entry-header><h2 class=entry-hint-parent>微积分与机器学习：从变化率到神经网络梯度的完整旅程</h2></header><div class=entry-content><p>引言：为什么需要微积分？ 想象你在山上，想找到最低点。你会怎么做？你会观察脚下的坡度，选择最陡峭的方向迈出一步，然后重复这个过程。这个简单的直觉——沿着负梯度方向走——正是现代人工智能的核心算法。
从ChatGPT的语言模型到AlphaGo的围棋策略，从图像识别到语音合成，所有这些技术背后都有一个共同的数学基础：微积分。
微积分研究的是变化。而机器学习本质上是关于优化——通过不断调整参数来减少错误。当我们在高维空间中优化复杂的神经网络时，微积分提供了描述和计算这种变化的精确语言。
这篇文章将带你深入理解微积分如何驱动现代人工智能。我们不会停留在表面，而是会深入到数学推导的核心，揭示梯度下降、反向传播等算法的数学本质。这是一次从17世纪牛顿和莱布尼茨的发明，到21世纪深度学习革命的完整旅程。
第一部分：微积分基础理论 1. 导数的本质：从变化率到瞬时变化率 1.1 变化率的直观理解 变化率是人类最早思考的数学问题之一。如果一辆车2小时行驶100公里，平均速度是50公里/小时。但它某一时刻的瞬时速度是多少？
微积分的答案是：用极限。考虑函数 $f(x)$ 在 $x_0$ 附近的平均变化率： $$ \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x} $$
当 $\Delta x \to 0$ 时，这个平均变化率的极限就是导数： $$ f^{\prime}(x_0) = \lim_{\Delta x \to 0} \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x} $$
1.2 导数的几何意义 几何直观：导数是切线的斜率。在 $x_0$ 处，曲线 $f(x)$ 可以用直线（切线）逼近： $$ f(x) \approx f(x_0) + f^{\prime}(x_0)(x - x_0) $$
这就是一阶泰勒公式，也是线性化的思想：局部用简单的线性函数逼近复杂的非线性函数。
严格定义（$\epsilon-\delta$ 语言）： $$ \forall \epsilon > 0, \exists \delta > 0 \text{ s.t. } |\Delta x| &lt; \delta \implies \left|\frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x} - f^{\prime}(x_0)\right| &lt; \epsilon $$
...</p></div><footer class=entry-footer><span title='2026-01-25 19:00:00 +0800 CST'>January 25, 2026</span>&nbsp;·&nbsp;<span>9 min</span>&nbsp;·&nbsp;<span>1716 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 微积分与机器学习：从变化率到神经网络梯度的完整旅程" href=https://s-ai-unix.github.io/posts/2026-01-25-calculus-ml-systematic-review/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>