<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>深度学习 | s-ai-unix's Blog</title><meta name=keywords content><meta name=description content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://s-ai-unix.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml title=rss><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="深度学习"><meta property="og:description" content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="深度学习"><meta name=twitter:description content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body class=list id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/tags/>Tags</a></div><h1>深度学习</h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/llama3-herd-cover.jpg alt="Llama 3 模型集群架构示意图"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作</h2></header><div class=entry-content><p>引言：开源 AI 的黎明 2024 年 7 月 23 日，Meta AI 发布了一篇重磅论文——《The Llama 3 Herd of Models》。这篇论文不仅介绍了一个拥有 4050 亿参数的巨型语言模型，更标志着开源人工智能正式迈入了与闭源巨头分庭抗礼的新纪元。
回想 2022 年底，ChatGPT 的横空出世让整个 AI 领域为之震动。然而，最强大的模型始终被封闭在 OpenAI、Google 等公司的围墙之内。研究者无法探究其内部机理，开发者无法自由定制，这种"黑箱"状态严重阻碍了 AI 技术的普惠发展。
Llama 3 的出现改变了这一切。Meta 不仅开源了完整的模型权重，还详细披露了从数据筛选到训练优化的每一个技术细节。这意味着，任何研究者和开发者都可以在自己的硬件上运行这个媲美 GPT-4 的模型，深入理解它的工作原理，甚至在此基础上进行创新。
本文将带领读者深入这篇 92 页的论文，从数据、规模、复杂性管理三个核心维度，层层剥开 Llama 3 的技术奥秘。
第一章：模型概览 —— “模型群"的设计理念 1.1 为什么叫 “Herd”（群）？ 论文标题中的 “Herd of Models” 并非随意命名。Meta 同时发布了三个不同规模的模型：
模型 参数量 上下文长度 目标场景 Llama 3 8B $8 \times 10^9$ 128K tokens 边缘设备、低延迟推理 Llama 3 70B $70 \times 10^9$ 128K tokens 平衡性能与效率 Llama 3 405B $405 \times 10^9$ 128K tokens 顶级性能、复杂推理 这种"群"策略的核心思想是：用一个旗舰模型（405B）指导整个家族的优化方向，同时让每个成员在特定场景下发挥最大价值。
...</p></div><footer class=entry-footer><span title='2026-01-31 09:30:00 +0800 CST'>January 31, 2026</span>&nbsp;·&nbsp;<span>6 min</span>&nbsp;·&nbsp;<span>1184 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作" href=https://s-ai-unix.github.io/posts/2026-01-31-ai-paper-llama3-herd-of-models/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/alphazero-cover.jpg alt="AlphaZero 国际象棋棋盘"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：AlphaZero - 从零开始的自我博弈通用算法</h2></header><div class=entry-content><p>引言：超越人类知识 2017年12月，一个历史性的事件发生在伦敦 DeepMind 的实验室里。一个名为 AlphaZero 的算法，在仅接受游戏规则、没有任何人类棋谱输入的情况下，通过短短 24 小时的自我对弈训练，不仅掌握了国际象棋，还击败了当时世界最强的国际象棋程序 Stockfish。
这不是科幻小说。2018 年 12 月，DeepMind 团队在《科学》杂志上发表了题为"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"的论文，向世界展示了这一突破。
AlphaZero 的意义远超它击败的对手。它证明了：一个通用的学习算法可以从随机初始状态开始，仅通过自我博弈，就能达到超越人类数千年积累的专业知识水平。这一成就不仅震撼了棋类世界，更深刻地影响了我们对机器学习和人工智能的认知。
第一章：从 AlphaGo 到 AlphaZero 1.1 AlphaGo 的局限 要理解 AlphaZero 的革命性，我们需要先回顾它的前辈 AlphaGo。
AlphaGo 在 2016 年击败了围棋世界冠军李世石，这是人工智能史上的里程碑。但 AlphaGo 的训练过程依赖于人类专家的知识：
监督学习阶段：使用 16 万盘人类高手棋谱训练策略网络 强化学习阶段：在监督学习基础上进一步优化 价值网络：需要人类棋谱数据进行训练 这种对人类数据的依赖带来了几个问题：
知识瓶颈：模型的上限受限于人类棋谱的质量 领域限制：针对围棋设计的架构难以迁移到其他游戏 数据成本：获取高质量人类棋谱需要大量资源 1.2 完全自主学习的愿景 AlphaZero 的核心突破在于：完全抛弃人类棋谱，从零开始学习。
这一想法的理论基础来自强化学习的一个核心洞察：如果环境是确定的，且我们能够模拟环境的动态，那么一个智能体可以通过与环境的交互来学习最优策略，而无需任何外部示范。
在棋类游戏中，这个条件完美满足：
规则完全已知且确定 可以完美模拟任意棋局的发展 胜负结果是明确的奖励信号 图 1：AlphaGo 与 AlphaZero 训练流程对比。AlphaGo 从人类棋谱开始，AlphaZero 则从随机初始化开始纯自我博弈
...</p></div><footer class=entry-footer><span title='2026-01-30 13:00:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>702 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：AlphaZero - 从零开始的自我博弈通用算法" href=https://s-ai-unix.github.io/posts/2026-01-30-alphazero-paper-interpretation/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/alphago-cover.jpg alt="AlphaGo 围棋人工智能"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：AlphaGo - 深度学习与树搜索征服围棋</h2></header><div class=entry-content><p>引言：最后的堡垒 2016年1月27日，伦敦。DeepMind 团队在《自然》杂志上发表了一篇注定要载入人工智能史册的论文：“Mastering the game of Go with deep neural networks and tree search”。这篇论文介绍了 AlphaGo——一个结合了深度神经网络和蒙特卡洛树搜索的计算机围棋程序。
就在论文发表两个月后，AlphaGo 以 4:1 的比分击败了世界围棋冠军李世石。这是人工智能历史上的一个转折点。在此之前，围棋被普遍认为是人工智能难以攻克的"最后的堡垒"。
为什么围棋如此困难？让我们从这个问题开始，逐步揭开 AlphaGo 的神秘面纱。
第一章：围棋——人工智能的终极挑战 1.1 搜索空间的爆炸性增长 围棋起源于中国，已有超过 2500 年的历史。它的规则极其简单：黑白双方轮流在 $19 \times 19$ 的棋盘交叉点上落子，以围地多者为胜。然而，这种简单规则却孕育出了近乎无穷的复杂性。
从数学角度分析，围棋的复杂度体现在两个维度：
分支因子：平均每步有约 250 种合法着法。相比之下，国际象棋约为 35。
对局长度：典型围棋对局约有 150 步。国际象棋约为 80 步。
游戏树的规模可以用 $b^d$ 来估计，其中 $b$ 是分支因子，$d$ 是深度。围棋的游戏树复杂度约为 $250^{150} \approx 10^{360}$，而国际象棋约为 $35^{80} \approx 10^{123}$。
为了理解这个数字的庞大程度，可以对比：
宇宙中估计的原子数量：约 $10^{80}$ 个 可观测宇宙的体积（以普朗克体积计）：约 $10^{185}$ 这意味着，即使使用穷举搜索——即使我们拥有由宇宙中所有原子构成的超级计算机，每颗原子每秒能进行 $10^{20}$ 次运算——也无法在宇宙年龄（约 138 亿年）内遍历完围棋的所有可能局面。
1.2 局面评估的困难 比搜索空间更棘手的是局面评估。在国际象棋中，程序员可以编写明确的评估函数：王的安全性、子力价值、控制中心等。这些启发式规则可以被形式化为可计算的函数。
但在围棋中，局面评估极其微妙。一个看似被围困的棋子群可能在 20 步后"起死回生"；一片看似稳固的领地可能因为一个隐蔽的劫争而化为乌有。人类棋手依靠直觉和"棋感"来判断局面优劣，而这种直觉很难被编码为显式规则。
...</p></div><footer class=entry-footer><span title='2026-01-30 12:30:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>667 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：AlphaGo - 深度学习与树搜索征服围棋" href=https://s-ai-unix.github.io/posts/2026-01-30-alphago-paper-interpretation/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/inception-v4-cover.jpg alt="AI 论文解读系列 Inception-v4 Going Deeper with Convolutions"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：Inception-v4 - Going Deeper with Convolutions</h2></header><div class=entry-content><p>AI 论文解读系列：Inception-v4 - Going Deeper with Convolutions 引言 2016年2月，Google 的 Christian Szegedy 等人在 arXiv 上发表了一篇名为《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》的论文。这篇论文不仅是 Inception 系列发展的重要里程碑，更提出了一种革命性的思路：将 Inception 的多尺度特征提取能力与 ResNet 的残差连接相结合。
让我们先回顾一下当时的背景。2015年，ResNet 横空出世，用简单的跳跃连接解决了深层网络的退化问题，将网络深度推向了一百层甚至上千层。与此同时，Inception-v3 以其独特的多分支结构，在计算效率和准确率之间取得了优异的平衡。一个自然的问题浮现出来：**这两种看似迥异的设计哲学能否融合？**如果能将 Inception 的高效特征提取与残差连接的优化优势结合起来，会发生什么？
本文将系统性地解读这篇经典论文，从 Inception 系列的演进脉络出发，深入剖析 Inception-v4 的架构设计原理，探讨 Inception-ResNet 的创新之处，以及残差缩放这一关键技术的数学本质。
图：Inception 系列演进历程与 ImageNet 竞赛 Top-5 错误率变化趋势
第一章：Inception 的演进之路 1.1 Inception-v1：多尺度特征提取的开创 要理解 Inception-v4，我们需要先回到2014年的 Inception-v1（GoogLeNet）。当时，深度学习领域的主流思路是"越深越好"——AlexNet 有8层，VGGNet 堆到了19层。但 Google 的研究者们提出了一个不同的观点：与其简单地堆叠相同的层，不如让网络自己选择如何组合不同尺度的特征。
Inception 模块的核心思想可以用一个简单的问题来概括：当我们观察一张图像时，我们究竟需要多大的感受野？
识别一只猫的脸，可能只需要一个 $3 \times 3$ 的区域就能看清它的眼睛和鼻子 但要判断这是一只完整卧着的猫，可能需要一个 $5 \times 5$ 的区域来捕捉整体轮廓 而对于更宏观的场景理解，甚至需要更大的视野 Inception 模块的解决方案是并行使用不同大小的卷积核，让网络自己学习每种尺度的权重。一个典型的 Inception 模块包含四个分支：
...</p></div><footer class=entry-footer><span title='2026-01-30 12:30:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>7 min</span>&nbsp;·&nbsp;<span>1455 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：Inception-v4 - Going Deeper with Convolutions" href=https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97-inception-v4-going-deeper-with-convolutions/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/bert-cover.jpg alt="BERT 自然语言处理"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：BERT - 预训练深度双向 Transformer 的革命</h2></header><div class=entry-content><p>引言：语言理解的瓶颈 2018年10月，Google AI Language 团队发布了一篇名为"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"的论文。这篇论文及其开源代码在 NLP 领域引发了一场革命。
在 BERT 出现之前，自然语言处理面临一个根本性难题：如何让机器真正理解语言的上下文含义？传统的语言模型只能从左到右（或从右到左）单向处理文本，就像阅读时只能看到当前词之前的所有词，却无法看到之后的词。这种"管中窥豹"的方式严重限制了模型的理解能力。
BERT 的核心突破在于它提出了深度双向表示的概念——通过一种新的预训练目标，让模型同时考虑词语的左右上下文，从而获得更丰富、更准确的语言理解能力。
本文将深入解读 BERT 的技术原理，从其核心思想出发，逐步揭示它如何改变了 NLP 的研究范式。
第一章：从上下文说起——为什么双向如此重要 1.1 一词多义的困境 自然语言的复杂性很大程度上源于一词多义。同一个词在不同的上下文中可能有完全不同的含义。考虑这两个句子：
“他在银行工作。"（金融机构） “河边的银行种满了柳树。"（河岸）
对于人类来说，区分这两个"银行"的含义轻而易举，因为我们能够同时看到这个词左右两侧的上下文。但对于单向语言模型来说，当它处理到"银行"这个词时，只能看到"他在"或"河边的”，无法获得足够的信息来做出准确判断。
1.2 传统语言模型的局限 传统的语言模型采用自回归（Autoregressive）方式建模，即基于前文预测下一个词：
$$ P(w_1, w_2, \ldots, w_n) = \prod_{i=1}^{n} P(w_i | w_1, \ldots, w_{i-1}) $$
GPT 等模型采用了这种从左到右的处理方式。虽然这种架构在生成任务（如机器翻译、文本摘要）中表现良好，但对于需要深度理解上下文的任务（如问答、情感分析）则存在天然的局限性。
另一种尝试是浅层双向，如 ELMo。它分别训练一个从左到右和一个从右到左的语言模型，然后将两者的表示拼接起来。这种方法虽然考虑了双向信息，但两个方向的表示是独立计算的，而非真正的深度交互。
图 1：语言模型架构对比。左图为单向模型只能看到左侧上下文，右图为 BERT 双向模型可以看到完整上下文
第二章：Transformer——BERT 的基石 在深入 BERT 之前，我们需要理解它的基础架构：Transformer。BERT 完全基于 Transformer 的 Encoder 部分构建。
2.1 注意力机制的魔力 Transformer 的核心是自注意力机制（Self-Attention）。与传统的循环神经网络（RNN）不同，自注意力允许模型直接建模序列中任意两个位置之间的关系，无论它们相距多远。
...</p></div><footer class=entry-footer><span title='2026-01-30 12:00:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>656 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：BERT - 预训练深度双向 Transformer 的革命" href=https://s-ai-unix.github.io/posts/2026-01-30-bert-paper-interpretation/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/seq2seq-cover.jpg alt="Seq2Seq 神经网络抽象图"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：Seq2Seq--从序列到序列的革命</h2></header><div class=entry-content><p>引言：翻译的困境 想象一下，你正在学习一门外语。当你听到一句法语 “Bonjour le monde” 时，你的大脑是如何将其转化为英语 “Hello world” 的？
这不是简单的逐词替换。“Bonjour” 对应 “Hello”，但 “le monde” 是 “the world” 的倒序。词序不同，语法结构不同，甚至可能一个词对应多个词。传统的机器翻译系统使用基于规则的方法或统计模型，需要大量的人工特征工程和复杂的对齐算法。
2014年，Ilya Sutskever、Oriol Vinyals 和 Quoc Le 在 Google 发表了一篇改变游戏规则的论文：“Sequence to Sequence Learning with Neural Networks”。他们提出的 Seq2Seq 架构，用一个统一的神经网络模型取代了复杂的流水线，让机器翻译的准确率跃升到了新的高度。
但这篇论文的意义远不止于翻译。它开创了序列转导（Sequence Transduction）这一全新的学习范式，为后来的注意力机制、Transformer 乃至大语言模型奠定了基础。
第一章：序列转导问题 1.1 什么让序列数据特殊 在深入 Seq2Seq 之前，让我们先理解序列数据的本质。
传统的机器学习任务，比如图像分类或房价预测，输入和输出的维度是固定的。一张图片永远是 $224 \times 224 \times 3$ 的像素矩阵，一套房子的特征永远是卧室数、面积、位置等固定字段。
但序列数据不同：
一句话可能有 5 个词，也可能有 50 个词 源语言和目标语言的词序可能不同 一个概念可能用一个词表达，也可能用多个词 上图展示了一个典型的机器翻译场景。输入序列 “Hello world this is a test” 需要被转换为 “Bonjour monde ceci est un test”。注意两个关键挑战：
...</p></div><footer class=entry-footer><span title='2026-01-30 09:00:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>763 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：Seq2Seq--从序列到序列的革命" href=https://s-ai-unix.github.io/posts/2026-01-30-seq2seq-paper-explained/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/word2vec-cover.jpg alt="Word2Vec 词向量可视化"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：Word2Vec - 词向量的革命</h2></header><div class=entry-content><p>“You shall know a word by the company it keeps.” — John Rupert Firth
引言：从符号到语义 想象一下，你正在阅读一篇关于"苹果"的文章。在"乔布斯推出了划时代的苹果产品"这句话中，“苹果"显然指的是一家公司；而在"我喜欢吃新鲜的苹果"中，它则是一种水果。人类能够毫不费力地根据上下文理解这种歧义，但对于计算机而言，这曾是一个巨大的挑战。
在 Word2Vec 出现之前，自然语言处理主要依赖独热编码（One-Hot Encoding）：每个词都被表示为一个高维稀疏向量，向量中只有对应位置为 $1$，其余全为 $0$。“苹果"可能是 $[0, 0, 1, 0, \ldots, 0]$，“香蕉"是 $[0, 0, 0, 1, \ldots, 0]$。这种方法的问题显而易见：任意两个词之间的余弦相似度都是 $0$，模型完全无法捕捉"苹果"和"香蕉"都是水果这一语义关系。
2013 年，Tomas Mikolov 等人在 Google 提出了 Word2Vec，这是一种能够从大规模语料库中学习词向量表示的浅层神经网络。其核心思想简单却深刻：语义相近的词，其上下文也相似。这一方法不仅在多项语义和语法任务上取得了当时最先进的性能，更开启了深度学习在自然语言处理领域的广泛应用。
本文将带你深入理解 Word2Vec 的数学原理，从神经概率语言模型出发，完整推导 CBOW 和 Skip-gram 两种架构，并探讨其在现代 NLP 中的深远影响。
第一章：从词袋到神经语言模型 1.1 统计语言模型的演进 语言模型的核心任务是计算一个句子出现的概率。对于包含 $n$ 个词的句子
$$w_1, w_2, \ldots, w_n$$ 其联合概率可以分解为：
$$P(w_1, w_2, \ldots, w_n) = \prod_{i=1}^{n} P(w_i \mid w_1, \ldots, w_{i-1})$$ 这个分解基于链式法则，但直接估计这些条件概率面临维度灾难——历史词的组合数是指数级的。
...</p></div><footer class=entry-footer><span title='2026-01-30 09:00:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>7 min</span>&nbsp;·&nbsp;<span>1442 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：Word2Vec - 词向量的革命" href=https://s-ai-unix.github.io/posts/2026-01-30-word2vec-paper-explained/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/vit-cover.jpg alt="AI 论文解读系列 Vision Transformer cover image"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：Vision Transformer 视觉Transformer</h2></header><div class=entry-content><p>AI 论文解读系列：Vision Transformer 视觉 Transformer 引言 2020 年，Google Research 发表了一篇极具颠覆性的论文《An Image is Worth 16$\times$16 Words: Transformers for Image Recognition at Scale》。这篇论文提出了 Vision Transformer（ViT），一个纯粹基于 Transformer 架构的视觉模型，在 ImageNet 分类任务上取得了与最先进的卷积神经网络（CNN）相媲美甚至超越的成绩。
这个成果的震撼之处在于：在计算机视觉领域统治了整整十年的卷积神经网络，终于遇到了真正的挑战者。CNN 凭借其归纳偏置（局部性、平移等变性）在视觉任务中表现出色，而 Transformer 原本是为自然语言处理设计的序列模型。ViT 的成功证明，只要有足够的数据和计算资源，纯粹的注意力机制同样可以在视觉任务中大放异彩。
本文将从注意力机制的基础出发，循序渐进地剖析 ViT 的架构设计、数学原理和训练策略，揭示为何"一张图片相当于 16$\times$16 个单词"这一简单想法能够改变计算机视觉的格局。
第一章：从 CNN 到 Transformer 的范式转移 1.1 卷积神经网络的统治时代 自 2012 年 AlexNet 在 ImageNet 竞赛中取得突破性成果以来，卷积神经网络（CNN）一直是计算机视觉领域的主流架构。CNN 的成功建立在几个关键设计之上：
局部感受野（Local Receptive Fields）：每个神经元只与输入的局部区域连接，捕捉局部特征如边缘、纹理。
权重共享（Weight Sharing）：同一个卷积核在整个输入上滑动，检测相同特征的不同位置。
平移等变性（Translation Equivariance）：输入图像平移，特征图也相应平移，保持空间关系。
这些归纳偏置（Inductive Bias）使 CNN 非常适合处理图像数据，但也带来了一些限制：
感受野有限，需要堆叠多层才能获取全局信息 对长距离依赖的建模能力较弱 难以直接捕捉空间上相距较远的像素之间的关系 1.2 Transformer 在自然语言处理中的成功 2017 年，Google 在论文《Attention Is All You Need》中提出了 Transformer 架构，彻底改变了自然语言处理（NLP）领域。Transformer 完全基于自注意力机制（Self-Attention），摒弃了循环和卷积结构。
...</p></div><footer class=entry-footer><span title='2026-01-30 08:46:42 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>986 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：Vision Transformer 视觉Transformer" href=https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97vision-transformer-%E8%A7%86%E8%A7%89transformer/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/resnet-cover.jpg alt="AI 论文解读系列 ResNet 深度残差学习 cover image"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：ResNet 深度残差学习</h2></header><div class=entry-content><p>AI 论文解读系列：ResNet 深度残差学习 引言 2015 年，微软研究院的何恺明等人在 ImageNet 竞赛中提出了一个看似简单却极具革命性的想法：如果神经网络学习的是残差而非直接的映射，会发生什么？这个想法催生了 ResNet（Residual Network），一个拥有 152 层甚至 1000 多层的深度网络，不仅赢得了 ImageNet 2015 的冠军，更重要的是，它解决了困扰深度学习领域多年的一个核心问题——深层网络的退化。
在 ResNet 出现之前，人们普遍认为更深的网络应该具有更强的表达能力。然而实践却给出了反直觉的结果：当网络层数增加到一定程度后，训练准确率反而下降。这不是过拟合，因为在训练集上的表现同样变差了。ResNet 的巧妙之处在于，它通过一个极其简单的跳跃连接（skip connection），让网络可以选择学习残差映射 $\mathcal{F}(\mathbf{x}) = \mathcal{H}(\mathbf{x}) - \mathbf{x}$，而非直接学习 $\mathcal{H}(\mathbf{x})$。
本文将系统性地解读这篇经典论文，从问题背景、核心思想、数学推导、架构设计到实验验证，循序渐进地揭示 ResNet 为何如此有效。
第一章：深层网络的困境 1.1 从浅层到深层：一个自然的假设 深度学习的成功在很大程度上归功于深层神经网络强大的表示能力。从 LeNet-5 的 5 层，到 AlexNet 的 8 层，再到 VGGNet 的 16-19 层，网络深度的增加似乎与性能提升正相关。这种趋势背后的直觉很简单：更深的网络可以学习更复杂的特征层次结构。
让我们形式化地思考这个问题。假设我们有一个浅层网络，它能够学习某个映射 $\mathcal{H}(\mathbf{x})$。如果我们在其后面添加更多层，直觉上，这些额外的层可以学习恒等映射（identity mapping），即直接输出输入：$\mathbf{y} = \mathbf{x}$。这样，深层网络至少应该和浅层网络表现一样好。
然而，实践观察到的却是另一番景象。
1.2 退化问题：理论与现实的鸿沟 2015 年之前的研究者发现，当网络层数超过 20 层后，出现了一个令人困惑的现象：随着网络加深，训练误差不降反升。
上图展示了在 CIFAR-10 数据集上的典型实验结果。20 层网络的训练误差约为 8%，而 56 层网络的训练误差却上升到了 20%。请注意，这是在训练集上的表现，因此这不是过拟合问题，而是优化问题。
这个现象被称为退化问题（Degradation Problem）。它的存在表明：
...</p></div><footer class=entry-footer><span title='2026-01-30 08:38:11 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>1008 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：ResNet 深度残差学习" href=https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97%E4%B9%8Bresnet-%E6%B7%B1%E5%BA%A6%E6%AE%8B%E5%B7%AE%E5%AD%A6%E4%B9%A0/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/tensor-cover.jpg alt=张量与多维数据></figure><header class=entry-header><h2 class=entry-hint-parent>张量：从数学抽象到深度学习核心的系统综述</h2></header><div class=entry-content><p>引言：多维世界的数学语言 想象你正在观察一个正在旋转的陀螺。描述它需要多少参数？
位置：$3$ 个坐标 $(x, y, z)$ 方向：$3$ 个欧拉角 角速度：$3$ 个分量 转动惯量：$9$ 个数（$3 \times 3$ 矩阵） 这些量不仅仅是数字的集合，它们有特定的变换规则。当坐标系旋转时，位置和角速度按向量规则变换，而转动惯量则按更复杂的规则变换——这就是张量。
在物理学中，张量是描述场的通用语言。爱因斯坦的广义相对论用张量写下：
$$G_{\mu\nu} + \Lambda g_{\mu\nu} = \frac{8\pi G}{c^4} T_{\mu\nu}$$
在深度学习中，一张 $224 \times 224$ 的彩色图像是 $224 \times 224 \times 3$ 的三阶张量。一批 $32$ 张这样的图像是 $32 \times 224 \times 224 \times 3$ 的四阶张量。
本文将带你走进张量的世界，从数学定义到物理直觉，从代数运算到现代应用，理解为什么张量成为描述复杂系统的核心工具。
第一章：张量的本质——超越矩阵的多维数组 1.1 从标量到张量 在数学中，我们熟悉不同维度的对象：
图 1：张量的维度层级。从0阶标量（单个数字）到1阶向量、2阶矩阵，再到3阶及更高阶张量，维度不断增加。
*0阶张量：标量
标量只有一个数值，没有方向：
$$a = 5, \quad T = 300\text{K}, \quad E = mc^2$$
标量在坐标变换下不变——无论你从哪个角度看，温度始终是 $300$K。
...</p></div><footer class=entry-footer><span title='2026-01-29 08:00:00 +0800 CST'>January 29, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>1019 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 张量：从数学抽象到深度学习核心的系统综述" href=https://s-ai-unix.github.io/posts/2026-01-29-tensor-comprehensive-guide/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://s-ai-unix.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>