<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>特征值分解 on s-ai-unix's Blog</title><link>https://s-ai-unix.github.io/tags/%E7%89%B9%E5%BE%81%E5%80%BC%E5%88%86%E8%A7%A3/</link><description>Recent content in 特征值分解 on s-ai-unix's Blog</description><generator>Hugo -- 0.154.5</generator><language>zh-cn</language><lastBuildDate>Sat, 24 Jan 2026 12:00:00 +0800</lastBuildDate><atom:link href="https://s-ai-unix.github.io/tags/%E7%89%B9%E5%BE%81%E5%80%BC%E5%88%86%E8%A7%A3/index.xml" rel="self" type="application/rss+xml"/><item><title>PCA 主成分分析：从数据降维的优雅艺术</title><link>https://s-ai-unix.github.io/posts/2026-01-24-pca-comprehensive-guide/</link><pubDate>Sat, 24 Jan 2026 12:00:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-24-pca-comprehensive-guide/</guid><description>深入探讨机器学习中的核心降维算法 PCA，从直观理解到数学推导，从两种等价的视角（最大化方差、最小化重构误差）揭示其本质，包含完整的证明和实际应用。</description></item></channel></rss>