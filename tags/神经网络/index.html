<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>神经网络 | s-ai-unix's Blog</title><meta name=keywords content><meta name=description content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://s-ai-unix.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.xml title=rss><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="神经网络"><meta property="og:description" content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="神经网络"><meta name=twitter:description content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body class=list id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/tags/>Tags</a></div><h1>神经网络</h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/llama3-herd-cover.jpg alt="Llama 3 模型集群架构示意图"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作</h2></header><div class=entry-content><p>引言：开源 AI 的黎明 2024 年 7 月 23 日，Meta AI 发布了一篇重磅论文——《The Llama 3 Herd of Models》。这篇论文不仅介绍了一个拥有 4050 亿参数的巨型语言模型，更标志着开源人工智能正式迈入了与闭源巨头分庭抗礼的新纪元。
回想 2022 年底，ChatGPT 的横空出世让整个 AI 领域为之震动。然而，最强大的模型始终被封闭在 OpenAI、Google 等公司的围墙之内。研究者无法探究其内部机理，开发者无法自由定制，这种"黑箱"状态严重阻碍了 AI 技术的普惠发展。
Llama 3 的出现改变了这一切。Meta 不仅开源了完整的模型权重，还详细披露了从数据筛选到训练优化的每一个技术细节。这意味着，任何研究者和开发者都可以在自己的硬件上运行这个媲美 GPT-4 的模型，深入理解它的工作原理，甚至在此基础上进行创新。
本文将带领读者深入这篇 92 页的论文，从数据、规模、复杂性管理三个核心维度，层层剥开 Llama 3 的技术奥秘。
第一章：模型概览 —— “模型群"的设计理念 1.1 为什么叫 “Herd”（群）？ 论文标题中的 “Herd of Models” 并非随意命名。Meta 同时发布了三个不同规模的模型：
模型 参数量 上下文长度 目标场景 Llama 3 8B $8 \times 10^9$ 128K tokens 边缘设备、低延迟推理 Llama 3 70B $70 \times 10^9$ 128K tokens 平衡性能与效率 Llama 3 405B $405 \times 10^9$ 128K tokens 顶级性能、复杂推理 这种"群"策略的核心思想是：用一个旗舰模型（405B）指导整个家族的优化方向，同时让每个成员在特定场景下发挥最大价值。
...</p></div><footer class=entry-footer><span title='2026-01-31 09:30:00 +0800 CST'>January 31, 2026</span>&nbsp;·&nbsp;<span>6 min</span>&nbsp;·&nbsp;<span>1184 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作" href=https://s-ai-unix.github.io/posts/2026-01-31-ai-paper-llama3-herd-of-models/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/alphago-cover.jpg alt="AlphaGo 围棋人工智能"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：AlphaGo - 深度学习与树搜索征服围棋</h2></header><div class=entry-content><p>引言：最后的堡垒 2016年1月27日，伦敦。DeepMind 团队在《自然》杂志上发表了一篇注定要载入人工智能史册的论文：“Mastering the game of Go with deep neural networks and tree search”。这篇论文介绍了 AlphaGo——一个结合了深度神经网络和蒙特卡洛树搜索的计算机围棋程序。
就在论文发表两个月后，AlphaGo 以 4:1 的比分击败了世界围棋冠军李世石。这是人工智能历史上的一个转折点。在此之前，围棋被普遍认为是人工智能难以攻克的"最后的堡垒"。
为什么围棋如此困难？让我们从这个问题开始，逐步揭开 AlphaGo 的神秘面纱。
第一章：围棋——人工智能的终极挑战 1.1 搜索空间的爆炸性增长 围棋起源于中国，已有超过 2500 年的历史。它的规则极其简单：黑白双方轮流在 $19 \times 19$ 的棋盘交叉点上落子，以围地多者为胜。然而，这种简单规则却孕育出了近乎无穷的复杂性。
从数学角度分析，围棋的复杂度体现在两个维度：
分支因子：平均每步有约 250 种合法着法。相比之下，国际象棋约为 35。
对局长度：典型围棋对局约有 150 步。国际象棋约为 80 步。
游戏树的规模可以用 $b^d$ 来估计，其中 $b$ 是分支因子，$d$ 是深度。围棋的游戏树复杂度约为 $250^{150} \approx 10^{360}$，而国际象棋约为 $35^{80} \approx 10^{123}$。
为了理解这个数字的庞大程度，可以对比：
宇宙中估计的原子数量：约 $10^{80}$ 个 可观测宇宙的体积（以普朗克体积计）：约 $10^{185}$ 这意味着，即使使用穷举搜索——即使我们拥有由宇宙中所有原子构成的超级计算机，每颗原子每秒能进行 $10^{20}$ 次运算——也无法在宇宙年龄（约 138 亿年）内遍历完围棋的所有可能局面。
1.2 局面评估的困难 比搜索空间更棘手的是局面评估。在国际象棋中，程序员可以编写明确的评估函数：王的安全性、子力价值、控制中心等。这些启发式规则可以被形式化为可计算的函数。
但在围棋中，局面评估极其微妙。一个看似被围困的棋子群可能在 20 步后"起死回生"；一片看似稳固的领地可能因为一个隐蔽的劫争而化为乌有。人类棋手依靠直觉和"棋感"来判断局面优劣，而这种直觉很难被编码为显式规则。
...</p></div><footer class=entry-footer><span title='2026-01-30 12:30:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>667 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：AlphaGo - 深度学习与树搜索征服围棋" href=https://s-ai-unix.github.io/posts/2026-01-30-alphago-paper-interpretation/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/inception-v4-cover.jpg alt="AI 论文解读系列 Inception-v4 Going Deeper with Convolutions"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：Inception-v4 - Going Deeper with Convolutions</h2></header><div class=entry-content><p>AI 论文解读系列：Inception-v4 - Going Deeper with Convolutions 引言 2016年2月，Google 的 Christian Szegedy 等人在 arXiv 上发表了一篇名为《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》的论文。这篇论文不仅是 Inception 系列发展的重要里程碑，更提出了一种革命性的思路：将 Inception 的多尺度特征提取能力与 ResNet 的残差连接相结合。
让我们先回顾一下当时的背景。2015年，ResNet 横空出世，用简单的跳跃连接解决了深层网络的退化问题，将网络深度推向了一百层甚至上千层。与此同时，Inception-v3 以其独特的多分支结构，在计算效率和准确率之间取得了优异的平衡。一个自然的问题浮现出来：**这两种看似迥异的设计哲学能否融合？**如果能将 Inception 的高效特征提取与残差连接的优化优势结合起来，会发生什么？
本文将系统性地解读这篇经典论文，从 Inception 系列的演进脉络出发，深入剖析 Inception-v4 的架构设计原理，探讨 Inception-ResNet 的创新之处，以及残差缩放这一关键技术的数学本质。
图：Inception 系列演进历程与 ImageNet 竞赛 Top-5 错误率变化趋势
第一章：Inception 的演进之路 1.1 Inception-v1：多尺度特征提取的开创 要理解 Inception-v4，我们需要先回到2014年的 Inception-v1（GoogLeNet）。当时，深度学习领域的主流思路是"越深越好"——AlexNet 有8层，VGGNet 堆到了19层。但 Google 的研究者们提出了一个不同的观点：与其简单地堆叠相同的层，不如让网络自己选择如何组合不同尺度的特征。
Inception 模块的核心思想可以用一个简单的问题来概括：当我们观察一张图像时，我们究竟需要多大的感受野？
识别一只猫的脸，可能只需要一个 $3 \times 3$ 的区域就能看清它的眼睛和鼻子 但要判断这是一只完整卧着的猫，可能需要一个 $5 \times 5$ 的区域来捕捉整体轮廓 而对于更宏观的场景理解，甚至需要更大的视野 Inception 模块的解决方案是并行使用不同大小的卷积核，让网络自己学习每种尺度的权重。一个典型的 Inception 模块包含四个分支：
...</p></div><footer class=entry-footer><span title='2026-01-30 12:30:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>7 min</span>&nbsp;·&nbsp;<span>1455 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：Inception-v4 - Going Deeper with Convolutions" href=https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97-inception-v4-going-deeper-with-convolutions/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/seq2seq-cover.jpg alt="Seq2Seq 神经网络抽象图"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：Seq2Seq--从序列到序列的革命</h2></header><div class=entry-content><p>引言：翻译的困境 想象一下，你正在学习一门外语。当你听到一句法语 “Bonjour le monde” 时，你的大脑是如何将其转化为英语 “Hello world” 的？
这不是简单的逐词替换。“Bonjour” 对应 “Hello”，但 “le monde” 是 “the world” 的倒序。词序不同，语法结构不同，甚至可能一个词对应多个词。传统的机器翻译系统使用基于规则的方法或统计模型，需要大量的人工特征工程和复杂的对齐算法。
2014年，Ilya Sutskever、Oriol Vinyals 和 Quoc Le 在 Google 发表了一篇改变游戏规则的论文：“Sequence to Sequence Learning with Neural Networks”。他们提出的 Seq2Seq 架构，用一个统一的神经网络模型取代了复杂的流水线，让机器翻译的准确率跃升到了新的高度。
但这篇论文的意义远不止于翻译。它开创了序列转导（Sequence Transduction）这一全新的学习范式，为后来的注意力机制、Transformer 乃至大语言模型奠定了基础。
第一章：序列转导问题 1.1 什么让序列数据特殊 在深入 Seq2Seq 之前，让我们先理解序列数据的本质。
传统的机器学习任务，比如图像分类或房价预测，输入和输出的维度是固定的。一张图片永远是 $224 \times 224 \times 3$ 的像素矩阵，一套房子的特征永远是卧室数、面积、位置等固定字段。
但序列数据不同：
一句话可能有 5 个词，也可能有 50 个词 源语言和目标语言的词序可能不同 一个概念可能用一个词表达，也可能用多个词 上图展示了一个典型的机器翻译场景。输入序列 “Hello world this is a test” 需要被转换为 “Bonjour monde ceci est un test”。注意两个关键挑战：
...</p></div><footer class=entry-footer><span title='2026-01-30 09:00:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>763 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：Seq2Seq--从序列到序列的革命" href=https://s-ai-unix.github.io/posts/2026-01-30-seq2seq-paper-explained/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/gpt3-paper-cover.jpg alt="GPT-3 论文解读封面"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：GPT-3——当语言模型学会举一反三</h2></header><div class=entry-content><p>引言：从海量数据中学习 2020 年 6 月，OpenAI 发表了一篇注定载入人工智能史册的论文：《Language Models are Few-Shot Learners》。这篇论文介绍了 GPT-3——一个拥有 1750 亿参数的巨型语言模型。这个数字意味着什么？如果将 GPT-3 的参数全部打印出来，使用标准字体，这些纸张可以从地球堆到月球——再返回地球好几个来回。
但 GPT-3 的真正革命性之处不在于它的规模，而在于它展现出的少样本学习能力（Few-Shot Learning）。在此之前，如果我们想让一个 AI 模型完成翻译任务，需要用成千上万对双语句子"教"它；而 GPT-3 只需要看几个例子，就能理解任务并给出合理的输出。
这篇文章将带你走进 GPT-3 的世界，理解它背后的数学原理、技术架构，以及它如何改变了我们对人工智能的认知。
第一章：从 GPT-1 到 GPT-3 的演进之路 1.1 语言的统计本质 在深入 GPT-3 之前，让我们先思考一个基本问题：什么是语言模型？
从数学角度看，语言模型试图回答这样一个问题：给定一段已出现的词序列
$$\mathbf{x}_{...</p></div><footer class=entry-footer><span title='2026-01-30 08:50:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>1 min</span>&nbsp;·&nbsp;<span>38 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：GPT-3——当语言模型学会举一反三" href=https://s-ai-unix.github.io/posts/2026-01-30-gpt3-few-shot-learners-paper/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/resnet-cover.jpg alt="AI 论文解读系列 ResNet 深度残差学习 cover image"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：ResNet 深度残差学习</h2></header><div class=entry-content><p>AI 论文解读系列：ResNet 深度残差学习 引言 2015 年，微软研究院的何恺明等人在 ImageNet 竞赛中提出了一个看似简单却极具革命性的想法：如果神经网络学习的是残差而非直接的映射，会发生什么？这个想法催生了 ResNet（Residual Network），一个拥有 152 层甚至 1000 多层的深度网络，不仅赢得了 ImageNet 2015 的冠军，更重要的是，它解决了困扰深度学习领域多年的一个核心问题——深层网络的退化。
在 ResNet 出现之前，人们普遍认为更深的网络应该具有更强的表达能力。然而实践却给出了反直觉的结果：当网络层数增加到一定程度后，训练准确率反而下降。这不是过拟合，因为在训练集上的表现同样变差了。ResNet 的巧妙之处在于，它通过一个极其简单的跳跃连接（skip connection），让网络可以选择学习残差映射 $\mathcal{F}(\mathbf{x}) = \mathcal{H}(\mathbf{x}) - \mathbf{x}$，而非直接学习 $\mathcal{H}(\mathbf{x})$。
本文将系统性地解读这篇经典论文，从问题背景、核心思想、数学推导、架构设计到实验验证，循序渐进地揭示 ResNet 为何如此有效。
第一章：深层网络的困境 1.1 从浅层到深层：一个自然的假设 深度学习的成功在很大程度上归功于深层神经网络强大的表示能力。从 LeNet-5 的 5 层，到 AlexNet 的 8 层，再到 VGGNet 的 16-19 层，网络深度的增加似乎与性能提升正相关。这种趋势背后的直觉很简单：更深的网络可以学习更复杂的特征层次结构。
让我们形式化地思考这个问题。假设我们有一个浅层网络，它能够学习某个映射 $\mathcal{H}(\mathbf{x})$。如果我们在其后面添加更多层，直觉上，这些额外的层可以学习恒等映射（identity mapping），即直接输出输入：$\mathbf{y} = \mathbf{x}$。这样，深层网络至少应该和浅层网络表现一样好。
然而，实践观察到的却是另一番景象。
1.2 退化问题：理论与现实的鸿沟 2015 年之前的研究者发现，当网络层数超过 20 层后，出现了一个令人困惑的现象：随着网络加深，训练误差不降反升。
上图展示了在 CIFAR-10 数据集上的典型实验结果。20 层网络的训练误差约为 8%，而 56 层网络的训练误差却上升到了 20%。请注意，这是在训练集上的表现，因此这不是过拟合问题，而是优化问题。
这个现象被称为退化问题（Degradation Problem）。它的存在表明：
...</p></div><footer class=entry-footer><span title='2026-01-30 08:38:11 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>1008 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：ResNet 深度残差学习" href=https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97%E4%B9%8Bresnet-%E6%B7%B1%E5%BA%A6%E6%AE%8B%E5%B7%AE%E5%AD%A6%E4%B9%A0/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/alexnet-cover.jpg alt=神经网络与深度学习></figure><header class=entry-header><h2 class=entry-hint-parent>AlexNet：开启深度学习革命的里程碑</h2></header><div class=entry-content><p>引言：一个时代的分水岭 $2012$ 年 $9$ 月 $30$ 日，多伦多大学的研究团队在 ImageNet 大规模视觉识别挑战赛（ILSVRC）上提交了一个卷积神经网络模型。当时，没有人意识到这将是一个历史性的时刻。
这个模型叫做 AlexNet，以第一作者 Alex Krizhevsky 的名字命名。它在图像分类任务上将 Top-5 错误率从上一年的 $25.8%$ 骤降至 $16.4%$——降幅接近 $10$ 个百分点，远超第二名近 $10%$。
这不是一次普通的进步，这是一次范式革命。
在此之前，深度学习经历了漫长的"寒冬"。尽管 $1986$ 年反向传播算法已被提出，$1998$ 年 LeCun 的 LeNet 已经证明了卷积神经网络的潜力，但深层网络的训练一直受困于梯度消失、计算资源匮乏和数据不足等问题。
AlexNet 的突破不仅在于它赢得了比赛，更在于它证明了：深度神经网络可以在大规模数据集上有效训练，并且性能远超传统方法。
这一证明，开启了人工智能的新纪元。
第一章：黎明前的黑暗——深度学习的寒冬 1.1 感知机的兴衰 要理解 AlexNet 的意义，我们需要回溯到神经网络的起源。
$1958$ 年，Frank Rosenblatt 提出了感知机（Perceptron），这是第一个能够学习的神经网络模型。Rosenblatt 乐观地宣称：“感知机最终将能够学习、做出决策和翻译语言。”
然而，$1969$ 年，Marvin Minsky 和 Seymour Papert 在《Perceptrons》一书中证明了感知机的局限性：它无法解决非线性可分问题，比如简单的异或（XOR）问题。
这个打击是致命的。神经网络研究陷入了第一次寒冬。
1.2 反向传播的曙光与困境 $1986$ 年，Rumelhart、Hinton 和 Williams 重新发现了反向传播算法（Backpropagation），为训练多层神经网络提供了理论基础。
反向传播的核心思想：
给定损失函数 $L$，网络参数 $\mathbf{W}$，反向传播通过链式法则计算梯度：
$$\frac{\partial L}{\partial w_{ij}^{(l)}} = \frac{\partial L}{\partial z_i^{(l)}} \cdot \frac{\partial z_i^{(l)}}{\partial w_{ij}^{(l)}} = \delta_i^{(l)} \cdot a_j^{(l-1)}$$
...</p></div><footer class=entry-footer><span title='2026-01-29 06:00:00 +0800 CST'>January 29, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>1021 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AlexNet：开启深度学习革命的里程碑" href=https://s-ai-unix.github.io/posts/2026-01-29-alexnet-deep-learning-revolution/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/vae-network.jpg alt=变分自编码器网络结构示意图></figure><header class=entry-header><h2 class=entry-hint-parent>变分自编码器：从概率建模到深度生成的优雅桥梁</h2></header><div class=entry-content><p>引言：概率与生成的交响曲 想象你在创作一幅肖像画。你观察模特的面容，记住她的眼睛形状、嘴角弧度、颧骨位置——这些是你观察到的具体特征。但当你拿起画笔时，你不仅仅是在复制这些特征，而是在大脑中提取出某种"风格特征"：一种抽象的、压缩的表示。然后，基于这个压缩表示，你重新生成一幅作品。
这就是自编码器（Autoencoder）的基本思想：将高维数据压缩到低维潜在空间，然后再从潜在空间重建原始数据。但传统的自编码器有一个致命缺陷：它学习的潜在空间是确定性的，这意味着我们无法从潜在空间中生成新的样本——我们只能重建已有的数据。
2013 年，Kingma 和 Welling 提出了变分自编码器（Variational Autoencoder，VAE），它将变分推断的思想引入深度学习，通过将潜在变量建模为概率分布，使得我们能够：
学习数据生成模型 从潜在空间采样生成新的、从未见过的样本 控制生成过程（通过操控潜在变量） 这不仅仅是一个算法，更是概率图模型与深度学习的完美结合。让我们一同踏上这段从变分推断到深度生成的优雅之旅。
第一章：自编码器基础 1.1 自编码器的直观理解 自编码器是一个神经网络，由两部分组成：
编码器（Encoder）：$z = f_{\text{enc}}(x)$，将输入 $x$ 映射到潜在表示 $z$ 解码器（Decoder）：$\hat{x} = f_{\text{dec}}(z)$，从潜在表示重建输入 训练目标是让重建误差最小化：
$$\mathcal{L}_{\text{AE}} = | x - \hat{x} |^2$$
1.2 标准自编码器的局限性 标准自编码器的编码器学习的是一个确定性映射：对于每个输入 $x$，潜在变量 $z$ 是一个固定的向量。这带来两个问题：
无法生成新样本：因为我们不知道潜在空间的概率分布，无法采样新的 $z$ 来生成 $\hat{x}$ 潜在空间不连续：即使输入 $x_1$ 和 $x_2$ 很相似，它们的潜在表示 $z_1$ 和 $z_2$ 可能相距很远 这些局限性推动我们思考：如果将潜在变量建模为概率分布，情况会怎样？
第二章：变分推断的核心思想 2.1 生成模型的框架 假设我们有一组观测数据 $\mathbf{x} = {x^{(1)}, x^{(2)}, \ldots, x^{(N)}}$，我们想要学习一个生成模型，其过程如下：
从某个先验分布 $p(z)$ 中采样潜在变量 $z$ 通过概率分布 $p(x|z)$ 生成观测数据 $x$ 这背后的概率图模型可以表示为：
...</p></div><footer class=entry-footer><span title='2026-01-24 18:30:00 +0800 CST'>January 24, 2026</span>&nbsp;·&nbsp;<span>6 min</span>&nbsp;·&nbsp;<span>1174 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 变分自编码器：从概率建模到深度生成的优雅桥梁" href=https://s-ai-unix.github.io/posts/2026-01-24-variational-autoencoder/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/gan-abstract.jpg alt=生成对抗网络的抽象艺术表现></figure><header class=entry-header><h2 class=entry-hint-parent>生成对抗网络：从混沌中创造秩序的博弈论</h2></header><div class=entry-content><p>引言：从混沌中创造秩序 想象你是一位艺术鉴赏家,正在试图辨别一幅画作是大师真迹还是现代仿品。你仔细观察笔触、色彩、构图,试图找出破绽。与此同时,另一位艺术家正在努力学习大师的风格,试图创作出能骗过你的作品。这是一个永恒的博弈:一方越来越擅长伪造,另一方越来越擅长辨别。
这正是生成对抗网络的核心思想。2014年,Ian Goodfellow 在一个学术研讨会上提出了这个想法,当时有人认为这是"在酒吧里想出来的疯狂主意"。然而,这个"疯狂的主意"彻底改变了生成式人工智能的格局。
第一章：生成问题的本质 在深入 GAN 之前,让我们先理解什么是"生成"问题。假设我们有一个数据集,比如一堆手写数字图片。我们希望创建一个模型,能够生成"看起来像"这些手写数字的新图片。
这个问题有两个核心挑战:
数据分布建模: 我们需要学习数据的概率分布 $p_{data}(\mathbf{x})$,其中 $\mathbf{x}$ 表示一个样本。 从分布中采样: 一旦我们学到了分布,我们需要能够从中采样来生成新样本。 1.1 传统生成方法 在 GAN 出现之前,研究者已经尝试了多种方法:
自编码器: 先将数据压缩到低维空间,然后试图从低维表示重建原始数据。但这种方法生成的样本往往模糊不清。
玻尔兹曼机: 基于能量函数的方法,通过马尔可夫链蒙特卡洛采样。但训练极其困难,采样效率低。
变分自编码器 (VAE): 通过变分推断近似后验分布。数学上优美,但生成的图像仍然不够真实。
这些方法都有一个共同点:它们试图显式地建模数据分布 $p_{data}(\mathbf{x})$。这就像试图精确描述"什么样的数字图像看起来像真实的",这本身就是一个极其困难的问题。
1.2 GAN 的突破思想 GAN 的革命性在于:不需要显式建模数据分布。
相反,GAN 将生成问题转化为一个对抗游戏:
生成器 (Generator, $G$): 从随机噪声 $\mathbf{z} \sim p_z(\mathbf{z})$ 出发,生成伪造样本 $\tilde{\mathbf{x}} = G(\mathbf{z})$。目标:让判别器无法区分真假。
判别器 (Discriminator, $D$): 接收一个样本 $\mathbf{x}$,判断它是来自真实数据($\mathbf{x} \sim p_{data}$)还是生成器($\tilde{\mathbf{x}} = G(\mathbf{z})$)。输出是概率 $D(\mathbf{x}) \in [0, 1]$。目标:准确区分真假。
这是一个零和博弈:生成器试图最小化判别器的准确率,而判别器试图最大化准确率。当两者达到平衡时,生成器就"学会"了生成真实样本。
flowchart LR subgraph 生成器_Generator Z[噪声 zz ~ p_z] G[生成器 G] Z --> G G --> Fake[伪造样本 x̃x̃ = Gz] end subgraph 判别器_Discriminator Real[真实样本 xx ~ p_data] FakeIn[伪造样本 x̃] D[判别器 D] Real --> D FakeIn --> D D --> Prob[概率 Dx ∈ 0,1] end Fake -.->|输入| FakeIn style Z fill:#FF6B6B,stroke:#FF6B6B,stroke-width:3px,color:#fff style G fill:#4ECDC4,stroke:#4ECDC4,stroke-width:2px,color:#fff style Fake fill:#FFE66D,stroke:#FFE66D,stroke-width:2px,color:#333 style Real fill:#95E1D3,stroke:#95E1D3,stroke-width:3px,color:#333 style D fill:#A8E6CF,stroke:#A8E6CF,stroke-width:2px,color:#333 style Prob fill:#DDA0DD,stroke:#DDA0DD,stroke-width:3px,color:#fff style FakeIn fill:#FFE66D,stroke:#FFE66D,stroke-width:2px,color:#333 图 1：GAN 的架构示意图。生成器将噪声映射为图像，判别器区分真实和伪造样本
...</p></div><footer class=entry-footer><span title='2026-01-24 11:45:00 +0800 CST'>January 24, 2026</span>&nbsp;·&nbsp;<span>7 min</span>&nbsp;·&nbsp;<span>1458 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 生成对抗网络：从混沌中创造秩序的博弈论" href=https://s-ai-unix.github.io/posts/2026-01-24-gan-comprehensive-guide/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/transformer-architecture.jpg alt="Transformer 架构的艺术化呈现"></figure><header class=entry-header><h2 class=entry-hint-parent>Transformer：重塑AI世界的架构革命</h2></header><div class=entry-content><p>引言 在人工智能的发展历程中，有几个时刻标志着技术范式的根本性转变。2017年10月就是这样一个时刻——Google Research 和多伦多大学的研究者们发表了一篇名为《Attention Is All You Need》的论文，提出了 Transformer 架构。
这篇论文的标题本身就是一种宣言：在这篇论文中，作者们向世界宣告，在处理序列数据时，注意力机制就是你所需要的一切。这篇论文不仅解决了长期困扰自然语言处理领域的难题，更开创了一个全新的 AI 时代。从 BERT 到 GPT 系列，从 PaLM 到 Claude，支撑现代大语言模型的核心架构都是 Transformer。
但 Transformer 到底是什么？它为什么如此重要？它是如何工作的？作为一个 AI 领域的深度从业者，我希望通过这篇文章，用最通俗易懂的方式，为你彻底解读这个重塑 AI 世界的重要架构。
第一章 背景：为什么我们需要 Transformer？ 1.1 序列数据处理的困境 在深入 Transformer 之前，让我们先理解它试图解决的问题。在自然语言处理、语音识别、机器翻译等任务中，我们面对的都是序列数据——句子是一系列词语的序列，语音是一系列声波的序列，DNA 是一系列碱基的序列。
对于序列数据的处理，传统的做法是使用循环神经网络（RNN）及其变体，如长短期记忆网络（LSTM）和门控循环单元（GRU）。这些网络的设计理念是：按顺序处理序列中的每个元素，将信息一步一步地传递下去。
RNN 的工作原理：想象你在读一本书。你的眼睛一次看一个字（或者一个词），然后大脑会记住这个字的意思，并结合之前记住的内容来理解整个句子。RNN 就是这样工作的——它按顺序处理输入序列，将之前的信息"记住"在隐藏状态中，然后用于处理下一个输入。
1.2 RNN 的致命缺陷 然而，RNN 存在几个根本性的问题：
第一个问题是长距离依赖问题。在处理长序列时，RNN 很难捕获序列前端和序列后端之间的关联。想象一个很长的句子：“那个在巴黎出生的，后来搬到纽约生活的，最后在北京去世的老人，年轻时是个著名的科学家。“要让 RNN 理解"老人"和"年轻时"之间的关系，信息需要从句子的一端传递到另一端。在这个过程中，信息会逐渐衰减，最终可能完全丢失。
第二个问题是计算效率问题。RNN 必须按顺序处理序列，这意味着第一步计算完成后才能开始第二步。这种串行计算的方式无法充分利用现代 GPU 的并行计算能力。在处理长序列时，计算变得非常耗时。
第三个问题是梯度消失和梯度爆炸问题。在反向传播过程中，梯度需要通过多个时间步传播。当序列很长时，梯度可能会变得非常小（消失）或非常大（爆炸），导致训练困难。
1.3 注意力机制的兴起 为了解决 RNN 的问题，研究者们提出了注意力机制（Attention Mechanism）。注意力机制的核心思想是：在处理序列中的每个元素时，我们不应该只依赖之前的信息，而应该能够"回顾"序列中的任意位置。
注意力的直观理解：想象你在嘈杂的咖啡馆里听朋友说话。即使周围很吵，你的大脑也能够聚焦于朋友的声音，而忽略背景噪音。注意力机制就是模拟这个过程——它让模型学会在处理每个词时，应该"关注"输入序列的哪些部分。
Bahdanau 等人在 2014 年提出了第一个注意力机制，用于机器翻译。这个注意力机制允许解码器在生成每个目标词时，关注源句子中的相关部分。这大大改善了机器翻译的性能。
但早期的注意力机制仍然是与 RNN 结合使用的。真正的革命性突破来自于 2017 年的那篇论文——作者们意识到，如果只使用注意力机制，我们就可以完全摆脱 RNN 的束缚。
...</p></div><footer class=entry-footer><span title='2026-01-21 10:00:00 +0800 CST'>January 21, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>985 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to Transformer：重塑AI世界的架构革命" href=https://s-ai-unix.github.io/posts/2026-01-21-transformer/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://s-ai-unix.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>