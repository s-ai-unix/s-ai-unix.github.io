<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>神经网络 on s-ai-unix's Blog</title><link>https://s-ai-unix.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><description>Recent content in 神经网络 on s-ai-unix's Blog</description><generator>Hugo -- 0.154.5</generator><language>zh-cn</language><lastBuildDate>Sat, 31 Jan 2026 09:30:00 +0800</lastBuildDate><atom:link href="https://s-ai-unix.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.xml" rel="self" type="application/rss+xml"/><item><title>AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作</title><link>https://s-ai-unix.github.io/posts/2026-01-31-ai-paper-llama3-herd-of-models/</link><pubDate>Sat, 31 Jan 2026 09:30:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-31-ai-paper-llama3-herd-of-models/</guid><description>深入解读 Meta AI 的 Llama 3 论文，从 Scaling Laws、模型架构到多模态扩展，全面剖析这个拥有 405B 参数的开源大模型集群的设计理念与技术细节。</description></item><item><title>AI 论文解读系列：AlphaGo - 深度学习与树搜索征服围棋</title><link>https://s-ai-unix.github.io/posts/2026-01-30-alphago-paper-interpretation/</link><pubDate>Fri, 30 Jan 2026 12:30:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-30-alphago-paper-interpretation/</guid><description>深入解读 DeepMind 发表于 Nature 的里程碑论文，剖析 AlphaGo 如何结合深度神经网络与蒙特卡洛树搜索，首次在围棋领域击败人类职业棋手</description></item><item><title>AI 论文解读系列：Inception-v4 - Going Deeper with Convolutions</title><link>https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97-inception-v4-going-deeper-with-convolutions/</link><pubDate>Fri, 30 Jan 2026 12:30:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97-inception-v4-going-deeper-with-convolutions/</guid><description>深入解读 Google 的 Inception-v4 论文，从 Inception 系列的演进历程出发，剖析 Inception-v4 的架构设计思想、多尺度特征提取原理，以及 Inception-ResNet 如何将残差连接与 Inception 模块融合，创造当时最强图像分类网络。</description></item><item><title>AI 论文解读系列：Seq2Seq--从序列到序列的革命</title><link>https://s-ai-unix.github.io/posts/2026-01-30-seq2seq-paper-explained/</link><pubDate>Fri, 30 Jan 2026 09:00:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-30-seq2seq-paper-explained/</guid><description>深入浅出解读 Seq2Seq 论文，从机器翻译的困境到编码器-解码器架构的突破，揭示深度学习处理序列数据的核心思想。</description></item><item><title>AI 论文解读系列：GPT-3——当语言模型学会举一反三</title><link>https://s-ai-unix.github.io/posts/2026-01-30-gpt3-few-shot-learners-paper/</link><pubDate>Fri, 30 Jan 2026 08:50:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-30-gpt3-few-shot-learners-paper/</guid><description>深入解读 OpenAI 里程碑式论文 GPT-3: Language Models are Few-Shot Learners，从 Transformer 架构到少样本学习的范式转变，探讨大规模语言模型的涌现能力与未来前景。</description></item><item><title>AI 论文解读系列：ResNet 深度残差学习</title><link>https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97%E4%B9%8Bresnet-%E6%B7%B1%E5%BA%A6%E6%AE%8B%E5%B7%AE%E5%AD%A6%E4%B9%A0/</link><pubDate>Fri, 30 Jan 2026 08:38:11 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97%E4%B9%8Bresnet-%E6%B7%B1%E5%BA%A6%E6%AE%8B%E5%B7%AE%E5%AD%A6%E4%B9%A0/</guid><description>深入解读何恺明等人的 ResNet 论文，从深度网络的退化问题出发，剖析残差学习的核心思想、数学原理和架构设计，揭示为何简单的跳跃连接能够训练出超深层神经网络。</description></item><item><title>AlexNet：开启深度学习革命的里程碑</title><link>https://s-ai-unix.github.io/posts/2026-01-29-alexnet-deep-learning-revolution/</link><pubDate>Thu, 29 Jan 2026 06:00:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-29-alexnet-deep-learning-revolution/</guid><description>深入浅出解析 AlexNet 的架构原理、关键技术创新和历史意义，从 ImageNet 挑战到深度学习革命，完整推导其数学原理</description></item><item><title>变分自编码器：从概率建模到深度生成的优雅桥梁</title><link>https://s-ai-unix.github.io/posts/2026-01-24-variational-autoencoder/</link><pubDate>Sat, 24 Jan 2026 18:30:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-24-variational-autoencoder/</guid><description>深入解析变分自编码器（VAE）的数学原理与推导，从变分推断到 ELBO 优化，从重参数化到生成应用，完整呈现 VAE 的理论框架与实践价值</description></item><item><title>生成对抗网络：从混沌中创造秩序的博弈论</title><link>https://s-ai-unix.github.io/posts/2026-01-24-gan-comprehensive-guide/</link><pubDate>Sat, 24 Jan 2026 11:45:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-24-gan-comprehensive-guide/</guid><description>深入探讨生成对抗网络（GAN）的数学原理、训练挑战与应用前景</description></item><item><title>Transformer：重塑AI世界的架构革命</title><link>https://s-ai-unix.github.io/posts/2026-01-21-transformer/</link><pubDate>Wed, 21 Jan 2026 10:00:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-21-transformer/</guid><description>深入解读 Transformer 架构的核心原理，从自注意力机制到多头注意力，探索这个重塑 AI 世界的重要架构</description></item><item><title>感知机的完整发展历程：从线性分类到深度学习的基石</title><link>https://s-ai-unix.github.io/posts/2026-01-21-perceptron-development-history/</link><pubDate>Wed, 21 Jan 2026 08:00:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-21-perceptron-development-history/</guid><description>系统综述感知机的发展历程，从早期的线性分类器到现代深度学习的基础，注重背景和演变过程的介绍，通俗易懂。</description></item><item><title>神经网络算法演进：从感知机到 Transformer 的七十年征程</title><link>https://s-ai-unix.github.io/posts/2026-01-15-neural-network-evolution/</link><pubDate>Thu, 15 Jan 2026 23:55:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-15-neural-network-evolution/</guid><description>回顾神经网络七十年发展历程，从感知机到 Transformer，详解核心算法的数学原理</description></item><item><title>大语言模型：为什么AI能这么快、这么聪明地回答问题</title><link>https://s-ai-unix.github.io/posts/2026-01-14-llm-principle-for-students/</link><pubDate>Wed, 14 Jan 2026 08:50:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-14-llm-principle-for-students/</guid><description>从预测下一个词的简单想法出发，解释大语言模型的工作原理，面向初中生和高中生的通俗易懂指南。</description></item><item><title>梯度、梯度下降与反向传播：从最优化到深度学习的数学引擎</title><link>https://s-ai-unix.github.io/posts/2026-01-14-gradient-descent-backpropagation-overview/</link><pubDate>Wed, 14 Jan 2026 08:34:44 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-14-gradient-descent-backpropagation-overview/</guid><description>系统介绍梯度、梯度下降、反向传播算法，以及梯度的其他应用，完整推导历史背景与应用场景，并详细对比梯度、散度、旋度三个核心概念。</description></item><item><title>基于神经网络的深度学习算法：从感知机到Transformer的完整指南</title><link>https://s-ai-unix.github.io/posts/2026-01-14-deep-learning-algorithms-comprehensive-guide/</link><pubDate>Wed, 14 Jan 2026 08:30:00 +0800</pubDate><guid>https://s-ai-unix.github.io/posts/2026-01-14-deep-learning-algorithms-comprehensive-guide/</guid><description>本文全面回顾深度学习算法的发展历程、数学原理、架构演进及未来前景，涵盖从基础神经网络到Transformer的完整演进路径。</description></item></channel></rss>