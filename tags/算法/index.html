<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>算法 | s-ai-unix's Blog</title><meta name=keywords content><meta name=description content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/tags/%E7%AE%97%E6%B3%95/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://s-ai-unix.github.io/tags/%E7%AE%97%E6%B3%95/index.xml title=rss><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/tags/%E7%AE%97%E6%B3%95/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/tags/%E7%AE%97%E6%B3%95/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="算法"><meta property="og:description" content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="算法"><meta name=twitter:description content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body class=list id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/tags/>Tags</a></div><h1>算法</h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/seq2seq-cover.jpg alt="Seq2Seq 神经网络抽象图"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：Seq2Seq--从序列到序列的革命</h2></header><div class=entry-content><p>引言：翻译的困境 想象一下，你正在学习一门外语。当你听到一句法语 “Bonjour le monde” 时，你的大脑是如何将其转化为英语 “Hello world” 的？
这不是简单的逐词替换。“Bonjour” 对应 “Hello”，但 “le monde” 是 “the world” 的倒序。词序不同，语法结构不同，甚至可能一个词对应多个词。传统的机器翻译系统使用基于规则的方法或统计模型，需要大量的人工特征工程和复杂的对齐算法。
2014年，Ilya Sutskever、Oriol Vinyals 和 Quoc Le 在 Google 发表了一篇改变游戏规则的论文：“Sequence to Sequence Learning with Neural Networks”。他们提出的 Seq2Seq 架构，用一个统一的神经网络模型取代了复杂的流水线，让机器翻译的准确率跃升到了新的高度。
但这篇论文的意义远不止于翻译。它开创了序列转导（Sequence Transduction）这一全新的学习范式，为后来的注意力机制、Transformer 乃至大语言模型奠定了基础。
第一章：序列转导问题 1.1 什么让序列数据特殊 在深入 Seq2Seq 之前，让我们先理解序列数据的本质。
传统的机器学习任务，比如图像分类或房价预测，输入和输出的维度是固定的。一张图片永远是 $224 \times 224 \times 3$ 的像素矩阵，一套房子的特征永远是卧室数、面积、位置等固定字段。
但序列数据不同：
一句话可能有 5 个词，也可能有 50 个词 源语言和目标语言的词序可能不同 一个概念可能用一个词表达，也可能用多个词 上图展示了一个典型的机器翻译场景。输入序列 “Hello world this is a test” 需要被转换为 “Bonjour monde ceci est un test”。注意两个关键挑战：
...</p></div><footer class=entry-footer><span title='2026-01-30 09:00:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>763 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：Seq2Seq--从序列到序列的革命" href=https://s-ai-unix.github.io/posts/2026-01-30-seq2seq-paper-explained/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/differential-geometry-autonomous-driving.jpg alt="微分几何与自动驾驶 cover image"></figure><header class=entry-header><h2 class=entry-hint-parent>弯曲的道路，智能的决策：微分几何如何赋能自动驾驶</h2></header><div class=entry-content><p>引言：当数学遇见自动驾驶 想象你正在驾驶一辆汽车行驶在蜿蜒的山路上。前方是一个急转弯，你需要减速、打方向、保持车道——这一系列看似简单的动作，实际上涉及复杂的几何判断：道路的曲率如何？转弯半径是多少？轮胎与地面的摩擦力能否提供足够的向心力？
现在，把驾驶员换成自动驾驶系统。它没有了人类的直觉和经验，必须依靠数学模型来理解这个世界。微分几何——这门研究曲线、曲面和弯曲空间的数学分支，正是自动驾驶系统的"眼睛"和"大脑"背后的理论基础。
从古希腊欧几里得研究直线和平面，到高斯发现曲面可以"内蕴地"研究，再到黎曼建立起 $n$ 维弯曲空间的一般理论，微分几何经历了两千多年的发展。而今天，这门古老的数学正以全新的方式赋能现代科技：它帮助自动驾驶汽车理解道路的几何结构，规划平滑的行驶轨迹，感知周围环境的三维形态。
本文将带你走进微分几何与自动驾驶的交汇点，看看抽象的数学概念如何在现实世界中大放异彩。
第一章：微分几何的核心概念回顾 1.1 曲线：道路的一维模型 一条道路可以抽象为三维空间中的一条参数曲线：
$$ \mathbf{r}(t) = (x(t), y(t), z(t)) $$
其中 $t$ 是参数，可以是时间，也可以是弧长。对于自动驾驶而言，我们最关心的是曲线的两个几何量：切向量和曲率。
切向量告诉我们道路在每一点的"方向"：
$$ \mathbf{T}(t) = \frac{d\mathbf{r}/dt}{\lVert d\mathbf{r}/dt \rVert} $$
汽车的前进方向应该与切向量对齐，这是最基本的控制要求。
曲率则告诉我们道路弯曲的程度。对于以弧长 $s$ 参数化的曲线，曲率定义为：
$$ \kappa(s) = \left\lVert \frac{d\mathbf{T}}{ds} \right\rVert = \left\lVert \frac{d^2\mathbf{r}}{ds^2} \right\rVert $$
曲率的倒数 $\rho = 1/\kappa$ 称为曲率半径。当汽车以速度 $v$ 通过曲率为 $\kappa$ 的路段时，所需的向心加速度为 $a = v^2 \kappa$。这就是为什么急转弯需要减速——曲率越大，所需的向心力越大。
1.2 曲面：路面的二维模型 实际的道路不是一个简单的曲线，而是一个曲面。我们可以用参数方程描述：
$$ \mathbf{r}(u, v) = (x(u, v), y(u, v), z(u, v)) $$
...</p></div><footer class=entry-footer><span title='2026-01-28 23:42:32 +0800 CST'>January 28, 2026</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>831 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 弯曲的道路，智能的决策：微分几何如何赋能自动驾驶" href=https://s-ai-unix.github.io/posts/2026-01-28-differential-geometry-autonomous-driving/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/jacobian-hessian-cover.jpg alt="Jacobian and Hessian Matrices cover image"></figure><header class=entry-header><h2 class=entry-hint-parent>雅可比矩阵与黑塞矩阵：多变量微积分的双璧</h2></header><div class=entry-content><p>引言 当我们从单变量微积分迈向多变量微积分时，一个核心问题浮现出来：如何描述多元函数的变化？在单变量情形中，导数 $f’(x)$ 告诉我们函数在某点的瞬时变化率。但当函数 $f: \mathbb{R}^n \to \mathbb{R}^m$ 拥有多个输入和输出时，情况变得复杂起来。
想象一下，你正在攀登一座山峰。在任何一个位置，你都想知道：
哪个方向最陡峭？（梯度的方向） 这个陡峭程度在各个方向如何变化？（曲率的描述） 雅可比矩阵和黑塞矩阵正是回答这些问题的数学工具。它们是多变量微积分中的"双璧"——一个描述一阶变化（线性近似），一个描述二阶变化（曲率特性）。从牛顿法到神经网络训练，从机器人运动学到广义相对论，这对"双璧"无处不在。
第一章：从一维到多维 1.1 单变量函数的局限性 回顾单变量微积分，函数 $f: \mathbb{R} \to \mathbb{R}$ 在点 $x$ 处的导数定义为：
$$ f’(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h} $$
这个定义告诉我们函数在 $x$ 处的瞬时变化率。几何上，它表示函数曲线在该点切线的斜率。
但当函数有多个输入时，例如 $f(x, y) = x^2 + y^2$，我们可以问：
沿 $x$ 方向的变化率是多少？ 沿 $y$ 方向的变化率是多少？ 沿任意方向的变化率是多少？ 这就引出了偏导数的概念。
1.2 偏导数与方向导数 函数 $f(x_1, x_2, \ldots, x_n)$ 关于 $x_i$ 的偏导数定义为：
$$ \frac{\partial f}{\partial x_i} = \lim_{h \to 0} \frac{f(x_1, \ldots, x_i+h, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h} $$
...</p></div><footer class=entry-footer><span title='2026-01-28 21:54:27 +0800 CST'>January 28, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>879 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 雅可比矩阵与黑塞矩阵：多变量微积分的双璧" href=https://s-ai-unix.github.io/posts/2026-01-28-jacobian-hessian-matrices/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/calculus-ml-journey.jpg alt=微积分的几何美感></figure><header class=entry-header><h2 class=entry-hint-parent>微积分与机器学习：从变化率到神经网络梯度的完整旅程</h2></header><div class=entry-content><p>引言：为什么需要微积分？ 想象你在山上，想找到最低点。你会怎么做？你会观察脚下的坡度，选择最陡峭的方向迈出一步，然后重复这个过程。这个简单的直觉——沿着负梯度方向走——正是现代人工智能的核心算法。
从ChatGPT的语言模型到AlphaGo的围棋策略，从图像识别到语音合成，所有这些技术背后都有一个共同的数学基础：微积分。
微积分研究的是变化。而机器学习本质上是关于优化——通过不断调整参数来减少错误。当我们在高维空间中优化复杂的神经网络时，微积分提供了描述和计算这种变化的精确语言。
这篇文章将带你深入理解微积分如何驱动现代人工智能。我们不会停留在表面，而是会深入到数学推导的核心，揭示梯度下降、反向传播等算法的数学本质。这是一次从17世纪牛顿和莱布尼茨的发明，到21世纪深度学习革命的完整旅程。
第一部分：微积分基础理论 1. 导数的本质：从变化率到瞬时变化率 1.1 变化率的直观理解 变化率是人类最早思考的数学问题之一。如果一辆车2小时行驶100公里，平均速度是50公里/小时。但它某一时刻的瞬时速度是多少？
微积分的答案是：用极限。考虑函数 $f(x)$ 在 $x_0$ 附近的平均变化率： $$ \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x} $$
当 $\Delta x \to 0$ 时，这个平均变化率的极限就是导数： $$ f^{\prime}(x_0) = \lim_{\Delta x \to 0} \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x} $$
1.2 导数的几何意义 几何直观：导数是切线的斜率。在 $x_0$ 处，曲线 $f(x)$ 可以用直线（切线）逼近： $$ f(x) \approx f(x_0) + f^{\prime}(x_0)(x - x_0) $$
这就是一阶泰勒公式，也是线性化的思想：局部用简单的线性函数逼近复杂的非线性函数。
严格定义（$\epsilon-\delta$ 语言）： $$ \forall \epsilon > 0, \exists \delta > 0 \text{ s.t. } |\Delta x| &lt; \delta \implies \left|\frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x} - f^{\prime}(x_0)\right| &lt; \epsilon $$
...</p></div><footer class=entry-footer><span title='2026-01-25 19:00:00 +0800 CST'>January 25, 2026</span>&nbsp;·&nbsp;<span>9 min</span>&nbsp;·&nbsp;<span>1716 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 微积分与机器学习：从变化率到神经网络梯度的完整旅程" href=https://s-ai-unix.github.io/posts/2026-01-25-calculus-ml-systematic-review/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/spectral-theorem.jpg alt=谱定理可视化></figure><header class=entry-header><h2 class=entry-hint-parent>谱定理：线性代数的优雅与机器学习的基石</h2></header><div class=entry-content><p>引言：对称性的数学之美 在数学的众多分支中，有一个深刻的原理反复出现：对称性带来简化。在物理学中，空间的对称性意味着守恒量；在群论中，对称结构导致简单的表示；在线性代数中，对称矩阵拥有最优雅的对角化理论——这就是谱定理。
想象你站在一个椭圆中心。如果你沿任意方向看出去，椭圆的"宽度"各不相同。但有两个特殊的方向——椭圆的长轴和短轴——沿这些方向，椭圆的形状最简单，只是一个被拉伸的圆。这两个正交的方向，就是椭圆的"主轴"，它们对应的拉伸倍数，就是"特征值"。
这个直观的几何图像，正是谱定理的核心。谱定理告诉我们：任何实对称矩阵都可以通过正交变换对角化。换句话说，在适当的坐标系下，对称矩阵描述的线性变换只是沿某些正交方向的简单拉伸。
在机器学习和深度学习中，谱定理无处不在。从主成分分析（PCA）到奇异值分解（SVD），从谱聚类到图神经网络，谱定理提供了理解数据和算法的理论基础。
在这篇文章中，我们将系统性地介绍谱定理的核心理论，从实对称矩阵的正交对角化到一般的奇异值分解，从PCA到谱聚类，深入浅出地推导每一个公式，并通过可视化图形直观理解这些概念。
第一章：谱定理的基础理论 1.1 特征值与特征向量：不变的方向 给定一个 $n \times n$ 矩阵 $A$，如果存在非零向量 $v \in \mathbb{R}^n$ 和标量 $\lambda \in \mathbb{R}$，使得
$$ Av = \lambda v $$
则称 $\lambda$ 是 $A$ 的特征值，$v$ 是对应的特征向量。
几何意义：特征向量 $v$ 是线性变换 $A$ 下的"不变方向"——变换后，这个向量只是被拉伸或压缩了 $\lambda$ 倍，方向保持不变。
特征多项式：特征值是特征方程的根
$$ \det(A - \lambda I) = 0 $$
对于 $n \times n$ 矩阵，这是一个 $n$ 次多项式，在复数域上有 $n$ 个根（计入重数）。
1.2 对称矩阵的特殊性质 实对称矩阵 $A \in \mathbb{R}^{n \times n}$（即 $A^\top = A$）拥有三个重要性质：
...</p></div><footer class=entry-footer><span title='2026-01-25 18:00:00 +0800 CST'>January 25, 2026</span>&nbsp;·&nbsp;<span>7 min</span>&nbsp;·&nbsp;<span>1458 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 谱定理：线性代数的优雅与机器学习的基石" href=https://s-ai-unix.github.io/posts/2026-01-25-spectral-theorem/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/bayesian-classifier.jpg alt=贝叶斯网络结构示意图></figure><header class=entry-header><h2 class=entry-hint-parent>贝叶斯分类器：从条件概率到智能决策的优雅之旅</h2></header><div class=entry-content><p>引言：不确定世界中的决策智慧 想象你在一家医院工作，面对一位病人。医生告诉你，这位病人有两种可能的疾病：疾病 A 和疾病 B。通过检查，你发现病人出现了某种症状 S。现在的关键问题是：这种症状的出现，是更倾向于指向疾病 A，还是疾病 B？
这就是分类问题的本质——根据观察到的特征，将样本划分到不同的类别中。而在众多分类算法中，贝叶斯分类器以其优美的数学形式和深刻的思想基础，始终占据着不可替代的位置。
它不依赖于复杂的神经网络或深度学习结构，仅仅基于概率论的基本原理，就能在许多实际应用中展现出令人惊讶的效果。更重要的是，它给了我们一种"在不确定情况下进行理性决策"的思维方式。
第一章：概率论的基石 在进入贝叶斯分类器的核心之前，让我们先回顾一些基础的概率概念。这些概念看似简单，却构成了整个贝叶斯理论的数学大厦。
1.1 条件概率 条件概率是贝叶斯理论的起点。它的直观含义是：在事件 B 发生的条件下，事件 A 发生的概率是多少？数学记为：
$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$
其中 $P(A \cap B)$ 表示 A 和 B 同时发生的概率，$P(B)$ 是事件 B 发生的概率。这个公式的直观理解是：如果我们把所有可能的情况看作一个空间，条件概率就是在"给定 B 发生"这个子空间内，A 所占的比重。
1.2 全概率公式 当我们面对一个复杂事件时，常常需要将其分解为若干互不相容的简单事件。这就是全概率公式的思想：
$$P(A) = \sum_{i=1}^{n} P(A|B_i) P(B_i)$$
其中 $B_1, B_2, \ldots, B_n$ 构成一个完备事件组（即它们互不相容且并集为整个样本空间）。全概率公式的几何直观是：将事件 A 的"面积"按照不同条件 $B_i$ 进行"切片"，然后将这些切片的面积加起来。
1.3 贝叶斯公式的诞生 将条件概率公式"反过来"使用，就得到了著名的贝叶斯公式：
$$P(B|A) = \frac{P(A|B) P(B)}{P(A)}$$
这个公式看似简单，却蕴含着深刻的哲学意义。它告诉我们：如果我们知道"在 B 发生的条件下 A 的概率"（$P(A|B)$），以及"先验概率" $P(B)$，就可以推导出"观察到 A 后，B 的概率"（$P(B|A)$）。
...</p></div><footer class=entry-footer><span title='2026-01-24 17:58:30 +0800 CST'>January 24, 2026</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>736 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 贝叶斯分类器：从条件概率到智能决策的优雅之旅" href=https://s-ai-unix.github.io/posts/2026-01-24-bayesian-classifier/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/monte-carlo-method.jpg alt=蒙特卡罗方法的随机性可视化></figure><header class=entry-header><h2 class=entry-hint-parent>蒙特卡罗算法：从原子弹到人工智能的随机之旅</h2></header><div class=entry-content><p>引言：掷骰子解方程 想象一下，有人告诉你：要计算一个复杂的定积分，不需要微积分，只需要掷足够多的骰子。你大概会觉得这个人疯了。然而，这正是二十世纪最伟大的计算方法之一——蒙特卡罗方法（Monte Carlo Method）的核心思想。
当我们面对那些传统方法难以处理的高维积分、复杂系统的模拟或者无法解析求解的概率问题时，蒙特卡罗方法给出了一个看似简单却深刻的答案：用随机性来求解确定性问题。这种方法已经深入到科学的方方面面——从核物理到金融工程，从生物进化到人工智能，无处不见它的身影。
让我们从一个最经典的例子开始：如何用"扔针"来计算 $\pi$ 的值。
第一章：蒙特卡罗的诞生——曼哈顿计划的秘密代号 1.1 摩纳哥的赌场与原子弹的秘密 “蒙特卡罗"这个名字，源自摩纳哥著名的赌城。1940 年代，在洛斯阿拉莫斯实验室，一群顶尖的科学家正在紧锣密鼓地研制世界上第一颗原子弹。在这个属于"曼哈顿计划"的绝密基地里，数学家约翰·冯·诺伊曼（John von Neumann）和斯坦尼斯拉夫·乌拉姆（Stanislaw Ulam）正在研究核裂变中的中子扩散问题。
这个问题极其复杂：中子在原子弹内部的行为是随机的，它们可能被原子核捕获，可能引发新的裂变，也可能逃逸出去。传统的方法根本无法处理这种复杂的随机过程。
乌拉姆后来回忆起他是如何产生这个想法的：
“当时我正因病康复，在玩纸牌接龙。我开始思考：如果把牌随机排列一百次，大概有多少次能成功接龙？相比于把所有可能的情况都计算出来，直接实验似乎更容易…”
这个看似简单的想法，孕育了一个全新的计算方法。由于这种方法涉及随机性，而蒙特卡罗又以赌场闻名，冯·诺伊曼就给它起了"蒙特卡罗"这个代号——既是保密的需要，也恰如其分地描述了方法的本质。
1.2 早期的思想萌芽 虽然蒙特卡罗方法在1940年代才正式命名，但用随机性来解决确定性问题的思想古已有之。
1777年，布丰投针实验
法国数学家乔治-路易·勒克莱尔，布丰伯爵（Georges-Louis Leclerc, Comte de Buffon）提出了第一个著名的随机实验：
在一张画满平行线的纸（线间距为 $d$）上随机投掷一根长度为 $l$ 的针（$l &lt; d$），针与任意一条平行线相交的概率是多少？
布丰证明了，这个概率是：
$$ P = \frac{2l}{\pi d} $$
这给出了一个计算 $\pi$ 的方法：如果我们投掷针 $N$ 次，其中 $n$ 次与线相交，那么：
$$ \frac{n}{N} \approx \frac{2l}{\pi d} \implies \pi \approx \frac{2lN}{nd} $$
这个实验被多次验证：1850年，沃尔夫在苏黎世投掷了5000次，得到 $\pi \approx 3.1596$；1901年，拉泽里尼投掷3408次，甚至得到了精确到小数点后6位的 $\pi$ 值（虽然有人怀疑他可能"选择性记录"了结果）。
19世纪末的统计学革命
随着统计学的发展，卡尔·皮尔逊（Karl Pearson）等人开始使用随机抽样来解决统计问题。但这些方法仍然主要用于验证已知的结果，而不是作为通用的计算工具。
第二章：数学基础——为什么随机性有效？ 要理解蒙特卡罗方法，我们需要先理解它的数学基础。这一切都建立在大数定律和中心极限定理这两大概率论支柱之上。
...</p></div><footer class=entry-footer><span title='2026-01-21 23:00:00 +0800 CST'>January 21, 2026</span>&nbsp;·&nbsp;<span>6 min</span>&nbsp;·&nbsp;<span>1171 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 蒙特卡罗算法：从原子弹到人工智能的随机之旅" href=https://s-ai-unix.github.io/posts/2026-01-21-monte-carlo-method/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/transformer-architecture.jpg alt="Transformer 架构的艺术化呈现"></figure><header class=entry-header><h2 class=entry-hint-parent>Transformer：重塑AI世界的架构革命</h2></header><div class=entry-content><p>引言 在人工智能的发展历程中，有几个时刻标志着技术范式的根本性转变。2017年10月就是这样一个时刻——Google Research 和多伦多大学的研究者们发表了一篇名为《Attention Is All You Need》的论文，提出了 Transformer 架构。
这篇论文的标题本身就是一种宣言：在这篇论文中，作者们向世界宣告，在处理序列数据时，注意力机制就是你所需要的一切。这篇论文不仅解决了长期困扰自然语言处理领域的难题，更开创了一个全新的 AI 时代。从 BERT 到 GPT 系列，从 PaLM 到 Claude，支撑现代大语言模型的核心架构都是 Transformer。
但 Transformer 到底是什么？它为什么如此重要？它是如何工作的？作为一个 AI 领域的深度从业者，我希望通过这篇文章，用最通俗易懂的方式，为你彻底解读这个重塑 AI 世界的重要架构。
第一章 背景：为什么我们需要 Transformer？ 1.1 序列数据处理的困境 在深入 Transformer 之前，让我们先理解它试图解决的问题。在自然语言处理、语音识别、机器翻译等任务中，我们面对的都是序列数据——句子是一系列词语的序列，语音是一系列声波的序列，DNA 是一系列碱基的序列。
对于序列数据的处理，传统的做法是使用循环神经网络（RNN）及其变体，如长短期记忆网络（LSTM）和门控循环单元（GRU）。这些网络的设计理念是：按顺序处理序列中的每个元素，将信息一步一步地传递下去。
RNN 的工作原理：想象你在读一本书。你的眼睛一次看一个字（或者一个词），然后大脑会记住这个字的意思，并结合之前记住的内容来理解整个句子。RNN 就是这样工作的——它按顺序处理输入序列，将之前的信息"记住"在隐藏状态中，然后用于处理下一个输入。
1.2 RNN 的致命缺陷 然而，RNN 存在几个根本性的问题：
第一个问题是长距离依赖问题。在处理长序列时，RNN 很难捕获序列前端和序列后端之间的关联。想象一个很长的句子：“那个在巴黎出生的，后来搬到纽约生活的，最后在北京去世的老人，年轻时是个著名的科学家。“要让 RNN 理解"老人"和"年轻时"之间的关系，信息需要从句子的一端传递到另一端。在这个过程中，信息会逐渐衰减，最终可能完全丢失。
第二个问题是计算效率问题。RNN 必须按顺序处理序列，这意味着第一步计算完成后才能开始第二步。这种串行计算的方式无法充分利用现代 GPU 的并行计算能力。在处理长序列时，计算变得非常耗时。
第三个问题是梯度消失和梯度爆炸问题。在反向传播过程中，梯度需要通过多个时间步传播。当序列很长时，梯度可能会变得非常小（消失）或非常大（爆炸），导致训练困难。
1.3 注意力机制的兴起 为了解决 RNN 的问题，研究者们提出了注意力机制（Attention Mechanism）。注意力机制的核心思想是：在处理序列中的每个元素时，我们不应该只依赖之前的信息，而应该能够"回顾"序列中的任意位置。
注意力的直观理解：想象你在嘈杂的咖啡馆里听朋友说话。即使周围很吵，你的大脑也能够聚焦于朋友的声音，而忽略背景噪音。注意力机制就是模拟这个过程——它让模型学会在处理每个词时，应该"关注"输入序列的哪些部分。
Bahdanau 等人在 2014 年提出了第一个注意力机制，用于机器翻译。这个注意力机制允许解码器在生成每个目标词时，关注源句子中的相关部分。这大大改善了机器翻译的性能。
但早期的注意力机制仍然是与 RNN 结合使用的。真正的革命性突破来自于 2017 年的那篇论文——作者们意识到，如果只使用注意力机制，我们就可以完全摆脱 RNN 的束缚。
...</p></div><footer class=entry-footer><span title='2026-01-21 10:00:00 +0800 CST'>January 21, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>985 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to Transformer：重塑AI世界的架构革命" href=https://s-ai-unix.github.io/posts/2026-01-21-transformer/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/shannon-entropy.jpg alt=信息熵与通信理论></figure><header class=entry-header><h2 class=entry-hint-parent>香农信息熵：不确定性的数学刻度</h2></header><div class=entry-content><p>引言：一条电报引发的思考 信息是什么？ 1844年5月24日，萨缪尔·摩斯（Samuel Morse）从华盛顿向巴尔的摩发出了人类历史上第一条电报：
“What hath God wrought!”
这四个单词穿越了64公里的铜线，开启了电信时代。但在庆祝之余，一个问题逐渐浮现：这条消息究竟包含了多少"信息"？
这个问题看似简单，实则深奥。“信息"是一个抽象的概念，如何用数学来量化它？一封情书和一份天气预报，哪一份包含更多"信息”？一条加密后的消息和原始消息，信息量是否相同？
这些问题的答案，隐藏在一位贝尔实验室工程师的伟大发现中。
香农的登场 1948年，克劳德·香农（Claude Shannon）发表了题为《通信的数学理论》的论文。这篇32页的论文，被誉为"数字时代的创世大宪章"。
在论文中，香农给出了"信息"的精确定义，并引入了一个核心概念——信息熵。这个名字借用了热力学中的"熵"，暗示了两者之间深刻的联系。
本文将带你踏上一段历史与数学交织的旅程，从电报时代的实际问题出发，逐步揭示信息熵的诞生、内涵及其深远影响。
第一章：信息时代的黎明——通信效率的困惑 1.1 摩斯电码中的智慧 在香农之前，通信工程师们已经面临着一个实际问题：如何用最少的符号传输最多的信息？
摩斯电码给出了一个直观的答案。观察摩斯电码的设计：
E: . (最常用) T: - (第二常用) A: .- Q: --.- (很少使用) Z: --.. 摩斯天才地意识到：常用的字母应该用较短的编码，不常用的字母可以用较长的编码。这个设计原则在今天看来理所当然，但在当时是革命性的。
但这引发了更深层的思考：如何精确衡量一个字母的"常用程度"？如何计算整个编码系统的效率？这些问题需要数学语言的精确描述。
1.2 电报的经济学问题 19世纪的电报按字收费，一条消息的成本与其长度直接相关。因此，压缩信息不仅是技术问题，更是经济问题。
工程师们开始思考：
如果我们能知道每个字母出现的概率，能否设计出最优的编码？ 通信线路的"容量"有没有理论极限？ 噪声（干扰）对信息传输的影响有多大？ 这些问题的答案，要等到20世纪才逐渐浮现。
flowchart LR subgraph A["19世纪通信挑战"] A1["摩斯电码1837"] A2["电报经济学按长度收费"] end subgraph B["20世纪理论突破"] B1["奈奎斯特1924"] B2["哈特利1928"] B3["香农1948"] end subgraph C["现代信息时代"] C1["数字通信"] C2["数据压缩"] C3["机器学习"] end A1 --> B1 A2 --> B2 B1 --> B3 B2 --> B3 B3 --> C1 B3 --> C2 B3 --> C3 style A1 fill:#34C759,color:#ffffff,stroke-width:2px style A2 fill:#34C759,color:#ffffff,stroke-width:2px style B1 fill:#007AFF,color:#ffffff,stroke-width:2px style B2 fill:#007AFF,color:#ffffff,stroke-width:2px style B3 fill:#007AFF,color:#ffffff,stroke-width:3px style C1 fill:#34C759,color:#ffffff,stroke-width:2px style C2 fill:#34C759,color:#ffffff,stroke-width:2px style C3 fill:#34C759,color:#ffffff,stroke-width:2px 第二章：先驱的脚步——奈奎斯特与哈特利 2.1 奈奎斯特的发现 1924年，贝尔实验室的哈里·奈奎斯特（Harry Nyquist）在研究电报传输时，做出了一个重要发现。
...</p></div><footer class=entry-footer><span title='2026-01-21 10:00:00 +0800 CST'>January 21, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>891 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 香农信息熵：不确定性的数学刻度" href=https://s-ai-unix.github.io/posts/2026-01-21-shannon-entropy-comprehensive-guide/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/perceptron-history.jpg alt=感知机发展历程></figure><header class=entry-header><h2 class=entry-hint-parent>感知机的完整发展历程：从线性分类到深度学习的基石</h2></header><div class=entry-content><p>引言：人工智能的原点 在人工智能的发展历程中，感知机（Perceptron）是一个具有里程碑意义的概念。它不仅是最早的机器学习算法之一，也是现代深度学习和神经网络的基础。
感知机的故事开始于 20 世纪中叶，当时计算机科学刚刚萌芽，科学家们开始探索如何让机器具备"学习"的能力。
第一章：感知机的诞生背景 1.1 早期人工智能研究的梦想 20 世纪 40 年代末到 50 年代初，随着计算机的诞生，科学家们开始思考：机器能否像人一样思考和学习？
图灵测试：1950 年，艾伦·图灵提出了著名的图灵测试，为人工智能的发展奠定了理论基础。 神经网络的早期构想：1943 年，麦卡洛克和皮茨提出了第一个人工神经网络模型，称为麦卡洛克-皮茨神经元。 1.2 罗森布拉特的突破 1957 年，美国心理学家弗兰克·罗森布拉特（Frank Rosenblatt）在康奈尔航空实验室提出了感知机模型。他将感知机描述为"能够通过经验自动学习的机器"。
罗森布拉特的工作受到了神经科学的启发，他试图模拟人类大脑中神经元的工作方式。
第二章：感知机的核心原理 2.1 感知机的基本结构 感知机是一个简单的线性分类器，它的结构非常简单：
graph TD A[输入] --> B[权重] C[偏置] --> D[求和] B --> D D --> E[激活函数] E --> F[输出] style A color:#ffffff,fill:#007AFF,stroke:#007AFF,stroke-width:3px style B color:#ffffff,fill:#34C759,stroke:#34C759,stroke-width:2px style C color:#ffffff,fill:#34C759,stroke:#34C759,stroke-width:2px style D color:#ffffff,fill:#007AFF,stroke:#007AFF,stroke-width:3px style E color:#ffffff,fill:#007AFF,stroke:#007AFF,stroke-width:3px style F color:#ffffff,fill:#007AFF,stroke:#007AFF,stroke-width:3px 2.2 感知机的工作原理 感知机的工作原理可以用以下公式表示：
...</p></div><footer class=entry-footer><span title='2026-01-21 08:00:00 +0800 CST'>January 21, 2026</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>749 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 感知机的完整发展历程：从线性分类到深度学习的基石" href=https://s-ai-unix.github.io/posts/2026-01-21-perceptron-development-history/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://s-ai-unix.github.io/tags/%E7%AE%97%E6%B3%95/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>