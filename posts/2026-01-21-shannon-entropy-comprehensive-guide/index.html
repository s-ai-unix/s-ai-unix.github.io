<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>香农信息熵：不确定性的数学刻度 | s-ai-unix's Blog</title><meta name=keywords content="数学史,综述,算法"><meta name=description content="从摩斯电码到信息时代，完整追溯香农信息熵的诞生历程。深入理解信息、熵与不确定性的本质联系，以及它们如何塑造了我们的数字世界。"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-21-shannon-entropy-comprehensive-guide/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-21-shannon-entropy-comprehensive-guide/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-21-shannon-entropy-comprehensive-guide/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="香农信息熵：不确定性的数学刻度"><meta property="og:description" content="从摩斯电码到信息时代，完整追溯香农信息熵的诞生历程。深入理解信息、熵与不确定性的本质联系，以及它们如何塑造了我们的数字世界。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-21T10:00:00+08:00"><meta property="article:modified_time" content="2026-01-21T10:00:00+08:00"><meta property="article:tag" content="数学史"><meta property="article:tag" content="综述"><meta property="article:tag" content="算法"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/shannon-entropy.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/shannon-entropy.jpg"><meta name=twitter:title content="香农信息熵：不确定性的数学刻度"><meta name=twitter:description content="从摩斯电码到信息时代，完整追溯香农信息熵的诞生历程。深入理解信息、熵与不确定性的本质联系，以及它们如何塑造了我们的数字世界。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"香农信息熵：不确定性的数学刻度","item":"https://s-ai-unix.github.io/posts/2026-01-21-shannon-entropy-comprehensive-guide/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"香农信息熵：不确定性的数学刻度","name":"香农信息熵：不确定性的数学刻度","description":"从摩斯电码到信息时代，完整追溯香农信息熵的诞生历程。深入理解信息、熵与不确定性的本质联系，以及它们如何塑造了我们的数字世界。","keywords":["数学史","综述","算法"],"articleBody":"引言：一条电报引发的思考 信息是什么？ 1844年5月24日，萨缪尔·摩斯（Samuel Morse）从华盛顿向巴尔的摩发出了人类历史上第一条电报：\n“What hath God wrought!”\n这四个单词穿越了64公里的铜线，开启了电信时代。但在庆祝之余，一个问题逐渐浮现：这条消息究竟包含了多少\"信息\"？\n这个问题看似简单，实则深奥。“信息\"是一个抽象的概念，如何用数学来量化它？一封情书和一份天气预报，哪一份包含更多\"信息”？一条加密后的消息和原始消息，信息量是否相同？\n这些问题的答案，隐藏在一位贝尔实验室工程师的伟大发现中。\n香农的登场 1948年，克劳德·香农（Claude Shannon）发表了题为《通信的数学理论》的论文。这篇32页的论文，被誉为\"数字时代的创世大宪章\"。\n在论文中，香农给出了\"信息\"的精确定义，并引入了一个核心概念——信息熵。这个名字借用了热力学中的\"熵\"，暗示了两者之间深刻的联系。\n本文将带你踏上一段历史与数学交织的旅程，从电报时代的实际问题出发，逐步揭示信息熵的诞生、内涵及其深远影响。\n第一章：信息时代的黎明——通信效率的困惑 1.1 摩斯电码中的智慧 在香农之前，通信工程师们已经面临着一个实际问题：如何用最少的符号传输最多的信息？\n摩斯电码给出了一个直观的答案。观察摩斯电码的设计：\nE: . (最常用) T: - (第二常用) A: .- Q: --.- (很少使用) Z: --.. 摩斯天才地意识到：常用的字母应该用较短的编码，不常用的字母可以用较长的编码。这个设计原则在今天看来理所当然，但在当时是革命性的。\n但这引发了更深层的思考：如何精确衡量一个字母的\"常用程度\"？如何计算整个编码系统的效率？这些问题需要数学语言的精确描述。\n1.2 电报的经济学问题 19世纪的电报按字收费，一条消息的成本与其长度直接相关。因此，压缩信息不仅是技术问题，更是经济问题。\n工程师们开始思考：\n如果我们能知道每个字母出现的概率，能否设计出最优的编码？ 通信线路的\"容量\"有没有理论极限？ 噪声（干扰）对信息传输的影响有多大？ 这些问题的答案，要等到20世纪才逐渐浮现。\nflowchart LR subgraph A[\"19世纪通信挑战\"] A1[\"摩斯电码1837\"] A2[\"电报经济学按长度收费\"] end subgraph B[\"20世纪理论突破\"] B1[\"奈奎斯特1924\"] B2[\"哈特利1928\"] B3[\"香农1948\"] end subgraph C[\"现代信息时代\"] C1[\"数字通信\"] C2[\"数据压缩\"] C3[\"机器学习\"] end A1 --\u003e B1 A2 --\u003e B2 B1 --\u003e B3 B2 --\u003e B3 B3 --\u003e C1 B3 --\u003e C2 B3 --\u003e C3 style A1 fill:#34C759,color:#ffffff,stroke-width:2px style A2 fill:#34C759,color:#ffffff,stroke-width:2px style B1 fill:#007AFF,color:#ffffff,stroke-width:2px style B2 fill:#007AFF,color:#ffffff,stroke-width:2px style B3 fill:#007AFF,color:#ffffff,stroke-width:3px style C1 fill:#34C759,color:#ffffff,stroke-width:2px style C2 fill:#34C759,color:#ffffff,stroke-width:2px style C3 fill:#34C759,color:#ffffff,stroke-width:2px 第二章：先驱的脚步——奈奎斯特与哈特利 2.1 奈奎斯特的发现 1924年，贝尔实验室的哈里·奈奎斯特（Harry Nyquist）在研究电报传输时，做出了一个重要发现。\n他认识到：** telegraph 信号的传输速率与信号的带宽成正比**。用数学语言表达：\n$$W = K \\cdot B$$\n其中 $W$ 是传输速率，$B$ 是带宽，$K$ 是一个常数。\n这是人类第一次用数学公式描述通信系统的\"能力\"。\n2.2 哈特利的推广 1928年，拉尔夫·哈特利（Ralph Hartley）进一步推广了奈奎斯特的工作。他意识到，信息量不仅与信号的数目有关，还与这些信号出现的概率有关。\n哈特利提出了一个重要的观察：如果有 $M$ 个可能的符号，每个符号携带的信息量应该是：\n$$I = \\log_2 M$$\n这里的对数形式至关重要——它反映了信息的可加性：如果我们将两个独立的消息组合，总信息量应该是各自信息量之和。\n为什么是对数？\n假设你有两套独立的符号系统，分别有 $M$ 和 $N$ 个符号。组合后共有 $MN$ 种可能：\n$$\\log_2(MN) = \\log_2 M + \\log_2 N$$\n这种可加性正是信息应有的特性！\n2.3 哈特利公式的局限 哈特利公式 $I = \\log_2 M$ 有一个隐含假设：所有符号出现的概率相等。\n但现实并非如此。在英语中，’e’ 出现的概率远高于 ‘z’；在天气预报中，“晴天\"的概率可能高于\"暴风雨”。\n哈特利不知道如何处理不等概率的情况，这个难题留给了香农。\n第三章：香农的突破——信息熵的诞生 3.1 1948年的伟大论文 1948年，香农在《贝尔系统技术杂志》上发表了《通信的数学理论》。这篇论文的开头就明确了目标：\n“The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point.”\n“通信的根本问题，是在一点精确或近似地重现另一点所选出的消息。”\n香农的伟大之处在于：他将\"信息\"从其具体内容中抽离出来，只关注其统计特性。\n3.2 信息量的直觉定义 香农首先思考：什么是一个合理的信息量度量？\n他提出了几个直观要求：\n连续性：概率的微小变化应导致信息量的微小变化 单调性：事件发生的概率越小，得知它发生时获得的信息量越大 可加性：两个独立事件同时发生的信息量，应该是各自信息量之和 根据这些要求，香农得出：如果一个事件发生的概率是 $p$，那么观察到这个事件所获得的信息量应该是：\n$$I(p) = -\\log_2 p$$\n负号的意义：由于 $0 \\leq p \\leq 1$，$\\log_2 p \\leq 0$，加上负号使信息量为正值。\n直观解释：\n如果 $p = 1$（必然事件），$I = 0$——告诉你明天太阳会升起，信息量为零 如果 $p = 0.5$（抛硬币），$I = 1$ 比特 如果 $p = 0.125$（从8个中选1个），$I = 3$ 比特 3.3 信息熵的定义 现在考虑一个随机变量 $X$，它可以取 $n$ 个不同的值 ${x_1, x_2, \\dots, x_n}$，对应的概率是 ${p_1, p_2, \\dots, p_n}$。\n每个可能取值的信息量是 $I(x_i) = -\\log_2 p_i$。\n平均信息量（即信息熵）是：\n$$H(X) = \\sum_{i=1}^{n} p_i \\cdot I(x_i) = -\\sum_{i=1}^{n} p_i \\log_2 p_i$$\n这就是香农熵的定义！\n符号的选择：香农选择符号 $H$ 来表示熵，是为了向玻尔兹曼（Boltzmann）的热力学熵致敬。\n第四章：深入理解信息熵 4.1 一个直观例子 假设有一个天气预报系统，每天预报三种天气：晴天、阴天、雨天。\n情况A：概率均等\n晴天：$p = 1/3$ 阴天：$p = 1/3$ 雨天：$p = 1/3$ $$H = -\\left(\\frac{1}{3}\\log_2\\frac{1}{3} + \\frac{1}{3}\\log_2\\frac{1}{3} + \\frac{1}{3}\\log_2\\frac{1}{3}\\right) = \\log_2 3 \\approx 1.58 \\text{ 比特}$$\n情况B：沙漠地区\n晴天：$p = 0.9$ 阴天：$p = 0.09$ 雨天：$p = 0.01$ $$H = -(0.9\\log_2 0.9 + 0.09\\log_2 0.09 + 0.01\\log_2 0.01) \\approx 0.52 \\text{ 比特}$$\n情况C：确定性系统\n晴天：$p = 1$ 阴天：$p = 0$ 雨天：$p = 0$ $$H = 0 \\text{ 比特}$$\n结论：熵衡量的是不确定性。概率分布越均匀，熵越大；分布越集中，熵越小；完全确定时，熵为零。\n4.2 熵的数学性质 香农证明了信息熵具有以下重要性质：\n性质1：非负性 $$H(X) \\geq 0$$\n性质2：最大值 对于 $n$ 个可能的结果，熵的最大值是 $\\log_2 n$，当所有概率相等时达到。\n性质3：凸性 熵是概率分布的凹函数，这意味着混合概率分布的熵不低于各分布熵的加权平均。\n4.3 与热力学熵的联系 香农最初想称这个量为\"信息\"，但冯·诺依曼（John von Neumann）建议：\n“You should call it entropy, for two reasons. In the first place your uncertainty function has been used in statistical mechanics under that name, so it already has a name. In the second place, and more important, no one knows what entropy really is, so in a debate you will always have the advantage.”\n“你应该称它为熵，有两个原因。第一，你的不确定性函数在统计力学中已经用这个名字了。第二，也是更重要的，没有人真正知道熵是什么，所以在辩论中你总是占优势。”\n事实上，两者的联系确实深刻。热力学熵的玻尔兹曼公式：\n$$S = k_B \\ln W$$\n其中 $W$ 是微观状态数，$k_B$ 是玻尔兹曼常数。\n如果将微观状态视为等概率的，香农熵与热力学熵本质上相同，只是单位不同。\n4.4 连续变量的熵：微分熵 对于连续随机变量，我们将求和替换为积分：\n$$h(X) = -\\int_{-\\infty}^{\\infty} f(x) \\log_2 f(x) , dx$$\n其中 $f(x)$ 是概率密度函数。\n这称为微分熵。需要注意的是，微分熵可以为负值，因为连续变量的信息量与坐标选择有关。\n第五章：信息熵的家族——相关概念 5.1 联合熵 两个随机变量 $X$ 和 $Y$ 的联合熵定义为：\n$$H(X, Y) = -\\sum_{x, y} p(x, y) \\log_2 p(x, y)$$\n这衡量了两个变量一起包含的不确定性。\n重要性质： $$H(X, Y) \\leq H(X) + H(Y)$$\n等号成立当且仅当 $X$ 和 $Y$ 相互独立。这意味着：变量之间的相关性减少了总的不确定性。\n5.2 条件熵 已知 $Y$ 的情况下，$X$ 的条件熵：\n$$H(X|Y) = -\\sum_{x, y} p(x, y) \\log_2 p(x|y)$$\n这表示\"在知道 $Y$ 之后，$X$ 还剩下多少不确定性\"。\n链式法则： $$H(X, Y) = H(Y) + H(X|Y)$$\n这个公式非常直观：要了解 $X$ 和 $Y$ 的全部信息，可以先了解 $Y$，然后了解在已知 $Y$ 的情况下 $X$ 的额外信息。\n5.3 互信息 $X$ 和 $Y$ 之间的互信息定义为：\n$$I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$$\n或者等价地：\n$$I(X; Y) = \\sum_{x, y} p(x, y) \\log_2 \\frac{p(x, y)}{p(x)p(y)}$$\n直观含义：互信息衡量的是两个变量之间共享的信息量。\n性质：\n$I(X; Y) \\geq 0$ $I(X; Y) = 0$ 当且仅当 $X$ 和 $Y$ 独立 $I(X; Y) = I(Y; X)$（对称性） 5.4 KL散度（相对熵） Kullback-Leibler 散度衡量两个概率分布 $P$ 和 $Q$ 之间的差异：\n$$D_{KL}(P | Q) = \\sum_{x} P(x) \\log_2 \\frac{P(x)}{Q(x)}$$\n重要性质：\n$D_{KL}(P | Q) \\geq 0$ $D_{KL}(P | Q) = 0$ 当且仅当 $P = Q$ $D_{KL}$ 不是距离（不对称，不满足三角不等式） KL散度在机器学习中极其重要，它是许多算法的核心。\n5.5 交叉熵 交叉熵定义为：\n$$H(P, Q) = -\\sum_{x} P(x) \\log_2 Q(x)$$\n它与KL散度的关系：\n$$H(P, Q) = H(P) + D_{KL}(P | Q)$$\n机器学习中的应用：在分类问题中，我们最小化交叉熵损失，实际上是在最小化预测分布与真实分布之间的KL散度（因为 $H(P)$ 是常数）。\nflowchart TD subgraph A[\"信息熵基础\"] A1[\"香农熵 H_X\"] A2[\"联合熵 H_XY\"] A3[\"条件熵 H_XY\"] end subgraph B[\"信息关系\"] B1[\"互信息 I_XY\"] B2[\"KL散度 D_KL\"] end subgraph C[\"应用领域\"] C1[\"数据压缩哈夫曼编码\"] C2[\"机器学习交叉熵损失\"] C3[\"通信系统信道容量\"] C4[\"统计推断最大熵原理\"] end A1 --\u003e B1 A1 --\u003e B2 A2 --\u003e B1 A3 --\u003e B1 B1 --\u003e C1 B1 --\u003e C3 B2 --\u003e C2 A1 --\u003e C4 style A1 fill:#007AFF,color:#ffffff,stroke-width:3px style A2 fill:#007AFF,color:#ffffff,stroke-width:2px style A3 fill:#007AFF,color:#ffffff,stroke-width:2px style B1 fill:#34C759,color:#ffffff,stroke-width:2px style B2 fill:#34C759,color:#ffffff,stroke-width:2px style C1 fill:#FF9500,color:#ffffff,stroke-width:2px style C2 fill:#FF9500,color:#ffffff,stroke-width:2px style C3 fill:#FF9500,color:#ffffff,stroke-width:2px style C4 fill:#FF9500,color:#ffffff,stroke-width:2px 第六章：从理论到应用——信息熵的广泛影响 6.1 信源编码定理 香农的第一定理，也称为无失真信源编码定理，给出了数据压缩的理论极限：\n一个熵为 $H$ 的信源，无法以低于 $H$ 比特/符号的速率进行无损编码；但可以任意接近 $H$。\n这个定理告诉我们：\nZIP、GZIP 等压缩算法的极限是什么 为什么有些文件无法被压缩 哈夫曼编码为什么是最优的前缀编码 6.2 信道编码定理 香农的第二定理，有噪信道编码定理，解决了通信的根本问题：\n任何通信信道都有确定的\"容量\" $C$。只要传输速率低于 $C$，就存在编码方式使错误率任意接近零。\n这个定理的革命性在于：它告诉我们可靠的通信是可能的，即使在有噪声的信道上！\n在此之前，人们认为减少错误的唯一方法是提高信号功率或降低传输速率。香农证明，通过巧妙的编码，我们可以在不增加功率的情况下实现可靠通信。\n6.3 最大熵原理 在统计推断中，最大熵原理提供了一种选择概率分布的方法：\n在所有与已知约束条件兼容的概率分布中，应该选择熵最大的那个。\n这个原理的直觉是：不要添加没有根据的信息。\n例如，如果你只知道一个分布的均值和方差，最大熵原理会告诉你选择高斯分布——因为这是在满足这些约束下最\"无偏\"的分布。\n6.4 机器学习中的应用 信息熵的概念在现代机器学习中无处不在：\n分类问题：交叉熵损失是训练神经网络的标准损失函数。\n特征选择：信息增益（互信息）用于决策树算法选择最佳分割特征。\n生成模型：变分推断使用KL散度来近似复杂的后验分布。\n强化学习：熵正则化鼓励探索，防止策略过早收敛。\n第七章：信息熵的计算实例 7.1 抛硬币的熵 公平硬币：$p(\\text{正面}) = p(\\text{反面}) = 0.5$\n$$H = -(0.5 \\log_2 0.5 + 0.5 \\log_2 0.5) = 1 \\text{ 比特}$$\n不公平硬币：$p(\\text{正面}) = 0.9$, $p(\\text{反面}) = 0.1$\n$$H = -(0.9 \\log_2 0.9 + 0.1 \\log_2 0.1) \\approx 0.47 \\text{ 比特}$$\n不公平硬币的熵更小，因为结果更可预测。\n7.2 英文字母的熵 如果我们统计英语文本中26个字母的出现频率，可以计算英语的\"一阶熵\"：\n$$H_1 \\approx 4.07 \\text{ 比特/字母}$$\n但考虑到字母之间的关联（如 ‘q’ 后面几乎总是 ‘u’），实际熵更低：\n$$H_0 = \\log_2 26 \\approx 4.75 \\text{ 比特/字母}$$ $$H_1 \\approx 4.07 \\text{ 比特/字母}$$ $$H_2 \\approx 3.5 \\text{ 比特/字母}$$（二阶近似）\n实际英语的熵大约是 1-1.5 比特/字母（考虑长距离关联）。\n7.3 DNA序列的熵 DNA由四个碱基组成：A、T、G、C。在不同物种和基因组区域中，碱基分布不同：\n人类基因组：A/T/G/C 约为 30%/30%/20%/20% $$H \\approx 1.97 \\text{ 比特/碱基}$$\n某些细菌：分布更均匀 $$H \\approx 2.0 \\text{ 比特/碱基}$$\n熵的计算可以帮助识别基因编码区和非编码区，因为它们有不同的统计特性。\n结语：香农的遗产 从比特到宇宙 1948年，香农在贝尔实验室的一间小屋里，用数学公式刻画了\"信息\"的本质。这个看似抽象的概念，后来成为了数字时代的基石。\n今天，香农熵的应用远远超出了通信工程：\n物理学：量子信息论将熵与量子纠缠联系起来 生物学：DNA序列分析、蛋白质结构预测 计算机科学：算法分析、数据结构、密码学 经济学：金融市场分析、风险管理 哲学：意识的度量、信息的本体论地位 信息的哲学 香农最重要的洞察或许是：信息是可以从意义中分离出来的。\n从摩斯的\"电报信息量\"到今天的\"数据流量\"，我们衡量的是符号的不确定性，而不是它们的语义。这种\"语义无关性\"恰恰是现代数字技术的基础——你的手机用相同的方式处理情书和银行账单，因为它不关心\"意义\"，只关心\"模式\"。\n但这提出了更深的问题：信息与意义是什么关系？一条消息可以携带大量信息却毫无意义吗？大脑处理的是信息还是意义？\n这些问题仍在探索中，而香农熵为我们提供了精确的语言来讨论它们。\n展望未来 随着量子计算的发展，量子信息论正在拓展我们对熵的理解。量子纠缠带来的新型相关性，产生了经典理论无法解释的现象。\n同时，在人工智能领域，信息几何、互信息神经网络等新方向正在兴起。香农的熵，这个诞生于电报时代的概念，仍在引领着技术的未来。\n参考文献与延伸阅读 Shannon, C. E. (1948). “A Mathematical Theory of Communication”. Bell System Technical Journal, 27, 379-423, 623-656. Cover, T. M., \u0026 Thomas, J. A. (2006). Elements of Information Theory (2nd ed.). Wiley. Pierce, J. R. (1980). An Introduction to Information Theory: Symbols, Signals and Noise. Dover. Gleick, J. (2011). The Information: A History, a Theory, a Flood. Pantheon. 香农曾说：“信息就是信息，不是物质，也不是能量。“在这个数据驱动的时代，理解信息的本质，或许是我们理解世界本质的关键一步。\n","wordCount":"891","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/shannon-entropy.jpg","datePublished":"2026-01-21T10:00:00+08:00","dateModified":"2026-01-21T10:00:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-21-shannon-entropy-comprehensive-guide/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">香农信息熵：不确定性的数学刻度</h1><div class=post-description>从摩斯电码到信息时代，完整追溯香农信息熵的诞生历程。深入理解信息、熵与不确定性的本质联系，以及它们如何塑造了我们的数字世界。</div><div class=post-meta><span title='2026-01-21 10:00:00 +0800 CST'>January 21, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>891 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/shannon-entropy.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/shannon-entropy.jpg alt=信息熵与通信理论></a><figcaption>香农信息熵：信息时代的数学基石</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e4%b8%80%e6%9d%a1%e7%94%b5%e6%8a%a5%e5%bc%95%e5%8f%91%e7%9a%84%e6%80%9d%e8%80%83 aria-label=引言：一条电报引发的思考>引言：一条电报引发的思考</a><ul><li><a href=#%e4%bf%a1%e6%81%af%e6%98%af%e4%bb%80%e4%b9%88 aria-label=信息是什么？>信息是什么？</a></li><li><a href=#%e9%a6%99%e5%86%9c%e7%9a%84%e7%99%bb%e5%9c%ba aria-label=香农的登场>香农的登场</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%80%e7%ab%a0%e4%bf%a1%e6%81%af%e6%97%b6%e4%bb%a3%e7%9a%84%e9%bb%8e%e6%98%8e%e9%80%9a%e4%bf%a1%e6%95%88%e7%8e%87%e7%9a%84%e5%9b%b0%e6%83%91 aria-label=第一章：信息时代的黎明——通信效率的困惑>第一章：信息时代的黎明——通信效率的困惑</a><ul><li><a href=#11-%e6%91%a9%e6%96%af%e7%94%b5%e7%a0%81%e4%b8%ad%e7%9a%84%e6%99%ba%e6%85%a7 aria-label="1.1 摩斯电码中的智慧">1.1 摩斯电码中的智慧</a></li><li><a href=#12-%e7%94%b5%e6%8a%a5%e7%9a%84%e7%bb%8f%e6%b5%8e%e5%ad%a6%e9%97%ae%e9%a2%98 aria-label="1.2 电报的经济学问题">1.2 电报的经济学问题</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e7%ab%a0%e5%85%88%e9%a9%b1%e7%9a%84%e8%84%9a%e6%ad%a5%e5%a5%88%e5%a5%8e%e6%96%af%e7%89%b9%e4%b8%8e%e5%93%88%e7%89%b9%e5%88%a9 aria-label=第二章：先驱的脚步——奈奎斯特与哈特利>第二章：先驱的脚步——奈奎斯特与哈特利</a><ul><li><a href=#21-%e5%a5%88%e5%a5%8e%e6%96%af%e7%89%b9%e7%9a%84%e5%8f%91%e7%8e%b0 aria-label="2.1 奈奎斯特的发现">2.1 奈奎斯特的发现</a></li><li><a href=#22-%e5%93%88%e7%89%b9%e5%88%a9%e7%9a%84%e6%8e%a8%e5%b9%bf aria-label="2.2 哈特利的推广">2.2 哈特利的推广</a></li><li><a href=#23-%e5%93%88%e7%89%b9%e5%88%a9%e5%85%ac%e5%bc%8f%e7%9a%84%e5%b1%80%e9%99%90 aria-label="2.3 哈特利公式的局限">2.3 哈特利公式的局限</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e7%ab%a0%e9%a6%99%e5%86%9c%e7%9a%84%e7%aa%81%e7%a0%b4%e4%bf%a1%e6%81%af%e7%86%b5%e7%9a%84%e8%af%9e%e7%94%9f aria-label=第三章：香农的突破——信息熵的诞生>第三章：香农的突破——信息熵的诞生</a><ul><li><a href=#31-1948%e5%b9%b4%e7%9a%84%e4%bc%9f%e5%a4%a7%e8%ae%ba%e6%96%87 aria-label="3.1 1948年的伟大论文">3.1 1948年的伟大论文</a></li><li><a href=#32-%e4%bf%a1%e6%81%af%e9%87%8f%e7%9a%84%e7%9b%b4%e8%a7%89%e5%ae%9a%e4%b9%89 aria-label="3.2 信息量的直觉定义">3.2 信息量的直觉定义</a></li><li><a href=#33-%e4%bf%a1%e6%81%af%e7%86%b5%e7%9a%84%e5%ae%9a%e4%b9%89 aria-label="3.3 信息熵的定义">3.3 信息熵的定义</a></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e7%ab%a0%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3%e4%bf%a1%e6%81%af%e7%86%b5 aria-label=第四章：深入理解信息熵>第四章：深入理解信息熵</a><ul><li><a href=#41-%e4%b8%80%e4%b8%aa%e7%9b%b4%e8%a7%82%e4%be%8b%e5%ad%90 aria-label="4.1 一个直观例子">4.1 一个直观例子</a></li><li><a href=#42-%e7%86%b5%e7%9a%84%e6%95%b0%e5%ad%a6%e6%80%a7%e8%b4%a8 aria-label="4.2 熵的数学性质">4.2 熵的数学性质</a></li><li><a href=#43-%e4%b8%8e%e7%83%ad%e5%8a%9b%e5%ad%a6%e7%86%b5%e7%9a%84%e8%81%94%e7%b3%bb aria-label="4.3 与热力学熵的联系">4.3 与热力学熵的联系</a></li><li><a href=#44-%e8%bf%9e%e7%bb%ad%e5%8f%98%e9%87%8f%e7%9a%84%e7%86%b5%e5%be%ae%e5%88%86%e7%86%b5 aria-label="4.4 连续变量的熵：微分熵">4.4 连续变量的熵：微分熵</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e7%ab%a0%e4%bf%a1%e6%81%af%e7%86%b5%e7%9a%84%e5%ae%b6%e6%97%8f%e7%9b%b8%e5%85%b3%e6%a6%82%e5%bf%b5 aria-label=第五章：信息熵的家族——相关概念>第五章：信息熵的家族——相关概念</a><ul><li><a href=#51-%e8%81%94%e5%90%88%e7%86%b5 aria-label="5.1 联合熵">5.1 联合熵</a></li><li><a href=#52-%e6%9d%a1%e4%bb%b6%e7%86%b5 aria-label="5.2 条件熵">5.2 条件熵</a></li><li><a href=#53-%e4%ba%92%e4%bf%a1%e6%81%af aria-label="5.3 互信息">5.3 互信息</a></li><li><a href=#54-kl%e6%95%a3%e5%ba%a6%e7%9b%b8%e5%af%b9%e7%86%b5 aria-label="5.4 KL散度（相对熵）">5.4 KL散度（相对熵）</a></li><li><a href=#55-%e4%ba%a4%e5%8f%89%e7%86%b5 aria-label="5.5 交叉熵">5.5 交叉熵</a></li></ul></li><li><a href=#%e7%ac%ac%e5%85%ad%e7%ab%a0%e4%bb%8e%e7%90%86%e8%ae%ba%e5%88%b0%e5%ba%94%e7%94%a8%e4%bf%a1%e6%81%af%e7%86%b5%e7%9a%84%e5%b9%bf%e6%b3%9b%e5%bd%b1%e5%93%8d aria-label=第六章：从理论到应用——信息熵的广泛影响>第六章：从理论到应用——信息熵的广泛影响</a><ul><li><a href=#61-%e4%bf%a1%e6%ba%90%e7%bc%96%e7%a0%81%e5%ae%9a%e7%90%86 aria-label="6.1 信源编码定理">6.1 信源编码定理</a></li><li><a href=#62-%e4%bf%a1%e9%81%93%e7%bc%96%e7%a0%81%e5%ae%9a%e7%90%86 aria-label="6.2 信道编码定理">6.2 信道编码定理</a></li><li><a href=#63-%e6%9c%80%e5%a4%a7%e7%86%b5%e5%8e%9f%e7%90%86 aria-label="6.3 最大熵原理">6.3 最大熵原理</a></li><li><a href=#64-%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8 aria-label="6.4 机器学习中的应用">6.4 机器学习中的应用</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%83%e7%ab%a0%e4%bf%a1%e6%81%af%e7%86%b5%e7%9a%84%e8%ae%a1%e7%ae%97%e5%ae%9e%e4%be%8b aria-label=第七章：信息熵的计算实例>第七章：信息熵的计算实例</a><ul><li><a href=#71-%e6%8a%9b%e7%a1%ac%e5%b8%81%e7%9a%84%e7%86%b5 aria-label="7.1 抛硬币的熵">7.1 抛硬币的熵</a></li><li><a href=#72-%e8%8b%b1%e6%96%87%e5%ad%97%e6%af%8d%e7%9a%84%e7%86%b5 aria-label="7.2 英文字母的熵">7.2 英文字母的熵</a></li><li><a href=#73-dna%e5%ba%8f%e5%88%97%e7%9a%84%e7%86%b5 aria-label="7.3 DNA序列的熵">7.3 DNA序列的熵</a></li></ul></li><li><a href=#%e7%bb%93%e8%af%ad%e9%a6%99%e5%86%9c%e7%9a%84%e9%81%97%e4%ba%a7 aria-label=结语：香农的遗产>结语：香农的遗产</a><ul><li><a href=#%e4%bb%8e%e6%af%94%e7%89%b9%e5%88%b0%e5%ae%87%e5%ae%99 aria-label=从比特到宇宙>从比特到宇宙</a></li><li><a href=#%e4%bf%a1%e6%81%af%e7%9a%84%e5%93%b2%e5%ad%a6 aria-label=信息的哲学>信息的哲学</a></li><li><a href=#%e5%b1%95%e6%9c%9b%e6%9c%aa%e6%9d%a5 aria-label=展望未来>展望未来</a></li></ul></li><li><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae%e4%b8%8e%e5%bb%b6%e4%bc%b8%e9%98%85%e8%af%bb aria-label=参考文献与延伸阅读>参考文献与延伸阅读</a></li></ul></div></details></div><div class=post-content><h2 id=引言一条电报引发的思考>引言：一条电报引发的思考<a hidden class=anchor aria-hidden=true href=#引言一条电报引发的思考>#</a></h2><h3 id=信息是什么>信息是什么？<a hidden class=anchor aria-hidden=true href=#信息是什么>#</a></h3><p>1844年5月24日，萨缪尔·摩斯（Samuel Morse）从华盛顿向巴尔的摩发出了人类历史上第一条电报：</p><blockquote><p>&ldquo;What hath God wrought!&rdquo;</p></blockquote><p>这四个单词穿越了64公里的铜线，开启了电信时代。但在庆祝之余，一个问题逐渐浮现：<strong>这条消息究竟包含了多少"信息"？</strong></p><p>这个问题看似简单，实则深奥。&ldquo;信息"是一个抽象的概念，如何用数学来量化它？一封情书和一份天气预报，哪一份包含更多"信息&rdquo;？一条加密后的消息和原始消息，信息量是否相同？</p><p>这些问题的答案，隐藏在一位贝尔实验室工程师的伟大发现中。</p><h3 id=香农的登场>香农的登场<a hidden class=anchor aria-hidden=true href=#香农的登场>#</a></h3><p>1948年，克劳德·香农（Claude Shannon）发表了题为《通信的数学理论》的论文。这篇32页的论文，被誉为"数字时代的创世大宪章"。</p><p>在论文中，香农给出了"信息"的精确定义，并引入了一个核心概念——<strong>信息熵</strong>。这个名字借用了热力学中的"熵"，暗示了两者之间深刻的联系。</p><p>本文将带你踏上一段历史与数学交织的旅程，从电报时代的实际问题出发，逐步揭示信息熵的诞生、内涵及其深远影响。</p><hr><h2 id=第一章信息时代的黎明通信效率的困惑>第一章：信息时代的黎明——通信效率的困惑<a hidden class=anchor aria-hidden=true href=#第一章信息时代的黎明通信效率的困惑>#</a></h2><h3 id=11-摩斯电码中的智慧>1.1 摩斯电码中的智慧<a hidden class=anchor aria-hidden=true href=#11-摩斯电码中的智慧>#</a></h3><p>在香农之前，通信工程师们已经面临着一个实际问题：<strong>如何用最少的符号传输最多的信息？</strong></p><p>摩斯电码给出了一个直观的答案。观察摩斯电码的设计：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>E: .          (最常用)
</span></span><span class=line><span class=cl>T: -          (第二常用)
</span></span><span class=line><span class=cl>A: .-
</span></span><span class=line><span class=cl>Q: --.-       (很少使用)
</span></span><span class=line><span class=cl>Z: --..
</span></span></code></pre></div><p>摩斯天才地意识到：<strong>常用的字母应该用较短的编码，不常用的字母可以用较长的编码</strong>。这个设计原则在今天看来理所当然，但在当时是革命性的。</p><p>但这引发了更深层的思考：如何精确衡量一个字母的"常用程度"？如何计算整个编码系统的效率？这些问题需要数学语言的精确描述。</p><h3 id=12-电报的经济学问题>1.2 电报的经济学问题<a hidden class=anchor aria-hidden=true href=#12-电报的经济学问题>#</a></h3><p>19世纪的电报按字收费，一条消息的成本与其长度直接相关。因此，<strong>压缩信息</strong>不仅是技术问题，更是经济问题。</p><p>工程师们开始思考：</p><ul><li>如果我们能知道每个字母出现的概率，能否设计出最优的编码？</li><li>通信线路的"容量"有没有理论极限？</li><li>噪声（干扰）对信息传输的影响有多大？</li></ul><p>这些问题的答案，要等到20世纪才逐渐浮现。</p><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>flowchart LR
subgraph A["19世纪通信挑战"]
A1["摩斯电码<br>1837"]
A2["电报经济学<br>按长度收费"]
end
subgraph B["20世纪理论突破"]
B1["奈奎斯特<br>1924"]
B2["哈特利<br>1928"]
B3["香农<br>1948"]
end
subgraph C["现代信息时代"]
C1["数字通信"]
C2["数据压缩"]
C3["机器学习"]
end
A1 --> B1
A2 --> B2
B1 --> B3
B2 --> B3
B3 --> C1
B3 --> C2
B3 --> C3
style A1 fill:#34C759,color:#ffffff,stroke-width:2px
style A2 fill:#34C759,color:#ffffff,stroke-width:2px
style B1 fill:#007AFF,color:#ffffff,stroke-width:2px
style B2 fill:#007AFF,color:#ffffff,stroke-width:2px
style B3 fill:#007AFF,color:#ffffff,stroke-width:3px
style C1 fill:#34C759,color:#ffffff,stroke-width:2px
style C2 fill:#34C759,color:#ffffff,stroke-width:2px
style C3 fill:#34C759,color:#ffffff,stroke-width:2px</div></div><hr><h2 id=第二章先驱的脚步奈奎斯特与哈特利>第二章：先驱的脚步——奈奎斯特与哈特利<a hidden class=anchor aria-hidden=true href=#第二章先驱的脚步奈奎斯特与哈特利>#</a></h2><h3 id=21-奈奎斯特的发现>2.1 奈奎斯特的发现<a hidden class=anchor aria-hidden=true href=#21-奈奎斯特的发现>#</a></h3><p>1924年，贝尔实验室的哈里·奈奎斯特（Harry Nyquist）在研究电报传输时，做出了一个重要发现。</p><p>他认识到：** telegraph 信号的传输速率与信号的带宽成正比**。用数学语言表达：</p><p>$$W = K \cdot B$$</p><p>其中 $W$ 是传输速率，$B$ 是带宽，$K$ 是一个常数。</p><p>这是人类第一次用数学公式描述通信系统的"能力"。</p><h3 id=22-哈特利的推广>2.2 哈特利的推广<a hidden class=anchor aria-hidden=true href=#22-哈特利的推广>#</a></h3><p>1928年，拉尔夫·哈特利（Ralph Hartley）进一步推广了奈奎斯特的工作。他意识到，信息量不仅与信号的数目有关，还与这些信号出现的<strong>概率</strong>有关。</p><p>哈特利提出了一个重要的观察：如果有 $M$ 个可能的符号，每个符号携带的信息量应该是：</p><p>$$I = \log_2 M$$</p><p>这里的对数形式至关重要——它反映了信息的<strong>可加性</strong>：如果我们将两个独立的消息组合，总信息量应该是各自信息量之和。</p><p><strong>为什么是对数？</strong></p><p>假设你有两套独立的符号系统，分别有 $M$ 和 $N$ 个符号。组合后共有 $MN$ 种可能：</p><p>$$\log_2(MN) = \log_2 M + \log_2 N$$</p><p>这种可加性正是信息应有的特性！</p><h3 id=23-哈特利公式的局限>2.3 哈特利公式的局限<a hidden class=anchor aria-hidden=true href=#23-哈特利公式的局限>#</a></h3><p>哈特利公式 $I = \log_2 M$ 有一个隐含假设：<strong>所有符号出现的概率相等</strong>。</p><p>但现实并非如此。在英语中，&rsquo;e&rsquo; 出现的概率远高于 &lsquo;z&rsquo;；在天气预报中，&ldquo;晴天"的概率可能高于"暴风雨&rdquo;。</p><p>哈特利不知道如何处理不等概率的情况，这个难题留给了香农。</p><hr><h2 id=第三章香农的突破信息熵的诞生>第三章：香农的突破——信息熵的诞生<a hidden class=anchor aria-hidden=true href=#第三章香农的突破信息熵的诞生>#</a></h2><h3 id=31-1948年的伟大论文>3.1 1948年的伟大论文<a hidden class=anchor aria-hidden=true href=#31-1948年的伟大论文>#</a></h3><p>1948年，香农在《贝尔系统技术杂志》上发表了《通信的数学理论》。这篇论文的开头就明确了目标：</p><blockquote><p>&ldquo;The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point.&rdquo;</p><p>&ldquo;通信的根本问题，是在一点精确或近似地重现另一点所选出的消息。&rdquo;</p></blockquote><p>香农的伟大之处在于：他将"信息"从其具体内容中抽离出来，只关注其<strong>统计特性</strong>。</p><h3 id=32-信息量的直觉定义>3.2 信息量的直觉定义<a hidden class=anchor aria-hidden=true href=#32-信息量的直觉定义>#</a></h3><p>香农首先思考：<strong>什么是一个合理的信息量度量？</strong></p><p>他提出了几个直观要求：</p><ol><li><strong>连续性</strong>：概率的微小变化应导致信息量的微小变化</li><li><strong>单调性</strong>：事件发生的概率越小，得知它发生时获得的信息量越大</li><li><strong>可加性</strong>：两个独立事件同时发生的信息量，应该是各自信息量之和</li></ol><p>根据这些要求，香农得出：如果一个事件发生的概率是 $p$，那么观察到这个事件所获得的信息量应该是：</p><p>$$I(p) = -\log_2 p$$</p><p><strong>负号的意义</strong>：由于 $0 \leq p \leq 1$，$\log_2 p \leq 0$，加上负号使信息量为正值。</p><p><strong>直观解释</strong>：</p><ul><li>如果 $p = 1$（必然事件），$I = 0$——告诉你明天太阳会升起，信息量为零</li><li>如果 $p = 0.5$（抛硬币），$I = 1$ 比特</li><li>如果 $p = 0.125$（从8个中选1个），$I = 3$ 比特</li></ul><h3 id=33-信息熵的定义>3.3 信息熵的定义<a hidden class=anchor aria-hidden=true href=#33-信息熵的定义>#</a></h3><p>现在考虑一个随机变量 $X$，它可以取 $n$ 个不同的值 ${x_1, x_2, \dots, x_n}$，对应的概率是 ${p_1, p_2, \dots, p_n}$。</p><p><strong>每个可能取值的信息量</strong>是 $I(x_i) = -\log_2 p_i$。</p><p><strong>平均信息量</strong>（即信息熵）是：</p><p>$$H(X) = \sum_{i=1}^{n} p_i \cdot I(x_i) = -\sum_{i=1}^{n} p_i \log_2 p_i$$</p><p>这就是<strong>香农熵</strong>的定义！</p><p><strong>符号的选择</strong>：香农选择符号 $H$ 来表示熵，是为了向玻尔兹曼（Boltzmann）的热力学熵致敬。</p><hr><h2 id=第四章深入理解信息熵>第四章：深入理解信息熵<a hidden class=anchor aria-hidden=true href=#第四章深入理解信息熵>#</a></h2><h3 id=41-一个直观例子>4.1 一个直观例子<a hidden class=anchor aria-hidden=true href=#41-一个直观例子>#</a></h3><p>假设有一个天气预报系统，每天预报三种天气：晴天、阴天、雨天。</p><p><strong>情况A：概率均等</strong></p><ul><li>晴天：$p = 1/3$</li><li>阴天：$p = 1/3$</li><li>雨天：$p = 1/3$</li></ul><p>$$H = -\left(\frac{1}{3}\log_2\frac{1}{3} + \frac{1}{3}\log_2\frac{1}{3} + \frac{1}{3}\log_2\frac{1}{3}\right) = \log_2 3 \approx 1.58 \text{ 比特}$$</p><p><strong>情况B：沙漠地区</strong></p><ul><li>晴天：$p = 0.9$</li><li>阴天：$p = 0.09$</li><li>雨天：$p = 0.01$</li></ul><p>$$H = -(0.9\log_2 0.9 + 0.09\log_2 0.09 + 0.01\log_2 0.01) \approx 0.52 \text{ 比特}$$</p><p><strong>情况C：确定性系统</strong></p><ul><li>晴天：$p = 1$</li><li>阴天：$p = 0$</li><li>雨天：$p = 0$</li></ul><p>$$H = 0 \text{ 比特}$$</p><p><strong>结论</strong>：熵衡量的是<strong>不确定性</strong>。概率分布越均匀，熵越大；分布越集中，熵越小；完全确定时，熵为零。</p><h3 id=42-熵的数学性质>4.2 熵的数学性质<a hidden class=anchor aria-hidden=true href=#42-熵的数学性质>#</a></h3><p>香农证明了信息熵具有以下重要性质：</p><p><strong>性质1：非负性</strong>
$$H(X) \geq 0$$</p><p><strong>性质2：最大值</strong>
对于 $n$ 个可能的结果，熵的最大值是 $\log_2 n$，当所有概率相等时达到。</p><p><strong>性质3：凸性</strong>
熵是概率分布的凹函数，这意味着混合概率分布的熵不低于各分布熵的加权平均。</p><h3 id=43-与热力学熵的联系>4.3 与热力学熵的联系<a hidden class=anchor aria-hidden=true href=#43-与热力学熵的联系>#</a></h3><p>香农最初想称这个量为"信息"，但冯·诺依曼（John von Neumann）建议：</p><blockquote><p>&ldquo;You should call it entropy, for two reasons. In the first place your uncertainty function has been used in statistical mechanics under that name, so it already has a name. In the second place, and more important, no one knows what entropy really is, so in a debate you will always have the advantage.&rdquo;</p><p>&ldquo;你应该称它为熵，有两个原因。第一，你的不确定性函数在统计力学中已经用这个名字了。第二，也是更重要的，没有人真正知道熵是什么，所以在辩论中你总是占优势。&rdquo;</p></blockquote><p>事实上，两者的联系确实深刻。热力学熵的玻尔兹曼公式：</p><p>$$S = k_B \ln W$$</p><p>其中 $W$ 是微观状态数，$k_B$ 是玻尔兹曼常数。</p><p>如果将微观状态视为等概率的，香农熵与热力学熵本质上相同，只是单位不同。</p><h3 id=44-连续变量的熵微分熵>4.4 连续变量的熵：微分熵<a hidden class=anchor aria-hidden=true href=#44-连续变量的熵微分熵>#</a></h3><p>对于连续随机变量，我们将求和替换为积分：</p><p>$$h(X) = -\int_{-\infty}^{\infty} f(x) \log_2 f(x) , dx$$</p><p>其中 $f(x)$ 是概率密度函数。</p><p>这称为<strong>微分熵</strong>。需要注意的是，微分熵可以为负值，因为连续变量的信息量与坐标选择有关。</p><hr><h2 id=第五章信息熵的家族相关概念>第五章：信息熵的家族——相关概念<a hidden class=anchor aria-hidden=true href=#第五章信息熵的家族相关概念>#</a></h2><h3 id=51-联合熵>5.1 联合熵<a hidden class=anchor aria-hidden=true href=#51-联合熵>#</a></h3><p>两个随机变量 $X$ 和 $Y$ 的联合熵定义为：</p><p>$$H(X, Y) = -\sum_{x, y} p(x, y) \log_2 p(x, y)$$</p><p>这衡量了两个变量一起包含的不确定性。</p><p><strong>重要性质</strong>：
$$H(X, Y) \leq H(X) + H(Y)$$</p><p>等号成立当且仅当 $X$ 和 $Y$ 相互独立。这意味着：<strong>变量之间的相关性减少了总的不确定性</strong>。</p><h3 id=52-条件熵>5.2 条件熵<a hidden class=anchor aria-hidden=true href=#52-条件熵>#</a></h3><p>已知 $Y$ 的情况下，$X$ 的条件熵：</p><p>$$H(X|Y) = -\sum_{x, y} p(x, y) \log_2 p(x|y)$$</p><p>这表示"在知道 $Y$ 之后，$X$ 还剩下多少不确定性"。</p><p><strong>链式法则</strong>：
$$H(X, Y) = H(Y) + H(X|Y)$$</p><p>这个公式非常直观：要了解 $X$ 和 $Y$ 的全部信息，可以先了解 $Y$，然后了解在已知 $Y$ 的情况下 $X$ 的额外信息。</p><h3 id=53-互信息>5.3 互信息<a hidden class=anchor aria-hidden=true href=#53-互信息>#</a></h3><p>$X$ 和 $Y$ 之间的互信息定义为：</p><p>$$I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$$</p><p>或者等价地：</p><p>$$I(X; Y) = \sum_{x, y} p(x, y) \log_2 \frac{p(x, y)}{p(x)p(y)}$$</p><p><strong>直观含义</strong>：互信息衡量的是两个变量之间<strong>共享</strong>的信息量。</p><p><strong>性质</strong>：</p><ul><li>$I(X; Y) \geq 0$</li><li>$I(X; Y) = 0$ 当且仅当 $X$ 和 $Y$ 独立</li><li>$I(X; Y) = I(Y; X)$（对称性）</li></ul><h3 id=54-kl散度相对熵>5.4 KL散度（相对熵）<a hidden class=anchor aria-hidden=true href=#54-kl散度相对熵>#</a></h3><p>Kullback-Leibler 散度衡量两个概率分布 $P$ 和 $Q$ 之间的差异：</p><p>$$D_{KL}(P | Q) = \sum_{x} P(x) \log_2 \frac{P(x)}{Q(x)}$$</p><p><strong>重要性质</strong>：</p><ul><li>$D_{KL}(P | Q) \geq 0$</li><li>$D_{KL}(P | Q) = 0$ 当且仅当 $P = Q$</li><li>$D_{KL}$ 不是距离（不对称，不满足三角不等式）</li></ul><p>KL散度在机器学习中极其重要，它是许多算法的核心。</p><h3 id=55-交叉熵>5.5 交叉熵<a hidden class=anchor aria-hidden=true href=#55-交叉熵>#</a></h3><p>交叉熵定义为：</p><p>$$H(P, Q) = -\sum_{x} P(x) \log_2 Q(x)$$</p><p>它与KL散度的关系：</p><p>$$H(P, Q) = H(P) + D_{KL}(P | Q)$$</p><p><strong>机器学习中的应用</strong>：在分类问题中，我们最小化交叉熵损失，实际上是在最小化预测分布与真实分布之间的KL散度（因为 $H(P)$ 是常数）。</p><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>flowchart TD
subgraph A["信息熵基础"]
A1["香农熵 H_X"]
A2["联合熵 H_XY"]
A3["条件熵 H_XY"]
end
subgraph B["信息关系"]
B1["互信息 I_XY"]
B2["KL散度 D_KL"]
end
subgraph C["应用领域"]
C1["数据压缩<br>哈夫曼编码"]
C2["机器学习<br>交叉熵损失"]
C3["通信系统<br>信道容量"]
C4["统计推断<br>最大熵原理"]
end
A1 --> B1
A1 --> B2
A2 --> B1
A3 --> B1
B1 --> C1
B1 --> C3
B2 --> C2
A1 --> C4
style A1 fill:#007AFF,color:#ffffff,stroke-width:3px
style A2 fill:#007AFF,color:#ffffff,stroke-width:2px
style A3 fill:#007AFF,color:#ffffff,stroke-width:2px
style B1 fill:#34C759,color:#ffffff,stroke-width:2px
style B2 fill:#34C759,color:#ffffff,stroke-width:2px
style C1 fill:#FF9500,color:#ffffff,stroke-width:2px
style C2 fill:#FF9500,color:#ffffff,stroke-width:2px
style C3 fill:#FF9500,color:#ffffff,stroke-width:2px
style C4 fill:#FF9500,color:#ffffff,stroke-width:2px</div></div><hr><h2 id=第六章从理论到应用信息熵的广泛影响>第六章：从理论到应用——信息熵的广泛影响<a hidden class=anchor aria-hidden=true href=#第六章从理论到应用信息熵的广泛影响>#</a></h2><h3 id=61-信源编码定理>6.1 信源编码定理<a hidden class=anchor aria-hidden=true href=#61-信源编码定理>#</a></h3><p>香农的第一定理，也称为<strong>无失真信源编码定理</strong>，给出了数据压缩的理论极限：</p><blockquote><p>一个熵为 $H$ 的信源，无法以低于 $H$ 比特/符号的速率进行无损编码；但可以任意接近 $H$。</p></blockquote><p>这个定理告诉我们：</p><ul><li>ZIP、GZIP 等压缩算法的极限是什么</li><li>为什么有些文件无法被压缩</li><li>哈夫曼编码为什么是最优的前缀编码</li></ul><h3 id=62-信道编码定理>6.2 信道编码定理<a hidden class=anchor aria-hidden=true href=#62-信道编码定理>#</a></h3><p>香农的第二定理，<strong>有噪信道编码定理</strong>，解决了通信的根本问题：</p><blockquote><p>任何通信信道都有确定的"容量" $C$。只要传输速率低于 $C$，就存在编码方式使错误率任意接近零。</p></blockquote><p>这个定理的革命性在于：它告诉我们<strong>可靠的通信是可能的</strong>，即使在有噪声的信道上！</p><p>在此之前，人们认为减少错误的唯一方法是提高信号功率或降低传输速率。香农证明，通过巧妙的编码，我们可以在不增加功率的情况下实现可靠通信。</p><h3 id=63-最大熵原理>6.3 最大熵原理<a hidden class=anchor aria-hidden=true href=#63-最大熵原理>#</a></h3><p>在统计推断中，<strong>最大熵原理</strong>提供了一种选择概率分布的方法：</p><blockquote><p>在所有与已知约束条件兼容的概率分布中，应该选择熵最大的那个。</p></blockquote><p>这个原理的直觉是：<strong>不要添加没有根据的信息</strong>。</p><p>例如，如果你只知道一个分布的均值和方差，最大熵原理会告诉你选择高斯分布——因为这是在满足这些约束下最"无偏"的分布。</p><h3 id=64-机器学习中的应用>6.4 机器学习中的应用<a hidden class=anchor aria-hidden=true href=#64-机器学习中的应用>#</a></h3><p>信息熵的概念在现代机器学习中无处不在：</p><p><strong>分类问题</strong>：交叉熵损失是训练神经网络的标准损失函数。</p><p><strong>特征选择</strong>：信息增益（互信息）用于决策树算法选择最佳分割特征。</p><p><strong>生成模型</strong>：变分推断使用KL散度来近似复杂的后验分布。</p><p><strong>强化学习</strong>：熵正则化鼓励探索，防止策略过早收敛。</p><hr><h2 id=第七章信息熵的计算实例>第七章：信息熵的计算实例<a hidden class=anchor aria-hidden=true href=#第七章信息熵的计算实例>#</a></h2><h3 id=71-抛硬币的熵>7.1 抛硬币的熵<a hidden class=anchor aria-hidden=true href=#71-抛硬币的熵>#</a></h3><p><strong>公平硬币</strong>：$p(\text{正面}) = p(\text{反面}) = 0.5$</p><p>$$H = -(0.5 \log_2 0.5 + 0.5 \log_2 0.5) = 1 \text{ 比特}$$</p><p><strong>不公平硬币</strong>：$p(\text{正面}) = 0.9$, $p(\text{反面}) = 0.1$</p><p>$$H = -(0.9 \log_2 0.9 + 0.1 \log_2 0.1) \approx 0.47 \text{ 比特}$$</p><p>不公平硬币的熵更小，因为结果更可预测。</p><h3 id=72-英文字母的熵>7.2 英文字母的熵<a hidden class=anchor aria-hidden=true href=#72-英文字母的熵>#</a></h3><p>如果我们统计英语文本中26个字母的出现频率，可以计算英语的"一阶熵"：</p><p>$$H_1 \approx 4.07 \text{ 比特/字母}$$</p><p>但考虑到字母之间的关联（如 &lsquo;q&rsquo; 后面几乎总是 &lsquo;u&rsquo;），实际熵更低：</p><p>$$H_0 = \log_2 26 \approx 4.75 \text{ 比特/字母}$$
$$H_1 \approx 4.07 \text{ 比特/字母}$$
$$H_2 \approx 3.5 \text{ 比特/字母}$$（二阶近似）</p><p>实际英语的熵大约是 <strong>1-1.5 比特/字母</strong>（考虑长距离关联）。</p><h3 id=73-dna序列的熵>7.3 DNA序列的熵<a hidden class=anchor aria-hidden=true href=#73-dna序列的熵>#</a></h3><p>DNA由四个碱基组成：A、T、G、C。在不同物种和基因组区域中，碱基分布不同：</p><p><strong>人类基因组</strong>：A/T/G/C 约为 30%/30%/20%/20%
$$H \approx 1.97 \text{ 比特/碱基}$$</p><p><strong>某些细菌</strong>：分布更均匀
$$H \approx 2.0 \text{ 比特/碱基}$$</p><p>熵的计算可以帮助识别基因编码区和非编码区，因为它们有不同的统计特性。</p><hr><h2 id=结语香农的遗产>结语：香农的遗产<a hidden class=anchor aria-hidden=true href=#结语香农的遗产>#</a></h2><h3 id=从比特到宇宙>从比特到宇宙<a hidden class=anchor aria-hidden=true href=#从比特到宇宙>#</a></h3><p>1948年，香农在贝尔实验室的一间小屋里，用数学公式刻画了"信息"的本质。这个看似抽象的概念，后来成为了数字时代的基石。</p><p>今天，香农熵的应用远远超出了通信工程：</p><ul><li><strong>物理学</strong>：量子信息论将熵与量子纠缠联系起来</li><li><strong>生物学</strong>：DNA序列分析、蛋白质结构预测</li><li><strong>计算机科学</strong>：算法分析、数据结构、密码学</li><li><strong>经济学</strong>：金融市场分析、风险管理</li><li><strong>哲学</strong>：意识的度量、信息的本体论地位</li></ul><h3 id=信息的哲学>信息的哲学<a hidden class=anchor aria-hidden=true href=#信息的哲学>#</a></h3><p>香农最重要的洞察或许是：<strong>信息是可以从意义中分离出来的</strong>。</p><p>从摩斯的"电报信息量"到今天的"数据流量"，我们衡量的是符号的不确定性，而不是它们的语义。这种"语义无关性"恰恰是现代数字技术的基础——你的手机用相同的方式处理情书和银行账单，因为它不关心"意义"，只关心"模式"。</p><p>但这提出了更深的问题：信息与意义是什么关系？一条消息可以携带大量信息却毫无意义吗？大脑处理的是信息还是意义？</p><p>这些问题仍在探索中，而香农熵为我们提供了精确的语言来讨论它们。</p><h3 id=展望未来>展望未来<a hidden class=anchor aria-hidden=true href=#展望未来>#</a></h3><p>随着量子计算的发展，量子信息论正在拓展我们对熵的理解。量子纠缠带来的新型相关性，产生了经典理论无法解释的现象。</p><p>同时，在人工智能领域，信息几何、互信息神经网络等新方向正在兴起。香农的熵，这个诞生于电报时代的概念，仍在引领着技术的未来。</p><hr><h2 id=参考文献与延伸阅读>参考文献与延伸阅读<a hidden class=anchor aria-hidden=true href=#参考文献与延伸阅读>#</a></h2><ol><li>Shannon, C. E. (1948). &ldquo;A Mathematical Theory of Communication&rdquo;. <em>Bell System Technical Journal</em>, 27, 379-423, 623-656.</li><li>Cover, T. M., & Thomas, J. A. (2006). <em>Elements of Information Theory</em> (2nd ed.). Wiley.</li><li>Pierce, J. R. (1980). <em>An Introduction to Information Theory: Symbols, Signals and Noise</em>. Dover.</li><li>Gleick, J. (2011). <em>The Information: A History, a Theory, a Flood</em>. Pantheon.</li></ol><hr><p><em>香农曾说：&ldquo;信息就是信息，不是物质，也不是能量。&ldquo;在这个数据驱动的时代，理解信息的本质，或许是我们理解世界本质的关键一步。</em></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E6%95%B0%E5%AD%A6%E5%8F%B2/>数学史</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%BB%BC%E8%BF%B0/>综述</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%AE%97%E6%B3%95/>算法</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-21-bayes-theorem/><span class=title>« Prev</span><br><span>贝叶斯公式：从牧师遗作到人工智能基石</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-21-perceptron-development-history/><span class=title>Next »</span><br><span>感知机的完整发展历程：从线性分类到深度学习的基石</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 香农信息熵：不确定性的数学刻度 on x" href="https://x.com/intent/tweet/?text=%e9%a6%99%e5%86%9c%e4%bf%a1%e6%81%af%e7%86%b5%ef%bc%9a%e4%b8%8d%e7%a1%ae%e5%ae%9a%e6%80%a7%e7%9a%84%e6%95%b0%e5%ad%a6%e5%88%bb%e5%ba%a6&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-shannon-entropy-comprehensive-guide%2f&amp;hashtags=%e6%95%b0%e5%ad%a6%e5%8f%b2%2c%e7%bb%bc%e8%bf%b0%2c%e7%ae%97%e6%b3%95"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 香农信息熵：不确定性的数学刻度 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-shannon-entropy-comprehensive-guide%2f&amp;title=%e9%a6%99%e5%86%9c%e4%bf%a1%e6%81%af%e7%86%b5%ef%bc%9a%e4%b8%8d%e7%a1%ae%e5%ae%9a%e6%80%a7%e7%9a%84%e6%95%b0%e5%ad%a6%e5%88%bb%e5%ba%a6&amp;summary=%e9%a6%99%e5%86%9c%e4%bf%a1%e6%81%af%e7%86%b5%ef%bc%9a%e4%b8%8d%e7%a1%ae%e5%ae%9a%e6%80%a7%e7%9a%84%e6%95%b0%e5%ad%a6%e5%88%bb%e5%ba%a6&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-shannon-entropy-comprehensive-guide%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 香农信息熵：不确定性的数学刻度 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-shannon-entropy-comprehensive-guide%2f&title=%e9%a6%99%e5%86%9c%e4%bf%a1%e6%81%af%e7%86%b5%ef%bc%9a%e4%b8%8d%e7%a1%ae%e5%ae%9a%e6%80%a7%e7%9a%84%e6%95%b0%e5%ad%a6%e5%88%bb%e5%ba%a6"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 香农信息熵：不确定性的数学刻度 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-shannon-entropy-comprehensive-guide%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>