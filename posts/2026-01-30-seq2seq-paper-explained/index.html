<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AI 论文解读系列：Seq2Seq--从序列到序列的革命 | s-ai-unix's Blog</title><meta name=keywords content="深度学习,神经网络,自然语言处理,算法"><meta name=description content="深入浅出解读 Seq2Seq 论文，从机器翻译的困境到编码器-解码器架构的突破，揭示深度学习处理序列数据的核心思想。"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-30-seq2seq-paper-explained/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-30-seq2seq-paper-explained/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-30-seq2seq-paper-explained/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="AI 论文解读系列：Seq2Seq--从序列到序列的革命"><meta property="og:description" content="深入浅出解读 Seq2Seq 论文，从机器翻译的困境到编码器-解码器架构的突破，揭示深度学习处理序列数据的核心思想。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-30T09:00:00+08:00"><meta property="article:modified_time" content="2026-01-30T09:00:00+08:00"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="神经网络"><meta property="article:tag" content="自然语言处理"><meta property="article:tag" content="算法"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/seq2seq-cover.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/seq2seq-cover.jpg"><meta name=twitter:title content="AI 论文解读系列：Seq2Seq--从序列到序列的革命"><meta name=twitter:description content="深入浅出解读 Seq2Seq 论文，从机器翻译的困境到编码器-解码器架构的突破，揭示深度学习处理序列数据的核心思想。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"AI 论文解读系列：Seq2Seq--从序列到序列的革命","item":"https://s-ai-unix.github.io/posts/2026-01-30-seq2seq-paper-explained/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"AI 论文解读系列：Seq2Seq--从序列到序列的革命","name":"AI 论文解读系列：Seq2Seq--从序列到序列的革命","description":"深入浅出解读 Seq2Seq 论文，从机器翻译的困境到编码器-解码器架构的突破，揭示深度学习处理序列数据的核心思想。","keywords":["深度学习","神经网络","自然语言处理","算法"],"articleBody":"引言：翻译的困境 想象一下，你正在学习一门外语。当你听到一句法语 “Bonjour le monde” 时，你的大脑是如何将其转化为英语 “Hello world” 的？\n这不是简单的逐词替换。“Bonjour” 对应 “Hello”，但 “le monde” 是 “the world” 的倒序。词序不同，语法结构不同，甚至可能一个词对应多个词。传统的机器翻译系统使用基于规则的方法或统计模型，需要大量的人工特征工程和复杂的对齐算法。\n2014年，Ilya Sutskever、Oriol Vinyals 和 Quoc Le 在 Google 发表了一篇改变游戏规则的论文：“Sequence to Sequence Learning with Neural Networks”。他们提出的 Seq2Seq 架构，用一个统一的神经网络模型取代了复杂的流水线，让机器翻译的准确率跃升到了新的高度。\n但这篇论文的意义远不止于翻译。它开创了序列转导（Sequence Transduction）这一全新的学习范式，为后来的注意力机制、Transformer 乃至大语言模型奠定了基础。\n第一章：序列转导问题 1.1 什么让序列数据特殊 在深入 Seq2Seq 之前，让我们先理解序列数据的本质。\n传统的机器学习任务，比如图像分类或房价预测，输入和输出的维度是固定的。一张图片永远是 $224 \\times 224 \\times 3$ 的像素矩阵，一套房子的特征永远是卧室数、面积、位置等固定字段。\n但序列数据不同：\n一句话可能有 5 个词，也可能有 50 个词 源语言和目标语言的词序可能不同 一个概念可能用一个词表达，也可能用多个词 上图展示了一个典型的机器翻译场景。输入序列 “Hello world this is a test” 需要被转换为 “Bonjour monde ceci est un test”。注意两个关键挑战：\n挑战一：长度不匹配\n输入和输出的长度可能不同。在更复杂的语言对中，比如英语到德语，这种差异更明显。\n挑战二：结构不对齐\n“this is” 对应 “ceci est”，词序相同，但这是幸运的情况。英语中的 “not only… but also” 在中文里可能需要完全重组语序。\n数学上，序列转导问题可以形式化为：给定输入序列 $\\mathbf{x} = (x_1, x_2, \\ldots, x_T)$，找到最可能的输出序列 $\\mathbf{y} = (y_1, y_2, \\ldots, y_{T’})$，其中 $T$ 和 $T’$ 可以不同。\n我们要求的是条件概率：\n$$P(y_1, y_2, \\ldots, y_{T’} \\mid x_1, x_2, \\ldots, x_T)$$\n根据链式法则，这个联合概率可以分解为：\n$$P(\\mathbf{y} \\mid \\mathbf{x}) = \\prod_{t=1}^{T’} P(y_t \\mid y_1, \\ldots, y_{t-1}, \\mathbf{x})$$\n这意味着，生成第 $t$ 个输出词时，我们需要考虑：\n已经生成的所有前面的词 $y_1, \\ldots, y_{t-1}$（自回归性质） 整个输入序列 $\\mathbf{x}$（条件性质） 1.2 RNN 的局限 循环神经网络（RNN）似乎是为序列数据而生的。它们通过隐藏状态 $h_t$ 传递历史信息：\n$$h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b)$$\n其中 $f$ 是激活函数（通常是 $\\tanh$ 或 ReLU），$W_{hh}$ 和 $W_{xh}$ 是权重矩阵，$b$ 是偏置。\n但标准 RNN 有两个致命弱点：\n梯度消失\n当序列很长时，反向传播的梯度需要经过很多时间步的连乘。如果激活函数的导数小于 1，梯度会指数级衰减。如左图所示，梯度在时间步 20 时已经衰减到接近零，这意味着模型几乎学不到远距离的依赖关系。\n数学上，损失函数 $L$ 对早期隐藏状态的梯度为：\n$$\\frac{\\partial L}{\\partial h_1} = \\frac{\\partial L}{\\partial h_T} \\prod_{t=2}^{T} \\frac{\\partial h_t}{\\partial h_{t-1}}$$\n如果每个 Jacobian 矩阵的范数小于 1，这个乘积会随着 $T$ 增大而指数衰减。\n长程依赖困难\n右图展示了信息保留率随序列距离的衰减。当两个相关词相距超过 20 个词时，模型能保留的信息已经不足 20%。这对于理解长文档或保持对话一致性是致命的。\n1.3 LSTM：为长序列而生 长短期记忆网络（LSTM）通过引入门控机制解决了这些问题。不再让每个隐藏状态都直接参与计算，LSTM 引入了一个专门的细胞状态（Cell State）$C_t$ 来传递长期信息。\nLSTM 的核心是三个门：\n遗忘门 $f_t$：决定从细胞状态中丢弃什么信息 $$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n输入门 $i_t$：决定什么新信息存入细胞状态 $$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n候选状态 $\\tilde{C}_t$：生成新的候选值 $$\\tilde{C}t = \\tanh(W_C \\cdot [h{t-1}, x_t] + b_C)$$\n更新细胞状态： $$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n其中 $\\odot$ 表示逐元素乘法（Hadamard 积）。\n输出门 $o_t$：决定基于细胞状态输出什么 $$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n$$h_t = o_t \\odot \\tanh(C_t)$$\n关键突破在于：细胞状态的更新几乎是线性的（只有逐元素乘法和加法），梯度可以更容易地反向传播，有效缓解了梯度消失问题。\n第二章：编码器-解码器架构 2.1 思想的突破 面对序列转导问题，Seq2Seq 的核心洞察是：将一个可变长度的序列压缩成一个固定维度的向量，再从这个向量解码出另一个可变长度的序列。\n这就像一个口译员：先听完一整段话（编码），理解其含义，然后用另一种语言复述出来（解码）。\n上图展示了 Seq2Seq 的基本架构，分为三个部分：\n编码器（Encoder）：由多层 LSTM 组成，从左到右（或从右到左）读取输入序列。每个时间步 $t$，编码器读取一个词 $x_t$，更新其隐藏状态。最终，最后一个隐藏状态（或最后一个细胞状态）被用作整个输入序列的上下文向量（Context Vector）$c$。\n上下文向量：固定维度的向量 $c \\in \\mathbb{R}^d$，编码了整个输入序列的语义信息。它是连接编码器和解码器的桥梁。\n解码器（Decoder）：另一个 LSTM（通常是独立的参数集合），以上下文向量 $c$ 为初始状态，逐词生成输出序列。每个时间步，解码器输出一个词 $y_t$，并将其作为下一个时间步的输入，直到生成特殊的结束标记 $\\langle\\text{EOS}\\rangle$。\n2.2 数学形式化 让我们用数学语言精确描述这个过程。\n编码器：\n对于输入序列 $\\mathbf{x} = (x_1, \\ldots, x_T)$，编码器 LSTM 计算：\n$$h_t^{\\text{enc}} = \\text{LSTM}{\\text{enc}}(h{t-1}^{\\text{enc}}, x_t), \\quad t = 1, \\ldots, T$$\n上下文向量通常取最后一个隐藏状态：\n$$c = h_T^{\\text{enc}}$$\n解码器：\n解码器以 $c$ 为初始状态，逐词生成输出：\n$$h_t^{\\text{dec}} = \\text{LSTM}{\\text{dec}}(h{t-1}^{\\text{dec}}, y_{t-1}), \\quad h_0^{\\text{dec}} = c$$\n输出词的概率分布通过 Softmax 获得：\n$$P(y_t \\mid y_{","wordCount":"763","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/seq2seq-cover.jpg","datePublished":"2026-01-30T09:00:00+08:00","dateModified":"2026-01-30T09:00:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-30-seq2seq-paper-explained/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">AI 论文解读系列：Seq2Seq--从序列到序列的革命</h1><div class=post-description>深入浅出解读 Seq2Seq 论文，从机器翻译的困境到编码器-解码器架构的突破，揭示深度学习处理序列数据的核心思想。</div><div class=post-meta><span title='2026-01-30 09:00:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>763 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/seq2seq-cover.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/seq2seq-cover.jpg alt="Seq2Seq 神经网络抽象图"></a><figcaption>神经网络连接的艺术</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e7%bf%bb%e8%af%91%e7%9a%84%e5%9b%b0%e5%a2%83 aria-label=引言：翻译的困境>引言：翻译的困境</a></li><li><a href=#%e7%ac%ac%e4%b8%80%e7%ab%a0%e5%ba%8f%e5%88%97%e8%bd%ac%e5%af%bc%e9%97%ae%e9%a2%98 aria-label=第一章：序列转导问题>第一章：序列转导问题</a><ul><li><a href=#11-%e4%bb%80%e4%b9%88%e8%ae%a9%e5%ba%8f%e5%88%97%e6%95%b0%e6%8d%ae%e7%89%b9%e6%ae%8a aria-label="1.1 什么让序列数据特殊">1.1 什么让序列数据特殊</a></li><li><a href=#12-rnn-%e7%9a%84%e5%b1%80%e9%99%90 aria-label="1.2 RNN 的局限">1.2 RNN 的局限</a></li><li><a href=#13-lstm%e4%b8%ba%e9%95%bf%e5%ba%8f%e5%88%97%e8%80%8c%e7%94%9f aria-label="1.3 LSTM：为长序列而生">1.3 LSTM：为长序列而生</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e7%ab%a0%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8%e6%9e%b6%e6%9e%84 aria-label=第二章：编码器-解码器架构>第二章：编码器-解码器架构</a><ul><li><a href=#21-%e6%80%9d%e6%83%b3%e7%9a%84%e7%aa%81%e7%a0%b4 aria-label="2.1 思想的突破">2.1 思想的突破</a></li><li><a href=#22-%e6%95%b0%e5%ad%a6%e5%bd%a2%e5%bc%8f%e5%8c%96 aria-label="2.2 数学形式化">2.2 数学形式化</a></li><li><a href=#23-%e8%be%93%e5%85%a5%e5%8f%8d%e8%bd%ac%e7%9a%84%e6%8a%80%e5%b7%a7 aria-label="2.3 输入反转的技巧">2.3 输入反转的技巧</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e7%ab%a0%e8%ae%ad%e7%bb%83%e4%b8%8e%e6%8e%a8%e7%90%86 aria-label=第三章：训练与推理>第三章：训练与推理</a><ul><li><a href=#31-%e5%a4%a7%e8%a7%84%e6%a8%a1%e8%ae%ad%e7%bb%83 aria-label="3.1 大规模训练">3.1 大规模训练</a></li><li><a href=#32-%e6%9d%9f%e6%90%9c%e7%b4%a2%e8%a7%a3%e7%a0%81 aria-label="3.2 束搜索解码">3.2 束搜索解码</a></li><li><a href=#33-%e5%ae%9e%e9%aa%8c%e7%bb%93%e6%9e%9c aria-label="3.3 实验结果">3.3 实验结果</a></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e7%ab%a0%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%9a%84%e9%bb%8e%e6%98%8e aria-label=第四章：注意力的黎明>第四章：注意力的黎明</a><ul><li><a href=#41-%e4%bf%a1%e6%81%af%e7%93%b6%e9%a2%88 aria-label="4.1 信息瓶颈">4.1 信息瓶颈</a></li><li><a href=#42-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e5%bc%95%e5%85%a5 aria-label="4.2 注意力机制的引入">4.2 注意力机制的引入</a></li><li><a href=#43-%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%9a%84%e5%8f%af%e8%a7%86%e5%8c%96 aria-label="4.3 注意力的可视化">4.3 注意力的可视化</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e7%ab%a0seq2seq-%e7%9a%84%e9%81%97%e4%ba%a7 aria-label="第五章：Seq2Seq 的遗产">第五章：Seq2Seq 的遗产</a><ul><li><a href=#51-%e8%b6%85%e8%b6%8a%e6%9c%ba%e5%99%a8%e7%bf%bb%e8%af%91 aria-label="5.1 超越机器翻译">5.1 超越机器翻译</a></li><li><a href=#52-%e9%80%9a%e5%90%91-transformer aria-label="5.2 通向 Transformer">5.2 通向 Transformer</a></li><li><a href=#53-%e6%a0%b8%e5%bf%83%e6%b4%9e%e8%a7%81%e5%9b%9e%e9%a1%be aria-label="5.3 核心洞见回顾">5.3 核心洞见回顾</a></li></ul></li><li><a href=#%e7%bb%93%e8%af%ad aria-label=结语>结语</a></li></ul></div></details></div><div class=post-content><h2 id=引言翻译的困境>引言：翻译的困境<a hidden class=anchor aria-hidden=true href=#引言翻译的困境>#</a></h2><p>想象一下，你正在学习一门外语。当你听到一句法语 &ldquo;Bonjour le monde&rdquo; 时，你的大脑是如何将其转化为英语 &ldquo;Hello world&rdquo; 的？</p><p>这不是简单的逐词替换。&ldquo;Bonjour&rdquo; 对应 &ldquo;Hello&rdquo;，但 &ldquo;le monde&rdquo; 是 &ldquo;the world&rdquo; 的倒序。词序不同，语法结构不同，甚至可能一个词对应多个词。传统的机器翻译系统使用基于规则的方法或统计模型，需要大量的人工特征工程和复杂的对齐算法。</p><p>2014年，Ilya Sutskever、Oriol Vinyals 和 Quoc Le 在 Google 发表了一篇改变游戏规则的论文：&ldquo;Sequence to Sequence Learning with Neural Networks&rdquo;。他们提出的 Seq2Seq 架构，用一个统一的神经网络模型取代了复杂的流水线，让机器翻译的准确率跃升到了新的高度。</p><p>但这篇论文的意义远不止于翻译。它开创了<strong>序列转导</strong>（Sequence Transduction）这一全新的学习范式，为后来的注意力机制、Transformer 乃至大语言模型奠定了基础。</p><h2 id=第一章序列转导问题>第一章：序列转导问题<a hidden class=anchor aria-hidden=true href=#第一章序列转导问题>#</a></h2><h3 id=11-什么让序列数据特殊>1.1 什么让序列数据特殊<a hidden class=anchor aria-hidden=true href=#11-什么让序列数据特殊>#</a></h3><p>在深入 Seq2Seq 之前，让我们先理解序列数据的本质。</p><p>传统的机器学习任务，比如图像分类或房价预测，输入和输出的维度是固定的。一张图片永远是 $224 \times 224 \times 3$ 的像素矩阵，一套房子的特征永远是卧室数、面积、位置等固定字段。</p><p>但序列数据不同：</p><ul><li>一句话可能有 5 个词，也可能有 50 个词</li><li>源语言和目标语言的词序可能不同</li><li>一个概念可能用一个词表达，也可能用多个词</li></ul><p><img alt=序列转导问题 loading=lazy src=/images/plots/sequence-transduction.png></p><p>上图展示了一个典型的机器翻译场景。输入序列 &ldquo;Hello world this is a test&rdquo; 需要被转换为 &ldquo;Bonjour monde ceci est un test&rdquo;。注意两个关键挑战：</p><p><strong>挑战一：长度不匹配</strong><br>输入和输出的长度可能不同。在更复杂的语言对中，比如英语到德语，这种差异更明显。</p><p><strong>挑战二：结构不对齐</strong><br>&ldquo;this is&rdquo; 对应 &ldquo;ceci est&rdquo;，词序相同，但这是幸运的情况。英语中的 &ldquo;not only&mldr; but also&rdquo; 在中文里可能需要完全重组语序。</p><p>数学上，序列转导问题可以形式化为：给定输入序列 $\mathbf{x} = (x_1, x_2, \ldots, x_T)$，找到最可能的输出序列 $\mathbf{y} = (y_1, y_2, \ldots, y_{T&rsquo;})$，其中 $T$ 和 $T&rsquo;$ 可以不同。</p><p>我们要求的是条件概率：</p><p>$$P(y_1, y_2, \ldots, y_{T&rsquo;} \mid x_1, x_2, \ldots, x_T)$$</p><p>根据链式法则，这个联合概率可以分解为：</p><p>$$P(\mathbf{y} \mid \mathbf{x}) = \prod_{t=1}^{T&rsquo;} P(y_t \mid y_1, \ldots, y_{t-1}, \mathbf{x})$$</p><p>这意味着，生成第 $t$ 个输出词时，我们需要考虑：</p><ul><li>已经生成的所有前面的词 $y_1, \ldots, y_{t-1}$（自回归性质）</li><li>整个输入序列 $\mathbf{x}$（条件性质）</li></ul><h3 id=12-rnn-的局限>1.2 RNN 的局限<a hidden class=anchor aria-hidden=true href=#12-rnn-的局限>#</a></h3><p>循环神经网络（RNN）似乎是为序列数据而生的。它们通过隐藏状态 $h_t$ 传递历史信息：</p><p>$$h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b)$$</p><p>其中 $f$ 是激活函数（通常是 $\tanh$ 或 ReLU），$W_{hh}$ 和 $W_{xh}$ 是权重矩阵，$b$ 是偏置。</p><p>但标准 RNN 有两个致命弱点：</p><p><img alt="RNN 局限性" loading=lazy src=/images/plots/rnn-limitations.png></p><p><strong>梯度消失</strong><br>当序列很长时，反向传播的梯度需要经过很多时间步的连乘。如果激活函数的导数小于 1，梯度会指数级衰减。如左图所示，梯度在时间步 20 时已经衰减到接近零，这意味着模型几乎学不到远距离的依赖关系。</p><p>数学上，损失函数 $L$ 对早期隐藏状态的梯度为：</p><p>$$\frac{\partial L}{\partial h_1} = \frac{\partial L}{\partial h_T} \prod_{t=2}^{T} \frac{\partial h_t}{\partial h_{t-1}}$$</p><p>如果每个 Jacobian 矩阵的范数小于 1，这个乘积会随着 $T$ 增大而指数衰减。</p><p><strong>长程依赖困难</strong><br>右图展示了信息保留率随序列距离的衰减。当两个相关词相距超过 20 个词时，模型能保留的信息已经不足 20%。这对于理解长文档或保持对话一致性是致命的。</p><h3 id=13-lstm为长序列而生>1.3 LSTM：为长序列而生<a hidden class=anchor aria-hidden=true href=#13-lstm为长序列而生>#</a></h3><p>长短期记忆网络（LSTM）通过引入<strong>门控机制</strong>解决了这些问题。不再让每个隐藏状态都直接参与计算，LSTM 引入了一个专门的<strong>细胞状态</strong>（Cell State）$C_t$ 来传递长期信息。</p><p><img alt="LSTM 单元结构" loading=lazy src=/images/plots/lstm-cell-structure.png></p><p>LSTM 的核心是三个门：</p><p><strong>遗忘门</strong> $f_t$：决定从细胞状态中丢弃什么信息
$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$</p><p><strong>输入门</strong> $i_t$：决定什么新信息存入细胞状态
$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$</p><p><strong>候选状态</strong> $\tilde{C}_t$：生成新的候选值
$$\tilde{C}<em>t = \tanh(W_C \cdot [h</em>{t-1}, x_t] + b_C)$$</p><p><strong>更新细胞状态</strong>：
$$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$$</p><p>其中 $\odot$ 表示逐元素乘法（Hadamard 积）。</p><p><strong>输出门</strong> $o_t$：决定基于细胞状态输出什么
$$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$</p><p>$$h_t = o_t \odot \tanh(C_t)$$</p><p>关键突破在于：细胞状态的更新几乎是线性的（只有逐元素乘法和加法），梯度可以更容易地反向传播，有效缓解了梯度消失问题。</p><h2 id=第二章编码器-解码器架构>第二章：编码器-解码器架构<a hidden class=anchor aria-hidden=true href=#第二章编码器-解码器架构>#</a></h2><h3 id=21-思想的突破>2.1 思想的突破<a hidden class=anchor aria-hidden=true href=#21-思想的突破>#</a></h3><p>面对序列转导问题，Seq2Seq 的核心洞察是：<strong>将一个可变长度的序列压缩成一个固定维度的向量，再从这个向量解码出另一个可变长度的序列</strong>。</p><p>这就像一个口译员：先听完一整段话（编码），理解其含义，然后用另一种语言复述出来（解码）。</p><p><img alt="Seq2Seq 架构" loading=lazy src=/images/plots/seq2seq-architecture.png></p><p>上图展示了 Seq2Seq 的基本架构，分为三个部分：</p><p><strong>编码器（Encoder）</strong>：由多层 LSTM 组成，从左到右（或从右到左）读取输入序列。每个时间步 $t$，编码器读取一个词 $x_t$，更新其隐藏状态。最终，最后一个隐藏状态（或最后一个细胞状态）被用作整个输入序列的<strong>上下文向量</strong>（Context Vector）$c$。</p><p><strong>上下文向量</strong>：固定维度的向量 $c \in \mathbb{R}^d$，编码了整个输入序列的语义信息。它是连接编码器和解码器的桥梁。</p><p><strong>解码器（Decoder）</strong>：另一个 LSTM（通常是独立的参数集合），以上下文向量 $c$ 为初始状态，逐词生成输出序列。每个时间步，解码器输出一个词 $y_t$，并将其作为下一个时间步的输入，直到生成特殊的结束标记 $\langle\text{EOS}\rangle$。</p><h3 id=22-数学形式化>2.2 数学形式化<a hidden class=anchor aria-hidden=true href=#22-数学形式化>#</a></h3><p>让我们用数学语言精确描述这个过程。</p><p><strong>编码器</strong>：</p><p>对于输入序列 $\mathbf{x} = (x_1, \ldots, x_T)$，编码器 LSTM 计算：</p><p>$$h_t^{\text{enc}} = \text{LSTM}<em>{\text{enc}}(h</em>{t-1}^{\text{enc}}, x_t), \quad t = 1, \ldots, T$$</p><p>上下文向量通常取最后一个隐藏状态：</p><p>$$c = h_T^{\text{enc}}$$</p><p><strong>解码器</strong>：</p><p>解码器以 $c$ 为初始状态，逐词生成输出：</p><p>$$h_t^{\text{dec}} = \text{LSTM}<em>{\text{dec}}(h</em>{t-1}^{\text{dec}}, y_{t-1}), \quad h_0^{\text{dec}} = c$$</p><p>输出词的概率分布通过 Softmax 获得：</p><p>$$P(y_t \mid y_{&lt;t}, \mathbf{x}) = \text{Softmax}(W_{\text{out}} h_t^{\text{dec}} + b_{\text{out}})$$</p><p>训练时，我们使用<strong>教师强制</strong>（Teacher Forcing）：解码器的输入不是它自己上一时刻的预测，而是真实的标签 $y_{t-1}^*$。这加速了训练收敛。</p><p>损失函数是交叉熵：</p><p>$$\mathcal{L} = -\sum_{t=1}^{T&rsquo;} \log P(y_t^* \mid y_{&lt;t}^*, \mathbf{x})$$</p><h3 id=23-输入反转的技巧>2.3 输入反转的技巧<a hidden class=anchor aria-hidden=true href=#23-输入反转的技巧>#</a></h3><p>论文中有一个看似奇怪但极其有效的技巧：<strong>将输入序列的词序反转</strong>。</p><p>例如，不是输入 &ldquo;A B C D&rdquo;，而是输入 &ldquo;D C B A&rdquo;。</p><p>为什么这有效？</p><p>考虑从法语 &ldquo;Je vais à l&rsquo;école&rdquo; 翻译到英语 &ldquo;I go to school&rdquo;。如果不反转：</p><ul><li>编码器先看到 &ldquo;Je&rdquo;，它的最后隐藏状态（编码整个句子的向量）距离 &ldquo;Je&rdquo; 很远</li><li>解码器需要先生成 &ldquo;I&rdquo;，但 &ldquo;I&rdquo; 对应的是 &ldquo;Je&rdquo;，而 &ldquo;Je&rdquo; 的信息在上下文向量中已经"稀释"了</li></ul><p>反转后：</p><ul><li>编码器最后看到的是 &ldquo;Je&rdquo;（现在是第一个词）</li><li>解码器首先生成 &ldquo;I&rdquo;，上下文向量中保留了更多关于 &ldquo;Je&rdquo; 的信息</li></ul><p>这种简单的技巧在 WMT'14 英法语翻译任务上将 BLEU 分数从 25.9 提升到 30.6，提升了近 5 个点！</p><h2 id=第三章训练与推理>第三章：训练与推理<a hidden class=anchor aria-hidden=true href=#第三章训练与推理>#</a></h2><h3 id=31-大规模训练>3.1 大规模训练<a hidden class=anchor aria-hidden=true href=#31-大规模训练>#</a></h3><p>Seq2Seq 的成功离不开大规模训练。论文使用了两组配置：</p><p><strong>深层 LSTM</strong>：4 层编码器 + 4 层解码器，每层 1000 个隐藏单元。这比当时常用的 1-2 层 RNN 深得多。</p><p><strong>词嵌入</strong>：输入词被映射为 1000 维的稠密向量。这些嵌入与网络一起端到端训练。</p><p><strong>正则化</strong>：</p><ul><li>Dropout：在输入到 LSTM 的循环连接上应用 0.2 的 Dropout</li><li>梯度裁剪：将梯度范数限制在 5 以内，防止梯度爆炸</li></ul><p><strong>优化</strong>：</p><ul><li>使用 SGD 带动量（初始学习率 0.7，每轮衰减）</li><li>批量大小 128</li><li>训练 7.5 轮（约 3.5 天在 8 块 NVIDIA K80 GPU 上）</li></ul><h3 id=32-束搜索解码>3.2 束搜索解码<a hidden class=anchor aria-hidden=true href=#32-束搜索解码>#</a></h3><p>训练时，我们知道真实的标签，可以使用教师强制。但推理时，我们需要模型自己生成整个序列。</p><p><strong>贪婪解码</strong>：每一步选择概率最高的词。简单但可能陷入局部最优——早期的错误会传播到后续。</p><p><strong>束搜索</strong>（Beam Search）：维护 $k$ 个最可能的候选序列（$k$ 是束宽，通常 5-10）。</p><p>算法流程：</p><ol><li>初始化：只有一个候选（开始标记），分数为 0</li><li>每一步：<ul><li>对每个候选，扩展出词汇表中所有可能的下一个词</li><li>计算新分数：$\text{score} + \log P(y_t \mid y_{&lt;t}, \mathbf{x})$</li><li>保留分数最高的 $k$ 个候选</li></ul></li><li>当候选生成结束标记时，将其加入最终候选集</li><li>返回分数最高的完整序列</li></ol><p>束搜索允许模型在早期"探索"不同的路径，避免贪婪策略的短视。论文中使用束宽 2 就显著提升了翻译质量。</p><h3 id=33-实验结果>3.3 实验结果<a hidden class=anchor aria-hidden=true href=#33-实验结果>#</a></h3><p>在 WMT'14 英语到法语翻译任务上，Seq2Seq 取得了突破性结果：</p><p><img alt="BLEU 分数对比" loading=lazy src=/images/plots/bleu-score-comparison.png></p><ul><li><strong>SMT Baseline</strong>：30.6 BLEU（当时的统计机器翻译系统）</li><li><strong>Neural LM</strong>：31.5 BLEU（仅使用神经语言模型）</li><li><strong>RNN Encoder-Decoder</strong>：31.8 BLEU（浅层 RNN）</li><li><strong>Seq2Seq + LSTM + Reverse</strong>：<strong>34.8 BLEU</strong></li></ul><p>最值得注意的是，简单的 Seq2Seq 模型（34.8 BLEU）已经超过了 WMT'14 比赛的最佳提交（33.3 BLEU），后者是复杂的集成系统，使用了大量人工特征。</p><p>当使用集成学习（5 个独立训练的模型投票）时，分数进一步提升到 <strong>36.5 BLEU</strong>。</p><h2 id=第四章注意力的黎明>第四章：注意力的黎明<a hidden class=anchor aria-hidden=true href=#第四章注意力的黎明>#</a></h2><h3 id=41-信息瓶颈>4.1 信息瓶颈<a hidden class=anchor aria-hidden=true href=#41-信息瓶颈>#</a></h3><p>Seq2Seq 有一个根本性的限制：<strong>上下文向量 $c$ 的维度是固定的</strong>，无论输入序列多长，都被压缩成同样大小的向量。</p><p>这就像让一个人听一整本书，然后只凭记忆复述。对于短段落可能还行，但对于长篇大论，信息必然丢失。</p><h3 id=42-注意力机制的引入>4.2 注意力机制的引入<a hidden class=anchor aria-hidden=true href=#42-注意力机制的引入>#</a></h3><p>2015年，Dzmitry Bahdanau 等人提出了<strong>注意力机制</strong>（Attention Mechanism），解决了这个问题。</p><p>核心思想：<strong>解码器在生成每个词时，动态地"关注"输入序列的不同部分</strong>。</p><p><img alt=注意力机制 loading=lazy src=/images/plots/attention-mechanism.png></p><p>上图展示了解码器生成 &ldquo;jour&rdquo; 时的注意力分布。编码器隐藏状态 $h_1, \ldots, h_4$ 分别对应输入词 &ldquo;SOS&rdquo;, &ldquo;Bon&rdquo;, &ldquo;bon&rdquo;, &ldquo;jour&rdquo;。注意力权重 $\alpha_{ti}$ 表示生成第 $t$ 个输出词时，应该给予第 $i$ 个输入隐藏状态多少关注。</p><p><strong>上下文向量的计算</strong>：</p><p>不再是固定的 $c = h_T^{\text{enc}}$，而是对每个解码步骤 $t$ 动态计算：</p><p>$$c_t = \sum_{i=1}^{T} \alpha_{ti} h_i^{\text{enc}}$$</p><p><strong>注意力权重</strong>：</p><p>$$\alpha_{ti} = \frac{\exp(e_{ti})}{\sum_{j=1}^{T} \exp(e_{tj})}$$</p><p>其中 $e_{ti}$ 是<strong>对齐分数</strong>（Alignment Score），衡量解码器状态 $s_{t-1}$ 与编码器隐藏状态 $h_i$ 的"匹配程度"：</p><p>$$e_{ti} = v_a^{\top} \tanh(W_s s_{t-1} + W_h h_i)$$</p><p>这被称为<strong>加性注意力</strong>（Additive Attention）或 Bahdanau 注意力。</p><p><strong>解码器更新</strong>：</p><p>现在解码器的输入不仅包含上一时刻的预测，还包含注意力加权后的上下文：</p><p>$$s_t = \text{LSTM}<em>{\text{dec}}(s</em>{t-1}, [y_{t-1}; c_t])$$</p><p>其中 $[\cdot; \cdot]$ 表示向量拼接。</p><h3 id=43-注意力的可视化>4.3 注意力的可视化<a hidden class=anchor aria-hidden=true href=#43-注意力的可视化>#</a></h3><p>注意力机制的一个美妙之处在于<strong>可解释性</strong>。通过可视化注意力权重矩阵，我们可以看到模型是如何"对齐"源语言和目标语言的。</p><p>例如，在翻译 &ldquo;the cat sat on the mat&rdquo; 到法语时：</p><ul><li>生成 &ldquo;le&rdquo; 时，注意力集中在 &ldquo;the&rdquo;（第一个）</li><li>生成 &ldquo;chat&rdquo; 时，注意力集中在 &ldquo;cat&rdquo;</li><li>生成 &ldquo;tapis&rdquo; 时，注意力集中在 &ldquo;mat&rdquo;</li></ul><p>这种软对齐（Soft Alignment）比传统统计机器翻译的硬对齐（Hard Alignment）更加灵活，能够处理词序差异和一对多映射。</p><h2 id=第五章seq2seq-的遗产>第五章：Seq2Seq 的遗产<a hidden class=anchor aria-hidden=true href=#第五章seq2seq-的遗产>#</a></h2><h3 id=51-超越机器翻译>5.1 超越机器翻译<a hidden class=anchor aria-hidden=true href=#51-超越机器翻译>#</a></h3><p>Seq2Seq 架构很快被应用到各种序列转导任务：</p><p><strong>语音识别</strong>：将声学特征序列（语音）转录为文本序列。DeepSpeech、Listen, Attend and Spell 等系统都基于 Seq2Seq。</p><p><strong>文本摘要</strong>：将长文档压缩为简短摘要。注意力机制帮助模型识别原文中的关键句子。</p><p><strong>对话系统</strong>：生成自然语言回复。编码器理解用户输入，解码器生成回复。</p><p><strong>代码生成</strong>：将自然语言描述转换为程序代码。GitHub Copilot 的早期版本就使用了 Seq2Seq。</p><h3 id=52-通向-transformer>5.2 通向 Transformer<a hidden class=anchor aria-hidden=true href=#52-通向-transformer>#</a></h3><p>Seq2Seq + 注意力为 2017 年的 Transformer 奠定了基础。Transformer 进一步革新了：</p><p><strong>自注意力</strong>（Self-Attention）：不再只是解码器关注编码器，序列内的每个位置都可以关注其他所有位置。</p><p><strong>并行化</strong>：RNN/LSTM 必须顺序处理序列，而自注意力可以并行计算，大大加速了训练。</p><p><strong>多头注意力</strong>：使用多组注意力机制，捕捉不同类型的依赖关系。</p><p>Transformer 的提出催生了 BERT、GPT 系列，最终引领我们进入了大语言模型的时代。</p><h3 id=53-核心洞见回顾>5.3 核心洞见回顾<a hidden class=anchor aria-hidden=true href=#53-核心洞见回顾>#</a></h3><p>Seq2Seq 论文之所以经典，在于它简洁而深刻的核心思想：</p><ol><li><strong>统一框架</strong>：用一个端到端的神经网络替代复杂的流水线</li><li><strong>编码器-解码器</strong>：将可变输入压缩为固定向量，再扩展为可变输出</li><li><strong>深度与容量</strong>：更深的网络（4 层 LSTM）配合大规模数据，释放神经网络的潜力</li><li><strong>为序列设计</strong>：LSTM 的门控机制专门解决序列建模的梯度问题</li></ol><p>这些洞见不仅适用于 2014 年的机器翻译，也适用于今天的大语言模型。从 Seq2Seq 到 GPT-4，我们始终在解决同一个问题：<strong>如何让机器理解并生成人类语言</strong>。Seq2Seq 是这段旅程的重要里程碑。</p><h2 id=结语>结语<a hidden class=anchor aria-hidden=true href=#结语>#</a></h2><p>当我们今天与 ChatGPT 对话，或使用 Google 翻译阅读外文文献时，很少会想起 2014 年的那篇论文。但正是 Seq2Seq 开创的编码器-解码器范式，让神经网络真正开始理解序列数据的本质。</p><p>从固定长度的上下文向量，到动态注意力，再到完全基于注意力的 Transformer，这是一条清晰的技术演进路线。每一步都建立在前一步的基础之上，每一代模型都解决了前一代的局限。</p><p>Seq2Seq 告诉我们：有时候，突破性的想法并不需要复杂的数学。将输入反转、使用更深的网络、端到端训练——这些看似简单的技巧，组合在一起就能产生惊人的效果。</p><p>在深度学习的历史长河中，Seq2Seq 是一颗璀璨的明珠。它不仅解决了机器翻译的问题，更开启了一个新时代：序列到序列学习的时代。</p><hr><p><strong>参考文献</strong></p><ol><li><p>Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. <em>Advances in Neural Information Processing Systems</em>, 27.</p></li><li><p>Bahdanau, D., Cho, K., & Bengio, Y. (2015). Neural machine translation by jointly learning to align and translate. <em>International Conference on Learning Representations</em>.</p></li><li><p>Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. <em>Neural Computation</em>, 9(8), 1735-1780.</p></li><li><p>Vaswani, A., et al. (2017). Attention is all you need. <em>Advances in Neural Information Processing Systems</em>, 30.</p></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a></li><li><a href=https://s-ai-unix.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/>自然语言处理</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%AE%97%E6%B3%95/>算法</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-30-bert-paper-interpretation/><span class=title>« Prev</span><br><span>AI 论文解读系列：BERT - 预训练深度双向 Transformer 的革命</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-30-word2vec-paper-explained/><span class=title>Next »</span><br><span>AI 论文解读系列：Word2Vec - 词向量的革命</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：Seq2Seq--从序列到序列的革命 on x" href="https://x.com/intent/tweet/?text=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aSeq2Seq--%e4%bb%8e%e5%ba%8f%e5%88%97%e5%88%b0%e5%ba%8f%e5%88%97%e7%9a%84%e9%9d%a9%e5%91%bd&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-seq2seq-paper-explained%2f&amp;hashtags=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%2c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%2c%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%e5%a4%84%e7%90%86%2c%e7%ae%97%e6%b3%95"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：Seq2Seq--从序列到序列的革命 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-seq2seq-paper-explained%2f&amp;title=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aSeq2Seq--%e4%bb%8e%e5%ba%8f%e5%88%97%e5%88%b0%e5%ba%8f%e5%88%97%e7%9a%84%e9%9d%a9%e5%91%bd&amp;summary=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aSeq2Seq--%e4%bb%8e%e5%ba%8f%e5%88%97%e5%88%b0%e5%ba%8f%e5%88%97%e7%9a%84%e9%9d%a9%e5%91%bd&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-seq2seq-paper-explained%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：Seq2Seq--从序列到序列的革命 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-seq2seq-paper-explained%2f&title=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aSeq2Seq--%e4%bb%8e%e5%ba%8f%e5%88%97%e5%88%b0%e5%ba%8f%e5%88%97%e7%9a%84%e9%9d%a9%e5%91%bd"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：Seq2Seq--从序列到序列的革命 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-seq2seq-paper-explained%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>