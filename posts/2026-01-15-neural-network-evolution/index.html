<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>神经网络算法演进：从感知机到 Transformer 的七十年征程 | s-ai-unix's Blog</title><meta name=keywords content="神经网络,深度学习,机器学习"><meta name=description content="回顾神经网络七十年发展历程，从感知机到 Transformer，详解核心算法的数学原理"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-15-neural-network-evolution/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-15-neural-network-evolution/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-15-neural-network-evolution/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="神经网络算法演进：从感知机到 Transformer 的七十年征程"><meta property="og:description" content="回顾神经网络七十年发展历程，从感知机到 Transformer，详解核心算法的数学原理"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-15T23:55:00+08:00"><meta property="article:modified_time" content="2026-01-15T23:55:00+08:00"><meta property="article:tag" content="神经网络"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="机器学习"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/neural-network-evolution.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/neural-network-evolution.jpg"><meta name=twitter:title content="神经网络算法演进：从感知机到 Transformer 的七十年征程"><meta name=twitter:description content="回顾神经网络七十年发展历程，从感知机到 Transformer，详解核心算法的数学原理"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"神经网络算法演进：从感知机到 Transformer 的七十年征程","item":"https://s-ai-unix.github.io/posts/2026-01-15-neural-network-evolution/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"神经网络算法演进：从感知机到 Transformer 的七十年征程","name":"神经网络算法演进：从感知机到 Transformer 的七十年征程","description":"回顾神经网络七十年发展历程，从感知机到 Transformer，详解核心算法的数学原理","keywords":["神经网络","深度学习","机器学习"],"articleBody":"引言：智慧的萌芽 想象一下 1957 年的夏天，康奈尔大学的弗兰克·罗森布拉特（Frank Rosenblatt）在实验室里调试着一台早期的电子计算机。他正在实现一个大胆的想法——能否用数学模型模拟人类的大脑神经元？\n这个想法在当时看起来近乎荒谬。人类大脑由数百亿个神经元组成，神经元之间通过突触连接，形成了一个令人眩晕的复杂网络。但罗森布拉特相信，如果我们能理解单个神经元的基本工作原理，就能一步步构建出能够学习的智能系统。\n那时的学术界对机器学习充满怀疑。“机器怎么可能思考？\"——这是当时的主流声音。但罗森布拉特和他的同道们坚持了下来，用数学公式编织着最初的神经之梦。\n今天，当我们面对能够写出论文、创作艺术、驾驶汽车的深度学习系统时，很容易忘记这一切都始于一个简单的线性分类器。让我们放慢脚步，回顾这七十年的征程，感受数学的力量与思想的演进。\n一、感知机：神经网络的起点（1957） 时间：1957 年 - 弗兰克·罗森布拉特 (Frank Rosenblatt)\n历史的起点 1957 年，弗兰克·罗森布拉特在康奈尔航空实验室发明了感知机（Perceptron）。这是第一个能够学习的神经网络模型，被誉为\"机器学习的开端”。\n1962 年的《纽约客》杂志甚至专门报道了这个发明，称它为\"会思考的机器\"。那时的媒体兴奋中充满了对人工智能未来的无限遐想。\n数学形式 单个神经元的工作原理 一个感知机神经元接收 $d$ 维输入 $\\mathbf{x} = (x_1, x_2, \\ldots, x_d)^T$，每个输入对应一个权重 $w_i$，还有一个偏置 $b$。\n神经元的输出是输入的加权和，然后通过激活函数：\n$$ y = f(z) = f\\left(\\sum_{i=1}^{d} w_i x_i + b\\right) = f(w^T x + b) $$\n其中 $z = \\mathbf{w}^T \\mathbf{x} + b$ 是净输入（net input）。\n激活函数 在最初的感知机中，激活函数是符号函数（sign function）：\n$$ f(z) = \\begin{cases} 1 \u0026 \\text{if } z \\geq 0 \\ -1 \u0026 \\text{if } z \u003c 0 \\end{cases} $$\n因此，感知机是一个二元分类器。\n感知机的学习规则：Rosenblatt 规则 感知机的学习非常直观。给定一个训练样本 $(\\mathbf{x}_i, y_i)$，其中 $y_i \\in {-1, +1}$。\n预测值为：\n$$ \\hat{y}_i = \\text{sign}(\\mathbf{w}^T \\mathbf{x}_i + b) $$\n如果预测正确（$\\hat{y}_i = y_i$），不更新权重。\n如果预测错误，按以下规则更新：\n$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta y_i \\mathbf{x}_i $$\n$$ b \\leftarrow b + \\eta y_i $$\n其中 $\\eta$ 是学习率。\n这个规则被称为Rosenblatt 规则，是梯度下降的一个简化形式。\n感知机的局限性：异或问题 1969 年，明斯基和佩伯特在《感知机》一书中证明了感知机的致命弱点：它无法解决非线性可分的问题，最著名的例子就是异或（XOR）问题。\n异或问题的真值表：\n$x_1$ $x_2$ $x_1 \\oplus x_2$ $-1$ $-1$ $-1$ $-1$ $+1$ $+1$ $+1$ $-1$ $+1$ $+1$ $+1$ $-1$ 如果我们尝试用一条直线（决策边界）来分类这四个点，你会发现这是不可能的。因为单层感知机只能产生线性决策边界。\n这个发现一度让神经网络研究进入寒冬。直到 1980 年代，多层感知机和非线性激活函数的引入才打破了僵局。\n二、多层感知机与反向传播：深度学习的复兴（1986） 时间：1986 年 - 大卫·鲁梅尔哈特 (David Rumelhart) 等\n寒冬后的复苏 在感知机被证明无法解决 XOR 问题后，神经网络研究沉寂了近二十年。直到 1986 年，大卫·鲁梅尔哈特、杰弗里·辛顿（Geoffrey Hinton）和罗纳德·威廉姆斯（Ronald Williams）在《Nature》上发表了题为《通过误差反向传播学习表征》的论文，提出了反向传播算法（Backpropagation）。\n这篇论文开启了现代深度学习的大门。它解决的核心问题是：当网络有多层时，如何高效地计算梯度？\n数学推导：反向传播的核心思想 前向传播（Forward Propagation） 考虑一个多层感知机（MLP），包含：\n输入层：d 个神经元 隐藏层：m 个神经元 输出层：c 个神经元（c 个类别） 设 $W^{(1)} \\in \\mathbb{R}^{m \\times d}$ 是输入层到隐藏层的权重矩阵，$b^{(1)} \\in \\mathbb{R}^m$ 是隐藏层的偏置向量。\n设 $W^{(2)} \\in \\mathbb{R}^{c \\times m}$ 是隐藏层到输出层的权重矩阵，$b^{(2)} \\in \\mathbb{R}^c$ 是输出层的偏置向量。\n隐藏层的净输入：\n$$ z^{(1)} = W^{(1)} x + b^{(1)} $$\n隐藏层的输出（使用非线性激活函数，如 sigmoid）：\n$$ a^{(1)} = \\sigma(z^{(1)}) $$\n其中 $\\sigma$ 逐元素应用 sigmoid 函数：\n$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n输出层的净输入：\n$$ z^{(2)} = W^{(2)} a^{(1)} + b^{(2)} $$\n输出层的输出（使用 softmax，用于多分类）：\n$$ \\hat{y} = \\text{softmax}(z^{(2)}) $$\n其中 softmax 函数将 $c$ 个实数转换为概率分布：\n$$ \\text{softmax}(z)j = \\frac{e^{z_j}}{\\sum{k=1}^{c} e^{z_k}} $$\n损失函数：交叉熵 使用交叉熵损失：\n$$ L = -\\sum_{i=1}^{c} y_i \\log(\\hat{y}_i) $$\n其中 $y$ 是 one-hot 编码的真实标签。\n反向传播：链式法则 核心思想：使用链式法则（Chain Rule）计算损失函数对每个参数的梯度。\n首先计算输出层的误差：\n$$ \\delta^{(2)} = \\hat{y} - y $$\n这是 softmax 交叉熵损失对净输入的导数（一个优雅的简化）。\n输出层权重的梯度：\n$$ \\frac{\\partial L}{\\partial W^{(2)}} = \\delta^{(2)} (a^{(1)})^T $$\n输出层偏置的梯度：\n$$ \\frac{\\partial L}{\\partial b^{(2)}} = \\delta^{(2)} $$\n然后，将误差反向传播到隐藏层。隐藏层的误差是：\n$$ \\delta^{(1)} = (W^{(2)})^T \\delta^{(2)} \\odot \\sigma’(z^{(1)}) $$\n其中 $\\odot$ 是逐元素乘法，$\\sigma’$ 是 sigmoid 的导数：\n$$ \\sigma’(z) = \\sigma(z)(1 - \\sigma(z)) $$\n隐藏层权重的梯度：\n$$ \\frac{\\partial L}{\\partial W^{(1)}} = \\delta^{(1)} x^T $$\n隐藏层偏置的梯度：\n$$ \\frac{\\partial L}{\\partial b^{(1)}} = \\delta^{(1)} $$\n为什么称为\"反向传播\"？ 前向传播是从输入层到输出层：输入 $\\to$ 隐藏层 $\\to$ 输出层。\n反向传播是从输出层到输入层：输出误差 $\\to$ 隐藏层误差 $\\to$ 输入层误差。\n这就像在计算器中\"反向\"流动，因此得名。\n参数更新 使用梯度下降更新参数：\n$$ W^{(1)} \\leftarrow W^{(1)} - \\eta \\frac{\\partial L}{\\partial W^{(1)}} $$\n$$ b^{(1)} \\leftarrow b^{(1)} - \\eta \\frac{\\partial L}{\\partial b^{(1)}} $$\n$$ W^{(2)} \\leftarrow W^{(2)} - \\eta \\frac{\\partial L}{\\partial W^{(2)}} $$\n$$ b^{(2)} \\leftarrow b^{(2)} - \\eta \\frac{\\partial L}{\\partial b^{(2)}} $$\n其中 $\\eta$ 是学习率。\n三、卷积神经网络：感受野的智慧（1998-2012） 时间：1998 年 - LeNet-5；2012 年 - AlexNet\n从视觉感知到卷积 1998 年，杨·勒昆（Yann LeCun）和他的团队提出了 LeNet-5，这是第一个成功的卷积神经网络（Convolutional Neural Network, CNN）。它在 MNIST 手写数字识别任务上达到了当时的最先进水平。\n卷积神经网络的核心洞察来自对生物视觉系统的研究：人类视觉皮层的神经元具有局部感受野（receptive field），即每个神经元只响应视野中的一小部分区域，而不是整个视野。\n数学形式 卷积操作（Convolution） 卷积神经网络的核心是卷积层（Convolutional Layer）。给定输入特征图 $X \\in \\mathbb{R}^{H \\times W \\times C_{\\text{in}}}$（高 $H$、宽 $W$、输入通道数 $C_{\\text{in}}$），卷积核 $K \\in \\mathbb{R}^{k \\times k \\times C_{\\text{in}} \\times C_{\\text{out}}}$（高 $k$、宽 $k$、$C_{\\text{out}}$ 个输出通道），卷积操作定义为：\n$$ (X * K){i,j,o} = \\sum{c=1}^{C_{\\text{in}}} \\sum_{p=1}^{k} \\sum_{q=1}^{k} X_{i+p-1, j+q-1, c} \\cdot K_{p,q,c,o} $$\n其中 $*$ 表示卷积运算，$i, j$ 是输出特征图的空间坐标，$o$ 是输出通道索引。\n更简洁的矩阵表示：\n$$ Y = X * K $$\n其中 $Y \\in \\mathbb{R}^{H’ \\times W’ \\times C_{\\text{out}}}$ 是输出特征图（空间大小为 $H’ = H - k + 1$, $W’ = W - k + 1$）。\n池化层（Pooling Layer） 为了减少计算量和参数数量，同时增加平移不变性，CNN 引入了池化层（Pooling Layer）。最常用的是最大池化（Max Pooling）：\n$$ Y_{i,j,o} = \\max_{p,q ∈ N_{i,j}} X_{p,q,o} $$\n其中 $N_{i,j}$ 是位置 $(i, j)$ 附近的窗口（如 $2 \\times 2$）。\nLeNet-5 架构 LeNet-5 的架构包括：\n输入层：$32 imes 32 灰度图像 卷积层 C1：6 个 $5 imes 5 卷积核，输出 $28 imes 28 imes 6 池化层 S2：$2 imes 2 最大池化，输出 $14 imes 14 imes 6 卷积层 C3：16 个 5×5 卷积核，输出 $10 imes 10 imes 16 池化层 S4：2×2 最大池化，输出 $5 imes 5 imes 16 全连接层 F5：120 个神经元 全连接层 F6：84 个神经元 输出层 F7：10 个神经元（对应 10 个数字） AlexNet 的革命（2012） 2012 年，Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 提出了 AlexNet，在 ImageNet 大规模视觉识别挑战赛（ILSVRC-2012）上以压倒性优势夺冠。\nAlexNet 的创新点：\n使用 ReLU（Rectified Linear Unit）激活函数，加速收敛： $$f(z) = \\max(0, z)$$ 使用 Dropout 随机失活，防止过拟合： 训练时以概率 p 随机将神经元的输出设为 0 测试时使用所有神经元，但输出乘以 p 使用 数据增强（Data Augmentation）：随机裁剪、水平翻转等 使用 GPU 并行计算 ReLU 的导数 ReLU 函数的导数是：\n$$ \\frac{\\partial f(z)}{\\partial z} = \\begin{cases} 1 \u0026 \\text{if } z \u003e 0 \\ 0 \u0026 \\text{if } z \\leq 0 \\end{cases} $$\nReLU 的优点：\n计算效率高（无指数运算） 缓解梯度消失问题（正值梯度恒为 1） 四、循环神经网络：记忆的艺术（1990s） 时间：1990 年 - 1997 年 LSTM\n序列数据的挑战 前馈神经网络（如 MLP 和 CNN）假设输入和输出之间是独立的映射关系。但对于序列数据（如语言、语音、时间序列），当前时刻的输入依赖于历史信息。\n循环神经网络（Recurrent Neural Network, RNN）的核心思想是：神经网络的输出不仅取决于当前输入，还取决于隐藏状态（hidden state），后者记忆了过去的信息。\n数学形式 RNN 的基本结构 考虑一个时间序列 $x_1, x_2, \\ldots, x_T$，每个时间步 $t$ 的输入是 $x_t \\in \\mathbb{R}^d$。\nRNN 维护一个隐藏状态 $h_t \\in \\mathbb{R}^m$，按时间递归更新：\n$$ h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h) $$\n其中：\n$W_{xh} \\in \\mathbb{R}^{m \\times d}$ 是输入到隐藏的权重矩阵 $W_{hh} \\in \\mathbb{R}^{m \\times m}$ 是隐藏到隐藏的权重矩阵 $b_h \\in \\mathbb{R}^m$ 是隐藏层的偏置 $f$ 是激活函数（如 $\\tanh$ 或 ReLU） 每个时间步的输出是：\n$$ y_t = g(W_{hy} h_t + b_y) $$\n其中：\n$W_{hy} \\in \\mathbb{R}^{c \\times m}$ 是隐藏到输出的权重矩阵（$c$ 是输出维度） $b_y \\in \\mathbb{R}^c$ 是输出的偏置 $g$ 是输出激活函数（如 softmax） 展开的时间图 将 RNN 按时间展开，可以看到信息如何从 $t=1$ 传递到 $t=T$：\n$$ h_1 = f(W_{xh} x_1 + b_h) $$\n$$ h_2 = f(W_{xh} x_2 + W_{hh} h_1 + b_h) $$\n$$ h_3 = f(W_{xh} x_3 + W_{hh} h_2 + b_h) $$\n$$ \\vdots $$\n$$ h_T = f(W_{xh} x_T + W_{hh} h_{T-1} + b_h) $$\n可以看到，$h_T$ 依赖于所有之前的输入 $x_1, x_2, \\ldots, x_T$，这就是 RNN 的记忆机制。\n反向传播通过时间（BPTT） RNN 的训练需要考虑时间依赖性，梯度需要反向传播通过时间（Backpropagation Through Time, BPTT）。\n损失函数：\n$$ L = \\sum_{t=1}^{T} \\ell(y_t, \\hat{y}_t) $$\n其中 ℓ 是单个时间步的损失（如交叉熵）。\n通过链式法则计算梯度：\n$$ \\frac{\\partial L}{\\partial W_{hh}} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial W_{hh}} $$\n其中 $\\partial h_t/\\partial W_{hh}$ 是递归的：\n$$ \\frac{\\partial h_t}{\\partial W_{hh}} = \\sum_{k=t}^{T} \\prod_{j=k+1}^{t} f’(z_j) W_{hh} $$\n这个求和表明：$W_{hh}$ 的梯度依赖于所有时间步，导致梯度消失（vanishing gradient）或梯度爆炸（exploding gradient）问题。\ntanh 的导数 tanh 激活函数的导数：\n$$ \\frac{\\partial \\tanh(z)}{\\partial z} = 1 - \\tanh^2(z) \\leq 1 $$\n如果 $W_{hh}$ 的特征值都小于 1，乘积会趋于 0，导致梯度消失。\n五、LSTM：长记忆的解决方案（1997） 时间：1997 年 - Sepp Hochreiter 和 Jürgen Schmidhuber\n梯度消失的问题 在长序列中，RNN 的梯度会呈指数衰减或增长。考虑 tanh 激活函数的导数：\n$$ \\tanh’(z) = 1 - \\tanh^2(z) \\leq 1 $$\n如果 $W_{hh}$ 的特征值都小于 1，乘积会趋于 0，导致梯度消失。\nLSTM 的核心创新 1997 年，Sepp Hochreiter 和 Jürgen Schmidhuber 提出了长短期记忆网络（Long Short-Term Memory, LSTM），通过引入门控机制（gating mechanism）解决梯度消失问题。\nLSTM 的细胞状态（cell state）和隐藏状态（hidden state）分离：\n$$ c_t = \\text{遗忘门} \\odot c_{t-1} + \\text{输入门} \\odot \\tilde{c}_t $$\n$$ h_t = \\text{输出门} \\odot \\tanh(c_t) $$\n其中 $\\odot$ 是逐元素乘法，$\\tilde{c}_t$ 是候选细胞状态。\n遗忘门（Forget Gate） 遗忘门决定保留多少旧信息：\n$$ f_t = \\sigma(W_f [h_{t-1}, x_t] + b_f) $$\n其中 $\\sigma$ 是 sigmoid 函数，输出在 $[0, 1]$ 之间。\n输入门（Input Gate） 输入门决定写入多少新信息：\n$$ i_t = \\sigma(W_i [h_{t-1}, x_t] + b_i) $$\n候选细胞状态（Candidate Cell State） $$ \\tilde{c}t = \\tanh(W_c [h{t-1}, x_t] + b_c) $$\n输出门（Output Gate） 输出门决定输出多少信息到隐藏状态：\n$$ o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o) $$\n细胞状态更新 $$ c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t $$\n隐藏状态更新 $$ h_t = o_t \\odot \\tanh(c_t) $$\n为什么 LSTM 解决了梯度消失问题？ 关键在于细胞状态的更新：\n$$ \\frac{\\partial c_t}{\\partial c_{t-1}} = f_t $$\n如果遗忘门 $f_t$ 接近 1，梯度几乎无损地传播。门控机制让网络学会何时保留和何时遗忘信息。\n六、注意力机制：打破序列依赖（2017） 时间：2017 年 - Vaswani 等\n从循环到注意力 在 Transformer 出现之前，序列建模主要依赖 RNN 及其变体（LSTM、GRU）。但 RNN 有两个根本限制：\n顺序计算：必须从 $t=1$ 计算到 $t=T$，无法并行 长距离依赖：即使有 LSTM，信息仍难以从 $t=1$ 传递到 $t=T$ 2017 年，Vaswani 等人在《Attention Is All You Need》中提出了Transformer，完全摒弃了循环结构，只用注意力机制（Attention Mechanism）。\n数学形式：自注意力（Self-Attention） 考虑一个序列 $X = [x_1, x_2, \\ldots, x_T] \\in \\mathbb{R}^{T \\times d}$，其中 $T$ 是序列长度，$d$ 是嵌入维度。\nQuery、Key、Value Transformer 的核心是自注意力（Self-Attention）。对于每个位置，我们计算三组向量：\n$$ Q_i = x_i W^Q, \\quad K_i = x_i W^K, \\quad V_i = x_i W^V $$\n其中 $W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d_k}$ 是可学习的参数矩阵，$d_k$ 是查询/键/值的维度。\n注意力分数 对于位置 $i$ 和 $j$，计算注意力分数（缩放点积）：\n$$ \\text{score}_{ij} = \\frac{Q_i \\cdot K_j}{\\sqrt{d_k}} $$\n注意力权重 将分数转换为概率分布（使用 softmax）：\n$$ \\alpha_{ij} = \\frac{\\exp(\\text{score}{ij})}{\\sum{k=1}^{T} \\exp(\\text{score}_{ik})} $$\n加权求和 对值向量加权求和，得到输出：\n$$ z_i = \\sum_{j=1}^{T} \\alpha_{ij} V_j $$\n矩阵形式 可以写成矩阵形式：\n$$ Z = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V $$\n其中：\n$Q = X W^Q \\in \\mathbb{R}^{T \\times d_k}$ $K = X W^K \\in \\mathbb{R}^{T \\times d_k}$ $V = X W^V \\in \\mathbb{R}^{T \\times d_v}$ 为什么称为\"注意力\"？ $\\alpha_{ij}$ 表示位置 $i$ 对位置 $j$ 的\"关注程度\"。如果 $\\alpha_{ij}$ 接近 1，说明位置 $i$ 通常关注位置 $j$。\n多头注意力（Multi-Head Attention） 为了捕获不同类型的关系，Transformer 使用多头注意力（Multi-Head Attention）：\n$$ \\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, \\text{head}_2, …, \\text{head}_h) W^O $$\n其中每个头是独立的自注意力：\n$$ \\text{head}_i = \\text{Attention}(X W_i^Q, X W_i^K, X W_i^V) $$\n$W^O \\in \\mathbb{R}^{h \\times d_v \\times d_{\\text{model}}}$ 是输出投影矩阵，$d_{\\text{model}}$ 是模型维度。\n七、残差连接：深层网络的关键（2015） 时间：2015 年 - 何恺明等\n深度网络的训练困境 随着网络深度增加，我们遇到了两个问题：\n梯度消失：深层网络的梯度难以传播到早期层 退化问题：网络深度增加后，训练误差反而增加（即使没有过拟合） 2015 年，何恺明等人提出了残差连接（Residual Connection），解决了这个问题。\n数学形式 残差块（Residual Block） 普通层的映射是 $F(x, {W_i})$，残差块学习的是残差映射：\n$$ R(x, {W_i}) = F(x, {W_i}) - x $$\n其中 $F(x, {W_i})$ 是残差函数（通常由 2-3 个卷积层组成），${W_i}$ 是可学习的权重。\n残差块的输出是：\n$$ y = F(x, {W_i}) + x $$\n这被称为跳跃连接（skip connection）或快捷路径（shortcut path）。\n为什么有效？ 考虑 L 层残差网络。前向传播可以写成：\n$$ x_{l+1} = x_l + F(x_l, W_l) $$\n因此：\n$$ x_L = x_0 + \\sum_{l=0}^{L-1} F(x_l, W_l) $$\n这意味着梯度可以直接从第 L 层传播到第 0 层：\n$$ \\frac{\\partial L}{\\partial x_0} = \\frac{\\partial L}{\\partial x_L} · \\prod_{l=0}^{L-1} \\left(1 + \\frac{\\partial F(x_l, W_l)}{\\partial x_l}\\right) $$\n残差连接中的恒等映射（identity mapping）确保梯度至少为 1，解决了梯度消失问题。\n八、现代架构：大模型的前奏（2018） 时间：2018 年 - BERT；2020 年 - GPT-3；2022 年 - ChatGPT\n从 NLP 到通用智能 2018 年，Google 提出了BERT（Bidirectional Encoder Representations from Transformers），将预训练-微调（pre-training and fine-tuning）范式推向主流。\nBERT 的创新点：\n掩码语言模型（Masked Language Model, MLM）：随机掩盖输入 tokens 的 15%，让模型预测 下一句预测（Next Sentence Prediction, NSP）：预测两个句子是否相邻 双向编码：使用 Transformer 的编码器，同时看到左右上下文 2020 年，OpenAI 发布了GPT-3（Generative Pre-trained Transformer 3），展示了超大规模模型的涌现能力。\nGPT-3 的关键：\n参数规模：1750 亿参数 少样本学习（Few-shot Learning）：只需几个例子就能学会新任务 零样本学习（Zero-shot Learning）：无需任何例子 Transformer 的编码器-解码器架构 编码器（Encoder） 编码器处理输入序列，输出固定维度的表示：\n$$ Z = \\text{Encoder}(X) $$\n其中 $Z \\in \\mathbb{R}^{T \\times d_{\\text{model}}}$ 是编码后的表示。\n解码器（Decoder） 解码器生成输出序列：\n$$ \\hat{y}t = \\text{softmax}(z_t W{vocab}) $$\n其中 $W_{\\text{vocab}} \\in \\mathbb{R}^{d_{\\text{model}} \\times |\\text{Vocab}|}$ 是词表矩阵，$z_t$ 是解码器在时间 $t$ 的隐藏状态。\n编码器-解码器注意力 解码器通过交叉注意力（Cross-Attention）关注编码器的输出：\n$$ z_t = \\text{Attention}(Q_t, K, V) $$\n其中 $Q_t = z_{t-1} W^Q$ 是解码器的查询，$K = Z W^K$ 和 $V = Z W^V$ 是编码器的键和值。\n结语：从单神经元到通用智能 1957 年的感知机只是一个线性分类器。但七十年后的今天，我们有：\n数千亿参数的模型 能理解复杂语言 能生成艺术作品 能辅助科学发现 这七十年的征程，本质上是数学和思想的演进：\n感知机：理解单个神经元 反向传播：理解如何学习多层网络 卷积网络：理解空间结构 循环网络：理解时间依赖 LSTM：理解长期记忆 注意力：理解全局关系 残差连接：理解深层网络 预训练模型：理解大规模学习 每一个突破都建立在前人的基础上，用数学公式表达了对智能的新理解。\n今天的深度学习仍有很多未解之谜：如何实现真正的推理？如何获得常识？如何解释模型的决策？\n但回望过去，我们有理由相信：只要坚持用数学和实验探索未知，终有一天，我们会解开这些谜题，创造出真正的通用智能。\n参考文献：\nRosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. Rumelhart, D. E., Hinton, G. E., \u0026 Williams, R. J. (1986). “Learning representations by back-propagating errors”. Nature. LeCun, Y., Bottou, L., Bengio, Y., \u0026 Haffner, P. (1998). “Gradient-based learning applied to document recognition”. Proceedings of the IEEE. Krizhevsky, A., Sutskever, I., \u0026 Hinton, G. E. (2012). “ImageNet classification with deep convolutional neural networks”. NIPS. Hochreiter, S., \u0026 Schmidhuber, J. (1997). “Long short-term memory”. Neural Computation. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., \u0026 Polosukhin, I. (2017). “Attention is all you need”. NIPS. He, K., Zhang, X., Ren, S., \u0026 Sun, J. (2016). “Deep residual learning for image recognition”. CVPR. Devlin, J., Chang, M. W., Lee, K., \u0026 Toutanova, K. (2019). “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. NAACL-HLT. ","wordCount":"1578","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/neural-network-evolution.jpg","datePublished":"2026-01-15T23:55:00+08:00","dateModified":"2026-01-15T23:55:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-15-neural-network-evolution/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">神经网络算法演进：从感知机到 Transformer 的七十年征程</h1><div class=post-description>回顾神经网络七十年发展历程，从感知机到 Transformer，详解核心算法的数学原理</div><div class=post-meta><span title='2026-01-15 23:55:00 +0800 CST'>January 15, 2026</span>&nbsp;·&nbsp;<span>8 min</span>&nbsp;·&nbsp;<span>1578 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/neural-network-evolution.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/neural-network-evolution.jpg alt=抽象神经网络连接图></a><figcaption>智能网络的演进</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e6%99%ba%e6%85%a7%e7%9a%84%e8%90%8c%e8%8a%bd aria-label=引言：智慧的萌芽>引言：智慧的萌芽</a></li><li><a href=#%e4%b8%80%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e8%b5%b7%e7%82%b91957 aria-label=一、感知机：神经网络的起点（1957）>一、感知机：神经网络的起点（1957）</a><ul><li><a href=#%e5%8e%86%e5%8f%b2%e7%9a%84%e8%b5%b7%e7%82%b9 aria-label=历史的起点>历史的起点</a></li><li><a href=#%e6%95%b0%e5%ad%a6%e5%bd%a2%e5%bc%8f aria-label=数学形式>数学形式</a><ul><li><a href=#%e5%8d%95%e4%b8%aa%e7%a5%9e%e7%bb%8f%e5%85%83%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86 aria-label=单个神经元的工作原理>单个神经元的工作原理</a></li><li><a href=#%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0 aria-label=激活函数>激活函数</a></li><li><a href=#%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e5%ad%a6%e4%b9%a0%e8%a7%84%e5%88%99rosenblatt-%e8%a7%84%e5%88%99 aria-label="感知机的学习规则：Rosenblatt 规则">感知机的学习规则：Rosenblatt 规则</a></li><li><a href=#%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e5%b1%80%e9%99%90%e6%80%a7%e5%bc%82%e6%88%96%e9%97%ae%e9%a2%98 aria-label=感知机的局限性：异或问题>感知机的局限性：异或问题</a></li></ul></li></ul></li><li><a href=#%e4%ba%8c%e5%a4%9a%e5%b1%82%e6%84%9f%e7%9f%a5%e6%9c%ba%e4%b8%8e%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%a4%8d%e5%85%b41986 aria-label=二、多层感知机与反向传播：深度学习的复兴（1986）>二、多层感知机与反向传播：深度学习的复兴（1986）</a><ul><li><a href=#%e5%af%92%e5%86%ac%e5%90%8e%e7%9a%84%e5%a4%8d%e8%8b%8f aria-label=寒冬后的复苏>寒冬后的复苏</a></li><li><a href=#%e6%95%b0%e5%ad%a6%e6%8e%a8%e5%af%bc%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%9a%84%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3 aria-label=数学推导：反向传播的核心思想>数学推导：反向传播的核心思想</a><ul><li><a href=#%e5%89%8d%e5%90%91%e4%bc%a0%e6%92%adforward-propagation aria-label="前向传播（Forward Propagation）">前向传播（Forward Propagation）</a></li><li><a href=#%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e4%ba%a4%e5%8f%89%e7%86%b5 aria-label=损失函数：交叉熵>损失函数：交叉熵</a></li><li><a href=#%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e9%93%be%e5%bc%8f%e6%b3%95%e5%88%99 aria-label=反向传播：链式法则>反向传播：链式法则</a></li><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e7%a7%b0%e4%b8%ba%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad aria-label='为什么称为"反向传播"？'>为什么称为"反向传播"？</a></li><li><a href=#%e5%8f%82%e6%95%b0%e6%9b%b4%e6%96%b0 aria-label=参数更新>参数更新</a></li></ul></li></ul></li><li><a href=#%e4%b8%89%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%84%9f%e5%8f%97%e9%87%8e%e7%9a%84%e6%99%ba%e6%85%a71998-2012 aria-label=三、卷积神经网络：感受野的智慧（1998-2012）>三、卷积神经网络：感受野的智慧（1998-2012）</a><ul><li><a href=#%e4%bb%8e%e8%a7%86%e8%a7%89%e6%84%9f%e7%9f%a5%e5%88%b0%e5%8d%b7%e7%a7%af aria-label=从视觉感知到卷积>从视觉感知到卷积</a></li><li><a href=#%e6%95%b0%e5%ad%a6%e5%bd%a2%e5%bc%8f-1 aria-label=数学形式>数学形式</a><ul><li><a href=#%e5%8d%b7%e7%a7%af%e6%93%8d%e4%bd%9cconvolution aria-label=卷积操作（Convolution）>卷积操作（Convolution）</a></li><li><a href=#%e6%b1%a0%e5%8c%96%e5%b1%82pooling-layer aria-label="池化层（Pooling Layer）">池化层（Pooling Layer）</a></li><li><a href=#lenet-5-%e6%9e%b6%e6%9e%84 aria-label="LeNet-5 架构">LeNet-5 架构</a></li><li><a href=#alexnet-%e7%9a%84%e9%9d%a9%e5%91%bd2012 aria-label="AlexNet 的革命（2012）">AlexNet 的革命（2012）</a></li><li><a href=#relu-%e7%9a%84%e5%af%bc%e6%95%b0 aria-label="ReLU 的导数">ReLU 的导数</a></li></ul></li></ul></li><li><a href=#%e5%9b%9b%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e8%ae%b0%e5%bf%86%e7%9a%84%e8%89%ba%e6%9c%af1990s aria-label=四、循环神经网络：记忆的艺术（1990s）>四、循环神经网络：记忆的艺术（1990s）</a><ul><li><a href=#%e5%ba%8f%e5%88%97%e6%95%b0%e6%8d%ae%e7%9a%84%e6%8c%91%e6%88%98 aria-label=序列数据的挑战>序列数据的挑战</a></li><li><a href=#%e6%95%b0%e5%ad%a6%e5%bd%a2%e5%bc%8f-2 aria-label=数学形式>数学形式</a><ul><li><a href=#rnn-%e7%9a%84%e5%9f%ba%e6%9c%ac%e7%bb%93%e6%9e%84 aria-label="RNN 的基本结构">RNN 的基本结构</a></li><li><a href=#%e5%b1%95%e5%bc%80%e7%9a%84%e6%97%b6%e9%97%b4%e5%9b%be aria-label=展开的时间图>展开的时间图</a></li><li><a href=#%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e9%80%9a%e8%bf%87%e6%97%b6%e9%97%b4bptt aria-label=反向传播通过时间（BPTT）>反向传播通过时间（BPTT）</a></li><li><a href=#tanh-%e7%9a%84%e5%af%bc%e6%95%b0 aria-label="tanh 的导数">tanh 的导数</a></li></ul></li></ul></li><li><a href=#%e4%ba%94lstm%e9%95%bf%e8%ae%b0%e5%bf%86%e7%9a%84%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%881997 aria-label=五、LSTM：长记忆的解决方案（1997）>五、LSTM：长记忆的解决方案（1997）</a><ul><li><a href=#%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1%e7%9a%84%e9%97%ae%e9%a2%98 aria-label=梯度消失的问题>梯度消失的问题</a></li><li><a href=#lstm-%e7%9a%84%e6%a0%b8%e5%bf%83%e5%88%9b%e6%96%b0 aria-label="LSTM 的核心创新">LSTM 的核心创新</a><ul><li><a href=#%e9%81%97%e5%bf%98%e9%97%a8forget-gate aria-label="遗忘门（Forget Gate）">遗忘门（Forget Gate）</a></li><li><a href=#%e8%be%93%e5%85%a5%e9%97%a8input-gate aria-label="输入门（Input Gate）">输入门（Input Gate）</a></li><li><a href=#%e5%80%99%e9%80%89%e7%bb%86%e8%83%9e%e7%8a%b6%e6%80%81candidate-cell-state aria-label="候选细胞状态（Candidate Cell State）">候选细胞状态（Candidate Cell State）</a></li><li><a href=#%e8%be%93%e5%87%ba%e9%97%a8output-gate aria-label="输出门（Output Gate）">输出门（Output Gate）</a></li><li><a href=#%e7%bb%86%e8%83%9e%e7%8a%b6%e6%80%81%e6%9b%b4%e6%96%b0 aria-label=细胞状态更新>细胞状态更新</a></li><li><a href=#%e9%9a%90%e8%97%8f%e7%8a%b6%e6%80%81%e6%9b%b4%e6%96%b0 aria-label=隐藏状态更新>隐藏状态更新</a></li></ul></li><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88-lstm-%e8%a7%a3%e5%86%b3%e4%ba%86%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1%e9%97%ae%e9%a2%98 aria-label="为什么 LSTM 解决了梯度消失问题？">为什么 LSTM 解决了梯度消失问题？</a></li></ul></li><li><a href=#%e5%85%ad%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e6%89%93%e7%a0%b4%e5%ba%8f%e5%88%97%e4%be%9d%e8%b5%962017 aria-label=六、注意力机制：打破序列依赖（2017）>六、注意力机制：打破序列依赖（2017）</a><ul><li><a href=#%e4%bb%8e%e5%be%aa%e7%8e%af%e5%88%b0%e6%b3%a8%e6%84%8f%e5%8a%9b aria-label=从循环到注意力>从循环到注意力</a></li><li><a href=#%e6%95%b0%e5%ad%a6%e5%bd%a2%e5%bc%8f%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9bself-attention aria-label=数学形式：自注意力（Self-Attention）>数学形式：自注意力（Self-Attention）</a><ul><li><a href=#querykeyvalue aria-label=Query、Key、Value>Query、Key、Value</a></li><li><a href=#%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%88%86%e6%95%b0 aria-label=注意力分数>注意力分数</a></li><li><a href=#%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9d%83%e9%87%8d aria-label=注意力权重>注意力权重</a></li><li><a href=#%e5%8a%a0%e6%9d%83%e6%b1%82%e5%92%8c aria-label=加权求和>加权求和</a></li><li><a href=#%e7%9f%a9%e9%98%b5%e5%bd%a2%e5%bc%8f aria-label=矩阵形式>矩阵形式</a></li><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e7%a7%b0%e4%b8%ba%e6%b3%a8%e6%84%8f%e5%8a%9b aria-label='为什么称为"注意力"？'>为什么称为"注意力"？</a></li><li><a href=#%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9bmulti-head-attention aria-label="多头注意力（Multi-Head Attention）">多头注意力（Multi-Head Attention）</a></li></ul></li></ul></li><li><a href=#%e4%b8%83%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5%e6%b7%b1%e5%b1%82%e7%bd%91%e7%bb%9c%e7%9a%84%e5%85%b3%e9%94%ae2015 aria-label=七、残差连接：深层网络的关键（2015）>七、残差连接：深层网络的关键（2015）</a><ul><li><a href=#%e6%b7%b1%e5%ba%a6%e7%bd%91%e7%bb%9c%e7%9a%84%e8%ae%ad%e7%bb%83%e5%9b%b0%e5%a2%83 aria-label=深度网络的训练困境>深度网络的训练困境</a></li><li><a href=#%e6%95%b0%e5%ad%a6%e5%bd%a2%e5%bc%8f-3 aria-label=数学形式>数学形式</a><ul><li><a href=#%e6%ae%8b%e5%b7%ae%e5%9d%97residual-block aria-label="残差块（Residual Block）">残差块（Residual Block）</a></li><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e6%9c%89%e6%95%88 aria-label=为什么有效？>为什么有效？</a></li></ul></li></ul></li><li><a href=#%e5%85%ab%e7%8e%b0%e4%bb%a3%e6%9e%b6%e6%9e%84%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%89%8d%e5%a5%8f2018 aria-label=八、现代架构：大模型的前奏（2018）>八、现代架构：大模型的前奏（2018）</a><ul><li><a href=#%e4%bb%8e-nlp-%e5%88%b0%e9%80%9a%e7%94%a8%e6%99%ba%e8%83%bd aria-label="从 NLP 到通用智能">从 NLP 到通用智能</a></li><li><a href=#transformer-%e7%9a%84%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8%e6%9e%b6%e6%9e%84 aria-label="Transformer 的编码器-解码器架构">Transformer 的编码器-解码器架构</a><ul><li><a href=#%e7%bc%96%e7%a0%81%e5%99%a8encoder aria-label=编码器（Encoder）>编码器（Encoder）</a></li><li><a href=#%e8%a7%a3%e7%a0%81%e5%99%a8decoder aria-label=解码器（Decoder）>解码器（Decoder）</a></li><li><a href=#%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8%e6%b3%a8%e6%84%8f%e5%8a%9b aria-label=编码器-解码器注意力>编码器-解码器注意力</a></li></ul></li></ul></li><li><a href=#%e7%bb%93%e8%af%ad%e4%bb%8e%e5%8d%95%e7%a5%9e%e7%bb%8f%e5%85%83%e5%88%b0%e9%80%9a%e7%94%a8%e6%99%ba%e8%83%bd aria-label=结语：从单神经元到通用智能>结语：从单神经元到通用智能</a></li></ul></div></details></div><div class=post-content><h2 id=引言智慧的萌芽>引言：智慧的萌芽<a hidden class=anchor aria-hidden=true href=#引言智慧的萌芽>#</a></h2><p>想象一下 1957 年的夏天，康奈尔大学的弗兰克·罗森布拉特（Frank Rosenblatt）在实验室里调试着一台早期的电子计算机。他正在实现一个大胆的想法——能否用数学模型模拟人类的大脑神经元？</p><p>这个想法在当时看起来近乎荒谬。人类大脑由数百亿个神经元组成，神经元之间通过突触连接，形成了一个令人眩晕的复杂网络。但罗森布拉特相信，如果我们能理解单个神经元的基本工作原理，就能一步步构建出能够学习的智能系统。</p><p>那时的学术界对机器学习充满怀疑。&ldquo;机器怎么可能思考？"——这是当时的主流声音。但罗森布拉特和他的同道们坚持了下来，用数学公式编织着最初的神经之梦。</p><p>今天，当我们面对能够写出论文、创作艺术、驾驶汽车的深度学习系统时，很容易忘记这一切都始于一个简单的线性分类器。让我们放慢脚步，回顾这七十年的征程，感受数学的力量与思想的演进。</p><hr><h2 id=一感知机神经网络的起点1957>一、感知机：神经网络的起点（1957）<a hidden class=anchor aria-hidden=true href=#一感知机神经网络的起点1957>#</a></h2><p><strong>时间：1957 年 - 弗兰克·罗森布拉特 (Frank Rosenblatt)</strong></p><h3 id=历史的起点>历史的起点<a hidden class=anchor aria-hidden=true href=#历史的起点>#</a></h3><p>1957 年，弗兰克·罗森布拉特在康奈尔航空实验室发明了感知机（Perceptron）。这是第一个能够学习的神经网络模型，被誉为"机器学习的开端&rdquo;。</p><p>1962 年的《纽约客》杂志甚至专门报道了这个发明，称它为"会思考的机器"。那时的媒体兴奋中充满了对人工智能未来的无限遐想。</p><h3 id=数学形式>数学形式<a hidden class=anchor aria-hidden=true href=#数学形式>#</a></h3><h4 id=单个神经元的工作原理>单个神经元的工作原理<a hidden class=anchor aria-hidden=true href=#单个神经元的工作原理>#</a></h4><p>一个感知机神经元接收 $d$ 维输入 $\mathbf{x} = (x_1, x_2, \ldots, x_d)^T$，每个输入对应一个权重 $w_i$，还有一个偏置 $b$。</p><p>神经元的输出是输入的加权和，然后通过<strong>激活函数</strong>：</p><p>$$
y = f(z) = f\left(\sum_{i=1}^{d} w_i x_i + b\right) = f(w^T x + b)
$$</p><p>其中 $z = \mathbf{w}^T \mathbf{x} + b$ 是净输入（net input）。</p><h4 id=激活函数>激活函数<a hidden class=anchor aria-hidden=true href=#激活函数>#</a></h4><p>在最初的感知机中，激活函数是<strong>符号函数</strong>（sign function）：</p><p>$$
f(z) = \begin{cases}
1 & \text{if } z \geq 0 \
-1 & \text{if } z &lt; 0
\end{cases}
$$</p><p>因此，感知机是一个<strong>二元分类器</strong>。</p><h4 id=感知机的学习规则rosenblatt-规则>感知机的学习规则：Rosenblatt 规则<a hidden class=anchor aria-hidden=true href=#感知机的学习规则rosenblatt-规则>#</a></h4><p>感知机的学习非常直观。给定一个训练样本 $(\mathbf{x}_i, y_i)$，其中 $y_i \in {-1, +1}$。</p><p>预测值为：</p><p>$$
\hat{y}_i = \text{sign}(\mathbf{w}^T \mathbf{x}_i + b)
$$</p><p>如果预测正确（$\hat{y}_i = y_i$），不更新权重。</p><p>如果预测错误，按以下规则更新：</p><p>$$
\mathbf{w} \leftarrow \mathbf{w} + \eta y_i \mathbf{x}_i
$$</p><p>$$
b \leftarrow b + \eta y_i
$$</p><p>其中 $\eta$ 是学习率。</p><p>这个规则被称为<strong>Rosenblatt 规则</strong>，是梯度下降的一个简化形式。</p><h4 id=感知机的局限性异或问题>感知机的局限性：异或问题<a hidden class=anchor aria-hidden=true href=#感知机的局限性异或问题>#</a></h4><p>1969 年，明斯基和佩伯特在《感知机》一书中证明了感知机的致命弱点：它无法解决<strong>非线性可分</strong>的问题，最著名的例子就是<strong>异或</strong>（XOR）问题。</p><p>异或问题的真值表：</p><table><thead><tr><th>$x_1$</th><th>$x_2$</th><th>$x_1 \oplus x_2$</th></tr></thead><tbody><tr><td>$-1$</td><td>$-1$</td><td>$-1$</td></tr><tr><td>$-1$</td><td>$+1$</td><td>$+1$</td></tr><tr><td>$+1$</td><td>$-1$</td><td>$+1$</td></tr><tr><td>$+1$</td><td>$+1$</td><td>$-1$</td></tr></tbody></table><p>如果我们尝试用一条直线（决策边界）来分类这四个点，你会发现这是不可能的。因为单层感知机只能产生线性决策边界。</p><p>这个发现一度让神经网络研究进入寒冬。直到 1980 年代，多层感知机和非线性激活函数的引入才打破了僵局。</p><hr><h2 id=二多层感知机与反向传播深度学习的复兴1986>二、多层感知机与反向传播：深度学习的复兴（1986）<a hidden class=anchor aria-hidden=true href=#二多层感知机与反向传播深度学习的复兴1986>#</a></h2><p><strong>时间：1986 年 - 大卫·鲁梅尔哈特 (David Rumelhart) 等</strong></p><h3 id=寒冬后的复苏>寒冬后的复苏<a hidden class=anchor aria-hidden=true href=#寒冬后的复苏>#</a></h3><p>在感知机被证明无法解决 XOR 问题后，神经网络研究沉寂了近二十年。直到 1986 年，大卫·鲁梅尔哈特、杰弗里·辛顿（Geoffrey Hinton）和罗纳德·威廉姆斯（Ronald Williams）在《Nature》上发表了题为《通过误差反向传播学习表征》的论文，提出了<strong>反向传播算法</strong>（Backpropagation）。</p><p>这篇论文开启了现代深度学习的大门。它解决的核心问题是：当网络有多层时，如何高效地计算梯度？</p><h3 id=数学推导反向传播的核心思想>数学推导：反向传播的核心思想<a hidden class=anchor aria-hidden=true href=#数学推导反向传播的核心思想>#</a></h3><h4 id=前向传播forward-propagation>前向传播（Forward Propagation）<a hidden class=anchor aria-hidden=true href=#前向传播forward-propagation>#</a></h4><p>考虑一个多层感知机（MLP），包含：</p><ul><li>输入层：d 个神经元</li><li>隐藏层：m 个神经元</li><li>输出层：c 个神经元（c 个类别）</li></ul><p>设 $W^{(1)} \in \mathbb{R}^{m \times d}$ 是输入层到隐藏层的权重矩阵，$b^{(1)} \in \mathbb{R}^m$ 是隐藏层的偏置向量。</p><p>设 $W^{(2)} \in \mathbb{R}^{c \times m}$ 是隐藏层到输出层的权重矩阵，$b^{(2)} \in \mathbb{R}^c$ 是输出层的偏置向量。</p><p><strong>隐藏层的净输入</strong>：</p><p>$$
z^{(1)} = W^{(1)} x + b^{(1)}
$$</p><p><strong>隐藏层的输出</strong>（使用非线性激活函数，如 sigmoid）：</p><p>$$
a^{(1)} = \sigma(z^{(1)})
$$</p><p>其中 $\sigma$ 逐元素应用 sigmoid 函数：</p><p>$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$</p><p><strong>输出层的净输入</strong>：</p><p>$$
z^{(2)} = W^{(2)} a^{(1)} + b^{(2)}
$$</p><p><strong>输出层的输出</strong>（使用 softmax，用于多分类）：</p><p>$$
\hat{y} = \text{softmax}(z^{(2)})
$$</p><p>其中 softmax 函数将 $c$ 个实数转换为概率分布：</p><p>$$
\text{softmax}(z)<em>j = \frac{e^{z_j}}{\sum</em>{k=1}^{c} e^{z_k}}
$$</p><h4 id=损失函数交叉熵>损失函数：交叉熵<a hidden class=anchor aria-hidden=true href=#损失函数交叉熵>#</a></h4><p>使用交叉熵损失：</p><p>$$
L = -\sum_{i=1}^{c} y_i \log(\hat{y}_i)
$$</p><p>其中 $y$ 是 one-hot 编码的真实标签。</p><h4 id=反向传播链式法则>反向传播：链式法则<a hidden class=anchor aria-hidden=true href=#反向传播链式法则>#</a></h4><p>核心思想：使用<strong>链式法则</strong>（Chain Rule）计算损失函数对每个参数的梯度。</p><p>首先计算输出层的误差：</p><p>$$
\delta^{(2)} = \hat{y} - y
$$</p><p>这是 softmax 交叉熵损失对净输入的导数（一个优雅的简化）。</p><p>输出层权重的梯度：</p><p>$$
\frac{\partial L}{\partial W^{(2)}} = \delta^{(2)} (a^{(1)})^T
$$</p><p>输出层偏置的梯度：</p><p>$$
\frac{\partial L}{\partial b^{(2)}} = \delta^{(2)}
$$</p><p>然后，将误差<strong>反向传播</strong>到隐藏层。隐藏层的误差是：</p><p>$$
\delta^{(1)} = (W^{(2)})^T \delta^{(2)} \odot \sigma&rsquo;(z^{(1)})
$$</p><p>其中 $\odot$ 是逐元素乘法，$\sigma&rsquo;$ 是 sigmoid 的导数：</p><p>$$
\sigma&rsquo;(z) = \sigma(z)(1 - \sigma(z))
$$</p><p>隐藏层权重的梯度：</p><p>$$
\frac{\partial L}{\partial W^{(1)}} = \delta^{(1)} x^T
$$</p><p>隐藏层偏置的梯度：</p><p>$$
\frac{\partial L}{\partial b^{(1)}} = \delta^{(1)}
$$</p><h4 id=为什么称为反向传播>为什么称为"反向传播"？<a hidden class=anchor aria-hidden=true href=#为什么称为反向传播>#</a></h4><p>前向传播是从输入层到输出层：输入 $\to$ 隐藏层 $\to$ 输出层。</p><p>反向传播是从输出层到输入层：输出误差 $\to$ 隐藏层误差 $\to$ 输入层误差。</p><p>这就像在计算器中"反向"流动，因此得名。</p><h4 id=参数更新>参数更新<a hidden class=anchor aria-hidden=true href=#参数更新>#</a></h4><p>使用梯度下降更新参数：</p><p>$$
W^{(1)} \leftarrow W^{(1)} - \eta \frac{\partial L}{\partial W^{(1)}}
$$</p><p>$$
b^{(1)} \leftarrow b^{(1)} - \eta \frac{\partial L}{\partial b^{(1)}}
$$</p><p>$$
W^{(2)} \leftarrow W^{(2)} - \eta \frac{\partial L}{\partial W^{(2)}}
$$</p><p>$$
b^{(2)} \leftarrow b^{(2)} - \eta \frac{\partial L}{\partial b^{(2)}}
$$</p><p>其中 $\eta$ 是学习率。</p><hr><h2 id=三卷积神经网络感受野的智慧1998-2012>三、卷积神经网络：感受野的智慧（1998-2012）<a hidden class=anchor aria-hidden=true href=#三卷积神经网络感受野的智慧1998-2012>#</a></h2><p><strong>时间：1998 年 - LeNet-5；2012 年 - AlexNet</strong></p><h3 id=从视觉感知到卷积>从视觉感知到卷积<a hidden class=anchor aria-hidden=true href=#从视觉感知到卷积>#</a></h3><p>1998 年，杨·勒昆（Yann LeCun）和他的团队提出了 LeNet-5，这是第一个成功的卷积神经网络（Convolutional Neural Network, CNN）。它在 MNIST 手写数字识别任务上达到了当时的最先进水平。</p><p>卷积神经网络的核心洞察来自对生物视觉系统的研究：人类视觉皮层的神经元具有<strong>局部感受野</strong>（receptive field），即每个神经元只响应视野中的一小部分区域，而不是整个视野。</p><h3 id=数学形式-1>数学形式<a hidden class=anchor aria-hidden=true href=#数学形式-1>#</a></h3><h4 id=卷积操作convolution>卷积操作（Convolution）<a hidden class=anchor aria-hidden=true href=#卷积操作convolution>#</a></h4><p>卷积神经网络的核心是<strong>卷积层</strong>（Convolutional Layer）。给定输入特征图 $X \in \mathbb{R}^{H \times W \times C_{\text{in}}}$（高 $H$、宽 $W$、输入通道数 $C_{\text{in}}$），卷积核 $K \in \mathbb{R}^{k \times k \times C_{\text{in}} \times C_{\text{out}}}$（高 $k$、宽 $k$、$C_{\text{out}}$ 个输出通道），卷积操作定义为：</p><p>$$
(X * K)<em>{i,j,o} = \sum</em>{c=1}^{C_{\text{in}}} \sum_{p=1}^{k} \sum_{q=1}^{k} X_{i+p-1, j+q-1, c} \cdot K_{p,q,c,o}
$$</p><p>其中 $*$ 表示卷积运算，$i, j$ 是输出特征图的空间坐标，$o$ 是输出通道索引。</p><p>更简洁的矩阵表示：</p><p>$$
Y = X * K
$$</p><p>其中 $Y \in \mathbb{R}^{H&rsquo; \times W&rsquo; \times C_{\text{out}}}$ 是输出特征图（空间大小为 $H&rsquo; = H - k + 1$, $W&rsquo; = W - k + 1$）。</p><h4 id=池化层pooling-layer>池化层（Pooling Layer）<a hidden class=anchor aria-hidden=true href=#池化层pooling-layer>#</a></h4><p>为了减少计算量和参数数量，同时增加平移不变性，CNN 引入了<strong>池化层</strong>（Pooling Layer）。最常用的是<strong>最大池化</strong>（Max Pooling）：</p><p>$$
Y_{i,j,o} = \max_{p,q ∈ N_{i,j}} X_{p,q,o}
$$</p><p>其中 $N_{i,j}$ 是位置 $(i, j)$ 附近的窗口（如 $2 \times 2$）。</p><h4 id=lenet-5-架构>LeNet-5 架构<a hidden class=anchor aria-hidden=true href=#lenet-5-架构>#</a></h4><p>LeNet-5 的架构包括：</p><ol><li>输入层：$32 imes 32 灰度图像</li><li>卷积层 C1：6 个 $5 imes 5 卷积核，输出 $28 imes 28 imes 6</li><li>池化层 S2：$2 imes 2 最大池化，输出 $14 imes 14 imes 6</li><li>卷积层 C3：16 个 5×5 卷积核，输出 $10 imes 10 imes 16</li><li>池化层 S4：2×2 最大池化，输出 $5 imes 5 imes 16</li><li>全连接层 F5：120 个神经元</li><li>全连接层 F6：84 个神经元</li><li>输出层 F7：10 个神经元（对应 10 个数字）</li></ol><h4 id=alexnet-的革命2012>AlexNet 的革命（2012）<a hidden class=anchor aria-hidden=true href=#alexnet-的革命2012>#</a></h4><p>2012 年，Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 提出了 AlexNet，在 ImageNet 大规模视觉识别挑战赛（ILSVRC-2012）上以压倒性优势夺冠。</p><p>AlexNet 的创新点：</p><ol><li>使用 <strong>ReLU</strong>（Rectified Linear Unit）激活函数，加速收敛：
$$f(z) = \max(0, z)$$</li><li>使用 <strong>Dropout</strong> 随机失活，防止过拟合：<ul><li>训练时以概率 p 随机将神经元的输出设为 0</li><li>测试时使用所有神经元，但输出乘以 p</li></ul></li><li>使用 <strong>数据增强</strong>（Data Augmentation）：随机裁剪、水平翻转等</li><li>使用 <strong>GPU</strong> 并行计算</li></ol><h4 id=relu-的导数>ReLU 的导数<a hidden class=anchor aria-hidden=true href=#relu-的导数>#</a></h4><p>ReLU 函数的导数是：</p><p>$$
\frac{\partial f(z)}{\partial z} = \begin{cases}
1 & \text{if } z > 0 \
0 & \text{if } z \leq 0
\end{cases}
$$</p><p>ReLU 的优点：</p><ul><li>计算效率高（无指数运算）</li><li>缓解梯度消失问题（正值梯度恒为 1）</li></ul><hr><h2 id=四循环神经网络记忆的艺术1990s>四、循环神经网络：记忆的艺术（1990s）<a hidden class=anchor aria-hidden=true href=#四循环神经网络记忆的艺术1990s>#</a></h2><p><strong>时间：1990 年 - 1997 年 LSTM</strong></p><h3 id=序列数据的挑战>序列数据的挑战<a hidden class=anchor aria-hidden=true href=#序列数据的挑战>#</a></h3><p>前馈神经网络（如 MLP 和 CNN）假设输入和输出之间是独立的映射关系。但对于序列数据（如语言、语音、时间序列），当前时刻的输入依赖于历史信息。</p><p>循环神经网络（Recurrent Neural Network, RNN）的核心思想是：神经网络的输出不仅取决于当前输入，还取决于<strong>隐藏状态</strong>（hidden state），后者记忆了过去的信息。</p><h3 id=数学形式-2>数学形式<a hidden class=anchor aria-hidden=true href=#数学形式-2>#</a></h3><h4 id=rnn-的基本结构>RNN 的基本结构<a hidden class=anchor aria-hidden=true href=#rnn-的基本结构>#</a></h4><p>考虑一个时间序列 $x_1, x_2, \ldots, x_T$，每个时间步 $t$ 的输入是 $x_t \in \mathbb{R}^d$。</p><p>RNN 维护一个隐藏状态 $h_t \in \mathbb{R}^m$，按时间递归更新：</p><p>$$
h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h)
$$</p><p>其中：</p><ul><li>$W_{xh} \in \mathbb{R}^{m \times d}$ 是输入到隐藏的权重矩阵</li><li>$W_{hh} \in \mathbb{R}^{m \times m}$ 是隐藏到隐藏的权重矩阵</li><li>$b_h \in \mathbb{R}^m$ 是隐藏层的偏置</li><li>$f$ 是激活函数（如 $\tanh$ 或 ReLU）</li></ul><p>每个时间步的输出是：</p><p>$$
y_t = g(W_{hy} h_t + b_y)
$$</p><p>其中：</p><ul><li>$W_{hy} \in \mathbb{R}^{c \times m}$ 是隐藏到输出的权重矩阵（$c$ 是输出维度）</li><li>$b_y \in \mathbb{R}^c$ 是输出的偏置</li><li>$g$ 是输出激活函数（如 softmax）</li></ul><h4 id=展开的时间图>展开的时间图<a hidden class=anchor aria-hidden=true href=#展开的时间图>#</a></h4><p>将 RNN 按时间展开，可以看到信息如何从 $t=1$ 传递到 $t=T$：</p><p>$$
h_1 = f(W_{xh} x_1 + b_h)
$$</p><p>$$
h_2 = f(W_{xh} x_2 + W_{hh} h_1 + b_h)
$$</p><p>$$
h_3 = f(W_{xh} x_3 + W_{hh} h_2 + b_h)
$$</p><p>$$
\vdots
$$</p><p>$$
h_T = f(W_{xh} x_T + W_{hh} h_{T-1} + b_h)
$$</p><p>可以看到，$h_T$ 依赖于所有之前的输入 $x_1, x_2, \ldots, x_T$，这就是 RNN 的记忆机制。</p><h4 id=反向传播通过时间bptt>反向传播通过时间（BPTT）<a hidden class=anchor aria-hidden=true href=#反向传播通过时间bptt>#</a></h4><p>RNN 的训练需要考虑时间依赖性，梯度需要<strong>反向传播通过时间</strong>（Backpropagation Through Time, BPTT）。</p><p>损失函数：</p><p>$$
L = \sum_{t=1}^{T} \ell(y_t, \hat{y}_t)
$$</p><p>其中 ℓ 是单个时间步的损失（如交叉熵）。</p><p>通过链式法则计算梯度：</p><p>$$
\frac{\partial L}{\partial W_{hh}} = \sum_{t=1}^{T} \frac{\partial L}{\partial h_t} \frac{\partial h_t}{\partial W_{hh}}
$$</p><p>其中 $\partial h_t/\partial W_{hh}$ 是递归的：</p><p>$$
\frac{\partial h_t}{\partial W_{hh}} = \sum_{k=t}^{T} \prod_{j=k+1}^{t} f&rsquo;(z_j) W_{hh}
$$</p><p>这个求和表明：$W_{hh}$ 的梯度依赖于所有时间步，导致<strong>梯度消失</strong>（vanishing gradient）或<strong>梯度爆炸</strong>（exploding gradient）问题。</p><h4 id=tanh-的导数>tanh 的导数<a hidden class=anchor aria-hidden=true href=#tanh-的导数>#</a></h4><p>tanh 激活函数的导数：</p><p>$$
\frac{\partial \tanh(z)}{\partial z} = 1 - \tanh^2(z) \leq 1
$$</p><p>如果 $W_{hh}$ 的特征值都小于 1，乘积会趋于 0，导致梯度消失。</p><hr><h2 id=五lstm长记忆的解决方案1997>五、LSTM：长记忆的解决方案（1997）<a hidden class=anchor aria-hidden=true href=#五lstm长记忆的解决方案1997>#</a></h2><p><strong>时间：1997 年 - Sepp Hochreiter 和 Jürgen Schmidhuber</strong></p><h3 id=梯度消失的问题>梯度消失的问题<a hidden class=anchor aria-hidden=true href=#梯度消失的问题>#</a></h3><p>在长序列中，RNN 的梯度会呈指数衰减或增长。考虑 tanh 激活函数的导数：</p><p>$$
\tanh&rsquo;(z) = 1 - \tanh^2(z) \leq 1
$$</p><p>如果 $W_{hh}$ 的特征值都小于 1，乘积会趋于 0，导致梯度消失。</p><h3 id=lstm-的核心创新>LSTM 的核心创新<a hidden class=anchor aria-hidden=true href=#lstm-的核心创新>#</a></h3><p>1997 年，Sepp Hochreiter 和 Jürgen Schmidhuber 提出了<strong>长短期记忆网络</strong>（Long Short-Term Memory, LSTM），通过引入<strong>门控机制</strong>（gating mechanism）解决梯度消失问题。</p><p>LSTM 的细胞状态（cell state）和隐藏状态（hidden state）分离：</p><p>$$
c_t = \text{遗忘门} \odot c_{t-1} + \text{输入门} \odot \tilde{c}_t
$$</p><p>$$
h_t = \text{输出门} \odot \tanh(c_t)
$$</p><p>其中 $\odot$ 是逐元素乘法，$\tilde{c}_t$ 是候选细胞状态。</p><h4 id=遗忘门forget-gate>遗忘门（Forget Gate）<a hidden class=anchor aria-hidden=true href=#遗忘门forget-gate>#</a></h4><p>遗忘门决定保留多少旧信息：</p><p>$$
f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)
$$</p><p>其中 $\sigma$ 是 sigmoid 函数，输出在 $[0, 1]$ 之间。</p><h4 id=输入门input-gate>输入门（Input Gate）<a hidden class=anchor aria-hidden=true href=#输入门input-gate>#</a></h4><p>输入门决定写入多少新信息：</p><p>$$
i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)
$$</p><h4 id=候选细胞状态candidate-cell-state>候选细胞状态（Candidate Cell State）<a hidden class=anchor aria-hidden=true href=#候选细胞状态candidate-cell-state>#</a></h4><p>$$
\tilde{c}<em>t = \tanh(W_c [h</em>{t-1}, x_t] + b_c)
$$</p><h4 id=输出门output-gate>输出门（Output Gate）<a hidden class=anchor aria-hidden=true href=#输出门output-gate>#</a></h4><p>输出门决定输出多少信息到隐藏状态：</p><p>$$
o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)
$$</p><h4 id=细胞状态更新>细胞状态更新<a hidden class=anchor aria-hidden=true href=#细胞状态更新>#</a></h4><p>$$
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t
$$</p><h4 id=隐藏状态更新>隐藏状态更新<a hidden class=anchor aria-hidden=true href=#隐藏状态更新>#</a></h4><p>$$
h_t = o_t \odot \tanh(c_t)
$$</p><h3 id=为什么-lstm-解决了梯度消失问题>为什么 LSTM 解决了梯度消失问题？<a hidden class=anchor aria-hidden=true href=#为什么-lstm-解决了梯度消失问题>#</a></h3><p>关键在于细胞状态的更新：</p><p>$$
\frac{\partial c_t}{\partial c_{t-1}} = f_t
$$</p><p>如果遗忘门 $f_t$ 接近 1，梯度几乎无损地传播。门控机制让网络学会<strong>何时保留</strong>和<strong>何时遗忘</strong>信息。</p><hr><h2 id=六注意力机制打破序列依赖2017>六、注意力机制：打破序列依赖（2017）<a hidden class=anchor aria-hidden=true href=#六注意力机制打破序列依赖2017>#</a></h2><p><strong>时间：2017 年 - Vaswani 等</strong></p><h3 id=从循环到注意力>从循环到注意力<a hidden class=anchor aria-hidden=true href=#从循环到注意力>#</a></h3><p>在 Transformer 出现之前，序列建模主要依赖 RNN 及其变体（LSTM、GRU）。但 RNN 有两个根本限制：</p><ol><li><strong>顺序计算</strong>：必须从 $t=1$ 计算到 $t=T$，无法并行</li><li><strong>长距离依赖</strong>：即使有 LSTM，信息仍难以从 $t=1$ 传递到 $t=T$</li></ol><p>2017 年，Vaswani 等人在《Attention Is All You Need》中提出了<strong>Transformer</strong>，完全摒弃了循环结构，只用<strong>注意力机制</strong>（Attention Mechanism）。</p><h3 id=数学形式自注意力self-attention>数学形式：自注意力（Self-Attention）<a hidden class=anchor aria-hidden=true href=#数学形式自注意力self-attention>#</a></h3><p>考虑一个序列 $X = [x_1, x_2, \ldots, x_T] \in \mathbb{R}^{T \times d}$，其中 $T$ 是序列长度，$d$ 是嵌入维度。</p><h4 id=querykeyvalue>Query、Key、Value<a hidden class=anchor aria-hidden=true href=#querykeyvalue>#</a></h4><p>Transformer 的核心是<strong>自注意力</strong>（Self-Attention）。对于每个位置，我们计算三组向量：</p><p>$$
Q_i = x_i W^Q, \quad K_i = x_i W^K, \quad V_i = x_i W^V
$$</p><p>其中 $W^Q, W^K, W^V \in \mathbb{R}^{d \times d_k}$ 是可学习的参数矩阵，$d_k$ 是查询/键/值的维度。</p><h4 id=注意力分数>注意力分数<a hidden class=anchor aria-hidden=true href=#注意力分数>#</a></h4><p>对于位置 $i$ 和 $j$，计算注意力分数（缩放点积）：</p><p>$$
\text{score}_{ij} = \frac{Q_i \cdot K_j}{\sqrt{d_k}}
$$</p><h4 id=注意力权重>注意力权重<a hidden class=anchor aria-hidden=true href=#注意力权重>#</a></h4><p>将分数转换为概率分布（使用 softmax）：</p><p>$$
\alpha_{ij} = \frac{\exp(\text{score}<em>{ij})}{\sum</em>{k=1}^{T} \exp(\text{score}_{ik})}
$$</p><h4 id=加权求和>加权求和<a hidden class=anchor aria-hidden=true href=#加权求和>#</a></h4><p>对值向量加权求和，得到输出：</p><p>$$
z_i = \sum_{j=1}^{T} \alpha_{ij} V_j
$$</p><h4 id=矩阵形式>矩阵形式<a hidden class=anchor aria-hidden=true href=#矩阵形式>#</a></h4><p>可以写成矩阵形式：</p><p>$$
Z = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$</p><p>其中：</p><ul><li>$Q = X W^Q \in \mathbb{R}^{T \times d_k}$</li><li>$K = X W^K \in \mathbb{R}^{T \times d_k}$</li><li>$V = X W^V \in \mathbb{R}^{T \times d_v}$</li></ul><h4 id=为什么称为注意力>为什么称为"注意力"？<a hidden class=anchor aria-hidden=true href=#为什么称为注意力>#</a></h4><p>$\alpha_{ij}$ 表示位置 $i$ 对位置 $j$ 的"关注程度"。如果 $\alpha_{ij}$ 接近 1，说明位置 $i$ 通常关注位置 $j$。</p><h4 id=多头注意力multi-head-attention>多头注意力（Multi-Head Attention）<a hidden class=anchor aria-hidden=true href=#多头注意力multi-head-attention>#</a></h4><p>为了捕获不同类型的关系，Transformer 使用<strong>多头注意力</strong>（Multi-Head Attention）：</p><p>$$
\text{MultiHead}(X) = \text{Concat}(\text{head}_1, \text{head}_2, &mldr;, \text{head}_h) W^O
$$</p><p>其中每个头是独立的自注意力：</p><p>$$
\text{head}_i = \text{Attention}(X W_i^Q, X W_i^K, X W_i^V)
$$</p><p>$W^O \in \mathbb{R}^{h \times d_v \times d_{\text{model}}}$ 是输出投影矩阵，$d_{\text{model}}$ 是模型维度。</p><hr><h2 id=七残差连接深层网络的关键2015>七、残差连接：深层网络的关键（2015）<a hidden class=anchor aria-hidden=true href=#七残差连接深层网络的关键2015>#</a></h2><p><strong>时间：2015 年 - 何恺明等</strong></p><h3 id=深度网络的训练困境>深度网络的训练困境<a hidden class=anchor aria-hidden=true href=#深度网络的训练困境>#</a></h3><p>随着网络深度增加，我们遇到了两个问题：</p><ol><li><strong>梯度消失</strong>：深层网络的梯度难以传播到早期层</li><li><strong>退化问题</strong>：网络深度增加后，训练误差反而增加（即使没有过拟合）</li></ol><p>2015 年，何恺明等人提出了<strong>残差连接</strong>（Residual Connection），解决了这个问题。</p><h3 id=数学形式-3>数学形式<a hidden class=anchor aria-hidden=true href=#数学形式-3>#</a></h3><h4 id=残差块residual-block>残差块（Residual Block）<a hidden class=anchor aria-hidden=true href=#残差块residual-block>#</a></h4><p>普通层的映射是 $F(x, {W_i})$，残差块学习的是<strong>残差映射</strong>：</p><p>$$
R(x, {W_i}) = F(x, {W_i}) - x
$$</p><p>其中 $F(x, {W_i})$ 是残差函数（通常由 2-3 个卷积层组成），${W_i}$ 是可学习的权重。</p><p>残差块的输出是：</p><p>$$
y = F(x, {W_i}) + x
$$</p><p>这被称为<strong>跳跃连接</strong>（skip connection）或<strong>快捷路径</strong>（shortcut path）。</p><h4 id=为什么有效>为什么有效？<a hidden class=anchor aria-hidden=true href=#为什么有效>#</a></h4><p>考虑 L 层残差网络。前向传播可以写成：</p><p>$$
x_{l+1} = x_l + F(x_l, W_l)
$$</p><p>因此：</p><p>$$
x_L = x_0 + \sum_{l=0}^{L-1} F(x_l, W_l)
$$</p><p>这意味着梯度可以直接从第 L 层传播到第 0 层：</p><p>$$
\frac{\partial L}{\partial x_0} = \frac{\partial L}{\partial x_L} · \prod_{l=0}^{L-1} \left(1 + \frac{\partial F(x_l, W_l)}{\partial x_l}\right)
$$</p><p>残差连接中的恒等映射（identity mapping）确保梯度至少为 1，解决了梯度消失问题。</p><hr><h2 id=八现代架构大模型的前奏2018>八、现代架构：大模型的前奏（2018）<a hidden class=anchor aria-hidden=true href=#八现代架构大模型的前奏2018>#</a></h2><p><strong>时间：2018 年 - BERT；2020 年 - GPT-3；2022 年 - ChatGPT</strong></p><h3 id=从-nlp-到通用智能>从 NLP 到通用智能<a hidden class=anchor aria-hidden=true href=#从-nlp-到通用智能>#</a></h3><p>2018 年，Google 提出了<strong>BERT</strong>（Bidirectional Encoder Representations from Transformers），将预训练-微调（pre-training and fine-tuning）范式推向主流。</p><p>BERT 的创新点：</p><ol><li><strong>掩码语言模型</strong>（Masked Language Model, MLM）：随机掩盖输入 tokens 的 15%，让模型预测</li><li><strong>下一句预测</strong>（Next Sentence Prediction, NSP）：预测两个句子是否相邻</li><li><strong>双向编码</strong>：使用 Transformer 的编码器，同时看到左右上下文</li></ol><p>2020 年，OpenAI 发布了<strong>GPT-3</strong>（Generative Pre-trained Transformer 3），展示了超大规模模型的涌现能力。</p><p>GPT-3 的关键：</p><ol><li><strong>参数规模</strong>：1750 亿参数</li><li><strong>少样本学习</strong>（Few-shot Learning）：只需几个例子就能学会新任务</li><li><strong>零样本学习</strong>（Zero-shot Learning）：无需任何例子</li></ol><h3 id=transformer-的编码器-解码器架构>Transformer 的编码器-解码器架构<a hidden class=anchor aria-hidden=true href=#transformer-的编码器-解码器架构>#</a></h3><h4 id=编码器encoder>编码器（Encoder）<a hidden class=anchor aria-hidden=true href=#编码器encoder>#</a></h4><p>编码器处理输入序列，输出固定维度的表示：</p><p>$$
Z = \text{Encoder}(X)
$$</p><p>其中 $Z \in \mathbb{R}^{T \times d_{\text{model}}}$ 是编码后的表示。</p><h4 id=解码器decoder>解码器（Decoder）<a hidden class=anchor aria-hidden=true href=#解码器decoder>#</a></h4><p>解码器生成输出序列：</p><p>$$
\hat{y}<em>t = \text{softmax}(z_t W</em>{vocab})
$$</p><p>其中 $W_{\text{vocab}} \in \mathbb{R}^{d_{\text{model}} \times |\text{Vocab}|}$ 是词表矩阵，$z_t$ 是解码器在时间 $t$ 的隐藏状态。</p><h4 id=编码器-解码器注意力>编码器-解码器注意力<a hidden class=anchor aria-hidden=true href=#编码器-解码器注意力>#</a></h4><p>解码器通过<strong>交叉注意力</strong>（Cross-Attention）关注编码器的输出：</p><p>$$
z_t = \text{Attention}(Q_t, K, V)
$$</p><p>其中 $Q_t = z_{t-1} W^Q$ 是解码器的查询，$K = Z W^K$ 和 $V = Z W^V$ 是编码器的键和值。</p><hr><h2 id=结语从单神经元到通用智能>结语：从单神经元到通用智能<a hidden class=anchor aria-hidden=true href=#结语从单神经元到通用智能>#</a></h2><p>1957 年的感知机只是一个线性分类器。但七十年后的今天，我们有：</p><ul><li>数千亿参数的模型</li><li>能理解复杂语言</li><li>能生成艺术作品</li><li>能辅助科学发现</li></ul><p>这七十年的征程，本质上是数学和思想的演进：</p><ol><li><strong>感知机</strong>：理解单个神经元</li><li><strong>反向传播</strong>：理解如何学习多层网络</li><li><strong>卷积网络</strong>：理解空间结构</li><li><strong>循环网络</strong>：理解时间依赖</li><li><strong>LSTM</strong>：理解长期记忆</li><li><strong>注意力</strong>：理解全局关系</li><li><strong>残差连接</strong>：理解深层网络</li><li><strong>预训练模型</strong>：理解大规模学习</li></ol><p>每一个突破都建立在前人的基础上，用数学公式表达了对智能的新理解。</p><p>今天的深度学习仍有很多未解之谜：如何实现真正的推理？如何获得常识？如何解释模型的决策？</p><p>但回望过去，我们有理由相信：只要坚持用数学和实验探索未知，终有一天，我们会解开这些谜题，创造出真正的通用智能。</p><hr><p><strong>参考文献</strong>：</p><ol><li>Rosenblatt, F. (1958). <em>The perceptron: A probabilistic model for information storage and organization in the brain</em>.</li><li>Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). &ldquo;Learning representations by back-propagating errors&rdquo;. <em>Nature</em>.</li><li>LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). &ldquo;Gradient-based learning applied to document recognition&rdquo;. <em>Proceedings of the IEEE</em>.</li><li>Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). &ldquo;ImageNet classification with deep convolutional neural networks&rdquo;. <em>NIPS</em>.</li><li>Hochreiter, S., & Schmidhuber, J. (1997). &ldquo;Long short-term memory&rdquo;. <em>Neural Computation</em>.</li><li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). &ldquo;Attention is all you need&rdquo;. <em>NIPS</em>.</li><li>He, K., Zhang, X., Ren, S., & Sun, J. (2016). &ldquo;Deep residual learning for image recognition&rdquo;. <em>CVPR</em>.</li><li>Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). &ldquo;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&rdquo;. <em>NAACL-HLT</em>.</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-16-hara-automotive-analysis/><span class=title>« Prev</span><br><span>汽车行业 HARA 分析综述：从理论到实践</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-15-traditional-ml-algorithms/><span class=title>Next »</span><br><span>深度学习前夜：十大传统机器学习算法的历史与数学之美</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 神经网络算法演进：从感知机到 Transformer 的七十年征程 on x" href="https://x.com/intent/tweet/?text=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%ae%97%e6%b3%95%e6%bc%94%e8%bf%9b%ef%bc%9a%e4%bb%8e%e6%84%9f%e7%9f%a5%e6%9c%ba%e5%88%b0%20Transformer%20%e7%9a%84%e4%b8%83%e5%8d%81%e5%b9%b4%e5%be%81%e7%a8%8b&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-15-neural-network-evolution%2f&amp;hashtags=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%2c%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%2c%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 神经网络算法演进：从感知机到 Transformer 的七十年征程 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-15-neural-network-evolution%2f&amp;title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%ae%97%e6%b3%95%e6%bc%94%e8%bf%9b%ef%bc%9a%e4%bb%8e%e6%84%9f%e7%9f%a5%e6%9c%ba%e5%88%b0%20Transformer%20%e7%9a%84%e4%b8%83%e5%8d%81%e5%b9%b4%e5%be%81%e7%a8%8b&amp;summary=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%ae%97%e6%b3%95%e6%bc%94%e8%bf%9b%ef%bc%9a%e4%bb%8e%e6%84%9f%e7%9f%a5%e6%9c%ba%e5%88%b0%20Transformer%20%e7%9a%84%e4%b8%83%e5%8d%81%e5%b9%b4%e5%be%81%e7%a8%8b&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-15-neural-network-evolution%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 神经网络算法演进：从感知机到 Transformer 的七十年征程 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-15-neural-network-evolution%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%ae%97%e6%b3%95%e6%bc%94%e8%bf%9b%ef%bc%9a%e4%bb%8e%e6%84%9f%e7%9f%a5%e6%9c%ba%e5%88%b0%20Transformer%20%e7%9a%84%e4%b8%83%e5%8d%81%e5%b9%b4%e5%be%81%e7%a8%8b"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 神经网络算法演进：从感知机到 Transformer 的七十年征程 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-15-neural-network-evolution%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>