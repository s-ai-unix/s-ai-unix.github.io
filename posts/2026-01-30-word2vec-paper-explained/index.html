<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AI 论文解读系列：Word2Vec - 词向量的革命 | s-ai-unix's Blog</title><meta name=keywords content="Word2Vec,自然语言处理,深度学习,机器学习"><meta name=description content="深入浅出解读 Mikolov 等人的 Word2Vec 论文，从词袋模型到神经语言模型，完整推导 CBOW 和 Skip-gram 的数学原理与应用。"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-30-word2vec-paper-explained/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-30-word2vec-paper-explained/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-30-word2vec-paper-explained/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="AI 论文解读系列：Word2Vec - 词向量的革命"><meta property="og:description" content="深入浅出解读 Mikolov 等人的 Word2Vec 论文，从词袋模型到神经语言模型，完整推导 CBOW 和 Skip-gram 的数学原理与应用。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-30T09:00:00+08:00"><meta property="article:modified_time" content="2026-01-30T09:00:00+08:00"><meta property="article:tag" content="Word2Vec"><meta property="article:tag" content="自然语言处理"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="机器学习"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/word2vec-cover.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/word2vec-cover.jpg"><meta name=twitter:title content="AI 论文解读系列：Word2Vec - 词向量的革命"><meta name=twitter:description content="深入浅出解读 Mikolov 等人的 Word2Vec 论文，从词袋模型到神经语言模型，完整推导 CBOW 和 Skip-gram 的数学原理与应用。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"AI 论文解读系列：Word2Vec - 词向量的革命","item":"https://s-ai-unix.github.io/posts/2026-01-30-word2vec-paper-explained/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"AI 论文解读系列：Word2Vec - 词向量的革命","name":"AI 论文解读系列：Word2Vec - 词向量的革命","description":"深入浅出解读 Mikolov 等人的 Word2Vec 论文，从词袋模型到神经语言模型，完整推导 CBOW 和 Skip-gram 的数学原理与应用。","keywords":["Word2Vec","自然语言处理","深度学习","机器学习"],"articleBody":" “You shall know a word by the company it keeps.” — John Rupert Firth\n引言：从符号到语义 想象一下，你正在阅读一篇关于\"苹果\"的文章。在\"乔布斯推出了划时代的苹果产品\"这句话中，“苹果\"显然指的是一家公司；而在\"我喜欢吃新鲜的苹果\"中，它则是一种水果。人类能够毫不费力地根据上下文理解这种歧义，但对于计算机而言，这曾是一个巨大的挑战。\n在 Word2Vec 出现之前，自然语言处理主要依赖独热编码（One-Hot Encoding）：每个词都被表示为一个高维稀疏向量，向量中只有对应位置为 $1$，其余全为 $0$。“苹果\"可能是 $[0, 0, 1, 0, \\ldots, 0]$，“香蕉\"是 $[0, 0, 0, 1, \\ldots, 0]$。这种方法的问题显而易见：任意两个词之间的余弦相似度都是 $0$，模型完全无法捕捉\"苹果\"和\"香蕉\"都是水果这一语义关系。\n2013 年，Tomas Mikolov 等人在 Google 提出了 Word2Vec，这是一种能够从大规模语料库中学习词向量表示的浅层神经网络。其核心思想简单却深刻：语义相近的词，其上下文也相似。这一方法不仅在多项语义和语法任务上取得了当时最先进的性能，更开启了深度学习在自然语言处理领域的广泛应用。\n本文将带你深入理解 Word2Vec 的数学原理，从神经概率语言模型出发，完整推导 CBOW 和 Skip-gram 两种架构，并探讨其在现代 NLP 中的深远影响。\n第一章：从词袋到神经语言模型 1.1 统计语言模型的演进 语言模型的核心任务是计算一个句子出现的概率。对于包含 $n$ 个词的句子\n$$w_1, w_2, \\ldots, w_n$$ 其联合概率可以分解为：\n$$P(w_1, w_2, \\ldots, w_n) = \\prod_{i=1}^{n} P(w_i \\mid w_1, \\ldots, w_{i-1})$$ 这个分解基于链式法则，但直接估计这些条件概率面临维度灾难——历史词的组合数是指数级的。\nn-gram 模型通过马尔可夫假设简化了这个问题：假设一个词只依赖于前 $n-1$ 个词。当 $n=2$ 时，就是二元模型（Bigram）：\n$$P(w_i \\mid w_1, \\ldots, w_{i-1}) \\approx P(w_i \\mid w_{i-1})$$ n-gram 模型简单高效，但存在明显缺陷：\n数据稀疏性：很多合理的词组合在训练语料中从未出现 无法捕捉长距离依赖：“虽然……但是……“这样的结构超出窗口范围 语义鸿沟：“猫\"和\"狗\"在模型中是完全无关的符号 1.2 分布式假说与词向量 1957 年，英国语言学家 J.R. Firth 提出了著名的分布式假说（Distributional Hypothesis）：“You shall know a word by the company it keeps.” 这句话揭示了一个深刻洞见——词的语义可以通过其上下文分布来刻画。\n想象你在一本被涂抹了部分文字的书中阅读。当看到\"我每天早上都会喝____来提神”，即使最后一个词被遮挡，你也能推断出它可能是\"咖啡”、“茶\"或\"可乐”。这说明词的语义确实蕴藏在上下文关系中。\n词向量（Word Embedding）正是基于这一思想的数学实现：\n将每个词映射到一个低维稠密向量 $\\mathbf{v} \\in \\mathbb{R}^d$（通常 $d = 50 \\sim 300$） 语义相似的词在向量空间中距离相近 向量可以捕捉丰富的语义关系 1.3 神经概率语言模型 2003 年，Yoshua Bengio 提出了神经概率语言模型（Neural Probabilistic Language Model, NPLM），首次用神经网络学习词向量。模型结构如下：\n输入层 $\\to$ 投影层（词向量查表）$\\to$ 隐藏层（$\\tanh$）$\\to$ 输出层（Softmax）\n对于给定的上下文词\n$$w_{i-n+1}, \\ldots, w_{i-1}$$ 模型预测目标词 $w_i$ 的概率：\n$$P(w_i \\mid w_{i-n+1}, \\ldots, w_{i-1}) = \\frac{\\exp(y_{w_i})}{\\sum_{w} \\exp(y_w)}$$ 其中 $y_w$ 是输出层对应词 $w$ 的得分。\nNPLM 的革命性在于：词向量作为模型的副产品被学习得到，相似的词会拥有相似的向量表示。但 NPLM 的计算复杂度很高，主要是因为：\n隐藏层和输出层的全连接计算 Softmax 需要遍历整个词汇表 这限制了它在更大规模数据上的应用。\n第二章：Word2Vec 的架构 Word2Vec 是对 NPLM 的简化与优化。Mikolov 等人发现，去除隐藏层不仅降低了计算复杂度，反而提高了词向量的质量。这一反直觉的发现源于：NPLM 的主要任务（语言建模）和学习词表示的目标并不完全一致。\nWord2Vec 提出了两种对称的架构：\nCBOW（Continuous Bag-of-Words）：用上下文预测中心词 Skip-gram：用中心词预测上下文 2.1 CBOW：上下文合成语义 CBOW 的核心思想是：一个词的语义由其周围词的语义\"合成\"而来。\n模型结构 flowchart TB subgraph 输入层[\"输入层 (上下文词)\"] W1[\"$w_{i-2}$\"] W2[\"$w_{i-1}$\"] W3[\"$w_{i+1}$\"] W4[\"$w_{i+2}$\"] end subgraph 投影层[\"投影层\"] AVG[\"平均 $\\mathbf{h} = (\\mathbf{v}_1+\\mathbf{v}_2+\\mathbf{v}_3+\\mathbf{v}_4)/4$\"] end subgraph 输出层[\"输出层 (预测)\"] TARGET[\"预测 $w_i$\"] end W1 --\u003e AVG W2 --\u003e AVG W3 --\u003e AVG W4 --\u003e AVG AVG --\u003e TARGET style W1 fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff style W2 fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff style W3 fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff style W4 fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff style AVG fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style TARGET fill:#FF9500,stroke:#FF9500,stroke-width:3px,color:#ffffff 图例说明：\n🔵 蓝色节点：上下文输入词（$w_{i-2}, w_{i-1}, w_{i+1}, w_{i+2}$） 🟢 绿色节点：投影层平均操作（$\\mathbf{h} = \\frac{1}{4}\\sum \\mathbf{v}_{w_k}$） 🟠 橙色节点：预测目标（中心词 $w_i$） 设窗口大小为 $c$（上下文词数），词汇表大小为 $V$，词向量维度为 $d$。\n输入：上下文词的独热编码\n$$\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_{2c} \\in \\{0, 1\\}^V$$ 投影层：词向量查表并求平均\n$$\\mathbf{h} = \\frac{1}{2c} \\sum_{k=1}^{2c} \\mathbf{W}^T \\mathbf{x}_k = \\frac{1}{2c} \\sum_{k=1}^{2c} \\mathbf{v}_{w_k}$$ 其中 $\\mathbf{W} \\in \\mathbb{R}^{V \\times d}$ 是输入词向量矩阵，$\\mathbf{v}_{w_k}$ 是词 $w_k$ 的输入向量。\n输出层：Softmax 预测中心词概率\n$$P(w_i \\mid \\text{context}) = \\frac{\\exp(\\mathbf{u}_{w_i}^T \\mathbf{h})}{\\sum_{w=1}^{V} \\exp(\\mathbf{u}_w^T \\mathbf{h})}$$ 其中 $\\mathbf{U} \\in \\mathbb{R}^{V \\times d}$ 是输出词向量矩阵，$\\mathbf{u}_w$ 是词 $w$ 的输出向量。\n数学推导 CBOW 的目标函数是最大化对数似然：\n$$\\mathcal{L} = \\frac{1}{T} \\sum_{t=1}^{T} \\log P(w_t \\mid w_{t-c}, \\ldots, w_{t-1}, w_{t+1}, \\ldots, w_{t+c})$$ 其中 $T$ 是训练语料的总词数。\n对于单个训练样本，损失函数为负对数似然：\n$$L = -\\log P(w_O \\mid w_{I,1}, \\ldots, w_{I,2c})$$ 其中 $w_O$ 是输出词（中心词），$w_{I,1}, \\ldots, w_{I,2c}$ 是输入词（上下文）。\n定义\n$$z_w = \\mathbf{u}_w^T \\mathbf{h}$$ 则：\n$$P(w_O \\mid \\text{context}) = \\frac{\\exp(z_{w_O})}{\\sum_{w=1}^{V} \\exp(z_w)} = \\frac{\\exp(z_{w_O})}{Z}$$ 其中 $Z = \\sum_{w=1}^{V} \\exp(z_w)$ 是配分函数。\n损失函数对 $z_w$ 的梯度：\n$$\\frac{\\partial L}{\\partial z_w} = \\begin{cases} P(w \\mid \\text{context}) - 1 \u0026 \\text{if } w = w_O \\\\ P(w \\mid \\text{context}) \u0026 \\text{otherwise} \\end{cases} = P(w \\mid \\text{context}) - \\mathbb{1}[w = w_O]$$ 这是一个漂亮的解释：梯度正比于预测概率与真实分布（one-hot）的差异。\n对输出向量 $\\mathbf{u}_w$ 的梯度：\n$$\\frac{\\partial L}{\\partial \\mathbf{u}_w} = \\frac{\\partial L}{\\partial z_w} \\cdot \\frac{\\partial z_w}{\\partial \\mathbf{u}_w} = (P(w \\mid \\text{context}) - \\mathbb{1}[w = w_O]) \\mathbf{h}$$ 对输入向量 $\\mathbf{v}{w{I,k}}$ 的梯度：\n$$\\frac{\\partial L}{\\partial \\mathbf{v}_{w_{I,k}}} = \\frac{1}{2c} \\mathbf{W}^T \\frac{\\partial L}{\\partial \\mathbf{h}} = \\frac{1}{2c} \\sum_{w=1}^{V} (P(w \\mid \\text{context}) - \\mathbb{1}[w = w_O]) \\mathbf{u}_w$$ 参数更新规则（学习率 $\\eta$）：\n$$\\mathbf{u}_w^{\\text{new}} = \\mathbf{u}_w^{\\text{old}} - \\eta \\cdot (P(w \\mid \\text{context}) - \\mathbb{1}[w = w_O]) \\mathbf{h}$$ $$\\mathbf{v}_{w_{I,k}}^{\\text{new}} = \\mathbf{v}_{w_{I,k}}^{\\text{old}} - \\eta \\cdot \\frac{1}{2c} \\sum_{w=1}^{V} (P(w \\mid \\text{context}) - \\mathbb{1}[w = w_O]) \\mathbf{u}_w$$ 2.2 Skip-gram：中心词辐射语义 Skip-gram 是 CBOW 的镜像：它用中心词预测周围的上下文词。直觉上，这迫使模型将更多信息编码到每个词的向量中。\n模型结构 flowchart LR CENTER[\"中心词 $w_i$\"] subgraph 上下文预测[\"预测多个上下文词\"] C1[\"预测 $w_{i-2}$\"] C2[\"预测 $w_{i-1}$\"] C3[\"预测 $w_{i+1}$\"] C4[\"预测 $w_{i+2}$\"] end CENTER --\u003e C1 CENTER --\u003e C2 CENTER --\u003e C3 CENTER --\u003e C4 style CENTER fill:#FF9500,stroke:#FF9500,stroke-width:3px,color:#ffffff style C1 fill:#007AFF,stroke:#007AFF,stroke-width:2px,color:#ffffff style C2 fill:#007AFF,stroke:#007AFF,stroke-width:2px,color:#ffffff style C3 fill:#007AFF,stroke:#007AFF,stroke-width:2px,color:#ffffff style C4 fill:#007AFF,stroke:#007AFF,stroke-width:2px,color:#ffffff 图例说明：\n🟠 橙色节点：中心词输入（$w_i$） 🔵 蓝色节点：独立预测的上下文词（$w_{i-2}, w_{i-1}, w_{i+1}, w_{i+2}$） 输入：中心词的独热编码 $\\mathbf{x} \\in {0, 1}^V$\n投影层：词向量查表\n$$\\mathbf{h} = \\mathbf{W}^T \\mathbf{x} = \\mathbf{v}_{w_I}$$ 输出层：对每个上下文位置独立预测\n假设上下文窗口为 $c$，Skip-gram 假设给定中心词时，各个上下文词的出现是条件独立的：\n$$P(w_{O,1}, \\ldots, w_{O,c} \\mid w_I) = \\prod_{j=1}^{c} P(w_{O,j} \\mid w_I)$$ 其中每个条件概率为：\n$$P(w_{O,j} \\mid w_I) = \\frac{\\exp(\\mathbf{u}_{w_{O,j}}^T \\mathbf{v}_{w_I})}{\\sum_{w=1}^{V} \\exp(\\mathbf{u}_w^T \\mathbf{v}_{w_I})}$$ 数学推导 目标函数：\n$$\\mathcal{L} = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log P(w_{t+j} \\mid w_t)$$ 对于单个上下文词 $w_O$，损失函数：\n$$L = -\\log P(w_O \\mid w_I) = -\\mathbf{u}_{w_O}^T \\mathbf{v}_{w_I} + \\log \\sum_{w=1}^{V} \\exp(\\mathbf{u}_w^T \\mathbf{v}_{w_I})$$ 对 $\\mathbf{v}_{w_I}$ 的梯度：\n$$\\frac{\\partial L}{\\partial \\mathbf{v}_{w_I}} = -\\mathbf{u}_{w_O} + \\frac{\\sum_{w=1}^{V} \\exp(\\mathbf{u}_w^T \\mathbf{v}_{w_I}) \\mathbf{u}_w}{\\sum_{w'=1}^{V} \\exp(\\mathbf{u}_{w'}^T \\mathbf{v}_{w_I})} = \\sum_{w=1}^{V} (P(w \\mid w_I) - \\mathbb{1}[w = w_O]) \\mathbf{u}_w$$ 对 $\\mathbf{u}_w$ 的梯度：\n$$\\frac{\\partial L}{\\partial \\mathbf{u}_w} = (P(w \\mid w_I) - \\mathbb{1}[w = w_O]) \\mathbf{v}_{w_I}$$ 2.3 CBOW vs Skip-gram 特性 CBOW Skip-gram 训练方向 上下文 $\\to$ 中心词 中心词 $\\to$ 上下文 训练速度 更快 较慢 对罕见词效果 一般 更好 对高频词效果 更好 一般 适用于 大规模语料 小规模语料 Mikolov 等人的实验表明：Skip-gram 在处理罕见词和捕捉精细语义关系方面表现更好，而 CBOW 训练速度更快，对高频词建模更平滑。\n第三章：训练优化策略 原始的 Softmax 需要遍历整个词汇表计算归一化因子，这对于大规模语料（$V \\sim 10^5 \\sim 10^7$）是不可接受的。Word2Vec 提出了两种优化策略：\n3.1 层次 Softmax（Hierarchical Softmax） 层次 Softmax 利用二叉树结构（通常是哈夫曼树）将计算复杂度从 $O(V)$ 降低到 $O(\\log V)$。\n哈夫曼树构建 每个词对应一个叶节点，权重为词频 高频词离根节点更近，路径更短 构建 Huffman 树，平均路径长度最小化 概率计算 在二叉树中，从根到叶节点的路径上的每个内部节点代表一个二分类决策。设 $n(w, j)$ 是从根到词 $w$ 的路径上第 $j$ 个节点，$L(w)$ 是路径长度。\n$$P(w \\mid w_I) = \\prod_{j=1}^{L(w)} \\sigma\\left([\\![ n(w, j+1) = \\text{ch}(n(w, j)) ]\\!] \\cdot (\\mathbf{v}'_{n(w,j)})^T \\mathbf{v}_{w_I}\\right)$$ 其中：\n$\\sigma(x) = \\frac{1}{1 + e^{-x}}$ 是 sigmoid 函数\n$[![ n(w, j+1) = \\text{ch}(n(w, j)) ]!]$ 是指示函数：\n$$[\\![ n(w, j+1) = \\text{ch}(n(w, j)) ]\\!] = \\begin{cases} 1 \u0026 \\text{如果 } n(w, j+1) \\text{ 是左子节点} \\\\ -1 \u0026 \\text{如果 } n(w, j+1) \\text{ 是右子节点} \\end{cases}$$ $\\mathbf{v}’_{n(w,j)}$ 是内部节点 $n(w,j)$ 的向量表示\n$\\mathbf{v}_{w_I}$ 是输入词 $w_I$ 的向量\n这样，计算 $P(w \\mid w_I)$ 只需要遍历路径上的 $O(\\log V)$ 个节点，而非全部 $V$ 个词。\n3.2 负采样（Negative Sampling） 负采样是另一种更简单的近似方法，也是实际应用中最常用的策略。\n核心思想 将多分类问题转化为二分类问题：\n正样本：真实的目标词对 $(w_I, w_O)$，标签为 $1$ 负样本：从噪声分布中采样的词对 $(w_I, w_{\\text{neg}})$，标签为 $0$ 目标函数 $$\\log \\sigma(\\mathbf{u}_{w_O}^T \\mathbf{v}_{w_I}) + \\sum_{i=1}^{k} \\mathbb{E}_{w_i \\sim P_n(w)}[\\log \\sigma(-\\mathbf{u}_{w_i}^T \\mathbf{v}_{w_I})]$$ 其中：\n$k$ 是负样本数量（通常 $5 \\sim 20$） $P_n(w)$ 是噪声分布，通常取 $P_n(w) \\propto f(w)^{3/4}$，$f(w)$ 是词频 $3/4$ 的幂次是为了降低高频词的采样概率，增加罕见词的采样机会 为什么负采样有效？ 负采样可以看作是对 Softmax 的近似，其理论基础是噪声对比估计（Noise Contrastive Estimation, NCE）。它将密度估计问题转化为区分真实数据和噪声数据的二分类问题。\n与层次 Softmax 相比，负采样的优势：\n实现更简单 对于小数据集和罕见词效果更好 训练速度更快（每次更新只需要处理 $k+1$ 个词） 3.3 子采样（Subsampling） 除了优化输出层的计算，Word2Vec 还对高频词进行了子采样。像\"的”、“是”、“在\"这样的词出现频率极高，但信息含量很低，而且会拖慢训练。\n子采样策略：以概率 $P(w_i)$ 丢弃词 $w_i$：\n$$P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}}$$ 其中 $f(w_i)$ 是词频，$t$ 是阈值（通常 $10^{-5}$）。当 $f(w_i) \u003e t$ 时，词被丢弃的概率随词频增加而增加。\n第四章：词向量的奇妙性质 训练完成后，Word2Vec 学到的词向量展现出令人惊叹的线性关系。\n4.1 语义关系的向量算术 Mikolov 等人发现，词向量能够捕捉各种语义和语法关系：\n图：词向量空间中的语义关系，king - man + woman ≈ queen\n$$\\mathbf{v}_{\\text{king}} - \\mathbf{v}_{\\text{man}} + \\mathbf{v}_{\\text{woman}} \\approx \\mathbf{v}_{\\text{queen}}$$ 这个经典例子表明：词向量不仅编码了词的语义，还编码了词之间的关系。“国王\"减去\"男人\"加上\"女人\"约等于\"女王”，这意味着向量空间中捕捉到了性别这一语义维度。\n类似的例子还包括：\n首都-国家：\n$$\\mathbf{v}_{\\text{Paris}} - \\mathbf{v}_{\\text{France}} + \\mathbf{v}_{\\text{Italy}} \\approx \\mathbf{v}_{\\text{Rome}}$$ 时态：\n$$\\mathbf{v}_{\\text{walking}} - \\mathbf{v}_{\\text{walked}} + \\mathbf{v}_{\\text{swam}} \\approx \\mathbf{v}_{\\text{swimming}}$$ 单复数：\n$$\\mathbf{v}_{\\text{apples}} - \\mathbf{v}_{\\text{apple}} + \\mathbf{v}_{\\text{car}} \\approx \\mathbf{v}_{\\text{cars}}$$ 比较级：\n$$\\mathbf{v}_{\\text{bigger}} - \\mathbf{v}_{\\text{big}} + \\mathbf{v}_{\\text{small}} \\approx \\mathbf{v}_{\\text{smaller}}$$ 4.2 为什么词向量有这种性质？ 这种线性关系的出现并非偶然，而是分布式假说的数学体现。考虑 Skip-gram 的目标：预测上下文词。如果\"国王\"和\"女王\"在相似的上下文中出现（”____统治着这个国家”），它们的向量就会相似。\n更重要的是，词向量编码了语义差异。“国王\"和\"女王\"的差向量大致等于\"男人\"和\"女人\"的差向量，因为它们都与\"性别\"这一概念相关。\n从几何角度看，Word2Vec 学习到的向量空间将语义关系编码为方向。每一个重要的语义维度（性别、时态、单复数等）对应向量空间中的一个方向。\n4.3 余弦相似度与词语类比 衡量词向量相似度的标准方法是余弦相似度：\n$$\\text{similarity}(\\mathbf{u}, \\mathbf{v}) = \\cos(\\theta) = \\frac{\\mathbf{u}^T \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}$$ 两个向量的夹角越小（方向越接近），余弦值越接近 $1$，表示语义越相似。\n词语类比任务（Word Analogy）是评估词向量的标准任务：\n图：词向量余弦相似度矩阵，展示语义相近词的关联程度\n对于关系\"A 之于 B，如同 C 之于 D”，寻找 D 等价于：\n$$\\mathbf{d} = \\arg\\max_{\\mathbf{d}'} \\frac{(\\mathbf{b} - \\mathbf{a} + \\mathbf{c})^T \\mathbf{d}'}{\\|\\mathbf{b} - \\mathbf{a} + \\mathbf{c}\\| \\|\\mathbf{d}'\\|}$$ Mikolov 等人报告，在包含 $1.6$ 亿词的训练数据上，Skip-gram 模型在语义类比任务上达到了 $55%$ 的准确率，在语法类比任务上达到了 $59%$ 的准确率。\n第五章：实现与实战 5.1 伪代码实现 以下是 Skip-gram 负采样的简化伪代码：\n# 初始化 V = vocabulary_size d = embedding_dim W_input = random(V, d) # 输入词向量矩阵 W_output = random(V, d) # 输出词向量矩阵 # 训练 for sentence in corpus: for i, target in enumerate(sentence): # 获取上下文窗口 context = sentence[max(0, i-window):i] + sentence[i+1:i+window+1] for context_word in context: # 正样本更新 z = sigmoid(dot(W_output[target], W_input[context_word])) g = (z - 1) * learning_rate W_output[target] -= g * W_input[context_word] W_input[context_word] -= g * W_output[target] # 负样本更新 for _ in range(negative_samples): negative = sample_from_noise_distribution() z = sigmoid(dot(W_output[negative], W_input[context_word])) g = z * learning_rate W_output[negative] -= g * W_input[context_word] W_input[context_word] -= g * W_output[negative] 5.2 超参数选择 图：CBOW 与 Skip-gram 训练过程中损失函数的变化\n超参数 推荐值 说明 词向量维度 $d$ $100 \\sim 300$ 维度越高表达能力越强，但也更容易过拟合 上下文窗口 $c$ $5 \\sim 10$ Skip-gram 可用较小窗口，CBOW 可用较大窗口 负采样数 $k$ $5 \\sim 20$ 小数据集用大值，大数据集用小值 学习率 $\\eta$ $0.01 \\sim 0.025$ 常用线性衰减策略 子采样阈值 $t$ $10^{-5}$ 控制高频词的丢弃率 最小词频 $5 \\sim 10$ 过滤罕见词，减少噪声 5.3 使用 Gensim 训练 实际应用中，我们通常使用成熟的库如 Gensim：\nfrom gensim.models import Word2Vec from gensim.utils import simple_preprocess # 准备语料（分词后的句子列表） sentences = [ [\"我\", \"喜欢\", \"自然\", \"语言\", \"处理\"], [\"机器\", \"学习\", \"是\", \"人工智能\", \"的\", \"分支\"], # ... 更多句子 ] # 训练模型 model = Word2Vec( sentences=sentences, vector_size=100, # 词向量维度 window=5, # 上下文窗口 min_count=5, # 最小词频 workers=4, # 并行线程数 sg=1, # 1=Skip-gram, 0=CBOW negative=5, # 负采样数 sample=1e-5, # 子采样阈值 epochs=5 # 训练轮数 ) # 获取词向量 vector = model.wv[\"机器学习\"] # 找最相似的词 similar = model.wv.most_similar(\"人工智能\", topn=5) # 词语类比 result = model.wv.most_similar( positive=[\"女王\", \"男人\"], negative=[\"国王\"], topn=1 ) 第六章：影响与演进 6.1 Word2Vec 的革命性意义 Word2Vec 的提出标志着自然语言处理进入了深度学习时代。它的影响可以从以下几个维度理解：\n1. 技术范式转变\n从符号到连续：将离散的词符号转化为连续的向量表示 从手工特征到自动学习：无需语言学知识，自动从数据中学习语义 从稀疏到稠密：低维稠密向量计算更高效，泛化能力更强 2. 工业应用落地\nWord2Vec 训练速度快、实现简单，很快在工业界广泛应用：\n搜索引擎：查询扩展、语义匹配 推荐系统：物品/用户向量表示 广告系统：关键词定向、受众画像 机器翻译：语义对齐、双语词典构建 3. 学术影响\n截至 2024 年，Mikolov 的 Word2Vec 论文被引用超过 $50{,}000$ 次，是 NLP 领域最具影响力的论文之一。\n6.2 后续发展 Word2Vec 开创了词嵌入的先河，后续研究在多个方向上进行拓展：\nGloVe（Global Vectors）\nPennington 等人在 2014 年提出 GloVe，结合了全局统计信息（共现矩阵）和局部上下文信息（窗口）。其目标函数直接优化共现矩阵与词向量内积的关系：\n$$J = \\sum_{i,j} f(X_{ij}) (\\mathbf{w}_i^T \\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j - \\log X_{ij})^2$$ GloVe 在某些任务上表现优于 Word2Vec，且训练更稳定。\nFastText\n2016 年，Facebook 提出 FastText，将词表示为字符 n-gram 的组合：\n$$\\mathbf{v}_w = \\sum_{g \\in \\mathcal{G}_w} \\mathbf{z}_g$$ 这种方法能够处理未登录词（OOV），并捕捉词的形态信息。\nContextualized Embeddings\nWord2Vec 是静态词向量：每个词只有一个固定的向量表示。这无法处理一词多义问题（如\"苹果\"公司 vs 水果）。\n2018 年前后，ELMo、GPT、BERT 等模型提出动态词向量（Contextualized Embeddings），根据上下文为每个词实例生成不同的表示：\n$$\\mathbf{h}_{w, \\text{context}} = \\text{Transformer}(w, \\text{context})$$ 这标志着 NLP 进入了预训练语言模型时代，但 Word2Vec 奠定的分布式语义基础依然适用。\n6.3 跨领域应用 Word2Vec 的核心思想——将离散符号嵌入连续向量空间——已被推广到众多领域：\n领域 应用 图神经网络 Node2Vec, DeepWalk（节点嵌入） 生物信息学 BioVec, ProtVec（蛋白质/DNA 序列） 社交网络 DeepWalk, LINE（用户/社区嵌入） 知识图谱 TransE, RotatE（实体/关系嵌入） 代码分析 Code2Vec, CodeBERT（代码嵌入） 推荐系统 Item2Vec, Prod2Vec（商品嵌入） 结语：一个词嵌入的时代 Word2Vec 不仅是一个算法，更是一种思想的胜利：语言的语义可以通过分布式的统计规律来捕捉。\n从 2013 年 Mikolov 等人的开创性论文，到今天动辄千亿参数的语言模型，词嵌入始终是自然语言处理的核心技术。无论是简单的文本分类，还是复杂的对话系统，将语言符号转化为机器可理解的向量表示都是不可或缺的第一步。\n回顾 Word2Vec 的发展历程，我们可以得到几点启示：\n简单即美：去除隐藏层的简化反而提升了性能，说明架构设计应当服务于目标任务。\n数据即知识：Word2Vec 不需要人工标注，从海量无标注文本中自动学习语义，体现了无监督学习的威力。\n几何即语义：词的语义关系编码在向量空间的几何结构中，这一洞见影响了后续所有表示学习研究。\n正如 Mikolov 在论文结尾所言：“我们的工作表明，简单的模型训练海量数据，往往能击败复杂的模型训练小量数据。” 这一哲学贯穿于深度学习的发展历程，从 Word2Vec 到 GPT-4，从未改变。\n“The meaning of a word is its use in the language.” — Ludwig Wittgenstein\n参考资料\nMikolov, T., Chen, K., Corrado, G., \u0026 Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. arXiv preprint arXiv:1301.3781.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G., \u0026 Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS.\nBengio, Y., Ducharme, R., Vincent, P., \u0026 Janvin, C. (2003). A Neural Probabilistic Language Model. JMLR.\nPennington, J., Socher, R., \u0026 Manning, C. (2014). GloVe: Global Vectors for Word Representation. EMNLP.\nRong, X. (2014). word2vec Parameter Learning Explained. arXiv preprint arXiv:1411.2738.\n","wordCount":"1442","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/word2vec-cover.jpg","datePublished":"2026-01-30T09:00:00+08:00","dateModified":"2026-01-30T09:00:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-30-word2vec-paper-explained/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">AI 论文解读系列：Word2Vec - 词向量的革命</h1><div class=post-description>深入浅出解读 Mikolov 等人的 Word2Vec 论文，从词袋模型到神经语言模型，完整推导 CBOW 和 Skip-gram 的数学原理与应用。</div><div class=post-meta><span title='2026-01-30 09:00:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>7 min</span>&nbsp;·&nbsp;<span>1442 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/word2vec-cover.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/word2vec-cover.jpg alt="Word2Vec 词向量可视化"></a><figcaption>图片来自 Unsplash</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e4%bb%8e%e7%ac%a6%e5%8f%b7%e5%88%b0%e8%af%ad%e4%b9%89 aria-label=引言：从符号到语义>引言：从符号到语义</a></li><li><a href=#%e7%ac%ac%e4%b8%80%e7%ab%a0%e4%bb%8e%e8%af%8d%e8%a2%8b%e5%88%b0%e7%a5%9e%e7%bb%8f%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b aria-label=第一章：从词袋到神经语言模型>第一章：从词袋到神经语言模型</a><ul><li><a href=#11-%e7%bb%9f%e8%ae%a1%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%bc%94%e8%bf%9b aria-label="1.1 统计语言模型的演进">1.1 统计语言模型的演进</a></li><li><a href=#12-%e5%88%86%e5%b8%83%e5%bc%8f%e5%81%87%e8%af%b4%e4%b8%8e%e8%af%8d%e5%90%91%e9%87%8f aria-label="1.2 分布式假说与词向量">1.2 分布式假说与词向量</a></li><li><a href=#13-%e7%a5%9e%e7%bb%8f%e6%a6%82%e7%8e%87%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b aria-label="1.3 神经概率语言模型">1.3 神经概率语言模型</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e7%ab%a0word2vec-%e7%9a%84%e6%9e%b6%e6%9e%84 aria-label="第二章：Word2Vec 的架构">第二章：Word2Vec 的架构</a><ul><li><a href=#21-cbow%e4%b8%8a%e4%b8%8b%e6%96%87%e5%90%88%e6%88%90%e8%af%ad%e4%b9%89 aria-label="2.1 CBOW：上下文合成语义">2.1 CBOW：上下文合成语义</a><ul><li><a href=#%e6%a8%a1%e5%9e%8b%e7%bb%93%e6%9e%84 aria-label=模型结构>模型结构</a></li><li><a href=#%e6%95%b0%e5%ad%a6%e6%8e%a8%e5%af%bc aria-label=数学推导>数学推导</a></li></ul></li><li><a href=#22-skip-gram%e4%b8%ad%e5%bf%83%e8%af%8d%e8%be%90%e5%b0%84%e8%af%ad%e4%b9%89 aria-label="2.2 Skip-gram：中心词辐射语义">2.2 Skip-gram：中心词辐射语义</a><ul><li><a href=#%e6%a8%a1%e5%9e%8b%e7%bb%93%e6%9e%84-1 aria-label=模型结构>模型结构</a></li><li><a href=#%e6%95%b0%e5%ad%a6%e6%8e%a8%e5%af%bc-1 aria-label=数学推导>数学推导</a></li></ul></li><li><a href=#23-cbow-vs-skip-gram aria-label="2.3 CBOW vs Skip-gram">2.3 CBOW vs Skip-gram</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e7%ab%a0%e8%ae%ad%e7%bb%83%e4%bc%98%e5%8c%96%e7%ad%96%e7%95%a5 aria-label=第三章：训练优化策略>第三章：训练优化策略</a><ul><li><a href=#31-%e5%b1%82%e6%ac%a1-softmaxhierarchical-softmax aria-label="3.1 层次 Softmax（Hierarchical Softmax）">3.1 层次 Softmax（Hierarchical Softmax）</a><ul><li><a href=#%e5%93%88%e5%a4%ab%e6%9b%bc%e6%a0%91%e6%9e%84%e5%bb%ba aria-label=哈夫曼树构建>哈夫曼树构建</a></li><li><a href=#%e6%a6%82%e7%8e%87%e8%ae%a1%e7%ae%97 aria-label=概率计算>概率计算</a></li></ul></li><li><a href=#32-%e8%b4%9f%e9%87%87%e6%a0%b7negative-sampling aria-label="3.2 负采样（Negative Sampling）">3.2 负采样（Negative Sampling）</a><ul><li><a href=#%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3 aria-label=核心思想>核心思想</a></li><li><a href=#%e7%9b%ae%e6%a0%87%e5%87%bd%e6%95%b0 aria-label=目标函数>目标函数</a></li><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e8%b4%9f%e9%87%87%e6%a0%b7%e6%9c%89%e6%95%88 aria-label=为什么负采样有效？>为什么负采样有效？</a></li></ul></li><li><a href=#33-%e5%ad%90%e9%87%87%e6%a0%b7subsampling aria-label="3.3 子采样（Subsampling）">3.3 子采样（Subsampling）</a></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e7%ab%a0%e8%af%8d%e5%90%91%e9%87%8f%e7%9a%84%e5%a5%87%e5%a6%99%e6%80%a7%e8%b4%a8 aria-label=第四章：词向量的奇妙性质>第四章：词向量的奇妙性质</a><ul><li><a href=#41-%e8%af%ad%e4%b9%89%e5%85%b3%e7%b3%bb%e7%9a%84%e5%90%91%e9%87%8f%e7%ae%97%e6%9c%af aria-label="4.1 语义关系的向量算术">4.1 语义关系的向量算术</a></li><li><a href=#42-%e4%b8%ba%e4%bb%80%e4%b9%88%e8%af%8d%e5%90%91%e9%87%8f%e6%9c%89%e8%bf%99%e7%a7%8d%e6%80%a7%e8%b4%a8 aria-label="4.2 为什么词向量有这种性质？">4.2 为什么词向量有这种性质？</a></li><li><a href=#43-%e4%bd%99%e5%bc%a6%e7%9b%b8%e4%bc%bc%e5%ba%a6%e4%b8%8e%e8%af%8d%e8%af%ad%e7%b1%bb%e6%af%94 aria-label="4.3 余弦相似度与词语类比">4.3 余弦相似度与词语类比</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e7%ab%a0%e5%ae%9e%e7%8e%b0%e4%b8%8e%e5%ae%9e%e6%88%98 aria-label=第五章：实现与实战>第五章：实现与实战</a><ul><li><a href=#51-%e4%bc%aa%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0 aria-label="5.1 伪代码实现">5.1 伪代码实现</a></li><li><a href=#52-%e8%b6%85%e5%8f%82%e6%95%b0%e9%80%89%e6%8b%a9 aria-label="5.2 超参数选择">5.2 超参数选择</a></li><li><a href=#53-%e4%bd%bf%e7%94%a8-gensim-%e8%ae%ad%e7%bb%83 aria-label="5.3 使用 Gensim 训练">5.3 使用 Gensim 训练</a></li></ul></li><li><a href=#%e7%ac%ac%e5%85%ad%e7%ab%a0%e5%bd%b1%e5%93%8d%e4%b8%8e%e6%bc%94%e8%bf%9b aria-label=第六章：影响与演进>第六章：影响与演进</a><ul><li><a href=#61-word2vec-%e7%9a%84%e9%9d%a9%e5%91%bd%e6%80%a7%e6%84%8f%e4%b9%89 aria-label="6.1 Word2Vec 的革命性意义">6.1 Word2Vec 的革命性意义</a></li><li><a href=#62-%e5%90%8e%e7%bb%ad%e5%8f%91%e5%b1%95 aria-label="6.2 后续发展">6.2 后续发展</a></li><li><a href=#63-%e8%b7%a8%e9%a2%86%e5%9f%9f%e5%ba%94%e7%94%a8 aria-label="6.3 跨领域应用">6.3 跨领域应用</a></li></ul></li><li><a href=#%e7%bb%93%e8%af%ad%e4%b8%80%e4%b8%aa%e8%af%8d%e5%b5%8c%e5%85%a5%e7%9a%84%e6%97%b6%e4%bb%a3 aria-label=结语：一个词嵌入的时代>结语：一个词嵌入的时代</a></li></ul></div></details></div><div class=post-content><blockquote><p>&ldquo;You shall know a word by the company it keeps.&rdquo; — John Rupert Firth</p></blockquote><h2 id=引言从符号到语义>引言：从符号到语义<a hidden class=anchor aria-hidden=true href=#引言从符号到语义>#</a></h2><p>想象一下，你正在阅读一篇关于"苹果"的文章。在"乔布斯推出了划时代的苹果产品"这句话中，&ldquo;苹果"显然指的是一家公司；而在"我喜欢吃新鲜的苹果"中，它则是一种水果。人类能够毫不费力地根据上下文理解这种歧义，但对于计算机而言，这曾是一个巨大的挑战。</p><p>在 Word2Vec 出现之前，自然语言处理主要依赖<strong>独热编码</strong>（One-Hot Encoding）：每个词都被表示为一个高维稀疏向量，向量中只有对应位置为 $1$，其余全为 $0$。&ldquo;苹果"可能是 $[0, 0, 1, 0, \ldots, 0]$，&ldquo;香蕉"是 $[0, 0, 0, 1, \ldots, 0]$。这种方法的问题显而易见：任意两个词之间的余弦相似度都是 $0$，模型完全无法捕捉"苹果"和"香蕉"都是水果这一语义关系。</p><p>2013 年，Tomas Mikolov 等人在 Google 提出了 Word2Vec，这是一种能够从大规模语料库中学习词向量表示的浅层神经网络。其核心思想简单却深刻：<strong>语义相近的词，其上下文也相似</strong>。这一方法不仅在多项语义和语法任务上取得了当时最先进的性能，更开启了深度学习在自然语言处理领域的广泛应用。</p><p>本文将带你深入理解 Word2Vec 的数学原理，从神经概率语言模型出发，完整推导 CBOW 和 Skip-gram 两种架构，并探讨其在现代 NLP 中的深远影响。</p><h2 id=第一章从词袋到神经语言模型>第一章：从词袋到神经语言模型<a hidden class=anchor aria-hidden=true href=#第一章从词袋到神经语言模型>#</a></h2><h3 id=11-统计语言模型的演进>1.1 统计语言模型的演进<a hidden class=anchor aria-hidden=true href=#11-统计语言模型的演进>#</a></h3><p>语言模型的核心任务是计算一个句子出现的概率。对于包含 $n$ 个词的句子</p><div class=math>$$w_1, w_2, \ldots, w_n$$</div><p>其联合概率可以分解为：</p><div class=math>$$P(w_1, w_2, \ldots, w_n) = \prod_{i=1}^{n} P(w_i \mid w_1, \ldots, w_{i-1})$$</div><p>这个分解基于<strong>链式法则</strong>，但直接估计这些条件概率面临维度灾难——历史词的组合数是指数级的。</p><p><strong>n-gram 模型</strong>通过马尔可夫假设简化了这个问题：假设一个词只依赖于前 $n-1$ 个词。当 $n=2$ 时，就是<strong>二元模型</strong>（Bigram）：</p><div class=math>$$P(w_i \mid w_1, \ldots, w_{i-1}) \approx P(w_i \mid w_{i-1})$$</div><p>n-gram 模型简单高效，但存在明显缺陷：</p><ul><li><strong>数据稀疏性</strong>：很多合理的词组合在训练语料中从未出现</li><li><strong>无法捕捉长距离依赖</strong>：&ldquo;虽然……但是……&ldquo;这样的结构超出窗口范围</li><li><strong>语义鸿沟</strong>：&ldquo;猫"和"狗"在模型中是完全无关的符号</li></ul><h3 id=12-分布式假说与词向量>1.2 分布式假说与词向量<a hidden class=anchor aria-hidden=true href=#12-分布式假说与词向量>#</a></h3><p>1957 年，英国语言学家 J.R. Firth 提出了著名的<strong>分布式假说</strong>（Distributional Hypothesis）：&ldquo;You shall know a word by the company it keeps.&rdquo; 这句话揭示了一个深刻洞见——词的语义可以通过其上下文分布来刻画。</p><p>想象你在一本被涂抹了部分文字的书中阅读。当看到"我每天早上都会喝____来提神&rdquo;，即使最后一个词被遮挡，你也能推断出它可能是"咖啡&rdquo;、&ldquo;茶"或"可乐&rdquo;。这说明词的语义确实蕴藏在上下文关系中。</p><p><strong>词向量</strong>（Word Embedding）正是基于这一思想的数学实现：</p><ul><li>将每个词映射到一个低维稠密向量 $\mathbf{v} \in \mathbb{R}^d$（通常 $d = 50 \sim 300$）</li><li>语义相似的词在向量空间中距离相近</li><li>向量可以捕捉丰富的语义关系</li></ul><h3 id=13-神经概率语言模型>1.3 神经概率语言模型<a hidden class=anchor aria-hidden=true href=#13-神经概率语言模型>#</a></h3><p>2003 年，Yoshua Bengio 提出了<strong>神经概率语言模型</strong>（Neural Probabilistic Language Model, NPLM），首次用神经网络学习词向量。模型结构如下：</p><p>输入层 $\to$ 投影层（词向量查表）$\to$ 隐藏层（$\tanh$）$\to$ 输出层（Softmax）</p><p>对于给定的上下文词</p><div class=math>$$w_{i-n+1}, \ldots, w_{i-1}$$</div><p>模型预测目标词 $w_i$ 的概率：</p><div class=math>$$P(w_i \mid w_{i-n+1}, \ldots, w_{i-1}) = \frac{\exp(y_{w_i})}{\sum_{w} \exp(y_w)}$$</div><p>其中 $y_w$ 是输出层对应词 $w$ 的得分。</p><p>NPLM 的革命性在于：<strong>词向量作为模型的副产品被学习得到</strong>，相似的词会拥有相似的向量表示。但 NPLM 的计算复杂度很高，主要是因为：</p><ol><li>隐藏层和输出层的全连接计算</li><li>Softmax 需要遍历整个词汇表</li></ol><p>这限制了它在更大规模数据上的应用。</p><h2 id=第二章word2vec-的架构>第二章：Word2Vec 的架构<a hidden class=anchor aria-hidden=true href=#第二章word2vec-的架构>#</a></h2><p>Word2Vec 是对 NPLM 的简化与优化。Mikolov 等人发现，<strong>去除隐藏层不仅降低了计算复杂度，反而提高了词向量的质量</strong>。这一反直觉的发现源于：NPLM 的主要任务（语言建模）和学习词表示的目标并不完全一致。</p><p>Word2Vec 提出了两种对称的架构：</p><ul><li><strong>CBOW</strong>（Continuous Bag-of-Words）：用上下文预测中心词</li><li><strong>Skip-gram</strong>：用中心词预测上下文</li></ul><h3 id=21-cbow上下文合成语义>2.1 CBOW：上下文合成语义<a hidden class=anchor aria-hidden=true href=#21-cbow上下文合成语义>#</a></h3><p>CBOW 的核心思想是：<strong>一个词的语义由其周围词的语义"合成"而来</strong>。</p><h4 id=模型结构>模型结构<a hidden class=anchor aria-hidden=true href=#模型结构>#</a></h4><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>flowchart TB
subgraph 输入层["输入层 (上下文词)"]
W1["$w_{i-2}$"]
W2["$w_{i-1}$"]
W3["$w_{i+1}$"]
W4["$w_{i+2}$"]
end
subgraph 投影层["投影层"]
AVG["平均 $\mathbf{h} = (\mathbf{v}_1+\mathbf{v}_2+\mathbf{v}_3+\mathbf{v}_4)/4$"]
end
subgraph 输出层["输出层 (预测)"]
TARGET["预测 $w_i$"]
end
W1 --> AVG
W2 --> AVG
W3 --> AVG
W4 --> AVG
AVG --> TARGET
style W1 fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff
style W2 fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff
style W3 fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff
style W4 fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff
style AVG fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style TARGET fill:#FF9500,stroke:#FF9500,stroke-width:3px,color:#ffffff</div></div><p><strong>图例说明</strong>：</p><ul><li>🔵 蓝色节点：上下文输入词（$w_{i-2}, w_{i-1}, w_{i+1}, w_{i+2}$）</li><li>🟢 绿色节点：投影层平均操作（$\mathbf{h} = \frac{1}{4}\sum \mathbf{v}_{w_k}$）</li><li>🟠 橙色节点：预测目标（中心词 $w_i$）</li></ul><p>设窗口大小为 $c$（上下文词数），词汇表大小为 $V$，词向量维度为 $d$。</p><p><strong>输入</strong>：上下文词的独热编码</p><div class=math>$$\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_{2c} \in \{0, 1\}^V$$</div><p><strong>投影层</strong>：词向量查表并求平均</p><div class=math>$$\mathbf{h} = \frac{1}{2c} \sum_{k=1}^{2c} \mathbf{W}^T \mathbf{x}_k = \frac{1}{2c} \sum_{k=1}^{2c} \mathbf{v}_{w_k}$$</div><p>其中 $\mathbf{W} \in \mathbb{R}^{V \times d}$ 是输入词向量矩阵，$\mathbf{v}_{w_k}$ 是词 $w_k$ 的输入向量。</p><p><strong>输出层</strong>：Softmax 预测中心词概率</p><div class=math>$$P(w_i \mid \text{context}) = \frac{\exp(\mathbf{u}_{w_i}^T \mathbf{h})}{\sum_{w=1}^{V} \exp(\mathbf{u}_w^T \mathbf{h})}$$</div><p>其中 $\mathbf{U} \in \mathbb{R}^{V \times d}$ 是输出词向量矩阵，$\mathbf{u}_w$ 是词 $w$ 的输出向量。</p><h4 id=数学推导>数学推导<a hidden class=anchor aria-hidden=true href=#数学推导>#</a></h4><p>CBOW 的目标函数是最大化对数似然：</p><div class=math>$$\mathcal{L} = \frac{1}{T} \sum_{t=1}^{T} \log P(w_t \mid w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c})$$</div><p>其中 $T$ 是训练语料的总词数。</p><p>对于单个训练样本，损失函数为负对数似然：</p><div class=math>$$L = -\log P(w_O \mid w_{I,1}, \ldots, w_{I,2c})$$</div><p>其中 $w_O$ 是输出词（中心词），$w_{I,1}, \ldots, w_{I,2c}$ 是输入词（上下文）。</p><p>定义</p><div class=math>$$z_w = \mathbf{u}_w^T \mathbf{h}$$</div><p>则：</p><div class=math>$$P(w_O \mid \text{context}) = \frac{\exp(z_{w_O})}{\sum_{w=1}^{V} \exp(z_w)} = \frac{\exp(z_{w_O})}{Z}$$</div><p>其中 $Z = \sum_{w=1}^{V} \exp(z_w)$ 是配分函数。</p><p>损失函数对 $z_w$ 的梯度：</p><div class=math>$$\frac{\partial L}{\partial z_w} = \begin{cases} P(w \mid \text{context}) - 1 & \text{if } w = w_O \\ P(w \mid \text{context}) & \text{otherwise} \end{cases} = P(w \mid \text{context}) - \mathbb{1}[w = w_O]$$</div><p>这是一个漂亮的解释：梯度正比于预测概率与真实分布（one-hot）的差异。</p><p>对输出向量 $\mathbf{u}_w$ 的梯度：</p><div class=math>$$\frac{\partial L}{\partial \mathbf{u}_w} = \frac{\partial L}{\partial z_w} \cdot \frac{\partial z_w}{\partial \mathbf{u}_w} = (P(w \mid \text{context}) - \mathbb{1}[w = w_O]) \mathbf{h}$$</div><p>对输入向量 $\mathbf{v}<em>{w</em>{I,k}}$ 的梯度：</p><div class=math>$$\frac{\partial L}{\partial \mathbf{v}_{w_{I,k}}} = \frac{1}{2c} \mathbf{W}^T \frac{\partial L}{\partial \mathbf{h}} = \frac{1}{2c} \sum_{w=1}^{V} (P(w \mid \text{context}) - \mathbb{1}[w = w_O]) \mathbf{u}_w$$</div><p>参数更新规则（学习率 $\eta$）：</p><div class=math>$$\mathbf{u}_w^{\text{new}} = \mathbf{u}_w^{\text{old}} - \eta \cdot (P(w \mid \text{context}) - \mathbb{1}[w = w_O]) \mathbf{h}$$</div><div class=math>$$\mathbf{v}_{w_{I,k}}^{\text{new}} = \mathbf{v}_{w_{I,k}}^{\text{old}} - \eta \cdot \frac{1}{2c} \sum_{w=1}^{V} (P(w \mid \text{context}) - \mathbb{1}[w = w_O]) \mathbf{u}_w$$</div><h3 id=22-skip-gram中心词辐射语义>2.2 Skip-gram：中心词辐射语义<a hidden class=anchor aria-hidden=true href=#22-skip-gram中心词辐射语义>#</a></h3><p>Skip-gram 是 CBOW 的镜像：它用中心词预测周围的上下文词。直觉上，这迫使模型将更多信息编码到每个词的向量中。</p><h4 id=模型结构-1>模型结构<a hidden class=anchor aria-hidden=true href=#模型结构-1>#</a></h4><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>flowchart LR
CENTER["中心词 $w_i$"]
subgraph 上下文预测["预测多个上下文词"]
C1["预测 $w_{i-2}$"]
C2["预测 $w_{i-1}$"]
C3["预测 $w_{i+1}$"]
C4["预测 $w_{i+2}$"]
end
CENTER --> C1
CENTER --> C2
CENTER --> C3
CENTER --> C4
style CENTER fill:#FF9500,stroke:#FF9500,stroke-width:3px,color:#ffffff
style C1 fill:#007AFF,stroke:#007AFF,stroke-width:2px,color:#ffffff
style C2 fill:#007AFF,stroke:#007AFF,stroke-width:2px,color:#ffffff
style C3 fill:#007AFF,stroke:#007AFF,stroke-width:2px,color:#ffffff
style C4 fill:#007AFF,stroke:#007AFF,stroke-width:2px,color:#ffffff</div></div><p><strong>图例说明</strong>：</p><ul><li>🟠 橙色节点：中心词输入（$w_i$）</li><li>🔵 蓝色节点：独立预测的上下文词（$w_{i-2}, w_{i-1}, w_{i+1}, w_{i+2}$）</li></ul><p><strong>输入</strong>：中心词的独热编码 $\mathbf{x} \in {0, 1}^V$</p><p><strong>投影层</strong>：词向量查表</p><div class=math>$$\mathbf{h} = \mathbf{W}^T \mathbf{x} = \mathbf{v}_{w_I}$$</div><p><strong>输出层</strong>：对每个上下文位置独立预测</p><p>假设上下文窗口为 $c$，Skip-gram 假设给定中心词时，各个上下文词的出现是条件独立的：</p><div class=math>$$P(w_{O,1}, \ldots, w_{O,c} \mid w_I) = \prod_{j=1}^{c} P(w_{O,j} \mid w_I)$$</div><p>其中每个条件概率为：</p><div class=math>$$P(w_{O,j} \mid w_I) = \frac{\exp(\mathbf{u}_{w_{O,j}}^T \mathbf{v}_{w_I})}{\sum_{w=1}^{V} \exp(\mathbf{u}_w^T \mathbf{v}_{w_I})}$$</div><h4 id=数学推导-1>数学推导<a hidden class=anchor aria-hidden=true href=#数学推导-1>#</a></h4><p>目标函数：</p><div class=math>$$\mathcal{L} = \frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log P(w_{t+j} \mid w_t)$$</div><p>对于单个上下文词 $w_O$，损失函数：</p><div class=math>$$L = -\log P(w_O \mid w_I) = -\mathbf{u}_{w_O}^T \mathbf{v}_{w_I} + \log \sum_{w=1}^{V} \exp(\mathbf{u}_w^T \mathbf{v}_{w_I})$$</div><p>对 $\mathbf{v}_{w_I}$ 的梯度：</p><div class=math>$$\frac{\partial L}{\partial \mathbf{v}_{w_I}} = -\mathbf{u}_{w_O} + \frac{\sum_{w=1}^{V} \exp(\mathbf{u}_w^T \mathbf{v}_{w_I}) \mathbf{u}_w}{\sum_{w'=1}^{V} \exp(\mathbf{u}_{w'}^T \mathbf{v}_{w_I})} = \sum_{w=1}^{V} (P(w \mid w_I) - \mathbb{1}[w = w_O]) \mathbf{u}_w$$</div><p>对 $\mathbf{u}_w$ 的梯度：</p><div class=math>$$\frac{\partial L}{\partial \mathbf{u}_w} = (P(w \mid w_I) - \mathbb{1}[w = w_O]) \mathbf{v}_{w_I}$$</div><h3 id=23-cbow-vs-skip-gram>2.3 CBOW vs Skip-gram<a hidden class=anchor aria-hidden=true href=#23-cbow-vs-skip-gram>#</a></h3><table><thead><tr><th>特性</th><th>CBOW</th><th>Skip-gram</th></tr></thead><tbody><tr><td>训练方向</td><td>上下文 $\to$ 中心词</td><td>中心词 $\to$ 上下文</td></tr><tr><td>训练速度</td><td>更快</td><td>较慢</td></tr><tr><td>对罕见词效果</td><td>一般</td><td>更好</td></tr><tr><td>对高频词效果</td><td>更好</td><td>一般</td></tr><tr><td>适用于</td><td>大规模语料</td><td>小规模语料</td></tr></tbody></table><p>Mikolov 等人的实验表明：Skip-gram 在处理罕见词和捕捉精细语义关系方面表现更好，而 CBOW 训练速度更快，对高频词建模更平滑。</p><h2 id=第三章训练优化策略>第三章：训练优化策略<a hidden class=anchor aria-hidden=true href=#第三章训练优化策略>#</a></h2><p>原始的 Softmax 需要遍历整个词汇表计算归一化因子，这对于大规模语料（$V \sim 10^5 \sim 10^7$）是不可接受的。Word2Vec 提出了两种优化策略：</p><h3 id=31-层次-softmaxhierarchical-softmax>3.1 层次 Softmax（Hierarchical Softmax）<a hidden class=anchor aria-hidden=true href=#31-层次-softmaxhierarchical-softmax>#</a></h3><p>层次 Softmax 利用二叉树结构（通常是哈夫曼树）将计算复杂度从 $O(V)$ 降低到 $O(\log V)$。</p><h4 id=哈夫曼树构建>哈夫曼树构建<a hidden class=anchor aria-hidden=true href=#哈夫曼树构建>#</a></h4><ul><li>每个词对应一个叶节点，权重为词频</li><li>高频词离根节点更近，路径更短</li><li>构建 Huffman 树，平均路径长度最小化</li></ul><h4 id=概率计算>概率计算<a hidden class=anchor aria-hidden=true href=#概率计算>#</a></h4><p>在二叉树中，从根到叶节点的路径上的每个内部节点代表一个二分类决策。设 $n(w, j)$ 是从根到词 $w$ 的路径上第 $j$ 个节点，$L(w)$ 是路径长度。</p><div class=math>$$P(w \mid w_I) = \prod_{j=1}^{L(w)} \sigma\left([\![ n(w, j+1) = \text{ch}(n(w, j)) ]\!] \cdot (\mathbf{v}'_{n(w,j)})^T \mathbf{v}_{w_I}\right)$$</div><p>其中：</p><ul><li><p>$\sigma(x) = \frac{1}{1 + e^{-x}}$ 是 sigmoid 函数</p></li><li><p>$[![ n(w, j+1) = \text{ch}(n(w, j)) ]!]$ 是指示函数：</p></li></ul><div class=math>$$[\![ n(w, j+1) = \text{ch}(n(w, j)) ]\!] = \begin{cases} 1 & \text{如果 } n(w, j+1) \text{ 是左子节点} \\ -1 & \text{如果 } n(w, j+1) \text{ 是右子节点} \end{cases}$$</div><ul><li><p>$\mathbf{v}&rsquo;_{n(w,j)}$ 是内部节点 $n(w,j)$ 的向量表示</p></li><li><p>$\mathbf{v}_{w_I}$ 是输入词 $w_I$ 的向量</p></li></ul><p>这样，计算 $P(w \mid w_I)$ 只需要遍历路径上的 $O(\log V)$ 个节点，而非全部 $V$ 个词。</p><h3 id=32-负采样negative-sampling>3.2 负采样（Negative Sampling）<a hidden class=anchor aria-hidden=true href=#32-负采样negative-sampling>#</a></h3><p>负采样是另一种更简单的近似方法，也是实际应用中最常用的策略。</p><h4 id=核心思想>核心思想<a hidden class=anchor aria-hidden=true href=#核心思想>#</a></h4><p>将多分类问题转化为二分类问题：</p><ul><li><strong>正样本</strong>：真实的目标词对 $(w_I, w_O)$，标签为 $1$</li><li><strong>负样本</strong>：从噪声分布中采样的词对 $(w_I, w_{\text{neg}})$，标签为 $0$</li></ul><h4 id=目标函数>目标函数<a hidden class=anchor aria-hidden=true href=#目标函数>#</a></h4><div class=math>$$\log \sigma(\mathbf{u}_{w_O}^T \mathbf{v}_{w_I}) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)}[\log \sigma(-\mathbf{u}_{w_i}^T \mathbf{v}_{w_I})]$$</div><p>其中：</p><ul><li>$k$ 是负样本数量（通常 $5 \sim 20$）</li><li>$P_n(w)$ 是噪声分布，通常取 $P_n(w) \propto f(w)^{3/4}$，$f(w)$ 是词频</li><li>$3/4$ 的幂次是为了降低高频词的采样概率，增加罕见词的采样机会</li></ul><h4 id=为什么负采样有效>为什么负采样有效？<a hidden class=anchor aria-hidden=true href=#为什么负采样有效>#</a></h4><p>负采样可以看作是对 Softmax 的近似，其理论基础是<strong>噪声对比估计</strong>（Noise Contrastive Estimation, NCE）。它将密度估计问题转化为区分真实数据和噪声数据的二分类问题。</p><p>与层次 Softmax 相比，负采样的优势：</p><ul><li>实现更简单</li><li>对于小数据集和罕见词效果更好</li><li>训练速度更快（每次更新只需要处理 $k+1$ 个词）</li></ul><h3 id=33-子采样subsampling>3.3 子采样（Subsampling）<a hidden class=anchor aria-hidden=true href=#33-子采样subsampling>#</a></h3><p>除了优化输出层的计算，Word2Vec 还对高频词进行了子采样。像"的&rdquo;、&ldquo;是&rdquo;、&ldquo;在"这样的词出现频率极高，但信息含量很低，而且会拖慢训练。</p><p>子采样策略：以概率 $P(w_i)$ 丢弃词 $w_i$：</p><div class=math>$$P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}}$$</div><p>其中 $f(w_i)$ 是词频，$t$ 是阈值（通常 $10^{-5}$）。当 $f(w_i) > t$ 时，词被丢弃的概率随词频增加而增加。</p><h2 id=第四章词向量的奇妙性质>第四章：词向量的奇妙性质<a hidden class=anchor aria-hidden=true href=#第四章词向量的奇妙性质>#</a></h2><p>训练完成后，Word2Vec 学到的词向量展现出令人惊叹的线性关系。</p><h3 id=41-语义关系的向量算术>4.1 语义关系的向量算术<a hidden class=anchor aria-hidden=true href=#41-语义关系的向量算术>#</a></h3><p>Mikolov 等人发现，词向量能够捕捉各种语义和语法关系：</p><p><img alt=词向量类比可视化 loading=lazy src=/images/plots/word2vec-analogy.png></p><p class=caption>图：词向量空间中的语义关系，king - man + woman ≈ queen</p><div class=math>$$\mathbf{v}_{\text{king}} - \mathbf{v}_{\text{man}} + \mathbf{v}_{\text{woman}} \approx \mathbf{v}_{\text{queen}}$$</div><p>这个经典例子表明：词向量不仅编码了词的语义，还编码了词之间的关系。&ldquo;国王"减去"男人"加上"女人"约等于"女王&rdquo;，这意味着向量空间中捕捉到了<strong>性别</strong>这一语义维度。</p><p>类似的例子还包括：</p><p><strong>首都-国家</strong>：</p><div class=math>$$\mathbf{v}_{\text{Paris}} - \mathbf{v}_{\text{France}} + \mathbf{v}_{\text{Italy}} \approx \mathbf{v}_{\text{Rome}}$$</div><p><strong>时态</strong>：</p><div class=math>$$\mathbf{v}_{\text{walking}} - \mathbf{v}_{\text{walked}} + \mathbf{v}_{\text{swam}} \approx \mathbf{v}_{\text{swimming}}$$</div><p><strong>单复数</strong>：</p><div class=math>$$\mathbf{v}_{\text{apples}} - \mathbf{v}_{\text{apple}} + \mathbf{v}_{\text{car}} \approx \mathbf{v}_{\text{cars}}$$</div><p><strong>比较级</strong>：</p><div class=math>$$\mathbf{v}_{\text{bigger}} - \mathbf{v}_{\text{big}} + \mathbf{v}_{\text{small}} \approx \mathbf{v}_{\text{smaller}}$$</div><h3 id=42-为什么词向量有这种性质>4.2 为什么词向量有这种性质？<a hidden class=anchor aria-hidden=true href=#42-为什么词向量有这种性质>#</a></h3><p>这种线性关系的出现并非偶然，而是分布式假说的数学体现。考虑 Skip-gram 的目标：预测上下文词。如果"国王"和"女王"在相似的上下文中出现（&rdquo;____统治着这个国家&rdquo;），它们的向量就会相似。</p><p>更重要的是，词向量编码了<strong>语义差异</strong>。&ldquo;国王"和"女王"的差向量大致等于"男人"和"女人"的差向量，因为它们都与"性别"这一概念相关。</p><p>从几何角度看，Word2Vec 学习到的向量空间将语义关系编码为方向。每一个重要的语义维度（性别、时态、单复数等）对应向量空间中的一个方向。</p><h3 id=43-余弦相似度与词语类比>4.3 余弦相似度与词语类比<a hidden class=anchor aria-hidden=true href=#43-余弦相似度与词语类比>#</a></h3><p>衡量词向量相似度的标准方法是<strong>余弦相似度</strong>：</p><div class=math>$$\text{similarity}(\mathbf{u}, \mathbf{v}) = \cos(\theta) = \frac{\mathbf{u}^T \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|}$$</div><p>两个向量的夹角越小（方向越接近），余弦值越接近 $1$，表示语义越相似。</p><p><strong>词语类比任务</strong>（Word Analogy）是评估词向量的标准任务：</p><p><img alt=词向量相似度热力图 loading=lazy src=/images/plots/word2vec-similarity.png></p><p class=caption>图：词向量余弦相似度矩阵，展示语义相近词的关联程度</p><p>对于关系"A 之于 B，如同 C 之于 D&rdquo;，寻找 D 等价于：</p><div class=math>$$\mathbf{d} = \arg\max_{\mathbf{d}'} \frac{(\mathbf{b} - \mathbf{a} + \mathbf{c})^T \mathbf{d}'}{\|\mathbf{b} - \mathbf{a} + \mathbf{c}\| \|\mathbf{d}'\|}$$</div><p>Mikolov 等人报告，在包含 $1.6$ 亿词的训练数据上，Skip-gram 模型在语义类比任务上达到了 $55%$ 的准确率，在语法类比任务上达到了 $59%$ 的准确率。</p><h2 id=第五章实现与实战>第五章：实现与实战<a hidden class=anchor aria-hidden=true href=#第五章实现与实战>#</a></h2><h3 id=51-伪代码实现>5.1 伪代码实现<a hidden class=anchor aria-hidden=true href=#51-伪代码实现>#</a></h3><p>以下是 Skip-gram 负采样的简化伪代码：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 初始化</span>
</span></span><span class=line><span class=cl><span class=n>V</span> <span class=o>=</span> <span class=n>vocabulary_size</span>
</span></span><span class=line><span class=cl><span class=n>d</span> <span class=o>=</span> <span class=n>embedding_dim</span>
</span></span><span class=line><span class=cl><span class=n>W_input</span> <span class=o>=</span> <span class=n>random</span><span class=p>(</span><span class=n>V</span><span class=p>,</span> <span class=n>d</span><span class=p>)</span>    <span class=c1># 输入词向量矩阵</span>
</span></span><span class=line><span class=cl><span class=n>W_output</span> <span class=o>=</span> <span class=n>random</span><span class=p>(</span><span class=n>V</span><span class=p>,</span> <span class=n>d</span><span class=p>)</span>   <span class=c1># 输出词向量矩阵</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 训练</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>sentence</span> <span class=ow>in</span> <span class=n>corpus</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>target</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>sentence</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 获取上下文窗口</span>
</span></span><span class=line><span class=cl>        <span class=n>context</span> <span class=o>=</span> <span class=n>sentence</span><span class=p>[</span><span class=nb>max</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=o>-</span><span class=n>window</span><span class=p>):</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=n>sentence</span><span class=p>[</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>:</span><span class=n>i</span><span class=o>+</span><span class=n>window</span><span class=o>+</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>context_word</span> <span class=ow>in</span> <span class=n>context</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 正样本更新</span>
</span></span><span class=line><span class=cl>            <span class=n>z</span> <span class=o>=</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>dot</span><span class=p>(</span><span class=n>W_output</span><span class=p>[</span><span class=n>target</span><span class=p>],</span> <span class=n>W_input</span><span class=p>[</span><span class=n>context_word</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>            <span class=n>g</span> <span class=o>=</span> <span class=p>(</span><span class=n>z</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=o>*</span> <span class=n>learning_rate</span>
</span></span><span class=line><span class=cl>            <span class=n>W_output</span><span class=p>[</span><span class=n>target</span><span class=p>]</span> <span class=o>-=</span> <span class=n>g</span> <span class=o>*</span> <span class=n>W_input</span><span class=p>[</span><span class=n>context_word</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>W_input</span><span class=p>[</span><span class=n>context_word</span><span class=p>]</span> <span class=o>-=</span> <span class=n>g</span> <span class=o>*</span> <span class=n>W_output</span><span class=p>[</span><span class=n>target</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>            <span class=c1># 负样本更新</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>negative_samples</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>negative</span> <span class=o>=</span> <span class=n>sample_from_noise_distribution</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=n>z</span> <span class=o>=</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>dot</span><span class=p>(</span><span class=n>W_output</span><span class=p>[</span><span class=n>negative</span><span class=p>],</span> <span class=n>W_input</span><span class=p>[</span><span class=n>context_word</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>                <span class=n>g</span> <span class=o>=</span> <span class=n>z</span> <span class=o>*</span> <span class=n>learning_rate</span>
</span></span><span class=line><span class=cl>                <span class=n>W_output</span><span class=p>[</span><span class=n>negative</span><span class=p>]</span> <span class=o>-=</span> <span class=n>g</span> <span class=o>*</span> <span class=n>W_input</span><span class=p>[</span><span class=n>context_word</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=n>W_input</span><span class=p>[</span><span class=n>context_word</span><span class=p>]</span> <span class=o>-=</span> <span class=n>g</span> <span class=o>*</span> <span class=n>W_output</span><span class=p>[</span><span class=n>negative</span><span class=p>]</span>
</span></span></code></pre></div><h3 id=52-超参数选择>5.2 超参数选择<a hidden class=anchor aria-hidden=true href=#52-超参数选择>#</a></h3><p><img alt="Word2Vec 训练曲线" loading=lazy src=/images/plots/word2vec-training.png></p><p class=caption>图：CBOW 与 Skip-gram 训练过程中损失函数的变化</p><table><thead><tr><th>超参数</th><th>推荐值</th><th>说明</th></tr></thead><tbody><tr><td>词向量维度 $d$</td><td>$100 \sim 300$</td><td>维度越高表达能力越强，但也更容易过拟合</td></tr><tr><td>上下文窗口 $c$</td><td>$5 \sim 10$</td><td>Skip-gram 可用较小窗口，CBOW 可用较大窗口</td></tr><tr><td>负采样数 $k$</td><td>$5 \sim 20$</td><td>小数据集用大值，大数据集用小值</td></tr><tr><td>学习率 $\eta$</td><td>$0.01 \sim 0.025$</td><td>常用线性衰减策略</td></tr><tr><td>子采样阈值 $t$</td><td>$10^{-5}$</td><td>控制高频词的丢弃率</td></tr><tr><td>最小词频</td><td>$5 \sim 10$</td><td>过滤罕见词，减少噪声</td></tr></tbody></table><h3 id=53-使用-gensim-训练>5.3 使用 Gensim 训练<a hidden class=anchor aria-hidden=true href=#53-使用-gensim-训练>#</a></h3><p>实际应用中，我们通常使用成熟的库如 Gensim：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>gensim.models</span> <span class=kn>import</span> <span class=n>Word2Vec</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>gensim.utils</span> <span class=kn>import</span> <span class=n>simple_preprocess</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 准备语料（分词后的句子列表）</span>
</span></span><span class=line><span class=cl><span class=n>sentences</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=s2>&#34;我&#34;</span><span class=p>,</span> <span class=s2>&#34;喜欢&#34;</span><span class=p>,</span> <span class=s2>&#34;自然&#34;</span><span class=p>,</span> <span class=s2>&#34;语言&#34;</span><span class=p>,</span> <span class=s2>&#34;处理&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=s2>&#34;机器&#34;</span><span class=p>,</span> <span class=s2>&#34;学习&#34;</span><span class=p>,</span> <span class=s2>&#34;是&#34;</span><span class=p>,</span> <span class=s2>&#34;人工智能&#34;</span><span class=p>,</span> <span class=s2>&#34;的&#34;</span><span class=p>,</span> <span class=s2>&#34;分支&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=c1># ... 更多句子</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 训练模型</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Word2Vec</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>sentences</span><span class=o>=</span><span class=n>sentences</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>vector_size</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>      <span class=c1># 词向量维度</span>
</span></span><span class=line><span class=cl>    <span class=n>window</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>             <span class=c1># 上下文窗口</span>
</span></span><span class=line><span class=cl>    <span class=n>min_count</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>          <span class=c1># 最小词频</span>
</span></span><span class=line><span class=cl>    <span class=n>workers</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>            <span class=c1># 并行线程数</span>
</span></span><span class=line><span class=cl>    <span class=n>sg</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>                 <span class=c1># 1=Skip-gram, 0=CBOW</span>
</span></span><span class=line><span class=cl>    <span class=n>negative</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>           <span class=c1># 负采样数</span>
</span></span><span class=line><span class=cl>    <span class=n>sample</span><span class=o>=</span><span class=mf>1e-5</span><span class=p>,</span>          <span class=c1># 子采样阈值</span>
</span></span><span class=line><span class=cl>    <span class=n>epochs</span><span class=o>=</span><span class=mi>5</span>              <span class=c1># 训练轮数</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 获取词向量</span>
</span></span><span class=line><span class=cl><span class=n>vector</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>wv</span><span class=p>[</span><span class=s2>&#34;机器学习&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 找最相似的词</span>
</span></span><span class=line><span class=cl><span class=n>similar</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>wv</span><span class=o>.</span><span class=n>most_similar</span><span class=p>(</span><span class=s2>&#34;人工智能&#34;</span><span class=p>,</span> <span class=n>topn</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 词语类比</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>wv</span><span class=o>.</span><span class=n>most_similar</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>positive</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;女王&#34;</span><span class=p>,</span> <span class=s2>&#34;男人&#34;</span><span class=p>],</span> 
</span></span><span class=line><span class=cl>    <span class=n>negative</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;国王&#34;</span><span class=p>],</span> 
</span></span><span class=line><span class=cl>    <span class=n>topn</span><span class=o>=</span><span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><h2 id=第六章影响与演进>第六章：影响与演进<a hidden class=anchor aria-hidden=true href=#第六章影响与演进>#</a></h2><h3 id=61-word2vec-的革命性意义>6.1 Word2Vec 的革命性意义<a hidden class=anchor aria-hidden=true href=#61-word2vec-的革命性意义>#</a></h3><p>Word2Vec 的提出标志着自然语言处理进入了深度学习时代。它的影响可以从以下几个维度理解：</p><p><strong>1. 技术范式转变</strong></p><ul><li><strong>从符号到连续</strong>：将离散的词符号转化为连续的向量表示</li><li><strong>从手工特征到自动学习</strong>：无需语言学知识，自动从数据中学习语义</li><li><strong>从稀疏到稠密</strong>：低维稠密向量计算更高效，泛化能力更强</li></ul><p><strong>2. 工业应用落地</strong></p><p>Word2Vec 训练速度快、实现简单，很快在工业界广泛应用：</p><ul><li><strong>搜索引擎</strong>：查询扩展、语义匹配</li><li><strong>推荐系统</strong>：物品/用户向量表示</li><li><strong>广告系统</strong>：关键词定向、受众画像</li><li><strong>机器翻译</strong>：语义对齐、双语词典构建</li></ul><p><strong>3. 学术影响</strong></p><p>截至 2024 年，Mikolov 的 Word2Vec 论文被引用超过 $50{,}000$ 次，是 NLP 领域最具影响力的论文之一。</p><h3 id=62-后续发展>6.2 后续发展<a hidden class=anchor aria-hidden=true href=#62-后续发展>#</a></h3><p>Word2Vec 开创了词嵌入的先河，后续研究在多个方向上进行拓展：</p><p><strong>GloVe（Global Vectors）</strong></p><p>Pennington 等人在 2014 年提出 GloVe，结合了全局统计信息（共现矩阵）和局部上下文信息（窗口）。其目标函数直接优化共现矩阵与词向量内积的关系：</p><div class=math>$$J = \sum_{i,j} f(X_{ij}) (\mathbf{w}_i^T \tilde{\mathbf{w}}_j + b_i + \tilde{b}_j - \log X_{ij})^2$$</div><p>GloVe 在某些任务上表现优于 Word2Vec，且训练更稳定。</p><p><strong>FastText</strong></p><p>2016 年，Facebook 提出 FastText，将词表示为字符 n-gram 的组合：</p><div class=math>$$\mathbf{v}_w = \sum_{g \in \mathcal{G}_w} \mathbf{z}_g$$</div><p>这种方法能够处理未登录词（OOV），并捕捉词的形态信息。</p><p><strong>Contextualized Embeddings</strong></p><p>Word2Vec 是<strong>静态词向量</strong>：每个词只有一个固定的向量表示。这无法处理一词多义问题（如"苹果"公司 vs 水果）。</p><p>2018 年前后，ELMo、GPT、BERT 等模型提出<strong>动态词向量</strong>（Contextualized Embeddings），根据上下文为每个词实例生成不同的表示：</p><div class=math>$$\mathbf{h}_{w, \text{context}} = \text{Transformer}(w, \text{context})$$</div><p>这标志着 NLP 进入了预训练语言模型时代，但 Word2Vec 奠定的分布式语义基础依然适用。</p><h3 id=63-跨领域应用>6.3 跨领域应用<a hidden class=anchor aria-hidden=true href=#63-跨领域应用>#</a></h3><p>Word2Vec 的核心思想——<strong>将离散符号嵌入连续向量空间</strong>——已被推广到众多领域：</p><table><thead><tr><th>领域</th><th>应用</th></tr></thead><tbody><tr><td>图神经网络</td><td>Node2Vec, DeepWalk（节点嵌入）</td></tr><tr><td>生物信息学</td><td>BioVec, ProtVec（蛋白质/DNA 序列）</td></tr><tr><td>社交网络</td><td>DeepWalk, LINE（用户/社区嵌入）</td></tr><tr><td>知识图谱</td><td>TransE, RotatE（实体/关系嵌入）</td></tr><tr><td>代码分析</td><td>Code2Vec, CodeBERT（代码嵌入）</td></tr><tr><td>推荐系统</td><td>Item2Vec, Prod2Vec（商品嵌入）</td></tr></tbody></table><h2 id=结语一个词嵌入的时代>结语：一个词嵌入的时代<a hidden class=anchor aria-hidden=true href=#结语一个词嵌入的时代>#</a></h2><p>Word2Vec 不仅是一个算法，更是一种思想的胜利：<strong>语言的语义可以通过分布式的统计规律来捕捉</strong>。</p><p>从 2013 年 Mikolov 等人的开创性论文，到今天动辄千亿参数的语言模型，词嵌入始终是自然语言处理的核心技术。无论是简单的文本分类，还是复杂的对话系统，将语言符号转化为机器可理解的向量表示都是不可或缺的第一步。</p><p>回顾 Word2Vec 的发展历程，我们可以得到几点启示：</p><ol><li><p><strong>简单即美</strong>：去除隐藏层的简化反而提升了性能，说明架构设计应当服务于目标任务。</p></li><li><p><strong>数据即知识</strong>：Word2Vec 不需要人工标注，从海量无标注文本中自动学习语义，体现了无监督学习的威力。</p></li><li><p><strong>几何即语义</strong>：词的语义关系编码在向量空间的几何结构中，这一洞见影响了后续所有表示学习研究。</p></li></ol><p>正如 Mikolov 在论文结尾所言：&ldquo;我们的工作表明，简单的模型训练海量数据，往往能击败复杂的模型训练小量数据。&rdquo; 这一哲学贯穿于深度学习的发展历程，从 Word2Vec 到 GPT-4，从未改变。</p><blockquote><p>&ldquo;The meaning of a word is its use in the language.&rdquo; — Ludwig Wittgenstein</p></blockquote><hr><p><strong>参考资料</strong></p><ol><li><p>Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. <em>arXiv preprint arXiv:1301.3781</em>.</p></li><li><p>Mikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. <em>NIPS</em>.</p></li><li><p>Bengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003). A Neural Probabilistic Language Model. <em>JMLR</em>.</p></li><li><p>Pennington, J., Socher, R., & Manning, C. (2014). GloVe: Global Vectors for Word Representation. <em>EMNLP</em>.</p></li><li><p>Rong, X. (2014). word2vec Parameter Learning Explained. <em>arXiv preprint arXiv:1411.2738</em>.</p></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/word2vec/>Word2Vec</a></li><li><a href=https://s-ai-unix.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/>自然语言处理</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-30-seq2seq-paper-explained/><span class=title>« Prev</span><br><span>AI 论文解读系列：Seq2Seq--从序列到序列的革命</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-30-gpt3-few-shot-learners-paper/><span class=title>Next »</span><br><span>AI 论文解读系列：GPT-3——当语言模型学会举一反三</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：Word2Vec - 词向量的革命 on x" href="https://x.com/intent/tweet/?text=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aWord2Vec%20-%20%e8%af%8d%e5%90%91%e9%87%8f%e7%9a%84%e9%9d%a9%e5%91%bd&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-word2vec-paper-explained%2f&amp;hashtags=Word2Vec%2c%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%e5%a4%84%e7%90%86%2c%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%2c%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：Word2Vec - 词向量的革命 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-word2vec-paper-explained%2f&amp;title=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aWord2Vec%20-%20%e8%af%8d%e5%90%91%e9%87%8f%e7%9a%84%e9%9d%a9%e5%91%bd&amp;summary=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aWord2Vec%20-%20%e8%af%8d%e5%90%91%e9%87%8f%e7%9a%84%e9%9d%a9%e5%91%bd&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-word2vec-paper-explained%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：Word2Vec - 词向量的革命 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-word2vec-paper-explained%2f&title=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aWord2Vec%20-%20%e8%af%8d%e5%90%91%e9%87%8f%e7%9a%84%e9%9d%a9%e5%91%bd"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：Word2Vec - 词向量的革命 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-word2vec-paper-explained%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>