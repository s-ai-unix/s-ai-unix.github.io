<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作 | s-ai-unix's Blog</title><meta name=keywords content="深度学习,综述,神经网络,大语言模型"><meta name=description content="深入解读 Meta AI 的 Llama 3 论文，从 Scaling Laws、模型架构到多模态扩展，全面剖析这个拥有 405B 参数的开源大模型集群的设计理念与技术细节。"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-31-ai-paper-llama3-herd-of-models/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-31-ai-paper-llama3-herd-of-models/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-31-ai-paper-llama3-herd-of-models/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作"><meta property="og:description" content="深入解读 Meta AI 的 Llama 3 论文，从 Scaling Laws、模型架构到多模态扩展，全面剖析这个拥有 405B 参数的开源大模型集群的设计理念与技术细节。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-31T09:30:00+08:00"><meta property="article:modified_time" content="2026-01-31T09:30:00+08:00"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="综述"><meta property="article:tag" content="神经网络"><meta property="article:tag" content="大语言模型"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/llama3-herd-cover.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/llama3-herd-cover.jpg"><meta name=twitter:title content="AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作"><meta name=twitter:description content="深入解读 Meta AI 的 Llama 3 论文，从 Scaling Laws、模型架构到多模态扩展，全面剖析这个拥有 405B 参数的开源大模型集群的设计理念与技术细节。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作","item":"https://s-ai-unix.github.io/posts/2026-01-31-ai-paper-llama3-herd-of-models/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作","name":"AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作","description":"深入解读 Meta AI 的 Llama 3 论文，从 Scaling Laws、模型架构到多模态扩展，全面剖析这个拥有 405B 参数的开源大模型集群的设计理念与技术细节。","keywords":["深度学习","综述","神经网络","大语言模型"],"articleBody":"引言：开源 AI 的黎明 2024 年 7 月 23 日，Meta AI 发布了一篇重磅论文——《The Llama 3 Herd of Models》。这篇论文不仅介绍了一个拥有 4050 亿参数的巨型语言模型，更标志着开源人工智能正式迈入了与闭源巨头分庭抗礼的新纪元。\n回想 2022 年底，ChatGPT 的横空出世让整个 AI 领域为之震动。然而，最强大的模型始终被封闭在 OpenAI、Google 等公司的围墙之内。研究者无法探究其内部机理，开发者无法自由定制，这种\"黑箱\"状态严重阻碍了 AI 技术的普惠发展。\nLlama 3 的出现改变了这一切。Meta 不仅开源了完整的模型权重，还详细披露了从数据筛选到训练优化的每一个技术细节。这意味着，任何研究者和开发者都可以在自己的硬件上运行这个媲美 GPT-4 的模型，深入理解它的工作原理，甚至在此基础上进行创新。\n本文将带领读者深入这篇 92 页的论文，从数据、规模、复杂性管理三个核心维度，层层剥开 Llama 3 的技术奥秘。\n第一章：模型概览 —— “模型群\"的设计理念 1.1 为什么叫 “Herd”（群）？ 论文标题中的 “Herd of Models” 并非随意命名。Meta 同时发布了三个不同规模的模型：\n模型 参数量 上下文长度 目标场景 Llama 3 8B $8 \\times 10^9$ 128K tokens 边缘设备、低延迟推理 Llama 3 70B $70 \\times 10^9$ 128K tokens 平衡性能与效率 Llama 3 405B $405 \\times 10^9$ 128K tokens 顶级性能、复杂推理 这种\"群\"策略的核心思想是：用一个旗舰模型（405B）指导整个家族的优化方向，同时让每个成员在特定场景下发挥最大价值。\n1.2 核心能力矩阵 Llama 3 原生支持四大核心能力：\nflowchart TD subgraph Capabilities [Llama 3 核心能力] A[多语言能力] --\u003e A1[支持 8+ 种语言] A --\u003e A2[跨语言推理] B[代码生成] --\u003e B1[Python/C++/Java] B --\u003e B2[复杂算法实现] C[数学推理] --\u003e C1[GSM8K 96.8%] C --\u003e C2[MATH 73.8%] D[工具使用] --\u003e D1[零样本调用] D --\u003e D2[多轮工具链] end style Capabilities fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff style A fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style B fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style C fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style D fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style A1 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff style A2 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff style B1 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff style B2 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff style C1 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff style C2 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff style D1 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff style D2 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff 图例说明：\n🔵 蓝色：核心概念 🟢 绿色：主要能力类别 🩵 浅蓝：具体能力指标 特别值得注意的是，Llama 3 8B 在某些基准测试上已经超过了 Llama 2 70B 的表现。这种\"以小博大\"的能力提升，正是数据质量和训练策略优化的直接体现。\n第二章：预训练的艺术 —— 数据为王 2.1 数据规模的跃升 Llama 3 的预训练数据量达到了惊人的 15.6 万亿 tokens，相比 Llama 2 的 1.8 万亿，增长了近 9 倍。这个数字是什么概念？\n假设一本书平均有 10 万字，15.6 万亿 tokens 大约相当于：\n$$ \\frac{15.6 \\times 10^{12}}{10^5} \\approx 1.56 \\times 10^8 \\text{ 本书} $$\n约 1.56 亿本书！这几乎涵盖了人类文明的绝大部分书面知识。\n2.2 数据质量筛选的五重关卡 Meta 构建了一套工业级的数据清洗流水线，包含五个关键步骤：\nflowchart LR A[原始网页数据] --\u003e B{安全过滤} B --\u003e|通过| C[HTML解析] B --\u003e|拒绝| X[丢弃] C --\u003e D[多层级去重] D --\u003e E[启发式过滤] E --\u003e F[模型质量评分] F --\u003e G[高质量语料库] style A fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff style B fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff style C fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style D fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style E fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style F fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style G fill:#32D74B,stroke:#32D74B,stroke-width:3px,color:#ffffff style X fill:#FF3B30,stroke:#FF3B30,stroke-width:2px,color:#ffffff 第一关：安全与隐私过滤 移除含有不安全内容的域名 过滤成人内容 清除个人身份信息（PII） 第二关：文本提取与清洗 Meta 开发了一个自定义 HTML 解析器，相比第三方工具（如 BeautifulSoup），它在两个关键指标上更优：\n模板去除精度：准确识别并移除导航栏、广告、版权声明等\"样板\"内容 内容召回率：确保不遗漏正文、数学公式、代码块等有价值信息 有趣的是，实验发现 Markdown 格式对模型性能有害，因此所有 Markdown 标记都被移除，仅保留纯文本。\n第三关：三级去重策略 去重级别 方法 目的 URL 级 全局 URL 去重 保留每页面的最新版本 文档级 MinHash 算法 删除近似重复文档 行级 桶内行频率统计 移除残留模板（如导航菜单） 行级去重采用激进策略：在每个 3000 万文档的桶中，出现超过 6 次的行会被删除。这虽然会误伤一些常用短语，但整体质量提升明显。\n第四关：启发式过滤 使用多种启发式规则剔除低质量内容：\n重复 $n$-gram 覆盖率：识别日志文件、错误消息等重复内容 脏词计数：捕获未被域名黑名单覆盖的成人内容 KL 散度异常检测：过滤含有过多异常 token 的文档 第五关：模型质量评分 这是最关键的一步。Meta 使用 Llama 2 作为\"教师模型”，训练了一系列质量分类器：\n通用质量分类器：基于 Llama 2 的判断，预测文档是否\"值得被 Wikipedia 引用\" 代码专用分类器：识别包含代码和自然语言交织的文档 数学推理分类器：筛选 STEM 领域的推理内容 2.3 数据配比的艺术 最终的数据组成大致为：\n网页数据：约 50%（经严格筛选） 代码数据：约 17% 多语言数据：约 17%（覆盖 8+ 种语言） 数学/推理数据：约 10% 其他：约 6% 这种精心设计的配比，确保了模型在各个维度上的均衡发展。\n第三章：Scaling Laws —— 规模的科学 3.1 Chinchilla 最优 vs Llama 3 策略 2022 年，DeepMind 的 Chinchilla 论文提出了著名的 Scaling Laws：\n$$ L(N, D) = \\frac{A}{N^{\\alpha}} + \\frac{B}{D^{\\beta}} + L_{\\infty} $$\n其中：\n$N$ 是模型参数量 $D$ 是训练数据量 $L$ 是验证损失 $\\alpha \\approx 0.5$, $\\beta \\approx 0.5$ 根据 Chinchilla 最优计算，给定 $3.8 \\times 10^{25}$ FLOPs 的算力预算，最优配置约为 400B 参数 + 400B tokens。\n然而，Llama 3 405B 使用了 15.6 万亿 tokens——这是计算最优配置的 39 倍！\n3.2 为什么\"过训练\"？ Meta 刻意选择了\"过训练\"（over-training）策略，原因有三：\n推理效率优先：在相同的推理预算下，小模型 + 多数据 往往优于 大模型 + 少数据 知识密度：更多的数据曝光让模型\"记住\"更多知识 下游任务泛化：充分训练使模型更好地内化语言规律 从图中可以看到，Llama 3 的三个模型都位于 Chinchilla 最优曲线的右上方——这意味着它们都经过了\"过训练\"。但这种策略换来了什么呢？\n答案是：更强的推理能力。对于实际部署，推理成本往往远超训练成本。一个训练充分的小模型，可以比同等推理成本下的大模型表现更好。\n3.3 训练计算量分布 Llama 3 405B 的总训练计算量约为 $3.8 \\times 10^{25}$ FLOPs，分布如下：\n预训练阶段消耗了绝大部分算力（约 90%），这符合\"数据密集型\"训练的特点。\n第四章：模型架构 —— 保守中的创新 4.1 坚持 Dense Transformer 当 GPT-4、Mixtral 等模型纷纷采用 MoE（Mixture of Experts，专家混合）架构时，Llama 3 选择了相对保守的 Dense Transformer——即每个参数都参与每次前向传播。\n这个决策背后的考量是：最大化训练稳定性。\nMoE 架构虽然可以在相同激活参数下扩展总参数量，但带来了额外的复杂性：\n路由机制的不稳定性 负载均衡的挑战 专家崩溃（expert collapse）风险 对于需要稳定训练数月、消耗数亿美元算力的旗舰模型，简单即美德。\n4.2 关键架构参数 参数 Llama 3 8B Llama 3 70B Llama 3 405B 层数 $L$ 32 80 126 模型维度 $d_{model}$ 4096 8192 16384 注意力头数 $n_h$ 32 64 128 每头维度 $d_h$ 128 128 128 上下文长度 128K 128K 128K 词汇表大小 $ V $ 128K 4.3 分组查询注意力（GQA） Llama 3 采用了 Grouped-Query Attention（GQA），这是提升推理效率的关键设计。\n标准多头注意力（MHA）中，每个注意力头都有独立的查询（Query）、键（Key）、值（Value）投影：\n$$ \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V} $$\n在 GQA 中，多个查询头共享同一组键和值头：\nLlama 3 8B：8 个 KV 头（4 个查询头共享 1 个 KV 头） Llama 3 70B/405B：8 个 KV 头（8/16 个查询头共享 1 个 KV 头） 这带来了显著的内存节省。对于长度为 $L$、维度为 $d$ 的序列，KV Cache 的内存占用从 $O(L \\cdot n_h \\cdot d)$ 降低到 $O(L \\cdot n_{kv} \\cdot d)$，其中 $n_{kv} \\ll n_h$。\n4.4 RoPE 位置编码的演进 Llama 3 继续使用 Rotary Position Embedding（RoPE），但对基础频率进行了调整：\n$$ \\text{RoPE}(\\mathbf{x}, m) = \\mathbf{x} \\odot e^{i m \\theta_j} $$\n其中 $\\theta_j = \\theta_{base}^{-2(j-1)/d}$，$j$ 是维度索引。\n关键调整：\n基础频率 $\\theta_{base}$ 从 Llama 2 的 10000 提升到 500000 这使得模型能更好地处理长距离依赖，支持长达 128K tokens 的上下文 第五章：长上下文 —— 从 8K 到 128K 的进化 5.1 渐进式扩展策略 Llama 3 并非一开始就训练 128K 上下文，而是采用了渐进式扩展策略：\nflowchart LR A[预训练\n8K 上下文] --\u003e B[继续预训练\n逐步扩展到 128K] B --\u003e C[长上下文微调\n高质量数据] style A fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff style B fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff style C fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff 具体步骤：\n预训练阶段：使用 8K 上下文进行主要训练 继续预训练：逐步增加上下文长度，从 8K → 16K → 32K → 64K → 128K 退火阶段：在高质量数据上进行长上下文微调 5.2 关键技术：文档掩码 长上下文训练的一个关键创新是文档间掩码（Inter-document Masking）：\n在标准预训练中，同一批次的不同文档被拼接成一个长序列。传统方法允许注意力跨越文档边界，但 Llama 3 引入了一种特殊的注意力掩码——禁止不同文档之间的注意力。\n这种设计的直觉是：\n来自不同文档的 token 没有语义关联 强制模型专注于文档内部的长期依赖 在长序列上保持训练稳定性 5.3 性能验证：Needle-in-Haystack 为了验证长上下文能力，Meta 使用了经典的 “大海捞针” 测试：在一个极长文档的随机位置插入一句特定的话（“针”），然后要求模型检索这句话。\n从右侧图可以看到，即使在 128K tokens 的全长度下，Llama 3 的检索准确率仍保持在 97% 左右。这意味着模型可以可靠地处理整本书、长篇文章或大量代码库。\n第六章：后训练的艺术 —— 从语言模型到助手 6.1 后训练流程概览 预训练后的模型虽然掌握了语言知识和世界知识，但它还不会按照指令行事。后训练的目标是将这个\"知识库\"转化为\"有用且安全的助手\"。\nLlama 3 的后训练采用了一个相对简单但有效的流程：\nflowchart TD subgraph Pretrain [预训练模型] PT[Llama 3 Base] end subgraph Posttrain [后训练流程] SFT1[SFT 第1轮] --\u003e RS1[拒绝采样] RS1 --\u003e DPO1[DPO 第1轮] DPO1 --\u003e SFT2[SFT 第2轮] SFT2 --\u003e RS2[拒绝采样] RS2 --\u003e DPO2[DPO 第2轮] DPO2 --\u003e SFTN[SFT 第N轮...] end PT --\u003e SFT1 SFTN --\u003e Final[最终模型] style PT fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff style SFT1 fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style RS1 fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style DPO1 fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style SFT2 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff style RS2 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff style DPO2 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff style SFTN fill:#8E8E93,stroke:#8E8E93,stroke-width:1px,color:#ffffff style Final fill:#32D74B,stroke:#32D74B,stroke-width:3px,color:#ffffff 6.2 监督微调（SFT） SFT 阶段使用人工标注的高质量指令数据。关键策略包括：\n多样化数据：涵盖问答、代码、数学、推理、工具使用等多种任务 多语言覆盖：确保非英语能力不退化 质量筛选：使用模型自动评估 + 人工审核，只保留高质量样本 6.3 拒绝采样（Rejection Sampling） 这是一个提升模型推理能力的巧妙技巧：\n给定一个问题，让当前模型生成 $N$ 个候选答案（$N$ 通常设为 4-8） 使用奖励模型或人工标准，评估每个答案的质量 只保留最高分答案，用它构造新的训练样本 用这些\"精选\"样本继续 SFT 这相当于让模型反复练习它最擅长的解法，类似于学生的\"错题本\"学习法。\n6.4 直接偏好优化（DPO） DPO 是 Llama 3 对齐人类偏好的核心算法。相比传统的 RLHF（基于人类反馈的强化学习），DPO 更简单高效。\n核心思想是：对于每个查询，收集一对回答 $(y_w, y_l)$，其中 $y_w$ 是人类偏好的\"胜\"回答，$y_l$ 是\"负\"回答。DPO 直接优化以下目标：\n$$ \\mathcal{L}{\\text{DPO}} = -\\mathbb{E}{(x, y_w, y_l)}\\left[\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\right] $$\n其中：\n$\\pi_\\theta$ 是当前策略（待优化的模型） $\\pi_{\\text{ref}}$ 是参考策略（通常是 SFT 后的模型） $\\beta$ 是温度参数，控制优化强度 直观理解：DPO 试图最大化胜回答与负回答之间的对数概率差距，同时不要让模型偏离参考策略太远。\n第七章：多模态扩展 —— 看得懂、听得见的 Llama 7.1 组合式架构设计 除了纯文本模型，论文还介绍了 Llama 3 的多模态扩展——图像理解和语音识别能力。关键在于组合式（Compositional）设计：\n不从头训练一个端到端多模态模型，而是将预训练的视觉/语音编码器与预训练的语言模型通过轻量级适配器连接起来。\nflowchart TB subgraph Vision [视觉分支] VInput[图像输入] --\u003e ViT[ViT 编码器] ViT --\u003e VAdapter[视觉适配器] end subgraph Speech [语音分支] SInput[语音输入] --\u003e SEncoder[语音编码器] SEncoder --\u003e SAdapter[语音适配器] end subgraph Core [核心] LLM[Llama 3\n语言模型] end VAdapter --\u003e LLM SAdapter --\u003e LLM TInput[文本输入] --\u003e LLM LLM --\u003e Output[文本输出] style VInput fill:#007AFF,stroke:#007AFF,stroke-width:2px,color:#ffffff style ViT fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style VAdapter fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff style SInput fill:#007AFF,stroke:#007AFF,stroke-width:2px,color:#ffffff style SEncoder fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style SAdapter fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff style LLM fill:#AF52DE,stroke:#AF52DE,stroke-width:3px,color:#ffffff style TInput fill:#007AFF,stroke:#007AFF,stroke-width:2px,color:#ffffff style Output fill:#32D74B,stroke:#32D74B,stroke-width:2px,color:#ffffff 图例说明：\n🔵 蓝色：输入模态 🟢 绿色：预训练编码器 🟠 橙色：可训练适配器 🟣 紫色：冻结的语言模型核心 🟢 浅绿：输出 7.2 视觉适配器训练 视觉分支采用 Cross-Attention 适配器：\n使用预训练的 Vision Transformer（ViT）提取图像特征 在语言模型的每一层插入 Cross-Attention 层 Cross-Attention 的 Query 来自语言模型，Key/Value 来自 ViT 关键：训练时只更新适配器和 ViT 的参数，语言模型参数冻结 这种设计的优势：\n保留语言模型的全部能力 避免多模态训练\"破坏\"语言能力 可独立优化视觉-语言对齐 7.3 视频理解扩展 在图像适配器基础上，通过添加时序聚合器（Temporal Aggregator）实现视频理解：\n采样多帧图像，分别通过图像编码器 使用时序注意力聚合帧间信息 支持时序推理（如\"视频中发生了什么\"） 实验表明，即使在较少的视频数据上训练，这种架构也能达到与专用视频模型竞争的性能。\n7.4 语音适配器 语音分支采用类似的策略：\n语音编码器：基于 Conformer 架构，预训练于海量语音数据 适配器：将语音表示映射到语言模型的 token 空间 训练：联合优化适配器和编码器，语言模型冻结 值得注意的是，语音适配器训练后，模型可以直接理解语音指令并生成文本回答，实现了端到端的语音交互。\n第八章：性能评估 —— 与 GPT-4 的正面对决 8.1 基准测试结果 Llama 3 405B 在主流基准测试上的表现：\n从图中可以观察到几个关键结论：\n规模效应显著：从 8B 到 405B，各维度能力几乎单调提升 代码能力突出：HumanEval 上 405B 达到 89%，超越 GPT-4 数学推理强劲：GSM8K 96.8%、MATH 73.8%，均达到顶尖水平 小模型也有竞争力：8B 版本在多项任务上超越 Llama 2 70B 8.2 人工评估 除了自动基准测试，Meta 还进行了大规模人工评估。评估者被给予同一问题的两个模型回答（盲测），需要选择更好的那个。\n结果：Llama 3 405B 在有用性和事实准确性上均与 GPT-4 持平，在代码生成和数学推理上甚至略有优势。\n8.3 安全性评估 Meta 对 Llama 3 进行了全面的安全评估：\nLlama Guard 3：内置的输入/输出安全分类器 越狱测试：包括多轮对话越狱、长上下文越狱等 偏见评估：在多个维度上评估模型的公平性 结果显示，Llama 3 在保持高有用性的同时，显著降低了有害输出率。这得益于后训练阶段的安全数据混合和对齐优化。\n第九章：工程实践 —— 训练 405B 模型的技术挑战 9.1 4D 并行策略 训练 405B 模型需要数千张 GPU 协同工作。Meta 采用了 4D 并行策略：\n并行维度 作用 通信特点 TP\n张量并行 将单层网络拆分到多卡 高带宽、低延迟（机内） CP\n上下文并行 将长序列分段处理 中等带宽（机内/机间） PP\n流水线并行 将模型分层拆分到多机 中等延迟容忍 DP\n数据并行 多副本并行处理不同数据 梯度同步（全局） 并行顺序经过精心设计：[TP, CP, PP, DP]。这种排序确保了：\n通信最密集的 TP 在 NVLink 内部完成 CP 利用机内高速互联 PP 和 DP 可以跨越机架，容忍更高延迟 9.2 FP8 量化训练 为了在 H100 GPU 上高效训练，Meta 采用了 FP8（8-bit 浮点）量化：\n前向传播：权重和激活使用 FP8 反向传播：梯度使用 FP8 关键层（第一层、最后一层）保持 BF16 使用逐行量化（Row-wise Quantization）和动态缩放保证精度 实验表明，FP8 训练相比 BF16 几乎没有精度损失，但吞吐量提升显著。\n9.3 故障恢复机制 在数月的大规模训练中，硬件故障不可避免。Meta 的应对策略：\n高频 checkpoint：每 50-100 步保存状态 异步 checkpoint：不阻塞训练流水线 自动故障检测：监控 GPU 健康状态 弹性重启：自动从最近 checkpoint 恢复 据统计，整个训练过程中经历了数百次各类故障，但都通过自动化机制无缝恢复。\n结语：开源 AI 的新纪元 Llama 3 的发布不仅是技术里程碑，更是 AI 发展范式的转折点。\n回顾这篇论文的核心贡献：\n数据：15.6 万亿 tokens 的高质量语料，证明了数据质量与数量的同等重要性 规模：405B 参数 + 3.8×10²⁵ FLOPs，展示了 Dense Transformer 架构的极限潜力 效率：GQA、4D 并行、FP8 量化等工程创新，让大规模训练成为可能 开放：完整开源模型权重和技术细节，推动 AI 民主化 更重要的是，Llama 3 证明了开源模型可以与闭源巨头抗衡。这不是终点，而是新起点——一个研究者可以自由探索、开发者可以按需定制、企业可以自主部署的 AI 新时代。\n未来，当我们回望 2024 年，或许会将其视为\"开源 AI 元年\"。而《The Llama 3 Herd of Models》这篇论文，就是这一历史转折的见证者和推动者。\n参考阅读 论文原文：The Llama 3 Herd of Models 模型下载：Llama 官网 技术博客：Meta AI Blog 本文图表说明：\n- Scaling Laws 对比图：展示了 Llama 3 各模型相对于 Chinchilla 最优曲线的位置 - 上下文演化图：呈现了从 8K 到 128K 的训练阶段和性能保持 - 基准对比图：Llama 3 系列与 GPT-4 在主流测试集上的性能比较 - 计算量分布图：各训练阶段消耗的算力比例 ","wordCount":"1184","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/llama3-herd-cover.jpg","datePublished":"2026-01-31T09:30:00+08:00","dateModified":"2026-01-31T09:30:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-31-ai-paper-llama3-herd-of-models/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作</h1><div class=post-description>深入解读 Meta AI 的 Llama 3 论文，从 Scaling Laws、模型架构到多模态扩展，全面剖析这个拥有 405B 参数的开源大模型集群的设计理念与技术细节。</div><div class=post-meta><span title='2026-01-31 09:30:00 +0800 CST'>January 31, 2026</span>&nbsp;·&nbsp;<span>6 min</span>&nbsp;·&nbsp;<span>1184 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/llama3-herd-cover.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/llama3-herd-cover.jpg alt="Llama 3 模型集群架构示意图"></a><figcaption>Llama 3：开源 AI 的新里程碑</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e5%bc%80%e6%ba%90-ai-%e7%9a%84%e9%bb%8e%e6%98%8e aria-label="引言：开源 AI 的黎明">引言：开源 AI 的黎明</a></li><li><a href=#%e7%ac%ac%e4%b8%80%e7%ab%a0%e6%a8%a1%e5%9e%8b%e6%a6%82%e8%a7%88--%e6%a8%a1%e5%9e%8b%e7%be%a4%e7%9a%84%e8%ae%be%e8%ae%a1%e7%90%86%e5%bf%b5 aria-label='第一章：模型概览 —— &ldquo;模型群"的设计理念'>第一章：模型概览 —— &ldquo;模型群"的设计理念</a><ul><li><a href=#11-%e4%b8%ba%e4%bb%80%e4%b9%88%e5%8f%ab-herd%e7%be%a4 aria-label="1.1 为什么叫 &ldquo;Herd&rdquo;（群）？">1.1 为什么叫 &ldquo;Herd&rdquo;（群）？</a></li><li><a href=#12-%e6%a0%b8%e5%bf%83%e8%83%bd%e5%8a%9b%e7%9f%a9%e9%98%b5 aria-label="1.2 核心能力矩阵">1.2 核心能力矩阵</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e7%ab%a0%e9%a2%84%e8%ae%ad%e7%bb%83%e7%9a%84%e8%89%ba%e6%9c%af--%e6%95%b0%e6%8d%ae%e4%b8%ba%e7%8e%8b aria-label="第二章：预训练的艺术 —— 数据为王">第二章：预训练的艺术 —— 数据为王</a><ul><li><a href=#21-%e6%95%b0%e6%8d%ae%e8%a7%84%e6%a8%a1%e7%9a%84%e8%b7%83%e5%8d%87 aria-label="2.1 数据规模的跃升">2.1 数据规模的跃升</a></li><li><a href=#22-%e6%95%b0%e6%8d%ae%e8%b4%a8%e9%87%8f%e7%ad%9b%e9%80%89%e7%9a%84%e4%ba%94%e9%87%8d%e5%85%b3%e5%8d%a1 aria-label="2.2 数据质量筛选的五重关卡">2.2 数据质量筛选的五重关卡</a><ul><li><a href=#%e7%ac%ac%e4%b8%80%e5%85%b3%e5%ae%89%e5%85%a8%e4%b8%8e%e9%9a%90%e7%a7%81%e8%bf%87%e6%bb%a4 aria-label=第一关：安全与隐私过滤>第一关：安全与隐私过滤</a></li><li><a href=#%e7%ac%ac%e4%ba%8c%e5%85%b3%e6%96%87%e6%9c%ac%e6%8f%90%e5%8f%96%e4%b8%8e%e6%b8%85%e6%b4%97 aria-label=第二关：文本提取与清洗>第二关：文本提取与清洗</a></li><li><a href=#%e7%ac%ac%e4%b8%89%e5%85%b3%e4%b8%89%e7%ba%a7%e5%8e%bb%e9%87%8d%e7%ad%96%e7%95%a5 aria-label=第三关：三级去重策略>第三关：三级去重策略</a></li><li><a href=#%e7%ac%ac%e5%9b%9b%e5%85%b3%e5%90%af%e5%8f%91%e5%bc%8f%e8%bf%87%e6%bb%a4 aria-label=第四关：启发式过滤>第四关：启发式过滤</a></li><li><a href=#%e7%ac%ac%e4%ba%94%e5%85%b3%e6%a8%a1%e5%9e%8b%e8%b4%a8%e9%87%8f%e8%af%84%e5%88%86 aria-label=第五关：模型质量评分>第五关：模型质量评分</a></li></ul></li><li><a href=#23-%e6%95%b0%e6%8d%ae%e9%85%8d%e6%af%94%e7%9a%84%e8%89%ba%e6%9c%af aria-label="2.3 数据配比的艺术">2.3 数据配比的艺术</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e7%ab%a0scaling-laws--%e8%a7%84%e6%a8%a1%e7%9a%84%e7%a7%91%e5%ad%a6 aria-label="第三章：Scaling Laws —— 规模的科学">第三章：Scaling Laws —— 规模的科学</a><ul><li><a href=#31-chinchilla-%e6%9c%80%e4%bc%98-vs-llama-3-%e7%ad%96%e7%95%a5 aria-label="3.1 Chinchilla 最优 vs Llama 3 策略">3.1 Chinchilla 最优 vs Llama 3 策略</a></li><li><a href=#32-%e4%b8%ba%e4%bb%80%e4%b9%88%e8%bf%87%e8%ae%ad%e7%bb%83 aria-label='3.2 为什么"过训练"？'>3.2 为什么"过训练"？</a></li><li><a href=#33-%e8%ae%ad%e7%bb%83%e8%ae%a1%e7%ae%97%e9%87%8f%e5%88%86%e5%b8%83 aria-label="3.3 训练计算量分布">3.3 训练计算量分布</a></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e7%ab%a0%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84--%e4%bf%9d%e5%ae%88%e4%b8%ad%e7%9a%84%e5%88%9b%e6%96%b0 aria-label="第四章：模型架构 —— 保守中的创新">第四章：模型架构 —— 保守中的创新</a><ul><li><a href=#41-%e5%9d%9a%e6%8c%81-dense-transformer aria-label="4.1 坚持 Dense Transformer">4.1 坚持 Dense Transformer</a></li><li><a href=#42-%e5%85%b3%e9%94%ae%e6%9e%b6%e6%9e%84%e5%8f%82%e6%95%b0 aria-label="4.2 关键架构参数">4.2 关键架构参数</a></li><li><a href=#43-%e5%88%86%e7%bb%84%e6%9f%a5%e8%af%a2%e6%b3%a8%e6%84%8f%e5%8a%9bgqa aria-label="4.3 分组查询注意力（GQA）">4.3 分组查询注意力（GQA）</a></li><li><a href=#44-rope-%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81%e7%9a%84%e6%bc%94%e8%bf%9b aria-label="4.4 RoPE 位置编码的演进">4.4 RoPE 位置编码的演进</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e7%ab%a0%e9%95%bf%e4%b8%8a%e4%b8%8b%e6%96%87--%e4%bb%8e-8k-%e5%88%b0-128k-%e7%9a%84%e8%bf%9b%e5%8c%96 aria-label="第五章：长上下文 —— 从 8K 到 128K 的进化">第五章：长上下文 —— 从 8K 到 128K 的进化</a><ul><li><a href=#51-%e6%b8%90%e8%bf%9b%e5%bc%8f%e6%89%a9%e5%b1%95%e7%ad%96%e7%95%a5 aria-label="5.1 渐进式扩展策略">5.1 渐进式扩展策略</a></li><li><a href=#52-%e5%85%b3%e9%94%ae%e6%8a%80%e6%9c%af%e6%96%87%e6%a1%a3%e6%8e%a9%e7%a0%81 aria-label="5.2 关键技术：文档掩码">5.2 关键技术：文档掩码</a></li><li><a href=#53-%e6%80%a7%e8%83%bd%e9%aa%8c%e8%af%81needle-in-haystack aria-label="5.3 性能验证：Needle-in-Haystack">5.3 性能验证：Needle-in-Haystack</a></li></ul></li><li><a href=#%e7%ac%ac%e5%85%ad%e7%ab%a0%e5%90%8e%e8%ae%ad%e7%bb%83%e7%9a%84%e8%89%ba%e6%9c%af--%e4%bb%8e%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%88%b0%e5%8a%a9%e6%89%8b aria-label="第六章：后训练的艺术 —— 从语言模型到助手">第六章：后训练的艺术 —— 从语言模型到助手</a><ul><li><a href=#61-%e5%90%8e%e8%ae%ad%e7%bb%83%e6%b5%81%e7%a8%8b%e6%a6%82%e8%a7%88 aria-label="6.1 后训练流程概览">6.1 后训练流程概览</a></li><li><a href=#62-%e7%9b%91%e7%9d%a3%e5%be%ae%e8%b0%83sft aria-label="6.2 监督微调（SFT）">6.2 监督微调（SFT）</a></li><li><a href=#63-%e6%8b%92%e7%bb%9d%e9%87%87%e6%a0%b7rejection-sampling aria-label="6.3 拒绝采样（Rejection Sampling）">6.3 拒绝采样（Rejection Sampling）</a></li><li><a href=#64-%e7%9b%b4%e6%8e%a5%e5%81%8f%e5%a5%bd%e4%bc%98%e5%8c%96dpo aria-label="6.4 直接偏好优化（DPO）">6.4 直接偏好优化（DPO）</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%83%e7%ab%a0%e5%a4%9a%e6%a8%a1%e6%80%81%e6%89%a9%e5%b1%95--%e7%9c%8b%e5%be%97%e6%87%82%e5%90%ac%e5%be%97%e8%a7%81%e7%9a%84-llama aria-label="第七章：多模态扩展 —— 看得懂、听得见的 Llama">第七章：多模态扩展 —— 看得懂、听得见的 Llama</a><ul><li><a href=#71-%e7%bb%84%e5%90%88%e5%bc%8f%e6%9e%b6%e6%9e%84%e8%ae%be%e8%ae%a1 aria-label="7.1 组合式架构设计">7.1 组合式架构设计</a></li><li><a href=#72-%e8%a7%86%e8%a7%89%e9%80%82%e9%85%8d%e5%99%a8%e8%ae%ad%e7%bb%83 aria-label="7.2 视觉适配器训练">7.2 视觉适配器训练</a></li><li><a href=#73-%e8%a7%86%e9%a2%91%e7%90%86%e8%a7%a3%e6%89%a9%e5%b1%95 aria-label="7.3 视频理解扩展">7.3 视频理解扩展</a></li><li><a href=#74-%e8%af%ad%e9%9f%b3%e9%80%82%e9%85%8d%e5%99%a8 aria-label="7.4 语音适配器">7.4 语音适配器</a></li></ul></li><li><a href=#%e7%ac%ac%e5%85%ab%e7%ab%a0%e6%80%a7%e8%83%bd%e8%af%84%e4%bc%b0--%e4%b8%8e-gpt-4-%e7%9a%84%e6%ad%a3%e9%9d%a2%e5%af%b9%e5%86%b3 aria-label="第八章：性能评估 —— 与 GPT-4 的正面对决">第八章：性能评估 —— 与 GPT-4 的正面对决</a><ul><li><a href=#81-%e5%9f%ba%e5%87%86%e6%b5%8b%e8%af%95%e7%bb%93%e6%9e%9c aria-label="8.1 基准测试结果">8.1 基准测试结果</a></li><li><a href=#82-%e4%ba%ba%e5%b7%a5%e8%af%84%e4%bc%b0 aria-label="8.2 人工评估">8.2 人工评估</a></li><li><a href=#83-%e5%ae%89%e5%85%a8%e6%80%a7%e8%af%84%e4%bc%b0 aria-label="8.3 安全性评估">8.3 安全性评估</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b9%9d%e7%ab%a0%e5%b7%a5%e7%a8%8b%e5%ae%9e%e8%b7%b5--%e8%ae%ad%e7%bb%83-405b-%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%8a%80%e6%9c%af%e6%8c%91%e6%88%98 aria-label="第九章：工程实践 —— 训练 405B 模型的技术挑战">第九章：工程实践 —— 训练 405B 模型的技术挑战</a><ul><li><a href=#91-4d-%e5%b9%b6%e8%a1%8c%e7%ad%96%e7%95%a5 aria-label="9.1 4D 并行策略">9.1 4D 并行策略</a></li><li><a href=#92-fp8-%e9%87%8f%e5%8c%96%e8%ae%ad%e7%bb%83 aria-label="9.2 FP8 量化训练">9.2 FP8 量化训练</a></li><li><a href=#93-%e6%95%85%e9%9a%9c%e6%81%a2%e5%a4%8d%e6%9c%ba%e5%88%b6 aria-label="9.3 故障恢复机制">9.3 故障恢复机制</a></li></ul></li><li><a href=#%e7%bb%93%e8%af%ad%e5%bc%80%e6%ba%90-ai-%e7%9a%84%e6%96%b0%e7%ba%aa%e5%85%83 aria-label="结语：开源 AI 的新纪元">结语：开源 AI 的新纪元</a></li><li><a href=#%e5%8f%82%e8%80%83%e9%98%85%e8%af%bb aria-label=参考阅读>参考阅读</a></li></ul></div></details></div><div class=post-content><h2 id=引言开源-ai-的黎明>引言：开源 AI 的黎明<a hidden class=anchor aria-hidden=true href=#引言开源-ai-的黎明>#</a></h2><p>2024 年 7 月 23 日，Meta AI 发布了一篇重磅论文——《The Llama 3 Herd of Models》。这篇论文不仅介绍了一个拥有 4050 亿参数的巨型语言模型，更标志着开源人工智能正式迈入了与闭源巨头分庭抗礼的新纪元。</p><p>回想 2022 年底，ChatGPT 的横空出世让整个 AI 领域为之震动。然而，最强大的模型始终被封闭在 OpenAI、Google 等公司的围墙之内。研究者无法探究其内部机理，开发者无法自由定制，这种"黑箱"状态严重阻碍了 AI 技术的普惠发展。</p><p>Llama 3 的出现改变了这一切。Meta 不仅开源了完整的模型权重，还详细披露了从数据筛选到训练优化的每一个技术细节。这意味着，任何研究者和开发者都可以在自己的硬件上运行这个媲美 GPT-4 的模型，深入理解它的工作原理，甚至在此基础上进行创新。</p><p>本文将带领读者深入这篇 92 页的论文，从<strong>数据、规模、复杂性管理</strong>三个核心维度，层层剥开 Llama 3 的技术奥秘。</p><h2 id=第一章模型概览--模型群的设计理念>第一章：模型概览 —— &ldquo;模型群"的设计理念<a hidden class=anchor aria-hidden=true href=#第一章模型概览--模型群的设计理念>#</a></h2><h3 id=11-为什么叫-herd群>1.1 为什么叫 &ldquo;Herd&rdquo;（群）？<a hidden class=anchor aria-hidden=true href=#11-为什么叫-herd群>#</a></h3><p>论文标题中的 &ldquo;Herd of Models&rdquo; 并非随意命名。Meta 同时发布了三个不同规模的模型：</p><table><thead><tr><th style=text-align:center>模型</th><th style=text-align:center>参数量</th><th style=text-align:center>上下文长度</th><th style=text-align:left>目标场景</th></tr></thead><tbody><tr><td style=text-align:center>Llama 3 8B</td><td style=text-align:center>$8 \times 10^9$</td><td style=text-align:center>128K tokens</td><td style=text-align:left>边缘设备、低延迟推理</td></tr><tr><td style=text-align:center>Llama 3 70B</td><td style=text-align:center>$70 \times 10^9$</td><td style=text-align:center>128K tokens</td><td style=text-align:left>平衡性能与效率</td></tr><tr><td style=text-align:center>Llama 3 405B</td><td style=text-align:center>$405 \times 10^9$</td><td style=text-align:center>128K tokens</td><td style=text-align:left>顶级性能、复杂推理</td></tr></tbody></table><p>这种"群"策略的核心思想是：<strong>用一个旗舰模型（405B）指导整个家族的优化方向，同时让每个成员在特定场景下发挥最大价值</strong>。</p><h3 id=12-核心能力矩阵>1.2 核心能力矩阵<a hidden class=anchor aria-hidden=true href=#12-核心能力矩阵>#</a></h3><p>Llama 3 原生支持四大核心能力：</p><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>flowchart TD
subgraph Capabilities [Llama 3 核心能力]
A[多语言能力] --> A1[支持 8+ 种语言]
A --> A2[跨语言推理]
B[代码生成] --> B1[Python/C++/Java]
B --> B2[复杂算法实现]
C[数学推理] --> C1[GSM8K 96.8%]
C --> C2[MATH 73.8%]
D[工具使用] --> D1[零样本调用]
D --> D2[多轮工具链]
end
style Capabilities fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff
style A fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style B fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style C fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style D fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style A1 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff
style A2 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff
style B1 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff
style B2 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff
style C1 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff
style C2 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff
style D1 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff
style D2 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff</div></div><p><strong>图例说明</strong>：</p><ul><li>🔵 蓝色：核心概念</li><li>🟢 绿色：主要能力类别</li><li>🩵 浅蓝：具体能力指标</li></ul><p>特别值得注意的是，Llama 3 8B 在某些基准测试上已经超过了 Llama 2 70B 的表现。这种"以小博大"的能力提升，正是数据质量和训练策略优化的直接体现。</p><h2 id=第二章预训练的艺术--数据为王>第二章：预训练的艺术 —— 数据为王<a hidden class=anchor aria-hidden=true href=#第二章预训练的艺术--数据为王>#</a></h2><h3 id=21-数据规模的跃升>2.1 数据规模的跃升<a hidden class=anchor aria-hidden=true href=#21-数据规模的跃升>#</a></h3><p>Llama 3 的预训练数据量达到了惊人的 <strong>15.6 万亿 tokens</strong>，相比 Llama 2 的 1.8 万亿，增长了近 9 倍。这个数字是什么概念？</p><p>假设一本书平均有 10 万字，15.6 万亿 tokens 大约相当于：</p><p>$$
\frac{15.6 \times 10^{12}}{10^5} \approx 1.56 \times 10^8 \text{ 本书}
$$</p><p>约 <strong>1.56 亿本书</strong>！这几乎涵盖了人类文明的绝大部分书面知识。</p><h3 id=22-数据质量筛选的五重关卡>2.2 数据质量筛选的五重关卡<a hidden class=anchor aria-hidden=true href=#22-数据质量筛选的五重关卡>#</a></h3><p>Meta 构建了一套工业级的数据清洗流水线，包含五个关键步骤：</p><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>flowchart LR
A[原始网页数据] --> B{安全过滤}
B -->|通过| C[HTML解析]
B -->|拒绝| X[丢弃]
C --> D[多层级去重]
D --> E[启发式过滤]
E --> F[模型质量评分]
F --> G[高质量语料库]
style A fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff
style B fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff
style C fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style D fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style E fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style F fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style G fill:#32D74B,stroke:#32D74B,stroke-width:3px,color:#ffffff
style X fill:#FF3B30,stroke:#FF3B30,stroke-width:2px,color:#ffffff</div></div><h4 id=第一关安全与隐私过滤>第一关：安全与隐私过滤<a hidden class=anchor aria-hidden=true href=#第一关安全与隐私过滤>#</a></h4><ul><li>移除含有不安全内容的域名</li><li>过滤成人内容</li><li>清除个人身份信息（PII）</li></ul><h4 id=第二关文本提取与清洗>第二关：文本提取与清洗<a hidden class=anchor aria-hidden=true href=#第二关文本提取与清洗>#</a></h4><p>Meta 开发了一个自定义 HTML 解析器，相比第三方工具（如 BeautifulSoup），它在两个关键指标上更优：</p><ul><li><strong>模板去除精度</strong>：准确识别并移除导航栏、广告、版权声明等"样板"内容</li><li><strong>内容召回率</strong>：确保不遗漏正文、数学公式、代码块等有价值信息</li></ul><p>有趣的是，<strong>实验发现 Markdown 格式对模型性能有害</strong>，因此所有 Markdown 标记都被移除，仅保留纯文本。</p><h4 id=第三关三级去重策略>第三关：三级去重策略<a hidden class=anchor aria-hidden=true href=#第三关三级去重策略>#</a></h4><table><thead><tr><th style=text-align:center>去重级别</th><th style=text-align:left>方法</th><th style=text-align:left>目的</th></tr></thead><tbody><tr><td style=text-align:center>URL 级</td><td style=text-align:left>全局 URL 去重</td><td style=text-align:left>保留每页面的最新版本</td></tr><tr><td style=text-align:center>文档级</td><td style=text-align:left>MinHash 算法</td><td style=text-align:left>删除近似重复文档</td></tr><tr><td style=text-align:center>行级</td><td style=text-align:left>桶内行频率统计</td><td style=text-align:left>移除残留模板（如导航菜单）</td></tr></tbody></table><p>行级去重采用激进策略：<strong>在每个 3000 万文档的桶中，出现超过 6 次的行会被删除</strong>。这虽然会误伤一些常用短语，但整体质量提升明显。</p><h4 id=第四关启发式过滤>第四关：启发式过滤<a hidden class=anchor aria-hidden=true href=#第四关启发式过滤>#</a></h4><p>使用多种启发式规则剔除低质量内容：</p><ul><li><strong>重复 $n$-gram 覆盖率</strong>：识别日志文件、错误消息等重复内容</li><li><strong>脏词计数</strong>：捕获未被域名黑名单覆盖的成人内容</li><li><strong>KL 散度异常检测</strong>：过滤含有过多异常 token 的文档</li></ul><h4 id=第五关模型质量评分>第五关：模型质量评分<a hidden class=anchor aria-hidden=true href=#第五关模型质量评分>#</a></h4><p>这是最关键的一步。Meta 使用 Llama 2 作为"教师模型&rdquo;，训练了一系列质量分类器：</p><ol><li><strong>通用质量分类器</strong>：基于 Llama 2 的判断，预测文档是否"值得被 Wikipedia 引用"</li><li><strong>代码专用分类器</strong>：识别包含代码和自然语言交织的文档</li><li><strong>数学推理分类器</strong>：筛选 STEM 领域的推理内容</li></ol><h3 id=23-数据配比的艺术>2.3 数据配比的艺术<a hidden class=anchor aria-hidden=true href=#23-数据配比的艺术>#</a></h3><p>最终的数据组成大致为：</p><ul><li><strong>网页数据</strong>：约 50%（经严格筛选）</li><li><strong>代码数据</strong>：约 17%</li><li><strong>多语言数据</strong>：约 17%（覆盖 8+ 种语言）</li><li><strong>数学/推理数据</strong>：约 10%</li><li><strong>其他</strong>：约 6%</li></ul><p>这种精心设计的配比，确保了模型在各个维度上的均衡发展。</p><h2 id=第三章scaling-laws--规模的科学>第三章：Scaling Laws —— 规模的科学<a hidden class=anchor aria-hidden=true href=#第三章scaling-laws--规模的科学>#</a></h2><h3 id=31-chinchilla-最优-vs-llama-3-策略>3.1 Chinchilla 最优 vs Llama 3 策略<a hidden class=anchor aria-hidden=true href=#31-chinchilla-最优-vs-llama-3-策略>#</a></h3><p>2022 年，DeepMind 的 Chinchilla 论文提出了著名的 Scaling Laws：</p><p>$$
L(N, D) = \frac{A}{N^{\alpha}} + \frac{B}{D^{\beta}} + L_{\infty}
$$</p><p>其中：</p><ul><li>$N$ 是模型参数量</li><li>$D$ 是训练数据量</li><li>$L$ 是验证损失</li><li>$\alpha \approx 0.5$, $\beta \approx 0.5$</li></ul><p>根据 Chinchilla 最优计算，给定 $3.8 \times 10^{25}$ FLOPs 的算力预算，最优配置约为 <strong>400B 参数 + 400B tokens</strong>。</p><p>然而，Llama 3 405B 使用了 <strong>15.6 万亿 tokens</strong>——这是计算最优配置的 <strong>39 倍</strong>！</p><h3 id=32-为什么过训练>3.2 为什么"过训练"？<a hidden class=anchor aria-hidden=true href=#32-为什么过训练>#</a></h3><p>Meta 刻意选择了"过训练"（over-training）策略，原因有三：</p><ol><li><strong>推理效率优先</strong>：在相同的推理预算下，小模型 + 多数据 往往优于 大模型 + 少数据</li><li><strong>知识密度</strong>：更多的数据曝光让模型"记住"更多知识</li><li><strong>下游任务泛化</strong>：充分训练使模型更好地内化语言规律</li></ol><p><img alt="Scaling Laws 对比图" loading=lazy src=/images/plots/llama3-scaling-laws.png></p><p>从图中可以看到，Llama 3 的三个模型都位于 Chinchilla 最优曲线的右上方——这意味着它们都经过了"过训练"。但这种策略换来了什么呢？</p><p><strong>答案是：更强的推理能力</strong>。对于实际部署，推理成本往往远超训练成本。一个训练充分的小模型，可以比同等推理成本下的大模型表现更好。</p><h3 id=33-训练计算量分布>3.3 训练计算量分布<a hidden class=anchor aria-hidden=true href=#33-训练计算量分布>#</a></h3><p>Llama 3 405B 的总训练计算量约为 $3.8 \times 10^{25}$ FLOPs，分布如下：</p><p><img alt=训练计算量分布 loading=lazy src=/images/plots/llama3-training-compute.png></p><p>预训练阶段消耗了绝大部分算力（约 90%），这符合"数据密集型"训练的特点。</p><h2 id=第四章模型架构--保守中的创新>第四章：模型架构 —— 保守中的创新<a hidden class=anchor aria-hidden=true href=#第四章模型架构--保守中的创新>#</a></h2><h3 id=41-坚持-dense-transformer>4.1 坚持 Dense Transformer<a hidden class=anchor aria-hidden=true href=#41-坚持-dense-transformer>#</a></h3><p>当 GPT-4、Mixtral 等模型纷纷采用 MoE（Mixture of Experts，专家混合）架构时，Llama 3 选择了相对保守的 <strong>Dense Transformer</strong>——即每个参数都参与每次前向传播。</p><p>这个决策背后的考量是：<strong>最大化训练稳定性</strong>。</p><p>MoE 架构虽然可以在相同激活参数下扩展总参数量，但带来了额外的复杂性：</p><ul><li>路由机制的不稳定性</li><li>负载均衡的挑战</li><li>专家崩溃（expert collapse）风险</li></ul><p>对于需要稳定训练数月、消耗数亿美元算力的旗舰模型，<strong>简单即美德</strong>。</p><h3 id=42-关键架构参数>4.2 关键架构参数<a hidden class=anchor aria-hidden=true href=#42-关键架构参数>#</a></h3><table><thead><tr><th style=text-align:left>参数</th><th style=text-align:center>Llama 3 8B</th><th style=text-align:center>Llama 3 70B</th><th style=text-align:center>Llama 3 405B</th></tr></thead><tbody><tr><td style=text-align:left>层数 $L$</td><td style=text-align:center>32</td><td style=text-align:center>80</td><td style=text-align:center>126</td></tr><tr><td style=text-align:left>模型维度 $d_{model}$</td><td style=text-align:center>4096</td><td style=text-align:center>8192</td><td style=text-align:center>16384</td></tr><tr><td style=text-align:left>注意力头数 $n_h$</td><td style=text-align:center>32</td><td style=text-align:center>64</td><td style=text-align:center>128</td></tr><tr><td style=text-align:left>每头维度 $d_h$</td><td style=text-align:center>128</td><td style=text-align:center>128</td><td style=text-align:center>128</td></tr><tr><td style=text-align:left>上下文长度</td><td style=text-align:center>128K</td><td style=text-align:center>128K</td><td style=text-align:center>128K</td></tr><tr><td style=text-align:left>词汇表大小 $</td><td style=text-align:center>V</td><td style=text-align:center>$</td><td style=text-align:center>128K</td></tr></tbody></table><h3 id=43-分组查询注意力gqa>4.3 分组查询注意力（GQA）<a hidden class=anchor aria-hidden=true href=#43-分组查询注意力gqa>#</a></h3><p>Llama 3 采用了 <strong>Grouped-Query Attention（GQA）</strong>，这是提升推理效率的关键设计。</p><p>标准多头注意力（MHA）中，每个注意力头都有独立的查询（Query）、键（Key）、值（Value）投影：</p><p>$$
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
$$</p><p>在 GQA 中，多个查询头共享同一组键和值头：</p><ul><li><strong>Llama 3 8B</strong>：8 个 KV 头（4 个查询头共享 1 个 KV 头）</li><li><strong>Llama 3 70B/405B</strong>：8 个 KV 头（8/16 个查询头共享 1 个 KV 头）</li></ul><p>这带来了显著的内存节省。对于长度为 $L$、维度为 $d$ 的序列，KV Cache 的内存占用从 $O(L \cdot n_h \cdot d)$ 降低到 $O(L \cdot n_{kv} \cdot d)$，其中 $n_{kv} \ll n_h$。</p><h3 id=44-rope-位置编码的演进>4.4 RoPE 位置编码的演进<a hidden class=anchor aria-hidden=true href=#44-rope-位置编码的演进>#</a></h3><p>Llama 3 继续使用 <strong>Rotary Position Embedding（RoPE）</strong>，但对基础频率进行了调整：</p><p>$$
\text{RoPE}(\mathbf{x}, m) = \mathbf{x} \odot e^{i m \theta_j}
$$</p><p>其中 $\theta_j = \theta_{base}^{-2(j-1)/d}$，$j$ 是维度索引。</p><p>关键调整：</p><ul><li><strong>基础频率</strong> $\theta_{base}$ 从 Llama 2 的 10000 提升到 <strong>500000</strong></li><li>这使得模型能更好地处理长距离依赖，支持长达 128K tokens 的上下文</li></ul><h2 id=第五章长上下文--从-8k-到-128k-的进化>第五章：长上下文 —— 从 8K 到 128K 的进化<a hidden class=anchor aria-hidden=true href=#第五章长上下文--从-8k-到-128k-的进化>#</a></h2><h3 id=51-渐进式扩展策略>5.1 渐进式扩展策略<a hidden class=anchor aria-hidden=true href=#51-渐进式扩展策略>#</a></h3><p>Llama 3 并非一开始就训练 128K 上下文，而是采用了<strong>渐进式扩展</strong>策略：</p><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>flowchart LR
A[预训练<br>8K 上下文] --> B[继续预训练<br>逐步扩展到 128K]
B --> C[长上下文微调<br>高质量数据]
style A fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff
style B fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff
style C fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff</div></div><p>具体步骤：</p><ol><li><strong>预训练阶段</strong>：使用 8K 上下文进行主要训练</li><li><strong>继续预训练</strong>：逐步增加上下文长度，从 8K → 16K → 32K → 64K → 128K</li><li><strong>退火阶段</strong>：在高质量数据上进行长上下文微调</li></ol><h3 id=52-关键技术文档掩码>5.2 关键技术：文档掩码<a hidden class=anchor aria-hidden=true href=#52-关键技术文档掩码>#</a></h3><p>长上下文训练的一个关键创新是<strong>文档间掩码</strong>（Inter-document Masking）：</p><p>在标准预训练中，同一批次的不同文档被拼接成一个长序列。传统方法允许注意力跨越文档边界，但 Llama 3 引入了一种特殊的注意力掩码——<strong>禁止不同文档之间的注意力</strong>。</p><p>这种设计的直觉是：</p><ul><li>来自不同文档的 token 没有语义关联</li><li>强制模型专注于文档内部的长期依赖</li><li>在长序列上保持训练稳定性</li></ul><h3 id=53-性能验证needle-in-haystack>5.3 性能验证：Needle-in-Haystack<a hidden class=anchor aria-hidden=true href=#53-性能验证needle-in-haystack>#</a></h3><p>为了验证长上下文能力，Meta 使用了经典的 &ldquo;大海捞针&rdquo; 测试：在一个极长文档的随机位置插入一句特定的话（&ldquo;针&rdquo;），然后要求模型检索这句话。</p><p><img alt=长上下文演化图 loading=lazy src=/images/plots/llama3-context-evolution.png></p><p>从右侧图可以看到，即使在 128K tokens 的全长度下，Llama 3 的检索准确率仍保持在 97% 左右。这意味着模型可以可靠地处理整本书、长篇文章或大量代码库。</p><h2 id=第六章后训练的艺术--从语言模型到助手>第六章：后训练的艺术 —— 从语言模型到助手<a hidden class=anchor aria-hidden=true href=#第六章后训练的艺术--从语言模型到助手>#</a></h2><h3 id=61-后训练流程概览>6.1 后训练流程概览<a hidden class=anchor aria-hidden=true href=#61-后训练流程概览>#</a></h3><p>预训练后的模型虽然掌握了语言知识和世界知识，但它还<strong>不会按照指令行事</strong>。后训练的目标是将这个"知识库"转化为"有用且安全的助手"。</p><p>Llama 3 的后训练采用了一个相对简单但有效的流程：</p><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>flowchart TD
subgraph Pretrain [预训练模型]
PT[Llama 3 Base]
end
subgraph Posttrain [后训练流程]
SFT1[SFT 第1轮] --> RS1[拒绝采样]
RS1 --> DPO1[DPO 第1轮]
DPO1 --> SFT2[SFT 第2轮]
SFT2 --> RS2[拒绝采样]
RS2 --> DPO2[DPO 第2轮]
DPO2 --> SFTN[SFT 第N轮...]
end
PT --> SFT1
SFTN --> Final[最终模型]
style PT fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff
style SFT1 fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style RS1 fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style DPO1 fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style SFT2 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff
style RS2 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff
style DPO2 fill:#5AC8FA,stroke:#5AC8FA,stroke-width:1px,color:#ffffff
style SFTN fill:#8E8E93,stroke:#8E8E93,stroke-width:1px,color:#ffffff
style Final fill:#32D74B,stroke:#32D74B,stroke-width:3px,color:#ffffff</div></div><h3 id=62-监督微调sft>6.2 监督微调（SFT）<a hidden class=anchor aria-hidden=true href=#62-监督微调sft>#</a></h3><p>SFT 阶段使用人工标注的高质量指令数据。关键策略包括：</p><ul><li><strong>多样化数据</strong>：涵盖问答、代码、数学、推理、工具使用等多种任务</li><li><strong>多语言覆盖</strong>：确保非英语能力不退化</li><li><strong>质量筛选</strong>：使用模型自动评估 + 人工审核，只保留高质量样本</li></ul><h3 id=63-拒绝采样rejection-sampling>6.3 拒绝采样（Rejection Sampling）<a hidden class=anchor aria-hidden=true href=#63-拒绝采样rejection-sampling>#</a></h3><p>这是一个提升模型推理能力的巧妙技巧：</p><ol><li>给定一个问题，让当前模型生成 $N$ 个候选答案（$N$ 通常设为 4-8）</li><li>使用奖励模型或人工标准，评估每个答案的质量</li><li>只保留最高分答案，用它构造新的训练样本</li><li>用这些"精选"样本继续 SFT</li></ol><p>这相当于让模型<strong>反复练习它最擅长的解法</strong>，类似于学生的"错题本"学习法。</p><h3 id=64-直接偏好优化dpo>6.4 直接偏好优化（DPO）<a hidden class=anchor aria-hidden=true href=#64-直接偏好优化dpo>#</a></h3><p>DPO 是 Llama 3 对齐人类偏好的核心算法。相比传统的 RLHF（基于人类反馈的强化学习），DPO 更简单高效。</p><p>核心思想是：对于每个查询，收集一对回答 $(y_w, y_l)$，其中 $y_w$ 是人类偏好的"胜"回答，$y_l$ 是"负"回答。DPO 直接优化以下目标：</p><p>$$
\mathcal{L}<em>{\text{DPO}} = -\mathbb{E}</em>{(x, y_w, y_l)}\left[\log \sigma\left(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)\right]
$$</p><p>其中：</p><ul><li>$\pi_\theta$ 是当前策略（待优化的模型）</li><li>$\pi_{\text{ref}}$ 是参考策略（通常是 SFT 后的模型）</li><li>$\beta$ 是温度参数，控制优化强度</li></ul><p>直观理解：DPO 试图<strong>最大化胜回答与负回答之间的对数概率差距</strong>，同时不要让模型偏离参考策略太远。</p><h2 id=第七章多模态扩展--看得懂听得见的-llama>第七章：多模态扩展 —— 看得懂、听得见的 Llama<a hidden class=anchor aria-hidden=true href=#第七章多模态扩展--看得懂听得见的-llama>#</a></h2><h3 id=71-组合式架构设计>7.1 组合式架构设计<a hidden class=anchor aria-hidden=true href=#71-组合式架构设计>#</a></h3><p>除了纯文本模型，论文还介绍了 Llama 3 的多模态扩展——图像理解和语音识别能力。关键在于<strong>组合式（Compositional）设计</strong>：</p><p>不从头训练一个端到端多模态模型，而是将<strong>预训练的视觉/语音编码器</strong>与<strong>预训练的语言模型</strong>通过轻量级适配器连接起来。</p><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>flowchart TB
subgraph Vision [视觉分支]
VInput[图像输入] --> ViT[ViT 编码器]
ViT --> VAdapter[视觉适配器]
end
subgraph Speech [语音分支]
SInput[语音输入] --> SEncoder[语音编码器]
SEncoder --> SAdapter[语音适配器]
end
subgraph Core [核心]
LLM[Llama 3<br>语言模型]
end
VAdapter --> LLM
SAdapter --> LLM
TInput[文本输入] --> LLM
LLM --> Output[文本输出]
style VInput fill:#007AFF,stroke:#007AFF,stroke-width:2px,color:#ffffff
style ViT fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style VAdapter fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff
style SInput fill:#007AFF,stroke:#007AFF,stroke-width:2px,color:#ffffff
style SEncoder fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style SAdapter fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff
style LLM fill:#AF52DE,stroke:#AF52DE,stroke-width:3px,color:#ffffff
style TInput fill:#007AFF,stroke:#007AFF,stroke-width:2px,color:#ffffff
style Output fill:#32D74B,stroke:#32D74B,stroke-width:2px,color:#ffffff</div></div><p><strong>图例说明</strong>：</p><ul><li>🔵 蓝色：输入模态</li><li>🟢 绿色：预训练编码器</li><li>🟠 橙色：可训练适配器</li><li>🟣 紫色：冻结的语言模型核心</li><li>🟢 浅绿：输出</li></ul><h3 id=72-视觉适配器训练>7.2 视觉适配器训练<a hidden class=anchor aria-hidden=true href=#72-视觉适配器训练>#</a></h3><p>视觉分支采用 <strong>Cross-Attention 适配器</strong>：</p><ol><li>使用预训练的 Vision Transformer（ViT）提取图像特征</li><li>在语言模型的每一层插入 Cross-Attention 层</li><li>Cross-Attention 的 Query 来自语言模型，Key/Value 来自 ViT</li><li><strong>关键</strong>：训练时只更新适配器和 ViT 的参数，<strong>语言模型参数冻结</strong></li></ol><p>这种设计的优势：</p><ul><li>保留语言模型的全部能力</li><li>避免多模态训练"破坏"语言能力</li><li>可独立优化视觉-语言对齐</li></ul><h3 id=73-视频理解扩展>7.3 视频理解扩展<a hidden class=anchor aria-hidden=true href=#73-视频理解扩展>#</a></h3><p>在图像适配器基础上，通过添加<strong>时序聚合器</strong>（Temporal Aggregator）实现视频理解：</p><ul><li>采样多帧图像，分别通过图像编码器</li><li>使用时序注意力聚合帧间信息</li><li>支持时序推理（如"视频中发生了什么"）</li></ul><p>实验表明，即使在较少的视频数据上训练，这种架构也能达到与专用视频模型竞争的性能。</p><h3 id=74-语音适配器>7.4 语音适配器<a hidden class=anchor aria-hidden=true href=#74-语音适配器>#</a></h3><p>语音分支采用类似的策略：</p><ol><li><strong>语音编码器</strong>：基于 Conformer 架构，预训练于海量语音数据</li><li><strong>适配器</strong>：将语音表示映射到语言模型的 token 空间</li><li><strong>训练</strong>：联合优化适配器和编码器，语言模型冻结</li></ol><p>值得注意的是，语音适配器训练后，模型可以直接理解语音指令并生成文本回答，实现了<strong>端到端的语音交互</strong>。</p><h2 id=第八章性能评估--与-gpt-4-的正面对决>第八章：性能评估 —— 与 GPT-4 的正面对决<a hidden class=anchor aria-hidden=true href=#第八章性能评估--与-gpt-4-的正面对决>#</a></h2><h3 id=81-基准测试结果>8.1 基准测试结果<a hidden class=anchor aria-hidden=true href=#81-基准测试结果>#</a></h3><p>Llama 3 405B 在主流基准测试上的表现：</p><p><img alt=模型性能对比 loading=lazy src=/images/plots/llama3-benchmark-comparison.png></p><p>从图中可以观察到几个关键结论：</p><ol><li><strong>规模效应显著</strong>：从 8B 到 405B，各维度能力几乎单调提升</li><li><strong>代码能力突出</strong>：HumanEval 上 405B 达到 89%，超越 GPT-4</li><li><strong>数学推理强劲</strong>：GSM8K 96.8%、MATH 73.8%，均达到顶尖水平</li><li><strong>小模型也有竞争力</strong>：8B 版本在多项任务上超越 Llama 2 70B</li></ol><h3 id=82-人工评估>8.2 人工评估<a hidden class=anchor aria-hidden=true href=#82-人工评估>#</a></h3><p>除了自动基准测试，Meta 还进行了大规模人工评估。评估者被给予同一问题的两个模型回答（盲测），需要选择更好的那个。</p><p>结果：Llama 3 405B 在<strong>有用性</strong>和<strong>事实准确性</strong>上均与 GPT-4 持平，在<strong>代码生成</strong>和<strong>数学推理</strong>上甚至略有优势。</p><h3 id=83-安全性评估>8.3 安全性评估<a hidden class=anchor aria-hidden=true href=#83-安全性评估>#</a></h3><p>Meta 对 Llama 3 进行了全面的安全评估：</p><ul><li><strong>Llama Guard 3</strong>：内置的输入/输出安全分类器</li><li><strong>越狱测试</strong>：包括多轮对话越狱、长上下文越狱等</li><li><strong>偏见评估</strong>：在多个维度上评估模型的公平性</li></ul><p>结果显示，Llama 3 在保持高有用性的同时，显著降低了有害输出率。这得益于后训练阶段的<strong>安全数据混合</strong>和<strong>对齐优化</strong>。</p><h2 id=第九章工程实践--训练-405b-模型的技术挑战>第九章：工程实践 —— 训练 405B 模型的技术挑战<a hidden class=anchor aria-hidden=true href=#第九章工程实践--训练-405b-模型的技术挑战>#</a></h2><h3 id=91-4d-并行策略>9.1 4D 并行策略<a hidden class=anchor aria-hidden=true href=#91-4d-并行策略>#</a></h3><p>训练 405B 模型需要数千张 GPU 协同工作。Meta 采用了 <strong>4D 并行</strong>策略：</p><table><thead><tr><th style=text-align:center>并行维度</th><th style=text-align:left>作用</th><th style=text-align:left>通信特点</th></tr></thead><tbody><tr><td style=text-align:center><strong>TP</strong><br>张量并行</td><td style=text-align:left>将单层网络拆分到多卡</td><td style=text-align:left>高带宽、低延迟（机内）</td></tr><tr><td style=text-align:center><strong>CP</strong><br>上下文并行</td><td style=text-align:left>将长序列分段处理</td><td style=text-align:left>中等带宽（机内/机间）</td></tr><tr><td style=text-align:center><strong>PP</strong><br>流水线并行</td><td style=text-align:left>将模型分层拆分到多机</td><td style=text-align:left>中等延迟容忍</td></tr><tr><td style=text-align:center><strong>DP</strong><br>数据并行</td><td style=text-align:left>多副本并行处理不同数据</td><td style=text-align:left>梯度同步（全局）</td></tr></tbody></table><p>并行顺序经过精心设计：<strong>[TP, CP, PP, DP]</strong>。这种排序确保了：</p><ul><li>通信最密集的 TP 在 NVLink 内部完成</li><li>CP 利用机内高速互联</li><li>PP 和 DP 可以跨越机架，容忍更高延迟</li></ul><h3 id=92-fp8-量化训练>9.2 FP8 量化训练<a hidden class=anchor aria-hidden=true href=#92-fp8-量化训练>#</a></h3><p>为了在 H100 GPU 上高效训练，Meta 采用了 <strong>FP8（8-bit 浮点）量化</strong>：</p><ul><li>前向传播：权重和激活使用 FP8</li><li>反向传播：梯度使用 FP8</li><li>关键层（第一层、最后一层）保持 BF16</li><li>使用<strong>逐行量化</strong>（Row-wise Quantization）和<strong>动态缩放</strong>保证精度</li></ul><p>实验表明，FP8 训练相比 BF16 几乎没有精度损失，但吞吐量提升显著。</p><h3 id=93-故障恢复机制>9.3 故障恢复机制<a hidden class=anchor aria-hidden=true href=#93-故障恢复机制>#</a></h3><p>在数月的大规模训练中，硬件故障不可避免。Meta 的应对策略：</p><ul><li><strong>高频 checkpoint</strong>：每 50-100 步保存状态</li><li><strong>异步 checkpoint</strong>：不阻塞训练流水线</li><li><strong>自动故障检测</strong>：监控 GPU 健康状态</li><li><strong>弹性重启</strong>：自动从最近 checkpoint 恢复</li></ul><p>据统计，整个训练过程中经历了数百次各类故障，但都通过自动化机制无缝恢复。</p><h2 id=结语开源-ai-的新纪元>结语：开源 AI 的新纪元<a hidden class=anchor aria-hidden=true href=#结语开源-ai-的新纪元>#</a></h2><p>Llama 3 的发布不仅是技术里程碑，更是 AI 发展范式的转折点。</p><p>回顾这篇论文的核心贡献：</p><ol><li><strong>数据</strong>：15.6 万亿 tokens 的高质量语料，证明了数据质量与数量的同等重要性</li><li><strong>规模</strong>：405B 参数 + 3.8×10²⁵ FLOPs，展示了 Dense Transformer 架构的极限潜力</li><li><strong>效率</strong>：GQA、4D 并行、FP8 量化等工程创新，让大规模训练成为可能</li><li><strong>开放</strong>：完整开源模型权重和技术细节，推动 AI 民主化</li></ol><p>更重要的是，Llama 3 证明了<strong>开源模型可以与闭源巨头抗衡</strong>。这不是终点，而是新起点——一个研究者可以自由探索、开发者可以按需定制、企业可以自主部署的 AI 新时代。</p><p>未来，当我们回望 2024 年，或许会将其视为"开源 AI 元年"。而《The Llama 3 Herd of Models》这篇论文，就是这一历史转折的见证者和推动者。</p><h2 id=参考阅读>参考阅读<a hidden class=anchor aria-hidden=true href=#参考阅读>#</a></h2><ul><li>论文原文：<a href=https://arxiv.org/abs/2407.21783>The Llama 3 Herd of Models</a></li><li>模型下载：<a href=https://llama.meta.com/>Llama 官网</a></li><li>技术博客：<a href=https://ai.meta.com/blog/>Meta AI Blog</a></li></ul><hr><p class=caption>本文图表说明：</p>- Scaling Laws 对比图：展示了 Llama 3 各模型相对于 Chinchilla 最优曲线的位置
- 上下文演化图：呈现了从 8K 到 128K 的训练阶段和性能保持
- 基准对比图：Llama 3 系列与 GPT-4 在主流测试集上的性能比较
- 计算量分布图：各训练阶段消耗的算力比例</div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%BB%BC%E8%BF%B0/>综述</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a></li><li><a href=https://s-ai-unix.github.io/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>大语言模型</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-02-01-epsilon-delta%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E7%9A%84%E4%B8%A5%E6%A0%BC%E5%8C%96%E9%9D%A9%E5%91%BD/><span class=title>« Prev</span><br><span>Epsilon-Delta：数学分析的严格化革命</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-30-alphazero-paper-interpretation/><span class=title>Next »</span><br><span>AI 论文解读系列：AlphaZero - 从零开始的自我博弈通用算法</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作 on x" href="https://x.com/intent/tweet/?text=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aThe%20Llama%203%20Herd%20of%20Models%20%e2%80%94%e2%80%94%20%e5%bc%80%e6%ba%90%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%b7%85%e5%b3%b0%e4%b9%8b%e4%bd%9c&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-31-ai-paper-llama3-herd-of-models%2f&amp;hashtags=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%2c%e7%bb%bc%e8%bf%b0%2c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%2c%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-31-ai-paper-llama3-herd-of-models%2f&amp;title=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aThe%20Llama%203%20Herd%20of%20Models%20%e2%80%94%e2%80%94%20%e5%bc%80%e6%ba%90%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%b7%85%e5%b3%b0%e4%b9%8b%e4%bd%9c&amp;summary=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aThe%20Llama%203%20Herd%20of%20Models%20%e2%80%94%e2%80%94%20%e5%bc%80%e6%ba%90%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%b7%85%e5%b3%b0%e4%b9%8b%e4%bd%9c&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-31-ai-paper-llama3-herd-of-models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-31-ai-paper-llama3-herd-of-models%2f&title=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aThe%20Llama%203%20Herd%20of%20Models%20%e2%80%94%e2%80%94%20%e5%bc%80%e6%ba%90%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%b7%85%e5%b3%b0%e4%b9%8b%e4%bd%9c"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：The Llama 3 Herd of Models —— 开源大模型的巅峰之作 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-31-ai-paper-llama3-herd-of-models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>