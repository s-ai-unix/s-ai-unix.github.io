<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>信息几何：在概率空间中寻找最短路径 | s-ai-unix's Blog</title><meta name=keywords content="信息几何,微分几何,机器学习,综述,深度学习"><meta name=description content="从 Fisher 信息度量到自然梯度，从黎曼流形到 Wasserstein 距离：全面介绍信息几何这一连接统计学、微分几何与深度学习的交叉领域"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-25-information-geometry/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-25-information-geometry/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-25-information-geometry/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="信息几何：在概率空间中寻找最短路径"><meta property="og:description" content="从 Fisher 信息度量到自然梯度，从黎曼流形到 Wasserstein 距离：全面介绍信息几何这一连接统计学、微分几何与深度学习的交叉领域"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-25T15:00:00+08:00"><meta property="article:modified_time" content="2026-01-25T15:00:00+08:00"><meta property="article:tag" content="信息几何"><meta property="article:tag" content="微分几何"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="综述"><meta property="article:tag" content="深度学习"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/ig-overview.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/ig-overview.jpg"><meta name=twitter:title content="信息几何：在概率空间中寻找最短路径"><meta name=twitter:description content="从 Fisher 信息度量到自然梯度，从黎曼流形到 Wasserstein 距离：全面介绍信息几何这一连接统计学、微分几何与深度学习的交叉领域"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"信息几何：在概率空间中寻找最短路径","item":"https://s-ai-unix.github.io/posts/2026-01-25-information-geometry/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"信息几何：在概率空间中寻找最短路径","name":"信息几何：在概率空间中寻找最短路径","description":"从 Fisher 信息度量到自然梯度，从黎曼流形到 Wasserstein 距离：全面介绍信息几何这一连接统计学、微分几何与深度学习的交叉领域","keywords":["信息几何","微分几何","机器学习","综述","深度学习"],"articleBody":"引言：当概率成为空间上的点 想象一下，你站在一个巨大的画廊里。墙上挂着无数幅画，每一幅画都是一张概率分布的直方图。如果你要量化两幅画之间的\"距离\"，你会怎么做？直接比较每个柱子的高度差异？还是考虑某种更本质的、统计学意义上的距离？\n这个问题触及了统计学的核心：如何量化两个概率分布之间的差异。传统的做法是使用 KL 散度或互信息，但这些度量缺乏几何直观——它们不是真正的\"距离\"，也不满足三角不等式。\n信息几何给出了一种全新的视角：将所有概率分布看作一个黎曼流形，每个分布是流形上的一个点，Fisher 信息矩阵定义了这个流形上的度量张量。在这个框架下，我们可以谈论\"两点之间的最短路径\"（测地线），可以计算\"梯度\"（自然梯度），可以定义\"曲率\"（统计流形的曲率）。\n这个领域的诞生可以追溯到 1945 年，印度统计学家 C. R. Rao 提出了 Fisher 信息度量可以作为微分几何的度量张量。此后，法国数学家 Amari 系统性地发展了信息几何的理论，并将其与神经网络、优化算法相结合。\n在这篇文章中，我们将从基础概念开始，系统性地介绍信息几何的核心理论，探讨其在深度学习中的应用，并对未来的发展方向做出展望。\n第一章：几何概率空间 1.1 概率分布作为流形 考虑一个简单的例子：所有零均值、单位方差的一维高斯分布 $\\mathcal{N}(0, \\sigma^2)$ 可以用一个参数 $\\sigma$ 来表示。但如果我们考虑所有可能的高斯分布 $\\mathcal{N}(\\mu, \\sigma^2)$，这就变成了一个二维的空间。\n更一般地，考虑一个参数族 $\\mathcal{P} = {p(x \\mid \\theta) : \\theta \\in \\Theta}$，其中 $\\theta \\in \\mathbb{R}^n$ 是参数。这个参数族可以看作一个 $n$ 维的流形——这就是统计流形。\n关键洞察：每个概率分布不是孤立的对象，而是镶嵌在无穷维分布空间中的一个点。信息几何的任务就是给这个流形装备一个自然的几何结构。\n1.2 Fisher 信息度量 1945 年，C. R. Rao 发现了一个重要的事实：Fisher 信息矩阵可以定义一个黎曼度量。\n定义：对于参数族 $p(x \\mid \\theta)$，Fisher 信息矩阵定义为：\n$$ I(\\theta){ij} = \\mathbb{E}{p(x \\mid \\theta)}\\left[\\frac{\\partial \\log p(x \\mid \\theta)}{\\partial \\theta_i} \\frac{\\partial \\log p(x \\mid \\theta)}{\\partial \\theta_j}\\right] $$\n在正则条件下，这个矩阵是正定的，因此可以定义一个黎曼度量：\n$$ ds^2 = \\sum_{i,j} I(\\theta)_{ij} d\\theta_i d\\theta_j $$\n直观理解：Fisher 信息度量告诉我们，参数空间中的\"距离\"应该如何衡量。如果两个参数在统计上很难区分（Fisher 信息小），它们之间的\"距离\"就远；如果容易区分（Fisher 信息大），它们之间的\"距离\"就近。\n图1：Fisher 信息椭球。不同相关系数下的高斯分布的 Fisher 信息椭球形状不同。椭球的长轴方向对应于方差最大的方向，短轴方向对应于方差最小的方向。\n1.3 测地线：概率分布之间的最短路径 在装备了 Fisher 信息度量后，统计流形成为了一个黎曼流形。我们可以计算两点之间的测地线——即概率分布之间的\"最短路径\"。\n对于正态分布的空间，测地线可以通过 Fisher 信息度量显式计算。有趣的是，沿着测地线插值得到的分布，与直接对参数进行线性插值得到的分布是不同的。\n图2：两个高斯分布之间的测地线。注意沿着测地线，分布平滑地从一个\"形态\"过渡到另一个\"形态\"，而线性插值则会产生不自然的中间分布。\n第二章：Fisher 信息度量的性质 2.1 不变性 Fisher 信息度量有一个美妙的性质：它在参数变换下是协变的。\n设 $\\eta = g(\\theta)$ 是一个参数变换，那么在新参数下的 Fisher 信息矩阵为：\n$$ I(\\eta) = J^{-T} I(\\theta) J^{-1} $$\n其中 $J$ 是雅可比矩阵。这意味着无论我们选择什么样的参数化，Fisher 信息度量给出的几何结构是内在的、不依赖于参数选择的。\n2.2 指数族的平坦性 在信息几何中，指数族（如高斯分布、泊松分布、伯努利分布等）占据着特殊的地位。它们是统计流形中的\"平坦空间\"——可以像欧几里得空间一样建立整体坐标系，曲率为零。\n指数族的形式：\n$$ p(x \\mid \\theta) = \\exp\\left(\\theta^\\top T(x) - \\psi(\\theta)\\right) h(x) $$\n其中 $T(x)$ 是充分统计量，$\\psi(\\theta)$ 是势函数。\n第三章：自然梯度下降 信息几何在优化中最重要的应用是自然梯度下降。\n3.1 标准梯度的问题 考虑优化目标函数 $L(\\theta)$。标准梯度下降的更新规则是：\n$$ \\theta_{t+1} = \\theta_t - \\alpha \\nabla L(\\theta_t) $$\n这在欧几里得空间中很自然，但在统计流形上就不那么合理了。问题在于：标准梯度假设所有参数方向上的步长是\"等价\"的，但从统计学的角度来看，不同参数方向的变化对分布的影响是不同的。\n3.2 自然梯度的思想 Amari 在 1998 年提出的自然梯度下降解决了这个问题。核心思想是：在统计流形上，梯度应该用 Fisher 信息度量的逆来\"预白化\"：\n$$ \\theta_{t+1} = \\theta_t - \\alpha I(\\theta_t)^{-1} \\nabla L(\\theta_t) $$\n直观理解：Fisher 信息矩阵的逆告诉我们每个参数方向上的\"敏感度\"。在敏感的方向上，我们应该用更小的步长；在不敏感的方向上，可以用更大的步长。\n图3：自然梯度下降（绿色）与标准梯度下降（红色）的对比。自然梯度沿着统计流形的测地线方向前进，通常能更快地收敛。\n3.3 在深度学习中的应用 自然梯度在深度学习中的应用面临计算 Fisher 信息矩阵的逆的挑战。针对这个问题，研究者们提出了多种近似方法：\nK-FAC（Kronecker-Factored Approximate Curvature）：假设 Fisher 信息矩阵可以分解为 Kronecker 乘积的形式 Adam 及其变种：虽然不是严格的自然梯度，但其自适应学习率的思想与自然梯度一脉相承 Shampoo：另一种二阶优化的近似方法 第四章：Wasserstein 距离与最优传输 信息几何的另一个重要分支是最优传输理论，以及由此导出的 Wasserstein 距离。\n4.1 从 Monge 问题到 Kantorovich 问题 最优传输问题的原始形式是：给定两个概率分布 $P$ 和 $Q$，找到一种\"运输方案\"，将 $P$ 变换成 $Q$，使得运输成本最小。\n1781 年，Gaspard Monge 提出了这个问题，但他的 formulations 太过刚性。1942 年，Leonid Kantorovich 放松了约束，允许将质量\"拆分\"运输，这才使得这个问题变得可解。\n4.2 Wasserstein 距离 Wasserstein 距离（也称为 Earth Mover’s Distance）定义为：\n$$ W_p(P, Q) = \\left(\\inf_{\\gamma \\in \\Gamma(P, Q)} \\int d(x, y)^p d\\gamma(x, y)\\right)^{1/p} $$\n其中 $\\Gamma(P, Q)$ 是所有边际分布分别为 $P$ 和 $Q$ 的联合分布的集合。\n图4：Wasserstein 距离的直观解释。灰色的箭头表示\"质量搬运\"的计划，箭头的长度表示搬运的距离。目标是找到总成本最小的搬运方案。\n4.3 在生成模型中的应用 Wasserstein 距离在生成模型中有重要应用：\nWGAN（Wasserstein GAN）：使用 Wasserstein 距离作为损失函数，解决了原始 GAN 的梯度消失问题 Wasserstein Dropout：通过 Wasserstein 距离正则化 Dropout Wasserstein Barycenter：计算多个分布的\"平均值\" 第五章：信息投影与变分推断 信息几何为变分推断提供了优雅的几何解释。\n5.1 信息投影 给定一个复杂的真实分布 $P$，我们想要用一个简单的近似分布族 $Q$ 中的分布来近似它。传统的做法是最小化 KL 散度 $D_{KL}(P | Q)$ 或 $D_{KL}(Q | P)$。\n信息几何引入了一种新的视角：信息投影。考虑两个不同的 KL 散度：\n前向 KL：$D_{KL}(P | Q)$ —— I-投影 反向 KL：$D_{KL}(Q | P)$ —— M-投影 这两种投影在几何上有不同的含义。I-投影保持支撑集不变，适合近似多峰分布；M-投影保持模式不变，适合寻找\"简单\"的近似。\n图5：信息投影：将复杂分布（蓝色）投影到指数族（橙色）。注意投影点不是参数空间中的欧几里得投影，而是在 Fisher 信息度量下的投影。\n5.2 变分自编码器 VAE 的学习过程可以理解为信息投影：编码器将数据映射到潜在空间，解码器从潜在空间重构数据。ELBO（Evidence Lower BOund）可以解释为自由能的变分近似。\n第六章：曲率与神经网络的优化景观 6.1 神经网络的\"景观\" 神经网络的损失函数曲面是一个非常复杂的高维非凸曲面。信息几何提供了一种工具来分析这个曲面的曲率性质。\nHessian 矩阵与曲率：损失函数的 Hessian 矩阵描述了函数的局部曲率。大的特征值对应于高曲率方向（陡峭的峡谷），小的特征值对应于低曲率方向（平坦的高原）。\n6.2 曲率与优化 理解损失函数的曲率有助于设计更好的优化算法：\n高曲率区域：需要使用较小的学习率，或者使用二阶方法 鞍点：高维空间中鞍点比局部最小值更常见，需要特殊的处理策略 路径曲率：优化轨迹的曲率可以用来指导学习率的调整 第七章：深度学习中的几何新方向 7.1 几何深度学习 几何深度学习是近年来兴起的领域，它考虑数据的几何结构。\n图神经网络：数据是图结构，利用图的几何性质进行学习 流形学习：假设数据分布在一个低维流形上，试图学习这个流形的结构 双曲空间嵌入：利用双曲几何的负曲率性质来建模层级结构 7.2 流形假说再审视 “流形假说\"认为真实数据分布在一个低维流形上。信息几何为这个假说提供了严格的数学框架，并提出了新的问题：\n如何估计数据流形的曲率？ 如何设计尊重流形几何结构的神经网络？ 流形的拓扑性质（如贝蒂数）如何影响学习？ 7.3 几何正则化 信息几何激发了几何正则化方法：\n拉普拉斯正则化：要求预测函数在数据流形上平滑变化 Wasserstein 正则化：保持编码器的雅可比矩阵与正交矩阵接近 信息瓶颈：限制信息流，强迫网络学习紧凑的表征 第八章：前沿与展望 8.1 当前挑战 信息几何与深度学习的结合仍面临诸多挑战：\n计算复杂性：Fisher 信息矩阵的逆是 $O(n^3)$ 的复杂度，对于深度神经网络来说不可行。当前的近似方法（如 K-FAC、Shampoo）虽然有效，但仍有改进空间。\n理论理解：我们对深度网络的损失函数曲率的理解还很有限。为什么随机梯度下降在实践中效果这么好？为什么过参数化的网络不会过拟合？\n新型架构：Transformer 等新型架构的几何性质是什么？注意力机制如何改变信息的流动？\n8.2 未来方向 几何引导的架构设计：未来的神经网络架构可能会更加注重几何性质。例如，设计具有良好曲率性质的激活函数，或者利用流形结构设计更高效的注意力机制。\n量子信息几何：量子力学与信息论的结合产生了量子信息论。量子机器学习中的几何结构是一个前沿方向。\n因果推断与几何：因果图可以看作一种特殊的几何结构。将因果推断与信息几何结合，可能产生更强大的推理算法。\n生物启发：大脑中的信息处理方式可能遵循某种几何原则。神经科学的发现可能启发新的机器学习算法。\n8.3 对几何与深度学习结合的判断 几何方法在深度学习中的重要性将持续增长：\n数据理解：几何视角帮助我们理解数据的本质结构，这是设计有效算法的前提\n算法设计：自然梯度等几何方法已经在某些任务上显示出超越标准方法的性能\n理论保证：几何分析可能为优化算法的收敛性、泛化性能提供理论保证\n新兴应用：生成模型、强化学习、因果推断等领域都对几何方法有强烈需求\n但同时需要注意：几何方法往往计算复杂度高，需要在理论和实践之间找到平衡。未来的方向可能是设计\"几何感知\"但计算高效的算法。\n结语 在这篇文章中，我们系统性地介绍了信息几何的核心理论，从 Fisher 信息度量到自然梯度，从 Wasserstein 距离到信息投影，最后探讨了与深度学习结合的前沿方向。\n信息几何的美在于它将三个看似不相关的领域——统计学、微分几何、信息论——统一在同一个框架下。在这个框架下，概率分布不再是抽象的数学对象，而是流形上的点；优化不再是黑箱算法，而是沿测地线的\"自然\"运动；两个分布之间的差异不再是单一的数字，而是可以用几何形状来量化的关系。\n随着深度学习的发展，几何视角将变得越来越重要。理解数据的几何结构、设计几何感知的算法、分析优化过程的几何性质，这些将是未来研究的重要方向。\n希望这篇文章能够帮助读者建立信息几何的整体认识，为更深入的学习和研究打下坚实的基础。\n参考文献 Amari, S. (2016). Information Geometry and Its Applications. Springer. Amari, S., \u0026 Nagaoka, H. (2000). Methods of Information Geometry. American Mathematical Society. Cover, T. M., \u0026 Thomas, J. A. (2006). Elements of Information Theory (2nd ed.). Wiley. Villani, C. (2009). Optimal Transport: Old and New. Springer. Peyré, G., \u0026 Cuturi, M. (2019). Computational Optimal Transport. Foundations and Trends in Machine Learning, 11(5-6), 355-607. Pascanu, R., et al. (2014). Natural Gradient Descent in Deep Neural Networks. ICML. Martens, J. (2020). New Insights and Perspectives on the Natural Gradient Method. Journal of Machine Learning Research, 21, 1-96. ","wordCount":"483","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/ig-overview.jpg","datePublished":"2026-01-25T15:00:00+08:00","dateModified":"2026-01-25T15:00:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-25-information-geometry/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">信息几何：在概率空间中寻找最短路径</h1><div class=post-description>从 Fisher 信息度量到自然梯度，从黎曼流形到 Wasserstein 距离：全面介绍信息几何这一连接统计学、微分几何与深度学习的交叉领域</div><div class=post-meta><span title='2026-01-25 15:00:00 +0800 CST'>January 25, 2026</span>&nbsp;·&nbsp;<span>3 min</span>&nbsp;·&nbsp;<span>483 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/ig-overview.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/ig-overview.jpg alt=信息几何可视化></a><figcaption>信息几何：概率空间的几何学</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e5%bd%93%e6%a6%82%e7%8e%87%e6%88%90%e4%b8%ba%e7%a9%ba%e9%97%b4%e4%b8%8a%e7%9a%84%e7%82%b9 aria-label=引言：当概率成为空间上的点>引言：当概率成为空间上的点</a></li><li><a href=#%e7%ac%ac%e4%b8%80%e7%ab%a0%e5%87%a0%e4%bd%95%e6%a6%82%e7%8e%87%e7%a9%ba%e9%97%b4 aria-label=第一章：几何概率空间>第一章：几何概率空间</a><ul><li><a href=#11-%e6%a6%82%e7%8e%87%e5%88%86%e5%b8%83%e4%bd%9c%e4%b8%ba%e6%b5%81%e5%bd%a2 aria-label="1.1 概率分布作为流形">1.1 概率分布作为流形</a></li><li><a href=#12-fisher-%e4%bf%a1%e6%81%af%e5%ba%a6%e9%87%8f aria-label="1.2 Fisher 信息度量">1.2 Fisher 信息度量</a></li><li><a href=#13-%e6%b5%8b%e5%9c%b0%e7%ba%bf%e6%a6%82%e7%8e%87%e5%88%86%e5%b8%83%e4%b9%8b%e9%97%b4%e7%9a%84%e6%9c%80%e7%9f%ad%e8%b7%af%e5%be%84 aria-label="1.3 测地线：概率分布之间的最短路径">1.3 测地线：概率分布之间的最短路径</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e7%ab%a0fisher-%e4%bf%a1%e6%81%af%e5%ba%a6%e9%87%8f%e7%9a%84%e6%80%a7%e8%b4%a8 aria-label="第二章：Fisher 信息度量的性质">第二章：Fisher 信息度量的性质</a><ul><li><a href=#21-%e4%b8%8d%e5%8f%98%e6%80%a7 aria-label="2.1 不变性">2.1 不变性</a></li><li><a href=#22-%e6%8c%87%e6%95%b0%e6%97%8f%e7%9a%84%e5%b9%b3%e5%9d%a6%e6%80%a7 aria-label="2.2 指数族的平坦性">2.2 指数族的平坦性</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e7%ab%a0%e8%87%aa%e7%84%b6%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d aria-label=第三章：自然梯度下降>第三章：自然梯度下降</a><ul><li><a href=#31-%e6%a0%87%e5%87%86%e6%a2%af%e5%ba%a6%e7%9a%84%e9%97%ae%e9%a2%98 aria-label="3.1 标准梯度的问题">3.1 标准梯度的问题</a></li><li><a href=#32-%e8%87%aa%e7%84%b6%e6%a2%af%e5%ba%a6%e7%9a%84%e6%80%9d%e6%83%b3 aria-label="3.2 自然梯度的思想">3.2 自然梯度的思想</a></li><li><a href=#33-%e5%9c%a8%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8 aria-label="3.3 在深度学习中的应用">3.3 在深度学习中的应用</a></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e7%ab%a0wasserstein-%e8%b7%9d%e7%a6%bb%e4%b8%8e%e6%9c%80%e4%bc%98%e4%bc%a0%e8%be%93 aria-label="第四章：Wasserstein 距离与最优传输">第四章：Wasserstein 距离与最优传输</a><ul><li><a href=#41-%e4%bb%8e-monge-%e9%97%ae%e9%a2%98%e5%88%b0-kantorovich-%e9%97%ae%e9%a2%98 aria-label="4.1 从 Monge 问题到 Kantorovich 问题">4.1 从 Monge 问题到 Kantorovich 问题</a></li><li><a href=#42-wasserstein-%e8%b7%9d%e7%a6%bb aria-label="4.2 Wasserstein 距离">4.2 Wasserstein 距离</a></li><li><a href=#43-%e5%9c%a8%e7%94%9f%e6%88%90%e6%a8%a1%e5%9e%8b%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8 aria-label="4.3 在生成模型中的应用">4.3 在生成模型中的应用</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e7%ab%a0%e4%bf%a1%e6%81%af%e6%8a%95%e5%bd%b1%e4%b8%8e%e5%8f%98%e5%88%86%e6%8e%a8%e6%96%ad aria-label=第五章：信息投影与变分推断>第五章：信息投影与变分推断</a><ul><li><a href=#51-%e4%bf%a1%e6%81%af%e6%8a%95%e5%bd%b1 aria-label="5.1 信息投影">5.1 信息投影</a></li><li><a href=#52-%e5%8f%98%e5%88%86%e8%87%aa%e7%bc%96%e7%a0%81%e5%99%a8 aria-label="5.2 变分自编码器">5.2 变分自编码器</a></li></ul></li><li><a href=#%e7%ac%ac%e5%85%ad%e7%ab%a0%e6%9b%b2%e7%8e%87%e4%b8%8e%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e4%bc%98%e5%8c%96%e6%99%af%e8%a7%82 aria-label=第六章：曲率与神经网络的优化景观>第六章：曲率与神经网络的优化景观</a><ul><li><a href=#61-%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e6%99%af%e8%a7%82 aria-label='6.1 神经网络的"景观"'>6.1 神经网络的"景观"</a></li><li><a href=#62-%e6%9b%b2%e7%8e%87%e4%b8%8e%e4%bc%98%e5%8c%96 aria-label="6.2 曲率与优化">6.2 曲率与优化</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%83%e7%ab%a0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e5%87%a0%e4%bd%95%e6%96%b0%e6%96%b9%e5%90%91 aria-label=第七章：深度学习中的几何新方向>第七章：深度学习中的几何新方向</a><ul><li><a href=#71-%e5%87%a0%e4%bd%95%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0 aria-label="7.1 几何深度学习">7.1 几何深度学习</a></li><li><a href=#72-%e6%b5%81%e5%bd%a2%e5%81%87%e8%af%b4%e5%86%8d%e5%ae%a1%e8%a7%86 aria-label="7.2 流形假说再审视">7.2 流形假说再审视</a></li><li><a href=#73-%e5%87%a0%e4%bd%95%e6%ad%a3%e5%88%99%e5%8c%96 aria-label="7.3 几何正则化">7.3 几何正则化</a></li></ul></li><li><a href=#%e7%ac%ac%e5%85%ab%e7%ab%a0%e5%89%8d%e6%b2%bf%e4%b8%8e%e5%b1%95%e6%9c%9b aria-label=第八章：前沿与展望>第八章：前沿与展望</a><ul><li><a href=#81-%e5%bd%93%e5%89%8d%e6%8c%91%e6%88%98 aria-label="8.1 当前挑战">8.1 当前挑战</a></li><li><a href=#82-%e6%9c%aa%e6%9d%a5%e6%96%b9%e5%90%91 aria-label="8.2 未来方向">8.2 未来方向</a></li><li><a href=#83-%e5%af%b9%e5%87%a0%e4%bd%95%e4%b8%8e%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%bb%93%e5%90%88%e7%9a%84%e5%88%a4%e6%96%ad aria-label="8.3 对几何与深度学习结合的判断">8.3 对几何与深度学习结合的判断</a></li></ul></li><li><a href=#%e7%bb%93%e8%af%ad aria-label=结语>结语</a></li><li><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae aria-label=参考文献>参考文献</a></li></ul></div></details></div><div class=post-content><h2 id=引言当概率成为空间上的点>引言：当概率成为空间上的点<a hidden class=anchor aria-hidden=true href=#引言当概率成为空间上的点>#</a></h2><p>想象一下，你站在一个巨大的画廊里。墙上挂着无数幅画，每一幅画都是一张概率分布的直方图。如果你要量化两幅画之间的"距离"，你会怎么做？直接比较每个柱子的高度差异？还是考虑某种更本质的、统计学意义上的距离？</p><p>这个问题触及了统计学的核心：如何量化两个概率分布之间的差异。传统的做法是使用 KL 散度或互信息，但这些度量缺乏几何直观——它们不是真正的"距离"，也不满足三角不等式。</p><p>信息几何给出了一种全新的视角：将所有概率分布看作一个黎曼流形，每个分布是流形上的一个点，Fisher 信息矩阵定义了这个流形上的度量张量。在这个框架下，我们可以谈论"两点之间的最短路径"（测地线），可以计算"梯度"（自然梯度），可以定义"曲率"（统计流形的曲率）。</p><p>这个领域的诞生可以追溯到 1945 年，印度统计学家 C. R. Rao 提出了 Fisher 信息度量可以作为微分几何的度量张量。此后，法国数学家 Amari 系统性地发展了信息几何的理论，并将其与神经网络、优化算法相结合。</p><p>在这篇文章中，我们将从基础概念开始，系统性地介绍信息几何的核心理论，探讨其在深度学习中的应用，并对未来的发展方向做出展望。</p><h2 id=第一章几何概率空间>第一章：几何概率空间<a hidden class=anchor aria-hidden=true href=#第一章几何概率空间>#</a></h2><h3 id=11-概率分布作为流形>1.1 概率分布作为流形<a hidden class=anchor aria-hidden=true href=#11-概率分布作为流形>#</a></h3><p>考虑一个简单的例子：所有零均值、单位方差的一维高斯分布 $\mathcal{N}(0, \sigma^2)$ 可以用一个参数 $\sigma$ 来表示。但如果我们考虑所有可能的高斯分布 $\mathcal{N}(\mu, \sigma^2)$，这就变成了一个二维的空间。</p><p>更一般地，考虑一个参数族 $\mathcal{P} = {p(x \mid \theta) : \theta \in \Theta}$，其中 $\theta \in \mathbb{R}^n$ 是参数。这个参数族可以看作一个 $n$ 维的流形——这就是统计流形。</p><p><strong>关键洞察</strong>：每个概率分布不是孤立的对象，而是镶嵌在无穷维分布空间中的一个点。信息几何的任务就是给这个流形装备一个自然的几何结构。</p><h3 id=12-fisher-信息度量>1.2 Fisher 信息度量<a hidden class=anchor aria-hidden=true href=#12-fisher-信息度量>#</a></h3><p>1945 年，C. R. Rao 发现了一个重要的事实：Fisher 信息矩阵可以定义一个黎曼度量。</p><p><strong>定义</strong>：对于参数族 $p(x \mid \theta)$，Fisher 信息矩阵定义为：</p><p>$$
I(\theta)<em>{ij} = \mathbb{E}</em>{p(x \mid \theta)}\left[\frac{\partial \log p(x \mid \theta)}{\partial \theta_i} \frac{\partial \log p(x \mid \theta)}{\partial \theta_j}\right]
$$</p><p>在正则条件下，这个矩阵是正定的，因此可以定义一个黎曼度量：</p><p>$$
ds^2 = \sum_{i,j} I(\theta)_{ij} d\theta_i d\theta_j
$$</p><p><strong>直观理解</strong>：Fisher 信息度量告诉我们，参数空间中的"距离"应该如何衡量。如果两个参数在统计上很难区分（Fisher 信息小），它们之间的"距离"就远；如果容易区分（Fisher 信息大），它们之间的"距离"就近。</p><p><img alt="Fisher 信息椭球" loading=lazy src=/images/math/ig-fisher-ellipsoid.png></p><p><em>图1：Fisher 信息椭球。不同相关系数下的高斯分布的 Fisher 信息椭球形状不同。椭球的长轴方向对应于方差最大的方向，短轴方向对应于方差最小的方向。</em></p><h3 id=13-测地线概率分布之间的最短路径>1.3 测地线：概率分布之间的最短路径<a hidden class=anchor aria-hidden=true href=#13-测地线概率分布之间的最短路径>#</a></h3><p>在装备了 Fisher 信息度量后，统计流形成为了一个黎曼流形。我们可以计算两点之间的测地线——即概率分布之间的"最短路径"。</p><p>对于正态分布的空间，测地线可以通过 Fisher 信息度量显式计算。有趣的是，沿着测地线插值得到的分布，与直接对参数进行线性插值得到的分布是不同的。</p><p><img alt=概率分布空间的测地线 loading=lazy src=/images/math/ig-geodesic.png></p><p><em>图2：两个高斯分布之间的测地线。注意沿着测地线，分布平滑地从一个"形态"过渡到另一个"形态"，而线性插值则会产生不自然的中间分布。</em></p><h2 id=第二章fisher-信息度量的性质>第二章：Fisher 信息度量的性质<a hidden class=anchor aria-hidden=true href=#第二章fisher-信息度量的性质>#</a></h2><h3 id=21-不变性>2.1 不变性<a hidden class=anchor aria-hidden=true href=#21-不变性>#</a></h3><p>Fisher 信息度量有一个美妙的性质：它在参数变换下是协变的。</p><p>设 $\eta = g(\theta)$ 是一个参数变换，那么在新参数下的 Fisher 信息矩阵为：</p><p>$$
I(\eta) = J^{-T} I(\theta) J^{-1}
$$</p><p>其中 $J$ 是雅可比矩阵。这意味着无论我们选择什么样的参数化，Fisher 信息度量给出的几何结构是内在的、不依赖于参数选择的。</p><h3 id=22-指数族的平坦性>2.2 指数族的平坦性<a hidden class=anchor aria-hidden=true href=#22-指数族的平坦性>#</a></h3><p>在信息几何中，指数族（如高斯分布、泊松分布、伯努利分布等）占据着特殊的地位。它们是统计流形中的"平坦空间"——可以像欧几里得空间一样建立整体坐标系，曲率为零。</p><p><strong>指数族的形式</strong>：</p><p>$$
p(x \mid \theta) = \exp\left(\theta^\top T(x) - \psi(\theta)\right) h(x)
$$</p><p>其中 $T(x)$ 是充分统计量，$\psi(\theta)$ 是势函数。</p><h2 id=第三章自然梯度下降>第三章：自然梯度下降<a hidden class=anchor aria-hidden=true href=#第三章自然梯度下降>#</a></h2><p>信息几何在优化中最重要的应用是自然梯度下降。</p><h3 id=31-标准梯度的问题>3.1 标准梯度的问题<a hidden class=anchor aria-hidden=true href=#31-标准梯度的问题>#</a></h3><p>考虑优化目标函数 $L(\theta)$。标准梯度下降的更新规则是：</p><p>$$
\theta_{t+1} = \theta_t - \alpha \nabla L(\theta_t)
$$</p><p>这在欧几里得空间中很自然，但在统计流形上就不那么合理了。问题在于：标准梯度假设所有参数方向上的步长是"等价"的，但从统计学的角度来看，不同参数方向的变化对分布的影响是不同的。</p><h3 id=32-自然梯度的思想>3.2 自然梯度的思想<a hidden class=anchor aria-hidden=true href=#32-自然梯度的思想>#</a></h3><p>Amari 在 1998 年提出的自然梯度下降解决了这个问题。核心思想是：在统计流形上，梯度应该用 Fisher 信息度量的逆来"预白化"：</p><p>$$
\theta_{t+1} = \theta_t - \alpha I(\theta_t)^{-1} \nabla L(\theta_t)
$$</p><p><strong>直观理解</strong>：Fisher 信息矩阵的逆告诉我们每个参数方向上的"敏感度"。在敏感的方向上，我们应该用更小的步长；在不敏感的方向上，可以用更大的步长。</p><p><img alt="自然梯度 vs 标准梯度" loading=lazy src=/images/math/ig-natural-gradient.png></p><p><em>图3：自然梯度下降（绿色）与标准梯度下降（红色）的对比。自然梯度沿着统计流形的测地线方向前进，通常能更快地收敛。</em></p><h3 id=33-在深度学习中的应用>3.3 在深度学习中的应用<a hidden class=anchor aria-hidden=true href=#33-在深度学习中的应用>#</a></h3><p>自然梯度在深度学习中的应用面临计算 Fisher 信息矩阵的逆的挑战。针对这个问题，研究者们提出了多种近似方法：</p><ul><li><strong>K-FAC（Kronecker-Factored Approximate Curvature）</strong>：假设 Fisher 信息矩阵可以分解为 Kronecker 乘积的形式</li><li><strong>Adam 及其变种</strong>：虽然不是严格的自然梯度，但其自适应学习率的思想与自然梯度一脉相承</li><li><strong>Shampoo</strong>：另一种二阶优化的近似方法</li></ul><h2 id=第四章wasserstein-距离与最优传输>第四章：Wasserstein 距离与最优传输<a hidden class=anchor aria-hidden=true href=#第四章wasserstein-距离与最优传输>#</a></h2><p>信息几何的另一个重要分支是最优传输理论，以及由此导出的 Wasserstein 距离。</p><h3 id=41-从-monge-问题到-kantorovich-问题>4.1 从 Monge 问题到 Kantorovich 问题<a hidden class=anchor aria-hidden=true href=#41-从-monge-问题到-kantorovich-问题>#</a></h3><p>最优传输问题的原始形式是：给定两个概率分布 $P$ 和 $Q$，找到一种"运输方案"，将 $P$ 变换成 $Q$，使得运输成本最小。</p><p>1781 年，Gaspard Monge 提出了这个问题，但他的 formulations 太过刚性。1942 年，Leonid Kantorovich 放松了约束，允许将质量"拆分"运输，这才使得这个问题变得可解。</p><h3 id=42-wasserstein-距离>4.2 Wasserstein 距离<a hidden class=anchor aria-hidden=true href=#42-wasserstein-距离>#</a></h3><p>Wasserstein 距离（也称为 Earth Mover&rsquo;s Distance）定义为：</p><p>$$
W_p(P, Q) = \left(\inf_{\gamma \in \Gamma(P, Q)} \int d(x, y)^p d\gamma(x, y)\right)^{1/p}
$$</p><p>其中 $\Gamma(P, Q)$ 是所有边际分布分别为 $P$ 和 $Q$ 的联合分布的集合。</p><p><img alt="Wasserstein 距离：最优传输计划" loading=lazy src=/images/math/ig-wasserstein.png></p><p><em>图4：Wasserstein 距离的直观解释。灰色的箭头表示"质量搬运"的计划，箭头的长度表示搬运的距离。目标是找到总成本最小的搬运方案。</em></p><h3 id=43-在生成模型中的应用>4.3 在生成模型中的应用<a hidden class=anchor aria-hidden=true href=#43-在生成模型中的应用>#</a></h3><p>Wasserstein 距离在生成模型中有重要应用：</p><ul><li><strong>WGAN（Wasserstein GAN）</strong>：使用 Wasserstein 距离作为损失函数，解决了原始 GAN 的梯度消失问题</li><li><strong>Wasserstein Dropout</strong>：通过 Wasserstein 距离正则化 Dropout</li><li><strong>Wasserstein Barycenter</strong>：计算多个分布的"平均值"</li></ul><h2 id=第五章信息投影与变分推断>第五章：信息投影与变分推断<a hidden class=anchor aria-hidden=true href=#第五章信息投影与变分推断>#</a></h2><p>信息几何为变分推断提供了优雅的几何解释。</p><h3 id=51-信息投影>5.1 信息投影<a hidden class=anchor aria-hidden=true href=#51-信息投影>#</a></h3><p>给定一个复杂的真实分布 $P$，我们想要用一个简单的近似分布族 $Q$ 中的分布来近似它。传统的做法是最小化 KL 散度 $D_{KL}(P | Q)$ 或 $D_{KL}(Q | P)$。</p><p>信息几何引入了一种新的视角：<strong>信息投影</strong>。考虑两个不同的 KL 散度：</p><ul><li><strong>前向 KL</strong>：$D_{KL}(P | Q)$ —— I-投影</li><li><strong>反向 KL</strong>：$D_{KL}(Q | P)$ —— M-投影</li></ul><p>这两种投影在几何上有不同的含义。I-投影保持支撑集不变，适合近似多峰分布；M-投影保持模式不变，适合寻找"简单"的近似。</p><p><img alt=信息投影 loading=lazy src=/images/math/ig-projection.png></p><p><em>图5：信息投影：将复杂分布（蓝色）投影到指数族（橙色）。注意投影点不是参数空间中的欧几里得投影，而是在 Fisher 信息度量下的投影。</em></p><h3 id=52-变分自编码器>5.2 变分自编码器<a hidden class=anchor aria-hidden=true href=#52-变分自编码器>#</a></h3><p>VAE 的学习过程可以理解为信息投影：编码器将数据映射到潜在空间，解码器从潜在空间重构数据。ELBO（Evidence Lower BOund）可以解释为自由能的变分近似。</p><h2 id=第六章曲率与神经网络的优化景观>第六章：曲率与神经网络的优化景观<a hidden class=anchor aria-hidden=true href=#第六章曲率与神经网络的优化景观>#</a></h2><h3 id=61-神经网络的景观>6.1 神经网络的"景观"<a hidden class=anchor aria-hidden=true href=#61-神经网络的景观>#</a></h3><p>神经网络的损失函数曲面是一个非常复杂的高维非凸曲面。信息几何提供了一种工具来分析这个曲面的曲率性质。</p><p><strong>Hessian 矩阵与曲率</strong>：损失函数的 Hessian 矩阵描述了函数的局部曲率。大的特征值对应于高曲率方向（陡峭的峡谷），小的特征值对应于低曲率方向（平坦的高原）。</p><h3 id=62-曲率与优化>6.2 曲率与优化<a hidden class=anchor aria-hidden=true href=#62-曲率与优化>#</a></h3><p>理解损失函数的曲率有助于设计更好的优化算法：</p><ul><li><strong>高曲率区域</strong>：需要使用较小的学习率，或者使用二阶方法</li><li><strong>鞍点</strong>：高维空间中鞍点比局部最小值更常见，需要特殊的处理策略</li><li><strong>路径曲率</strong>：优化轨迹的曲率可以用来指导学习率的调整</li></ul><h2 id=第七章深度学习中的几何新方向>第七章：深度学习中的几何新方向<a hidden class=anchor aria-hidden=true href=#第七章深度学习中的几何新方向>#</a></h2><h3 id=71-几何深度学习>7.1 几何深度学习<a hidden class=anchor aria-hidden=true href=#71-几何深度学习>#</a></h3><p>几何深度学习是近年来兴起的领域，它考虑数据的几何结构。</p><ul><li><strong>图神经网络</strong>：数据是图结构，利用图的几何性质进行学习</li><li><strong>流形学习</strong>：假设数据分布在一个低维流形上，试图学习这个流形的结构</li><li><strong>双曲空间嵌入</strong>：利用双曲几何的负曲率性质来建模层级结构</li></ul><h3 id=72-流形假说再审视>7.2 流形假说再审视<a hidden class=anchor aria-hidden=true href=#72-流形假说再审视>#</a></h3><p>&ldquo;流形假说"认为真实数据分布在一个低维流形上。信息几何为这个假说提供了严格的数学框架，并提出了新的问题：</p><ul><li>如何估计数据流形的曲率？</li><li>如何设计尊重流形几何结构的神经网络？</li><li>流形的拓扑性质（如贝蒂数）如何影响学习？</li></ul><h3 id=73-几何正则化>7.3 几何正则化<a hidden class=anchor aria-hidden=true href=#73-几何正则化>#</a></h3><p>信息几何激发了几何正则化方法：</p><ul><li><strong>拉普拉斯正则化</strong>：要求预测函数在数据流形上平滑变化</li><li><strong>Wasserstein 正则化</strong>：保持编码器的雅可比矩阵与正交矩阵接近</li><li><strong>信息瓶颈</strong>：限制信息流，强迫网络学习紧凑的表征</li></ul><h2 id=第八章前沿与展望>第八章：前沿与展望<a hidden class=anchor aria-hidden=true href=#第八章前沿与展望>#</a></h2><h3 id=81-当前挑战>8.1 当前挑战<a hidden class=anchor aria-hidden=true href=#81-当前挑战>#</a></h3><p>信息几何与深度学习的结合仍面临诸多挑战：</p><p><strong>计算复杂性</strong>：Fisher 信息矩阵的逆是 $O(n^3)$ 的复杂度，对于深度神经网络来说不可行。当前的近似方法（如 K-FAC、Shampoo）虽然有效，但仍有改进空间。</p><p><strong>理论理解</strong>：我们对深度网络的损失函数曲率的理解还很有限。为什么随机梯度下降在实践中效果这么好？为什么过参数化的网络不会过拟合？</p><p><strong>新型架构</strong>：Transformer 等新型架构的几何性质是什么？注意力机制如何改变信息的流动？</p><h3 id=82-未来方向>8.2 未来方向<a hidden class=anchor aria-hidden=true href=#82-未来方向>#</a></h3><p><strong>几何引导的架构设计</strong>：未来的神经网络架构可能会更加注重几何性质。例如，设计具有良好曲率性质的激活函数，或者利用流形结构设计更高效的注意力机制。</p><p><strong>量子信息几何</strong>：量子力学与信息论的结合产生了量子信息论。量子机器学习中的几何结构是一个前沿方向。</p><p><strong>因果推断与几何</strong>：因果图可以看作一种特殊的几何结构。将因果推断与信息几何结合，可能产生更强大的推理算法。</p><p><strong>生物启发</strong>：大脑中的信息处理方式可能遵循某种几何原则。神经科学的发现可能启发新的机器学习算法。</p><h3 id=83-对几何与深度学习结合的判断>8.3 对几何与深度学习结合的判断<a hidden class=anchor aria-hidden=true href=#83-对几何与深度学习结合的判断>#</a></h3><p>几何方法在深度学习中的重要性将持续增长：</p><ol><li><p><strong>数据理解</strong>：几何视角帮助我们理解数据的本质结构，这是设计有效算法的前提</p></li><li><p><strong>算法设计</strong>：自然梯度等几何方法已经在某些任务上显示出超越标准方法的性能</p></li><li><p><strong>理论保证</strong>：几何分析可能为优化算法的收敛性、泛化性能提供理论保证</p></li><li><p><strong>新兴应用</strong>：生成模型、强化学习、因果推断等领域都对几何方法有强烈需求</p></li></ol><p>但同时需要注意：几何方法往往计算复杂度高，需要在理论和实践之间找到平衡。未来的方向可能是设计"几何感知"但计算高效的算法。</p><h2 id=结语>结语<a hidden class=anchor aria-hidden=true href=#结语>#</a></h2><p>在这篇文章中，我们系统性地介绍了信息几何的核心理论，从 Fisher 信息度量到自然梯度，从 Wasserstein 距离到信息投影，最后探讨了与深度学习结合的前沿方向。</p><p>信息几何的美在于它将三个看似不相关的领域——统计学、微分几何、信息论——统一在同一个框架下。在这个框架下，概率分布不再是抽象的数学对象，而是流形上的点；优化不再是黑箱算法，而是沿测地线的"自然"运动；两个分布之间的差异不再是单一的数字，而是可以用几何形状来量化的关系。</p><p>随着深度学习的发展，几何视角将变得越来越重要。理解数据的几何结构、设计几何感知的算法、分析优化过程的几何性质，这些将是未来研究的重要方向。</p><p>希望这篇文章能够帮助读者建立信息几何的整体认识，为更深入的学习和研究打下坚实的基础。</p><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><ol><li>Amari, S. (2016). <em>Information Geometry and Its Applications</em>. Springer.</li><li>Amari, S., & Nagaoka, H. (2000). <em>Methods of Information Geometry</em>. American Mathematical Society.</li><li>Cover, T. M., & Thomas, J. A. (2006). <em>Elements of Information Theory</em> (2nd ed.). Wiley.</li><li>Villani, C. (2009). <em>Optimal Transport: Old and New</em>. Springer.</li><li>Peyré, G., & Cuturi, M. (2019). Computational Optimal Transport. <em>Foundations and Trends in Machine Learning</em>, 11(5-6), 355-607.</li><li>Pascanu, R., et al. (2014). Natural Gradient Descent in Deep Neural Networks. <em>ICML</em>.</li><li>Martens, J. (2020). New Insights and Perspectives on the Natural Gradient Method. <em>Journal of Machine Learning Research</em>, 21, 1-96.</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E4%BF%A1%E6%81%AF%E5%87%A0%E4%BD%95/>信息几何</a></li><li><a href=https://s-ai-unix.github.io/tags/%E5%BE%AE%E5%88%86%E5%87%A0%E4%BD%95/>微分几何</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%BB%BC%E8%BF%B0/>综述</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-25-intrinsic-extrinsic-geometry/><span class=title>« Prev</span><br><span>内蕴与外蕴：几何学的两种视角</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-25-pde-overview/><span class=title>Next »</span><br><span>偏微分方程：描述物理世界的数学语言</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 信息几何：在概率空间中寻找最短路径 on x" href="https://x.com/intent/tweet/?text=%e4%bf%a1%e6%81%af%e5%87%a0%e4%bd%95%ef%bc%9a%e5%9c%a8%e6%a6%82%e7%8e%87%e7%a9%ba%e9%97%b4%e4%b8%ad%e5%af%bb%e6%89%be%e6%9c%80%e7%9f%ad%e8%b7%af%e5%be%84&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-25-information-geometry%2f&amp;hashtags=%e4%bf%a1%e6%81%af%e5%87%a0%e4%bd%95%2c%e5%be%ae%e5%88%86%e5%87%a0%e4%bd%95%2c%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%2c%e7%bb%bc%e8%bf%b0%2c%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 信息几何：在概率空间中寻找最短路径 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-25-information-geometry%2f&amp;title=%e4%bf%a1%e6%81%af%e5%87%a0%e4%bd%95%ef%bc%9a%e5%9c%a8%e6%a6%82%e7%8e%87%e7%a9%ba%e9%97%b4%e4%b8%ad%e5%af%bb%e6%89%be%e6%9c%80%e7%9f%ad%e8%b7%af%e5%be%84&amp;summary=%e4%bf%a1%e6%81%af%e5%87%a0%e4%bd%95%ef%bc%9a%e5%9c%a8%e6%a6%82%e7%8e%87%e7%a9%ba%e9%97%b4%e4%b8%ad%e5%af%bb%e6%89%be%e6%9c%80%e7%9f%ad%e8%b7%af%e5%be%84&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-25-information-geometry%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 信息几何：在概率空间中寻找最短路径 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-25-information-geometry%2f&title=%e4%bf%a1%e6%81%af%e5%87%a0%e4%bd%95%ef%bc%9a%e5%9c%a8%e6%a6%82%e7%8e%87%e7%a9%ba%e9%97%b4%e4%b8%ad%e5%af%bb%e6%89%be%e6%9c%80%e7%9f%ad%e8%b7%af%e5%be%84"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 信息几何：在概率空间中寻找最短路径 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-25-information-geometry%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>