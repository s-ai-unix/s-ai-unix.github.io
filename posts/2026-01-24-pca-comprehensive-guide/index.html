<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>PCA 主成分分析：从数据降维的优雅艺术 | s-ai-unix's Blog</title><meta name=keywords content="PCA,主成分分析,特征值分解,协方差矩阵,降维,机器学习"><meta name=description content="深入探讨机器学习中的核心降维算法 PCA，从直观理解到数学推导，从两种等价的视角（最大化方差、最小化重构误差）揭示其本质，包含完整的证明和实际应用。"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-24-pca-comprehensive-guide/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-24-pca-comprehensive-guide/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-24-pca-comprehensive-guide/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="PCA 主成分分析：从数据降维的优雅艺术"><meta property="og:description" content="深入探讨机器学习中的核心降维算法 PCA，从直观理解到数学推导，从两种等价的视角（最大化方差、最小化重构误差）揭示其本质，包含完整的证明和实际应用。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-24T12:00:00+08:00"><meta property="article:modified_time" content="2026-01-24T12:00:00+08:00"><meta property="article:tag" content="PCA"><meta property="article:tag" content="主成分分析"><meta property="article:tag" content="特征值分解"><meta property="article:tag" content="协方差矩阵"><meta property="article:tag" content="降维"><meta property="article:tag" content="机器学习"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/pca-visualization.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/pca-visualization.jpg"><meta name=twitter:title content="PCA 主成分分析：从数据降维的优雅艺术"><meta name=twitter:description content="深入探讨机器学习中的核心降维算法 PCA，从直观理解到数学推导，从两种等价的视角（最大化方差、最小化重构误差）揭示其本质，包含完整的证明和实际应用。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"PCA 主成分分析：从数据降维的优雅艺术","item":"https://s-ai-unix.github.io/posts/2026-01-24-pca-comprehensive-guide/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"PCA 主成分分析：从数据降维的优雅艺术","name":"PCA 主成分分析：从数据降维的优雅艺术","description":"深入探讨机器学习中的核心降维算法 PCA，从直观理解到数学推导，从两种等价的视角（最大化方差、最小化重构误差）揭示其本质，包含完整的证明和实际应用。","keywords":["PCA","主成分分析","特征值分解","协方差矩阵","降维","机器学习"],"articleBody":"引言：从混沌中寻找秩序 想象你是一个天文学家，正在观测一群恒星的位置。这些恒星在三维空间中分布，你记录了每颗恒星到地球的距离、赤经和赤纬——这就是一个典型的三维数据集。但是，你想理解这些恒星的分布规律，三维空间太复杂了。你突然意识到：这些恒星实际上分布在一个接近平面的薄层上！如果能找到这个平面，你就可以用二维坐标来描述每颗恒星的位置，大大简化问题。\n这个看似简单的思想——在高维数据中找到最能代表数据的低维子空间——就是主成分分析（Principal Component Analysis, PCA）的核心。\n在机器学习、数据科学和统计学中，我们经常面临\"维度灾难\"：数据维度越高，计算越复杂，噪声越多，模型越容易过拟合。PCA 提供了一种优雅的解决方案：它不丢弃任何原始特征的信息，而是将数据投影到新的坐标系中，在这个新坐标系中，前几个坐标轴（主成分）包含了数据的大部分信息。\n本文将带你深入 PCA 的世界。我们从直观的几何理解开始，穿越历史的长河，探索两种等价的数学推导视角，最终抵达实际应用的海岸。准备好了吗？让我们开始这场降维之旅。\nPCA 的直观理解：投影的智慧 为什么需要降维？ 在深入数学之前，让我们先理解为什么降维如此重要。\n假设你有一个包含 $1000$ 个人的数据集，每个人有 $100$ 个特征（身高、体重、血压、血糖、血细胞计数等）。这些特征之间往往存在相关性：身高和体重相关，血压和血糖相关。如果我们直接用 $100$ 个特征来分析，会遇到以下问题：\n计算复杂度：随着维度增加，算法的运行时间呈指数级增长。 过拟合风险：特征越多，模型越容易记住训练数据，泛化能力下降。 存储压力：$1000$ 个人 $\\times$ $100$ 个特征 $= 100,000$ 个数据点，存储和传输成本高。 可视化困难：我们只能在三维空间中直接观察数据，超过三维就无法直观理解。 PCA 的目标是找到一个低维表示，保留数据的大部分信息。关键问题是：如何衡量\"信息保留\"？答案是方差。\n方差作为信息度量 在一个数据集中，方差大的方向包含更多的信息。考虑一个简单的例子：假设我们有一个二维数据集，点的分布如图所示。\n图 1：PCA 的核心思想：将数据投影到方差最大的方向\n如果我们把这些点投影到不同的直线上，哪种投影方式能最好地保留原始数据的信息？\n直觉告诉我们：应该投影到数据\"伸展\"最厉害的方向上。在这个方向上，投影点的分布范围最广，方差最大，这意味着投影后保留了更多的原始信息。\n让我们用数学语言来表述这个直觉。设 $n$ 个 $d$ 维数据点 $\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n \\in \\mathbb{R}^d$，我们想找到一个单位向量 $\\mathbf{w} \\in \\mathbb{R}^d$（$|\\mathbf{w}| = 1$），使得数据投影到 $\\mathbf{w}$ 上的方差最大。\n数据点 $\\mathbf{x}_i$ 投影到 $\\mathbf{w}$ 上的值是：\n$$ z_i = \\mathbf{w}^{\\top} \\mathbf{x}_i $$\n投影值的均值是：\n$$ \\bar{z} = \\frac{1}{n}\\sum_{i=1}^{n} z_i = \\mathbf{w}^{\\top} \\bar{\\mathbf{x}} $$\n其中 $\\bar{\\mathbf{x}} = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbf{x}_i$ 是数据的均值。\n投影的方差是：\n$$ \\text{Var}(z) = \\frac{1}{n}\\sum_{i=1}^{n} (z_i - \\bar{z})^2 = \\frac{1}{n}\\sum_{i=1}^{n} (\\mathbf{w}^{\\top} (\\mathbf{x}_i - \\bar{\\mathbf{x}}))^2 $$\n让我们定义中心化的数据 $\\tilde{\\mathbf{x}}_i = \\mathbf{x}_i - \\bar{\\mathbf{x}}$，则：\n$$ \\text{Var}(z) = \\frac{1}{n}\\sum_{i=1}^{n} (\\mathbf{w}^{\\top} \\tilde{\\mathbf{x}}i)^2 = \\frac{1}{n}\\sum{i=1}^{n} \\mathbf{w}^{\\top} \\tilde{\\mathbf{x}}_i \\tilde{\\mathbf{x}}i^{\\top} \\mathbf{w} = \\mathbf{w}^{\\top} \\left(\\frac{1}{n}\\sum{i=1}^{n} \\tilde{\\mathbf{x}}_i \\tilde{\\mathbf{x}}_i^{\\top}\\right) \\mathbf{w} $$\n注意到 $\\frac{1}{n}\\sum_{i=1}^{n} \\tilde{\\mathbf{x}}_i \\tilde{\\mathbf{x}}_i^{\\top}$ 正是数据的协方差矩阵：\n$$ \\mathbf{\\Sigma} = \\frac{1}{n}\\sum_{i=1}^{n} \\tilde{\\mathbf{x}}_i \\tilde{\\mathbf{x}}_i^{\\top} $$\n因此，PCA 的优化问题是：\n$$ \\max_{\\mathbf{w}} \\mathbf{w}^{\\top} \\mathbf{\\Sigma} \\mathbf{w}, \\quad \\text{约束：} \\mathbf{w}^{\\top} \\mathbf{w} = 1 $$\n这就是 PCA 的第一个视角：寻找使投影方差最大的方向。\n历史背景：从 Pearson 到 Hotelling PCA 的历史可以追溯到 20 世纪初的统计学领域，两位数学家的贡献奠定了这个算法的基础。\nKarl Pearson：几何解释的开创者 1901 年，英国统计学家 Karl Pearson 发表了一篇题为\"On lines and planes of closest fit to systems of points in space\"（论空间点系的最优拟合线和面）的论文。Pearson 从几何角度思考：给定一组散布在空间中的点，如何找到一条直线或一个平面，使得这些点到这条线或平面的距离平方和最小？\n这个看似纯粹的几何问题，实际上等价于我们刚才讨论的\"最大化投影方差\"问题！Pearson 证明了，最优的投影方向正是数据的\"主轴\"——数据\"伸展\"最厉害的方向。\nPearson 的贡献不仅在于问题的表述，更在于他发现这个问题的解可以通过特征值分解来获得。虽然当时线性代数的理论还不像今天这样成熟，但 Pearson 已经直觉地抓住了问题的本质。\nHarold Hotelling：统计视角的完善 1933 年，美国数学家和统计学家 Harold Hotelling 独立地重新发现了 PCA，但从一个不同的角度。他将这个问题命名为\"主成分分析\"（Principal Component Analysis），并给出了完整的统计学解释。\nHotelling 的论文\"Analysis of a complex of statistical variables into principal components\"（将统计变量复合体分解为主成分）不仅解决了如何找到主成分的问题，还给出了一个清晰的解释：每个主成分都是原始变量的线性组合，这些主成分按照解释数据方差的大小排序，且彼此之间不相关。\nHotelling 的贡献使得 PCA 从一个纯粹的几何方法发展成为一个完整的统计工具，并广泛应用于实际数据分析中。\n值得一提的是，Hotelling 的 $T^2$ 统计量——一种多变量检验方法——至今仍在质量控制中广泛应用。\nPCA 的数学推导（视角一）：最大化方差 现在，让我们严谨地推导 PCA 的数学公式。我们将从\"最大化投影方差\"这个视角出发，逐步建立完整的理论框架。\n优化问题的建立 我们有 $n$ 个 $d$ 维数据点 $\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n \\in \\mathbb{R}^d$。首先，对数据进行中心化：\n$$ \\tilde{\\mathbf{x}}_i = \\mathbf{x}i - \\bar{\\mathbf{x}}, \\quad \\bar{\\mathbf{x}} = \\frac{1}{n}\\sum{i=1}^{n} \\mathbf{x}_i $$\n计算协方差矩阵：\n$$ \\mathbf{\\Sigma} = \\frac{1}{n}\\sum_{i=1}^{n} \\tilde{\\mathbf{x}}_i \\tilde{\\mathbf{x}}_i^{\\top} \\in \\mathbb{R}^{d \\times d} $$\n注意：$\\mathbf{\\Sigma}$ 是一个对称半正定矩阵。\n我们要找到第一个主成分的方向 $\\mathbf{w}_1$，使得投影方差最大：\n$$ \\mathbf{w}1 = \\arg\\max{\\mathbf{w}} \\mathbf{w}^{\\top} \\mathbf{\\Sigma} \\mathbf{w}, \\quad \\text{s.t. } \\mathbf{w}^{\\top} \\mathbf{w} = 1 $$\n使用拉格朗日乘数法 这是一个带约束的优化问题，我们可以使用拉格朗日乘数法求解。定义拉格朗日函数：\n$$ \\mathcal{L}(\\mathbf{w}, \\lambda) = \\mathbf{w}^{\\top} \\mathbf{\\Sigma} \\mathbf{w} - \\lambda (\\mathbf{w}^{\\top} \\mathbf{w} - 1) $$\n其中 $\\lambda$ 是拉格朗日乘数。\n对 $\\mathbf{w}$ 求梯度并设为零：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} = 2\\mathbf{\\Sigma}\\mathbf{w} - 2\\lambda \\mathbf{w} = 0 $$\n简化得到：\n$$ \\mathbf{\\Sigma} \\mathbf{w} = \\lambda \\mathbf{w} $$\n这正是特征值问题！$\\mathbf{w}$ 是 $\\mathbf{\\Sigma}$ 的特征向量，$\\lambda$ 是对应的特征值。\n理解特征值的含义 让我们计算投影方差的值。将特征方程 $\\mathbf{\\Sigma} \\mathbf{w} = \\lambda \\mathbf{w}$ 代入方差表达式：\n$$ \\mathbf{w}^{\\top} \\mathbf{\\Sigma} \\mathbf{w} = \\mathbf{w}^{\\top} \\lambda \\mathbf{w} = \\lambda \\mathbf{w}^{\\top} \\mathbf{w} = \\lambda $$\n因为 $\\mathbf{w}$ 是单位向量，$\\mathbf{w}^{\\top} \\mathbf{w} = 1$。\n这说明：投影方差等于对应的特征值！\n因此，要使投影方差最大，我们需要选择 $\\mathbf{\\Sigma}$ 的最大特征值对应的特征向量。\n第二个主成分及后续主成分 找到第一个主成分 $\\mathbf{w}_1$ 后，我们希望找到第二个主成分 $\\mathbf{w}_2$，它应该满足：\n最大化投影方差 与 $\\mathbf{w}_1$ 正交（不相关） 数学表述为：\n$$ \\mathbf{w}2 = \\arg\\max{\\mathbf{w}} \\mathbf{w}^{\\top} \\mathbf{\\Sigma} \\mathbf{w}, \\quad \\text{s.t. } \\mathbf{w}^{\\top} \\mathbf{w} = 1, \\mathbf{w}^{\\top} \\mathbf{w}_1 = 0 $$\n同样使用拉格朗日乘数法：\n$$ \\mathcal{L}(\\mathbf{w}, \\lambda, \\mu) = \\mathbf{w}^{\\top} \\mathbf{\\Sigma} \\mathbf{w} - \\lambda (\\mathbf{w}^{\\top} \\mathbf{w} - 1) - \\mu \\mathbf{w}^{\\top} \\mathbf{w}_1 $$\n对 $\\mathbf{w}$ 求梯度：\n$$ 2\\mathbf{\\Sigma}\\mathbf{w} - 2\\lambda \\mathbf{w} - \\mu \\mathbf{w}_1 = 0 $$\n左乘 $\\mathbf{w}_1^\\top$：\n$$ 2\\mathbf{w}_1^{\\top} \\mathbf{\\Sigma}\\mathbf{w} - 2\\lambda \\mathbf{w}_1^{\\top} \\mathbf{w} - \\mu \\mathbf{w}_1^{\\top} \\mathbf{w}_1 = 0 $$\n由于 $\\mathbf{\\Sigma}\\mathbf{w}_1 = \\lambda_1 \\mathbf{w}_1$（其中 $\\lambda_1$ 是最大特征值）：\n$$ 2\\lambda_1 \\mathbf{w}_1^{\\top} \\mathbf{w} - 2\\lambda \\mathbf{w}_1^{\\top} \\mathbf{w} - \\mu = 0 $$\n但 $\\mathbf{w}_1^{\\top} \\mathbf{w} = 0$（正交约束），所以 $\\mu = 0$。\n因此，我们得到：\n$$ \\mathbf{\\Sigma}\\mathbf{w} = \\lambda \\mathbf{w} $$\n这说明 $\\mathbf{w}_2$ 也必须是 $\\mathbf{\\Sigma}$ 的特征向量！但由于 $\\mathbf{w}_2$ 必须与 $\\mathbf{w}_1$ 正交，$\\mathbf{w}_2$ 只能选择第二大特征值对应的特征向量。\n同理，第 $k$ 个主成分 $\\mathbf{w}_k$ 是 $\\mathbf{\\Sigma}$ 的第 $k$ 大特征值 $\\lambda_k$ 对应的特征向量。\n总结 从\"最大化方差\"的视角，我们得出：\n协方差矩阵 $\\mathbf{\\Sigma}$ 的特征向量是 PCA 的主成分方向 特征值 $\\lambda_k$ 是投影到第 $k$ 个主成分的方差 主成分排序：按特征值从大到小排列 主成分正交：不同特征值对应的特征向量相互正交 这个推导优雅而简洁，将 PCA 的几何直觉与线性代数的特征值理论完美地结合在一起。\nPCA 的数学推导（视角二）：最小化重构误差 现在，我们从另一个角度理解 PCA：最小化重构误差。这个视角虽然与前一个等价，但提供了不同的物理直觉。\n重构问题的建立 假设我们想将 $d$ 维数据降维到 $k$ 维（$k \u003c d$）。设我们找到了 $k$ 个正交单位向量 $\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_k$，作为新坐标系的基。\n对于数据点 $\\mathbf{x}_i$，它的降维表示（编码）是：\n$$ \\mathbf{z}i = (z{i1}, z_{i2}, \\ldots, z_{ik})^\\top $$\n其中 $z_{ij} = \\mathbf{w}_j^{\\top} \\tilde{\\mathbf{x}}_i$ 是 $\\tilde{\\mathbf{x}}_i$ 在 $\\mathbf{w}_j$ 上的投影。\n从低维表示重构（解码）回原始空间：\n$$ \\hat{\\mathbf{x}}i = \\bar{\\mathbf{x}} + \\sum{j=1}^{k} z_{ij} \\mathbf{w}_j $$\n这就是将中心化数据投影到前 $k$ 个主成分张成的子空间，然后再加回均值。\n重构误差定义为原始数据与重构数据的欧氏距离平方：\n$$ \\text{Error} = \\sum_{i=1}^{n} |\\mathbf{x}i - \\hat{\\mathbf{x}}i|^2 = \\sum{i=1}^{n} |\\tilde{\\mathbf{x}}i - \\sum{j=1}^{k} z{ij} \\mathbf{w}_j|^2 $$\n我们的目标是最小化这个误差：\n$$ \\min_{\\mathbf{w}_1, \\ldots, \\mathbf{w}k} \\sum{i=1}^{n} \\left|\\tilde{\\mathbf{x}}i - \\sum{j=1}^{k} \\mathbf{w}_j (\\mathbf{w}_j^{\\top} \\tilde{\\mathbf{x}}_i)\\right|^2 $$\n利用正交投影的性质 记 $\\mathbf{W} = [\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_k] \\in \\mathbb{R}^{d \\times k}$，则重构误差可以写成：\n$$ \\text{Error} = \\sum_{i=1}^{n} |\\tilde{\\mathbf{x}}_i - \\mathbf{W}\\mathbf{W}^{\\top} \\tilde{\\mathbf{x}}_i|^2 $$\n利用矩阵迹的性质 $\\sum_{i=1}^{n} |\\mathbf{a}i|^2 = \\text{tr}\\left(\\sum{i=1}^{n} \\mathbf{a}_i \\mathbf{a}_i^{\\top}\\right)$：\n$$ \\begin{aligned} \\text{Error} \u0026= \\sum_{i=1}^{n} \\text{tr}\\left((\\tilde{\\mathbf{x}}_i - \\mathbf{W}\\mathbf{W}^\\top \\tilde{\\mathbf{x}}_i)(\\tilde{\\mathbf{x}}_i - \\mathbf{W}\\mathbf{W}^\\top \\tilde{\\mathbf{x}}i)^\\top\\right) \\ \u0026= \\text{tr}\\left(\\sum{i=1}^{n} \\tilde{\\mathbf{x}}_i \\tilde{\\mathbf{x}}i^\\top\\right) - 2\\text{tr}\\left(\\mathbf{W}^\\top \\sum{i=1}^{n} \\tilde{\\mathbf{x}}_i \\tilde{\\mathbf{x}}i^\\top \\mathbf{W}\\right) + \\text{tr}\\left(\\mathbf{W}^\\top \\sum{i=1}^{n} \\tilde{\\mathbf{x}}_i \\tilde{\\mathbf{x}}_i^\\top \\mathbf{W}\\right) \\ \u0026= \\text{tr}(n\\mathbf{\\Sigma}) - \\text{tr}(\\mathbf{W}^\\top n \\mathbf{\\Sigma} \\mathbf{W}) \\ \u0026= n \\left[\\text{tr}(\\mathbf{\\Sigma}) - \\text{tr}(\\mathbf{W}^\\top \\mathbf{\\Sigma} \\mathbf{W})\\right] \\end{aligned} $$\n这里我们利用了 $\\mathbf{W}^{\\top} \\mathbf{W} = \\mathbf{I}$（正交矩阵）。\n两种视角的等价性 最小化重构误差等价于最大化 $\\text{tr}(\\mathbf{W}^{\\top} \\mathbf{\\Sigma} \\mathbf{W})$。\n展开：\n$$ \\text{tr}(\\mathbf{W}^{\\top} \\mathbf{\\Sigma} \\mathbf{W}) = \\sum_{j=1}^{k} \\mathbf{w}_j^{\\top} \\mathbf{\\Sigma} \\mathbf{w}_j $$\n这正是前 $k$ 个主成分的投影方差之和！\n因此，“最小化重构误差\"等价于\"最大化投影方差”，两种视角殊途同归。\n为什么这是等价的？ 从物理直觉上理解：\n最大化方差视角：我们想找到一个方向，使得数据在这个方向上\"展开\"得最厉害，这样才能尽可能多地保留原始信息。 最小化重构误差视角：我们想找到一个低维子空间，使得将数据投影到这个子空间后，重构的误差最小。 这两个目标是互补的：如果投影方差大，说明数据在这个方向上的变化范围广，自然重构误差就小（因为大部分信息都保留在这个方向上了）。\n数学上，这体现了投影定理：在欧氏空间中，一个向量到子空间的最近点，就是它在该子空间上的正交投影。最小化重构误差找到的子空间，正是使得数据在这个子空间上的投影方差最大的子空间。\nPCA 算法的完整步骤 现在，我们已经从两个等价的视角完整推导了 PCA 的理论基础。让我们总结 PCA 的算法步骤。\n步骤 1：数据标准化（可选但推荐） 对于不同的特征，它们的单位和量纲可能不同。例如，身高的单位是厘米，体重的单位是千克，如果直接计算协方差，方差大的特征会主导主成分。\n有两种常见的标准化方法：\n中心化（必须有）：\n$$ \\tilde{\\mathbf{x}}_i = \\mathbf{x}_i - \\bar{\\mathbf{x}} $$\n归一化（推荐）：\n$$ \\mathbf{x}_i’ = \\frac{\\mathbf{x}_i - \\bar{\\mathbf{x}}}{\\sqrt{\\text{Var}(\\mathbf{x}_i)}} $$\n其中 $\\text{Var}(\\mathbf{x}_i)$ 是第 $i$ 个特征的方差。\n步骤 2：计算协方差矩阵 对于中心化数据 $\\tilde{\\mathbf{X}} = [\\tilde{\\mathbf{x}}_1, \\tilde{\\mathbf{x}}_2, \\ldots, \\tilde{\\mathbf{x}}_n]^{\\top} \\in \\mathbb{R}^{n \\times d}$：\n$$ \\mathbf{\\Sigma} = \\frac{1}{n}\\tilde{\\mathbf{X}}^{\\top} \\tilde{\\mathbf{X}} \\in \\mathbb{R}^{d \\times d} $$\n协方差矩阵的元素 $\\Sigma_{ij}$ 表示第 $i$ 个特征和第 $j$ 个特征的协方差：\n$$ \\Sigma_{ij} = \\frac{1}{n}\\sum_{k=1}^{n} \\tilde{x}{ki} \\tilde{x}{kj} $$\n对角线元素 $\\Sigma_{ii}$ 是第 $i$ 个特征的方差。\n步骤 3：特征值分解 求解协方差矩阵的特征值分解：\n$$ \\mathbf{\\Sigma} \\mathbf{w}_k = \\lambda_k \\mathbf{w}_k, \\quad k = 1, 2, \\ldots, d $$\n将特征值按从大到小排序：\n$$ \\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_d \\geq 0 $$\n对应的特征向量 $\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_d$ 就是主成分方向。\n步骤 4：选择前 $k$ 个主成分 选择前 $k$ 个主成分，通常有以下几种方法：\n方法一：方差贡献率\n前 $k$ 个主成分解释的方差占总方差的比例：\n$$ \\text{贡献率} = \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{d} \\lambda_i} $$\n选择最小的 $k$，使得贡献率达到某个阈值（如 $90%$ 或 $95%$）。\n方法二：肘部法则\n绘制特征值随主成分数量的变化图，找到曲线\"肘部\"的位置。\n方法三：交叉验证\n在实际任务中，用不同 $k$ 值训练模型，选择性能最佳的 $k$。\n步骤 5：降维投影 构造投影矩阵 $\\mathbf{W} = [\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_k] \\in \\mathbb{R}^{d \\times k}$。\n将原始数据投影到 $k$ 维：\n$$ \\mathbf{Z} = \\tilde{\\mathbf{X}} \\mathbf{W} \\in \\mathbb{R}^{n \\times k} $$\n每行 $\\mathbf{z}_i$ 是 $\\mathbf{x}_i$ 的 $k$ 维表示。\n步骤 6：数据重构（可选） 如果需要从低维表示重构回原始空间：\n$$ \\hat{\\mathbf{X}} = \\mathbf{Z} \\mathbf{W}^{\\top} = \\tilde{\\mathbf{X}} \\mathbf{W} \\mathbf{W}^{\\top} $$\n加上均值：\n$$ \\hat{\\mathbf{X}}’ = \\hat{\\mathbf{X}} + \\bar{\\mathbf{X}} $$\n几何直观：椭圆与投影 PCA 有一个优美的几何解释：数据的主成分方向，就是拟合数据分布的椭圆的长轴和短轴方向。\n椭圆方程 考虑二维数据，协方差矩阵 $\\mathbf{\\Sigma} = \\begin{pmatrix} \\sigma_x^2 \u0026 \\sigma_{xy} \\ \\sigma_{xy} \u0026 \\sigma_y^2 \\end{pmatrix}$。\n数据分布的等密度椭圆（假设数据服从高斯分布）是：\n$$ \\mathbf{x}^{\\top} \\mathbf{\\Sigma}^{-1} \\mathbf{x} = c $$\n其中 $c$ 是常数。\n椭圆的长轴方向是 $\\mathbf{\\Sigma}$ 的最大特征值对应的特征向量，短轴方向是最小特征值对应的特征向量。\n3D 椭球面的例子 下图展示了三维数据的主成分方向：\n图 2：三维数据的 PCA：椭圆的主轴就是主成分方向\n在这个图中，红色箭头表示第一个主成分（数据\"伸展\"最厉害的方向），绿色箭头表示第二个主成分（垂直于第一个主成分，且方差次大），蓝色箭头表示第三个主成分（方差最小）。\n投影的几何意义 当我们把数据投影到前两个主成分张成的平面上时，实际上是做了一个正交投影。这个投影可以理解为：从第三个主成分的方向\"看\"数据，看到的\"影子\"就是降维后的数据。\n投影后的数据保留了前两个主成分的信息，但丢失了第三个主成分方向的变异。不过，由于第三个主成分的方差最小，丢失的信息通常不多。\nPCA 的实际应用 PCA 在实际中有广泛的应用，让我们看几个具体的例子。\n应用一：数据可视化 高维数据难以直接观察。PCA 可以将高维数据投影到二维或三维，使我们能够直观地看到数据的分布。\n例子：鸢尾花数据集\n鸢尾花数据集有四个特征：花萼长度、花萼宽度、花瓣长度、花瓣宽度。我们可以用 PCA 将其降维到二维：\nfrom sklearn.datasets import load_iris from sklearn.decomposition import PCA import numpy as np # 加载数据 iris = load_iris() X = iris.data # PCA 降维到 2 维 pca = PCA(n_components=2) X_pca = pca.fit_transform(X) # 查看解释方差 print(f\"前两个主成分解释的方差比例: {sum(pca.explained_variance_ratio_):.2%}\") 输出结果可能是：\n前两个主成分解释的方差比例: 95.81% 这意味着用两个主成分就能保留原始数据 $95.81%$ 的信息！\n可视化后，我们会发现不同品种的鸢尾花在二维平面上的分布有明显差异，这有助于分类。\n应用二：图像压缩 数字图像可以看作高维数据。一张 $m \\times n$ 的灰度图像是一个 $m \\times n$ 维向量。PCA 可以用于图像压缩。\n例子：人脸图像压缩\n假设我们有 $100$ 张 $100 \\times 100$ 的人脸图像，每张图像看作 $10000$ 维向量。PCA 找到的主成分（在人脸识别中称为\"特征脸\"）反映了人脸的主要变化模式。\n前几个主成分可能对应：整体亮度、水平对比度、面部对称性等。\n压缩过程：\n计算所有人脸图像的 PCA 选择前 $k$ 个主成分（如 $k = 50$） 每张人脸图像用 $50$ 个系数表示，而不是 $10000$ 个像素值 压缩比为 $10000 : 50 = 200 : 1$ 重构时，用这 $50$ 个系数和对应的特征脸重建图像。\n应用三：噪声过滤 如果数据中包含噪声，且噪声在所有方向上的方差大致相等，而信号只存在于少数主成分方向上，那么 PCA 可以用于噪声过滤。\n方法：\n对含噪数据进行 PCA 只保留前 $k$ 个主成分（假设信号主要在前 $k$ 个主成分中） 用前 $k$ 个主成分重构数据 重构后的数据会去除噪声 这个方法等价于低通滤波：保留低频（变化缓慢、方差大）成分，去除高频（快速变化、方差小但包含噪声）成分。\n应用四：特征提取与降维 在机器学习中，高维特征会导致计算复杂度高、过拟合等问题。PCA 可以用于降维，提取最重要的特征。\n例子：手写数字识别\nMNIST 数据集中的每张数字图像是 $28 \\times 28 = 784$ 维。如果直接用原始像素作为特征，维度太高。\n使用 PCA 降维：\n保留 $95%$ 的方差可能只需要 $50$-$100$ 个主成分 这样可以将 $784$ 维降到 $50$-$100$ 维，大大降低计算复杂度 同时保留了数字的主要形状信息 应用五：金融风险建模 在金融领域，股票收益率之间存在相关性。PCA 可以用于：\n识别市场因子：第一个主成分通常对应\"市场整体走势\"，后续主成分可能对应特定行业或因子的风险。 降维建模：将数百只股票的收益率降维到少数几个因子，用因子模型建模。 风险分散：通过分析主成分的分布，构建多元化的投资组合。 PCA 的优缺点与改进 PCA 的优点 简单高效：算法只涉及矩阵运算，计算复杂度主要是特征值分解。 可解释性：主成分是原始特征的线性组合，可以通过载荷分析理解。 无监督：不需要标签数据，适用于各种场景。 理论基础完备：从几何、统计、优化等多个角度都有清晰的解释。 PCA 的缺点 线性变换：PCA 只能捕获线性关系。如果数据中有非线性结构，PCA 可能失效。 方差不等于信息：在某些情况下，方差小的方向可能包含重要信息（如类别信息）。 对尺度敏感：不同特征的量纲会影响主成分。 可解释性有限：虽然主成分是线性组合，但高维情况下理解主成分的物理意义仍然困难。 改进方法 针对 PCA 的局限性，有许多改进方法：\n核 PCA（Kernel PCA）： 将数据映射到高维空间（使用核函数），在高维空间中做 PCA。这样可以捕获非线性关系。\nt-SNE： 用于数据可视化，特别擅长保持数据的局部结构。\nUMAP： 类似 t-SNE，但计算更快，且能更好地保持全局结构。\n独立成分分析（ICA）： 假设成分是统计独立的（而 PCA 只要求不相关），在某些任务中效果更好。\n总结：从数据中提取智慧 PCA 是一个美丽而强大的算法。它用简单的线性代数工具，解决了高维数据分析中的一个核心问题：如何找到数据的\"主轴\"。\n我们从两个等价的视角理解了 PCA：\n最大化投影方差：找到数据\"伸展\"最厉害的方向，使得降维后保留最多的信息。 最小化重构误差：找到最优的低维子空间，使得降维再重构的误差最小。 这两种视角殊途同归，都指向了同一个数学核心：协方差矩阵的特征值分解。\nPCA 的优雅在于它的通用性。从天文学中的恒星分布，到生物学中的基因表达，从金融学中的股票收益，到计算机视觉中的人脸识别，PCA 都有广泛应用。它的核心思想——从复杂的高维数据中提取主要的变化模式——是数据分析的一个永恒主题。\n但 PCA 也有局限性：它是线性的，对尺度敏感，且不能保证方差大的方向包含所有重要信息。因此，在实际应用中，我们需要根据具体问题选择合适的方法，或者将 PCA 与其他技术结合使用。\n从 Pearson 的几何直觉，到 Hotelling 的统计完善，PCA 已经发展了一个多世纪。但它的核心思想依然闪耀着智慧的光芒：在复杂的混沌中寻找简洁的秩序，在冗余的信息中提取本质的模式。这不仅是 PCA 的哲学，也是所有数据科学方法的终极目标。\n当我们面对海量数据时，PCA 提醒我们：不要被复杂性所淹没，寻找隐藏在数据背后的主轴，那些主轴将引导我们走向理解和洞察的彼岸。\n","wordCount":"1140","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/pca-visualization.jpg","datePublished":"2026-01-24T12:00:00+08:00","dateModified":"2026-01-24T12:00:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-24-pca-comprehensive-guide/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">PCA 主成分分析：从数据降维的优雅艺术</h1><div class=post-description>深入探讨机器学习中的核心降维算法 PCA，从直观理解到数学推导，从两种等价的视角（最大化方差、最小化重构误差）揭示其本质，包含完整的证明和实际应用。</div><div class=post-meta><span title='2026-01-24 12:00:00 +0800 CST'>January 24, 2026</span>&nbsp;·&nbsp;<span>6 min</span>&nbsp;·&nbsp;<span>1140 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/pca-visualization.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/pca-visualization.jpg alt="PCA 可视化"></a><figcaption>降维的艺术</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e4%bb%8e%e6%b7%b7%e6%b2%8c%e4%b8%ad%e5%af%bb%e6%89%be%e7%a7%a9%e5%ba%8f aria-label=引言：从混沌中寻找秩序>引言：从混沌中寻找秩序</a></li><li><a href=#pca-%e7%9a%84%e7%9b%b4%e8%a7%82%e7%90%86%e8%a7%a3%e6%8a%95%e5%bd%b1%e7%9a%84%e6%99%ba%e6%85%a7 aria-label="PCA 的直观理解：投影的智慧">PCA 的直观理解：投影的智慧</a><ul><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e9%99%8d%e7%bb%b4 aria-label=为什么需要降维？>为什么需要降维？</a></li><li><a href=#%e6%96%b9%e5%b7%ae%e4%bd%9c%e4%b8%ba%e4%bf%a1%e6%81%af%e5%ba%a6%e9%87%8f aria-label=方差作为信息度量>方差作为信息度量</a></li></ul></li><li><a href=#%e5%8e%86%e5%8f%b2%e8%83%8c%e6%99%af%e4%bb%8e-pearson-%e5%88%b0-hotelling aria-label="历史背景：从 Pearson 到 Hotelling">历史背景：从 Pearson 到 Hotelling</a><ul><li><a href=#karl-pearson%e5%87%a0%e4%bd%95%e8%a7%a3%e9%87%8a%e7%9a%84%e5%bc%80%e5%88%9b%e8%80%85 aria-label="Karl Pearson：几何解释的开创者">Karl Pearson：几何解释的开创者</a></li><li><a href=#harold-hotelling%e7%bb%9f%e8%ae%a1%e8%a7%86%e8%a7%92%e7%9a%84%e5%ae%8c%e5%96%84 aria-label="Harold Hotelling：统计视角的完善">Harold Hotelling：统计视角的完善</a></li></ul></li><li><a href=#pca-%e7%9a%84%e6%95%b0%e5%ad%a6%e6%8e%a8%e5%af%bc%e8%a7%86%e8%a7%92%e4%b8%80%e6%9c%80%e5%a4%a7%e5%8c%96%e6%96%b9%e5%b7%ae aria-label="PCA 的数学推导（视角一）：最大化方差">PCA 的数学推导（视角一）：最大化方差</a><ul><li><a href=#%e4%bc%98%e5%8c%96%e9%97%ae%e9%a2%98%e7%9a%84%e5%bb%ba%e7%ab%8b aria-label=优化问题的建立>优化问题的建立</a></li><li><a href=#%e4%bd%bf%e7%94%a8%e6%8b%89%e6%a0%bc%e6%9c%97%e6%97%a5%e4%b9%98%e6%95%b0%e6%b3%95 aria-label=使用拉格朗日乘数法>使用拉格朗日乘数法</a></li><li><a href=#%e7%90%86%e8%a7%a3%e7%89%b9%e5%be%81%e5%80%bc%e7%9a%84%e5%90%ab%e4%b9%89 aria-label=理解特征值的含义>理解特征值的含义</a></li><li><a href=#%e7%ac%ac%e4%ba%8c%e4%b8%aa%e4%b8%bb%e6%88%90%e5%88%86%e5%8f%8a%e5%90%8e%e7%bb%ad%e4%b8%bb%e6%88%90%e5%88%86 aria-label=第二个主成分及后续主成分>第二个主成分及后续主成分</a></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a></li></ul></li><li><a href=#pca-%e7%9a%84%e6%95%b0%e5%ad%a6%e6%8e%a8%e5%af%bc%e8%a7%86%e8%a7%92%e4%ba%8c%e6%9c%80%e5%b0%8f%e5%8c%96%e9%87%8d%e6%9e%84%e8%af%af%e5%b7%ae aria-label="PCA 的数学推导（视角二）：最小化重构误差">PCA 的数学推导（视角二）：最小化重构误差</a><ul><li><a href=#%e9%87%8d%e6%9e%84%e9%97%ae%e9%a2%98%e7%9a%84%e5%bb%ba%e7%ab%8b aria-label=重构问题的建立>重构问题的建立</a></li><li><a href=#%e5%88%a9%e7%94%a8%e6%ad%a3%e4%ba%a4%e6%8a%95%e5%bd%b1%e7%9a%84%e6%80%a7%e8%b4%a8 aria-label=利用正交投影的性质>利用正交投影的性质</a></li><li><a href=#%e4%b8%a4%e7%a7%8d%e8%a7%86%e8%a7%92%e7%9a%84%e7%ad%89%e4%bb%b7%e6%80%a7 aria-label=两种视角的等价性>两种视角的等价性</a></li><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e8%bf%99%e6%98%af%e7%ad%89%e4%bb%b7%e7%9a%84 aria-label=为什么这是等价的？>为什么这是等价的？</a></li></ul></li><li><a href=#pca-%e7%ae%97%e6%b3%95%e7%9a%84%e5%ae%8c%e6%95%b4%e6%ad%a5%e9%aa%a4 aria-label="PCA 算法的完整步骤">PCA 算法的完整步骤</a><ul><li><a href=#%e6%ad%a5%e9%aa%a4-1%e6%95%b0%e6%8d%ae%e6%a0%87%e5%87%86%e5%8c%96%e5%8f%af%e9%80%89%e4%bd%86%e6%8e%a8%e8%8d%90 aria-label="步骤 1：数据标准化（可选但推荐）">步骤 1：数据标准化（可选但推荐）</a></li><li><a href=#%e6%ad%a5%e9%aa%a4-2%e8%ae%a1%e7%ae%97%e5%8d%8f%e6%96%b9%e5%b7%ae%e7%9f%a9%e9%98%b5 aria-label="步骤 2：计算协方差矩阵">步骤 2：计算协方差矩阵</a></li><li><a href=#%e6%ad%a5%e9%aa%a4-3%e7%89%b9%e5%be%81%e5%80%bc%e5%88%86%e8%a7%a3 aria-label="步骤 3：特征值分解">步骤 3：特征值分解</a></li><li><a href=#%e6%ad%a5%e9%aa%a4-4%e9%80%89%e6%8b%a9%e5%89%8d-k-%e4%b8%aa%e4%b8%bb%e6%88%90%e5%88%86 aria-label="步骤 4：选择前 $k$ 个主成分">步骤 4：选择前 $k$ 个主成分</a></li><li><a href=#%e6%ad%a5%e9%aa%a4-5%e9%99%8d%e7%bb%b4%e6%8a%95%e5%bd%b1 aria-label="步骤 5：降维投影">步骤 5：降维投影</a></li><li><a href=#%e6%ad%a5%e9%aa%a4-6%e6%95%b0%e6%8d%ae%e9%87%8d%e6%9e%84%e5%8f%af%e9%80%89 aria-label="步骤 6：数据重构（可选）">步骤 6：数据重构（可选）</a></li></ul></li><li><a href=#%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%82%e6%a4%ad%e5%9c%86%e4%b8%8e%e6%8a%95%e5%bd%b1 aria-label=几何直观：椭圆与投影>几何直观：椭圆与投影</a><ul><li><a href=#%e6%a4%ad%e5%9c%86%e6%96%b9%e7%a8%8b aria-label=椭圆方程>椭圆方程</a></li><li><a href=#3d-%e6%a4%ad%e7%90%83%e9%9d%a2%e7%9a%84%e4%be%8b%e5%ad%90 aria-label="3D 椭球面的例子">3D 椭球面的例子</a></li><li><a href=#%e6%8a%95%e5%bd%b1%e7%9a%84%e5%87%a0%e4%bd%95%e6%84%8f%e4%b9%89 aria-label=投影的几何意义>投影的几何意义</a></li></ul></li><li><a href=#pca-%e7%9a%84%e5%ae%9e%e9%99%85%e5%ba%94%e7%94%a8 aria-label="PCA 的实际应用">PCA 的实际应用</a><ul><li><a href=#%e5%ba%94%e7%94%a8%e4%b8%80%e6%95%b0%e6%8d%ae%e5%8f%af%e8%a7%86%e5%8c%96 aria-label=应用一：数据可视化>应用一：数据可视化</a></li><li><a href=#%e5%ba%94%e7%94%a8%e4%ba%8c%e5%9b%be%e5%83%8f%e5%8e%8b%e7%bc%a9 aria-label=应用二：图像压缩>应用二：图像压缩</a></li><li><a href=#%e5%ba%94%e7%94%a8%e4%b8%89%e5%99%aa%e5%a3%b0%e8%bf%87%e6%bb%a4 aria-label=应用三：噪声过滤>应用三：噪声过滤</a></li><li><a href=#%e5%ba%94%e7%94%a8%e5%9b%9b%e7%89%b9%e5%be%81%e6%8f%90%e5%8f%96%e4%b8%8e%e9%99%8d%e7%bb%b4 aria-label=应用四：特征提取与降维>应用四：特征提取与降维</a></li><li><a href=#%e5%ba%94%e7%94%a8%e4%ba%94%e9%87%91%e8%9e%8d%e9%a3%8e%e9%99%a9%e5%bb%ba%e6%a8%a1 aria-label=应用五：金融风险建模>应用五：金融风险建模</a></li></ul></li><li><a href=#pca-%e7%9a%84%e4%bc%98%e7%bc%ba%e7%82%b9%e4%b8%8e%e6%94%b9%e8%bf%9b aria-label="PCA 的优缺点与改进">PCA 的优缺点与改进</a><ul><li><a href=#pca-%e7%9a%84%e4%bc%98%e7%82%b9 aria-label="PCA 的优点">PCA 的优点</a></li><li><a href=#pca-%e7%9a%84%e7%bc%ba%e7%82%b9 aria-label="PCA 的缺点">PCA 的缺点</a></li><li><a href=#%e6%94%b9%e8%bf%9b%e6%96%b9%e6%b3%95 aria-label=改进方法>改进方法</a></li></ul></li><li><a href=#%e6%80%bb%e7%bb%93%e4%bb%8e%e6%95%b0%e6%8d%ae%e4%b8%ad%e6%8f%90%e5%8f%96%e6%99%ba%e6%85%a7 aria-label=总结：从数据中提取智慧>总结：从数据中提取智慧</a></li></ul></div></details></div><div class=post-content><h2 id=引言从混沌中寻找秩序>引言：从混沌中寻找秩序<a hidden class=anchor aria-hidden=true href=#引言从混沌中寻找秩序>#</a></h2><p>想象你是一个天文学家，正在观测一群恒星的位置。这些恒星在三维空间中分布，你记录了每颗恒星到地球的距离、赤经和赤纬——这就是一个典型的三维数据集。但是，你想理解这些恒星的分布规律，三维空间太复杂了。你突然意识到：这些恒星实际上分布在一个接近平面的薄层上！如果能找到这个平面，你就可以用二维坐标来描述每颗恒星的位置，大大简化问题。</p><p>这个看似简单的思想——在高维数据中找到最能代表数据的低维子空间——就是主成分分析（Principal Component Analysis, PCA）的核心。</p><p>在机器学习、数据科学和统计学中，我们经常面临"维度灾难"：数据维度越高，计算越复杂，噪声越多，模型越容易过拟合。PCA 提供了一种优雅的解决方案：它不丢弃任何原始特征的信息，而是将数据投影到新的坐标系中，在这个新坐标系中，前几个坐标轴（主成分）包含了数据的大部分信息。</p><p>本文将带你深入 PCA 的世界。我们从直观的几何理解开始，穿越历史的长河，探索两种等价的数学推导视角，最终抵达实际应用的海岸。准备好了吗？让我们开始这场降维之旅。</p><h2 id=pca-的直观理解投影的智慧>PCA 的直观理解：投影的智慧<a hidden class=anchor aria-hidden=true href=#pca-的直观理解投影的智慧>#</a></h2><h3 id=为什么需要降维>为什么需要降维？<a hidden class=anchor aria-hidden=true href=#为什么需要降维>#</a></h3><p>在深入数学之前，让我们先理解为什么降维如此重要。</p><p>假设你有一个包含 $1000$ 个人的数据集，每个人有 $100$ 个特征（身高、体重、血压、血糖、血细胞计数等）。这些特征之间往往存在相关性：身高和体重相关，血压和血糖相关。如果我们直接用 $100$ 个特征来分析，会遇到以下问题：</p><ol><li><strong>计算复杂度</strong>：随着维度增加，算法的运行时间呈指数级增长。</li><li><strong>过拟合风险</strong>：特征越多，模型越容易记住训练数据，泛化能力下降。</li><li><strong>存储压力</strong>：$1000$ 个人 $\times$ $100$ 个特征 $= 100,000$ 个数据点，存储和传输成本高。</li><li><strong>可视化困难</strong>：我们只能在三维空间中直接观察数据，超过三维就无法直观理解。</li></ol><p>PCA 的目标是找到一个低维表示，保留数据的大部分信息。关键问题是：如何衡量"信息保留"？答案是<strong>方差</strong>。</p><h3 id=方差作为信息度量>方差作为信息度量<a hidden class=anchor aria-hidden=true href=#方差作为信息度量>#</a></h3><p>在一个数据集中，方差大的方向包含更多的信息。考虑一个简单的例子：假设我们有一个二维数据集，点的分布如图所示。</p><p><img alt="PCA 投影示意图" loading=lazy src=/images/math/pca-projection-intuition.png></p><p><em>图 1：PCA 的核心思想：将数据投影到方差最大的方向</em></p><p>如果我们把这些点投影到不同的直线上，哪种投影方式能最好地保留原始数据的信息？</p><p>直觉告诉我们：应该投影到数据"伸展"最厉害的方向上。在这个方向上，投影点的分布范围最广，方差最大，这意味着投影后保留了更多的原始信息。</p><p>让我们用数学语言来表述这个直觉。设 $n$ 个 $d$ 维数据点 $\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n \in \mathbb{R}^d$，我们想找到一个单位向量 $\mathbf{w} \in \mathbb{R}^d$（$|\mathbf{w}| = 1$），使得数据投影到 $\mathbf{w}$ 上的方差最大。</p><p>数据点 $\mathbf{x}_i$ 投影到 $\mathbf{w}$ 上的值是：</p><p>$$
z_i = \mathbf{w}^{\top} \mathbf{x}_i
$$</p><p>投影值的均值是：</p><p>$$
\bar{z} = \frac{1}{n}\sum_{i=1}^{n} z_i = \mathbf{w}^{\top} \bar{\mathbf{x}}
$$</p><p>其中 $\bar{\mathbf{x}} = \frac{1}{n}\sum_{i=1}^{n} \mathbf{x}_i$ 是数据的均值。</p><p>投影的方差是：</p><p>$$
\text{Var}(z) = \frac{1}{n}\sum_{i=1}^{n} (z_i - \bar{z})^2 = \frac{1}{n}\sum_{i=1}^{n} (\mathbf{w}^{\top} (\mathbf{x}_i - \bar{\mathbf{x}}))^2
$$</p><p>让我们定义中心化的数据 $\tilde{\mathbf{x}}_i = \mathbf{x}_i - \bar{\mathbf{x}}$，则：</p><p>$$
\text{Var}(z) = \frac{1}{n}\sum_{i=1}^{n} (\mathbf{w}^{\top} \tilde{\mathbf{x}}<em>i)^2 = \frac{1}{n}\sum</em>{i=1}^{n} \mathbf{w}^{\top} \tilde{\mathbf{x}}_i \tilde{\mathbf{x}}<em>i^{\top} \mathbf{w} = \mathbf{w}^{\top} \left(\frac{1}{n}\sum</em>{i=1}^{n} \tilde{\mathbf{x}}_i \tilde{\mathbf{x}}_i^{\top}\right) \mathbf{w}
$$</p><p>注意到 $\frac{1}{n}\sum_{i=1}^{n} \tilde{\mathbf{x}}_i \tilde{\mathbf{x}}_i^{\top}$ 正是数据的<strong>协方差矩阵</strong>：</p><p>$$
\mathbf{\Sigma} = \frac{1}{n}\sum_{i=1}^{n} \tilde{\mathbf{x}}_i \tilde{\mathbf{x}}_i^{\top}
$$</p><p>因此，PCA 的优化问题是：</p><p>$$
\max_{\mathbf{w}} \mathbf{w}^{\top} \mathbf{\Sigma} \mathbf{w}, \quad \text{约束：} \mathbf{w}^{\top} \mathbf{w} = 1
$$</p><p>这就是 PCA 的第一个视角：<strong>寻找使投影方差最大的方向</strong>。</p><h2 id=历史背景从-pearson-到-hotelling>历史背景：从 Pearson 到 Hotelling<a hidden class=anchor aria-hidden=true href=#历史背景从-pearson-到-hotelling>#</a></h2><p>PCA 的历史可以追溯到 20 世纪初的统计学领域，两位数学家的贡献奠定了这个算法的基础。</p><h3 id=karl-pearson几何解释的开创者>Karl Pearson：几何解释的开创者<a hidden class=anchor aria-hidden=true href=#karl-pearson几何解释的开创者>#</a></h3><p>1901 年，英国统计学家 Karl Pearson 发表了一篇题为"On lines and planes of closest fit to systems of points in space"（论空间点系的最优拟合线和面）的论文。Pearson 从几何角度思考：给定一组散布在空间中的点，如何找到一条直线或一个平面，使得这些点到这条线或平面的距离平方和最小？</p><p>这个看似纯粹的几何问题，实际上等价于我们刚才讨论的"最大化投影方差"问题！Pearson 证明了，最优的投影方向正是数据的"主轴"——数据"伸展"最厉害的方向。</p><p>Pearson 的贡献不仅在于问题的表述，更在于他发现这个问题的解可以通过<strong>特征值分解</strong>来获得。虽然当时线性代数的理论还不像今天这样成熟，但 Pearson 已经直觉地抓住了问题的本质。</p><h3 id=harold-hotelling统计视角的完善>Harold Hotelling：统计视角的完善<a hidden class=anchor aria-hidden=true href=#harold-hotelling统计视角的完善>#</a></h3><p>1933 年，美国数学家和统计学家 Harold Hotelling 独立地重新发现了 PCA，但从一个不同的角度。他将这个问题命名为"主成分分析"（Principal Component Analysis），并给出了完整的统计学解释。</p><p>Hotelling 的论文"Analysis of a complex of statistical variables into principal components"（将统计变量复合体分解为主成分）不仅解决了如何找到主成分的问题，还给出了一个清晰的解释：每个主成分都是原始变量的线性组合，这些主成分按照解释数据方差的大小排序，且彼此之间<strong>不相关</strong>。</p><p>Hotelling 的贡献使得 PCA 从一个纯粹的几何方法发展成为一个完整的统计工具，并广泛应用于实际数据分析中。</p><p>值得一提的是，Hotelling 的 $T^2$ 统计量——一种多变量检验方法——至今仍在质量控制中广泛应用。</p><h2 id=pca-的数学推导视角一最大化方差>PCA 的数学推导（视角一）：最大化方差<a hidden class=anchor aria-hidden=true href=#pca-的数学推导视角一最大化方差>#</a></h2><p>现在，让我们严谨地推导 PCA 的数学公式。我们将从"最大化投影方差"这个视角出发，逐步建立完整的理论框架。</p><h3 id=优化问题的建立>优化问题的建立<a hidden class=anchor aria-hidden=true href=#优化问题的建立>#</a></h3><p>我们有 $n$ 个 $d$ 维数据点 $\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n \in \mathbb{R}^d$。首先，对数据进行中心化：</p><p>$$
\tilde{\mathbf{x}}_i = \mathbf{x}<em>i - \bar{\mathbf{x}}, \quad \bar{\mathbf{x}} = \frac{1}{n}\sum</em>{i=1}^{n} \mathbf{x}_i
$$</p><p>计算协方差矩阵：</p><p>$$
\mathbf{\Sigma} = \frac{1}{n}\sum_{i=1}^{n} \tilde{\mathbf{x}}_i \tilde{\mathbf{x}}_i^{\top} \in \mathbb{R}^{d \times d}
$$</p><p>注意：$\mathbf{\Sigma}$ 是一个对称半正定矩阵。</p><p>我们要找到第一个主成分的方向 $\mathbf{w}_1$，使得投影方差最大：</p><p>$$
\mathbf{w}<em>1 = \arg\max</em>{\mathbf{w}} \mathbf{w}^{\top} \mathbf{\Sigma} \mathbf{w}, \quad \text{s.t. } \mathbf{w}^{\top} \mathbf{w} = 1
$$</p><h3 id=使用拉格朗日乘数法>使用拉格朗日乘数法<a hidden class=anchor aria-hidden=true href=#使用拉格朗日乘数法>#</a></h3><p>这是一个带约束的优化问题，我们可以使用拉格朗日乘数法求解。定义拉格朗日函数：</p><p>$$
\mathcal{L}(\mathbf{w}, \lambda) = \mathbf{w}^{\top} \mathbf{\Sigma} \mathbf{w} - \lambda (\mathbf{w}^{\top} \mathbf{w} - 1)
$$</p><p>其中 $\lambda$ 是拉格朗日乘数。</p><p>对 $\mathbf{w}$ 求梯度并设为零：</p><p>$$
\frac{\partial \mathcal{L}}{\partial \mathbf{w}} = 2\mathbf{\Sigma}\mathbf{w} - 2\lambda \mathbf{w} = 0
$$</p><p>简化得到：</p><p>$$
\mathbf{\Sigma} \mathbf{w} = \lambda \mathbf{w}
$$</p><p>这正是<strong>特征值问题</strong>！$\mathbf{w}$ 是 $\mathbf{\Sigma}$ 的特征向量，$\lambda$ 是对应的特征值。</p><h3 id=理解特征值的含义>理解特征值的含义<a hidden class=anchor aria-hidden=true href=#理解特征值的含义>#</a></h3><p>让我们计算投影方差的值。将特征方程 $\mathbf{\Sigma} \mathbf{w} = \lambda \mathbf{w}$ 代入方差表达式：</p><p>$$
\mathbf{w}^{\top} \mathbf{\Sigma} \mathbf{w} = \mathbf{w}^{\top} \lambda \mathbf{w} = \lambda \mathbf{w}^{\top} \mathbf{w} = \lambda
$$</p><p>因为 $\mathbf{w}$ 是单位向量，$\mathbf{w}^{\top} \mathbf{w} = 1$。</p><p>这说明：<strong>投影方差等于对应的特征值</strong>！</p><p>因此，要使投影方差最大，我们需要选择 $\mathbf{\Sigma}$ 的最大特征值对应的特征向量。</p><h3 id=第二个主成分及后续主成分>第二个主成分及后续主成分<a hidden class=anchor aria-hidden=true href=#第二个主成分及后续主成分>#</a></h3><p>找到第一个主成分 $\mathbf{w}_1$ 后，我们希望找到第二个主成分 $\mathbf{w}_2$，它应该满足：</p><ol><li>最大化投影方差</li><li>与 $\mathbf{w}_1$ 正交（不相关）</li></ol><p>数学表述为：</p><p>$$
\mathbf{w}<em>2 = \arg\max</em>{\mathbf{w}} \mathbf{w}^{\top} \mathbf{\Sigma} \mathbf{w}, \quad \text{s.t. } \mathbf{w}^{\top} \mathbf{w} = 1, \mathbf{w}^{\top} \mathbf{w}_1 = 0
$$</p><p>同样使用拉格朗日乘数法：</p><p>$$
\mathcal{L}(\mathbf{w}, \lambda, \mu) = \mathbf{w}^{\top} \mathbf{\Sigma} \mathbf{w} - \lambda (\mathbf{w}^{\top} \mathbf{w} - 1) - \mu \mathbf{w}^{\top} \mathbf{w}_1
$$</p><p>对 $\mathbf{w}$ 求梯度：</p><p>$$
2\mathbf{\Sigma}\mathbf{w} - 2\lambda \mathbf{w} - \mu \mathbf{w}_1 = 0
$$</p><p>左乘 $\mathbf{w}_1^\top$：</p><p>$$
2\mathbf{w}_1^{\top} \mathbf{\Sigma}\mathbf{w} - 2\lambda \mathbf{w}_1^{\top} \mathbf{w} - \mu \mathbf{w}_1^{\top} \mathbf{w}_1 = 0
$$</p><p>由于 $\mathbf{\Sigma}\mathbf{w}_1 = \lambda_1 \mathbf{w}_1$（其中 $\lambda_1$ 是最大特征值）：</p><p>$$
2\lambda_1 \mathbf{w}_1^{\top} \mathbf{w} - 2\lambda \mathbf{w}_1^{\top} \mathbf{w} - \mu = 0
$$</p><p>但 $\mathbf{w}_1^{\top} \mathbf{w} = 0$（正交约束），所以 $\mu = 0$。</p><p>因此，我们得到：</p><p>$$
\mathbf{\Sigma}\mathbf{w} = \lambda \mathbf{w}
$$</p><p>这说明 $\mathbf{w}_2$ 也必须是 $\mathbf{\Sigma}$ 的特征向量！但由于 $\mathbf{w}_2$ 必须与 $\mathbf{w}_1$ 正交，$\mathbf{w}_2$ 只能选择<strong>第二大特征值</strong>对应的特征向量。</p><p>同理，第 $k$ 个主成分 $\mathbf{w}_k$ 是 $\mathbf{\Sigma}$ 的第 $k$ 大特征值 $\lambda_k$ 对应的特征向量。</p><h3 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h3><p>从"最大化方差"的视角，我们得出：</p><ol><li><strong>协方差矩阵</strong> $\mathbf{\Sigma}$ 的特征向量是 PCA 的主成分方向</li><li><strong>特征值</strong> $\lambda_k$ 是投影到第 $k$ 个主成分的方差</li><li><strong>主成分排序</strong>：按特征值从大到小排列</li><li><strong>主成分正交</strong>：不同特征值对应的特征向量相互正交</li></ol><p>这个推导优雅而简洁，将 PCA 的几何直觉与线性代数的特征值理论完美地结合在一起。</p><h2 id=pca-的数学推导视角二最小化重构误差>PCA 的数学推导（视角二）：最小化重构误差<a hidden class=anchor aria-hidden=true href=#pca-的数学推导视角二最小化重构误差>#</a></h2><p>现在，我们从另一个角度理解 PCA：<strong>最小化重构误差</strong>。这个视角虽然与前一个等价，但提供了不同的物理直觉。</p><h3 id=重构问题的建立>重构问题的建立<a hidden class=anchor aria-hidden=true href=#重构问题的建立>#</a></h3><p>假设我们想将 $d$ 维数据降维到 $k$ 维（$k &lt; d$）。设我们找到了 $k$ 个正交单位向量 $\mathbf{w}_1, \mathbf{w}_2, \ldots, \mathbf{w}_k$，作为新坐标系的基。</p><p>对于数据点 $\mathbf{x}_i$，它的降维表示（编码）是：</p><p>$$
\mathbf{z}<em>i = (z</em>{i1}, z_{i2}, \ldots, z_{ik})^\top
$$</p><p>其中 $z_{ij} = \mathbf{w}_j^{\top} \tilde{\mathbf{x}}_i$ 是 $\tilde{\mathbf{x}}_i$ 在 $\mathbf{w}_j$ 上的投影。</p><p>从低维表示重构（解码）回原始空间：</p><p>$$
\hat{\mathbf{x}}<em>i = \bar{\mathbf{x}} + \sum</em>{j=1}^{k} z_{ij} \mathbf{w}_j
$$</p><p>这就是将中心化数据投影到前 $k$ 个主成分张成的子空间，然后再加回均值。</p><p>重构误差定义为原始数据与重构数据的欧氏距离平方：</p><p>$$
\text{Error} = \sum_{i=1}^{n} |\mathbf{x}<em>i - \hat{\mathbf{x}}<em>i|^2 = \sum</em>{i=1}^{n} |\tilde{\mathbf{x}}<em>i - \sum</em>{j=1}^{k} z</em>{ij} \mathbf{w}_j|^2
$$</p><p>我们的目标是最小化这个误差：</p><p>$$
\min_{\mathbf{w}_1, \ldots, \mathbf{w}<em>k} \sum</em>{i=1}^{n} \left|\tilde{\mathbf{x}}<em>i - \sum</em>{j=1}^{k} \mathbf{w}_j (\mathbf{w}_j^{\top} \tilde{\mathbf{x}}_i)\right|^2
$$</p><h3 id=利用正交投影的性质>利用正交投影的性质<a hidden class=anchor aria-hidden=true href=#利用正交投影的性质>#</a></h3><p>记 $\mathbf{W} = [\mathbf{w}_1, \mathbf{w}_2, \ldots, \mathbf{w}_k] \in \mathbb{R}^{d \times k}$，则重构误差可以写成：</p><p>$$
\text{Error} = \sum_{i=1}^{n} |\tilde{\mathbf{x}}_i - \mathbf{W}\mathbf{W}^{\top} \tilde{\mathbf{x}}_i|^2
$$</p><p>利用矩阵迹的性质 $\sum_{i=1}^{n} |\mathbf{a}<em>i|^2 = \text{tr}\left(\sum</em>{i=1}^{n} \mathbf{a}_i \mathbf{a}_i^{\top}\right)$：</p><p>$$
\begin{aligned}
\text{Error} &= \sum_{i=1}^{n} \text{tr}\left((\tilde{\mathbf{x}}_i - \mathbf{W}\mathbf{W}^\top \tilde{\mathbf{x}}_i)(\tilde{\mathbf{x}}_i - \mathbf{W}\mathbf{W}^\top \tilde{\mathbf{x}}<em>i)^\top\right) \
&= \text{tr}\left(\sum</em>{i=1}^{n} \tilde{\mathbf{x}}_i \tilde{\mathbf{x}}<em>i^\top\right) - 2\text{tr}\left(\mathbf{W}^\top \sum</em>{i=1}^{n} \tilde{\mathbf{x}}_i \tilde{\mathbf{x}}<em>i^\top \mathbf{W}\right) + \text{tr}\left(\mathbf{W}^\top \sum</em>{i=1}^{n} \tilde{\mathbf{x}}_i \tilde{\mathbf{x}}_i^\top \mathbf{W}\right) \
&= \text{tr}(n\mathbf{\Sigma}) - \text{tr}(\mathbf{W}^\top n \mathbf{\Sigma} \mathbf{W}) \
&= n \left[\text{tr}(\mathbf{\Sigma}) - \text{tr}(\mathbf{W}^\top \mathbf{\Sigma} \mathbf{W})\right]
\end{aligned}
$$</p><p>这里我们利用了 $\mathbf{W}^{\top} \mathbf{W} = \mathbf{I}$（正交矩阵）。</p><h3 id=两种视角的等价性>两种视角的等价性<a hidden class=anchor aria-hidden=true href=#两种视角的等价性>#</a></h3><p>最小化重构误差等价于最大化 $\text{tr}(\mathbf{W}^{\top} \mathbf{\Sigma} \mathbf{W})$。</p><p>展开：</p><p>$$
\text{tr}(\mathbf{W}^{\top} \mathbf{\Sigma} \mathbf{W}) = \sum_{j=1}^{k} \mathbf{w}_j^{\top} \mathbf{\Sigma} \mathbf{w}_j
$$</p><p>这正是前 $k$ 个主成分的投影方差之和！</p><p>因此，&ldquo;最小化重构误差"等价于"最大化投影方差&rdquo;，两种视角殊途同归。</p><h3 id=为什么这是等价的>为什么这是等价的？<a hidden class=anchor aria-hidden=true href=#为什么这是等价的>#</a></h3><p>从物理直觉上理解：</p><ul><li><strong>最大化方差视角</strong>：我们想找到一个方向，使得数据在这个方向上"展开"得最厉害，这样才能尽可能多地保留原始信息。</li><li><strong>最小化重构误差视角</strong>：我们想找到一个低维子空间，使得将数据投影到这个子空间后，重构的误差最小。</li></ul><p>这两个目标是互补的：如果投影方差大，说明数据在这个方向上的变化范围广，自然重构误差就小（因为大部分信息都保留在这个方向上了）。</p><p>数学上，这体现了投影定理：在欧氏空间中，一个向量到子空间的最近点，就是它在该子空间上的正交投影。最小化重构误差找到的子空间，正是使得数据在这个子空间上的投影方差最大的子空间。</p><h2 id=pca-算法的完整步骤>PCA 算法的完整步骤<a hidden class=anchor aria-hidden=true href=#pca-算法的完整步骤>#</a></h2><p>现在，我们已经从两个等价的视角完整推导了 PCA 的理论基础。让我们总结 PCA 的算法步骤。</p><h3 id=步骤-1数据标准化可选但推荐>步骤 1：数据标准化（可选但推荐）<a hidden class=anchor aria-hidden=true href=#步骤-1数据标准化可选但推荐>#</a></h3><p>对于不同的特征，它们的单位和量纲可能不同。例如，身高的单位是厘米，体重的单位是千克，如果直接计算协方差，方差大的特征会主导主成分。</p><p>有两种常见的标准化方法：</p><p><strong>中心化</strong>（必须有）：</p><p>$$
\tilde{\mathbf{x}}_i = \mathbf{x}_i - \bar{\mathbf{x}}
$$</p><p><strong>归一化</strong>（推荐）：</p><p>$$
\mathbf{x}_i&rsquo; = \frac{\mathbf{x}_i - \bar{\mathbf{x}}}{\sqrt{\text{Var}(\mathbf{x}_i)}}
$$</p><p>其中 $\text{Var}(\mathbf{x}_i)$ 是第 $i$ 个特征的方差。</p><h3 id=步骤-2计算协方差矩阵>步骤 2：计算协方差矩阵<a hidden class=anchor aria-hidden=true href=#步骤-2计算协方差矩阵>#</a></h3><p>对于中心化数据 $\tilde{\mathbf{X}} = [\tilde{\mathbf{x}}_1, \tilde{\mathbf{x}}_2, \ldots, \tilde{\mathbf{x}}_n]^{\top} \in \mathbb{R}^{n \times d}$：</p><p>$$
\mathbf{\Sigma} = \frac{1}{n}\tilde{\mathbf{X}}^{\top} \tilde{\mathbf{X}} \in \mathbb{R}^{d \times d}
$$</p><p>协方差矩阵的元素 $\Sigma_{ij}$ 表示第 $i$ 个特征和第 $j$ 个特征的协方差：</p><p>$$
\Sigma_{ij} = \frac{1}{n}\sum_{k=1}^{n} \tilde{x}<em>{ki} \tilde{x}</em>{kj}
$$</p><p>对角线元素 $\Sigma_{ii}$ 是第 $i$ 个特征的方差。</p><h3 id=步骤-3特征值分解>步骤 3：特征值分解<a hidden class=anchor aria-hidden=true href=#步骤-3特征值分解>#</a></h3><p>求解协方差矩阵的特征值分解：</p><p>$$
\mathbf{\Sigma} \mathbf{w}_k = \lambda_k \mathbf{w}_k, \quad k = 1, 2, \ldots, d
$$</p><p>将特征值按从大到小排序：</p><p>$$
\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_d \geq 0
$$</p><p>对应的特征向量 $\mathbf{w}_1, \mathbf{w}_2, \ldots, \mathbf{w}_d$ 就是主成分方向。</p><h3 id=步骤-4选择前-k-个主成分>步骤 4：选择前 $k$ 个主成分<a hidden class=anchor aria-hidden=true href=#步骤-4选择前-k-个主成分>#</a></h3><p>选择前 $k$ 个主成分，通常有以下几种方法：</p><p><strong>方法一：方差贡献率</strong></p><p>前 $k$ 个主成分解释的方差占总方差的比例：</p><p>$$
\text{贡献率} = \frac{\sum_{i=1}^{k} \lambda_i}{\sum_{i=1}^{d} \lambda_i}
$$</p><p>选择最小的 $k$，使得贡献率达到某个阈值（如 $90%$ 或 $95%$）。</p><p><strong>方法二：肘部法则</strong></p><p>绘制特征值随主成分数量的变化图，找到曲线"肘部"的位置。</p><p><strong>方法三：交叉验证</strong></p><p>在实际任务中，用不同 $k$ 值训练模型，选择性能最佳的 $k$。</p><h3 id=步骤-5降维投影>步骤 5：降维投影<a hidden class=anchor aria-hidden=true href=#步骤-5降维投影>#</a></h3><p>构造投影矩阵 $\mathbf{W} = [\mathbf{w}_1, \mathbf{w}_2, \ldots, \mathbf{w}_k] \in \mathbb{R}^{d \times k}$。</p><p>将原始数据投影到 $k$ 维：</p><p>$$
\mathbf{Z} = \tilde{\mathbf{X}} \mathbf{W} \in \mathbb{R}^{n \times k}
$$</p><p>每行 $\mathbf{z}_i$ 是 $\mathbf{x}_i$ 的 $k$ 维表示。</p><h3 id=步骤-6数据重构可选>步骤 6：数据重构（可选）<a hidden class=anchor aria-hidden=true href=#步骤-6数据重构可选>#</a></h3><p>如果需要从低维表示重构回原始空间：</p><p>$$
\hat{\mathbf{X}} = \mathbf{Z} \mathbf{W}^{\top} = \tilde{\mathbf{X}} \mathbf{W} \mathbf{W}^{\top}
$$</p><p>加上均值：</p><p>$$
\hat{\mathbf{X}}&rsquo; = \hat{\mathbf{X}} + \bar{\mathbf{X}}
$$</p><h2 id=几何直观椭圆与投影>几何直观：椭圆与投影<a hidden class=anchor aria-hidden=true href=#几何直观椭圆与投影>#</a></h2><p>PCA 有一个优美的几何解释：数据的主成分方向，就是拟合数据分布的椭圆的长轴和短轴方向。</p><h3 id=椭圆方程>椭圆方程<a hidden class=anchor aria-hidden=true href=#椭圆方程>#</a></h3><p>考虑二维数据，协方差矩阵 $\mathbf{\Sigma} = \begin{pmatrix} \sigma_x^2 & \sigma_{xy} \ \sigma_{xy} & \sigma_y^2 \end{pmatrix}$。</p><p>数据分布的等密度椭圆（假设数据服从高斯分布）是：</p><p>$$
\mathbf{x}^{\top} \mathbf{\Sigma}^{-1} \mathbf{x} = c
$$</p><p>其中 $c$ 是常数。</p><p>椭圆的长轴方向是 $\mathbf{\Sigma}$ 的最大特征值对应的特征向量，短轴方向是最小特征值对应的特征向量。</p><h3 id=3d-椭球面的例子>3D 椭球面的例子<a hidden class=anchor aria-hidden=true href=#3d-椭球面的例子>#</a></h3><p>下图展示了三维数据的主成分方向：</p><p><img alt="PCA 3D 可视化" loading=lazy src=/images/math/pca-3d-visualization.png></p><p><em>图 2：三维数据的 PCA：椭圆的主轴就是主成分方向</em></p><p>在这个图中，红色箭头表示第一个主成分（数据"伸展"最厉害的方向），绿色箭头表示第二个主成分（垂直于第一个主成分，且方差次大），蓝色箭头表示第三个主成分（方差最小）。</p><h3 id=投影的几何意义>投影的几何意义<a hidden class=anchor aria-hidden=true href=#投影的几何意义>#</a></h3><p>当我们把数据投影到前两个主成分张成的平面上时，实际上是做了一个正交投影。这个投影可以理解为：从第三个主成分的方向"看"数据，看到的"影子"就是降维后的数据。</p><p>投影后的数据保留了前两个主成分的信息，但丢失了第三个主成分方向的变异。不过，由于第三个主成分的方差最小，丢失的信息通常不多。</p><h2 id=pca-的实际应用>PCA 的实际应用<a hidden class=anchor aria-hidden=true href=#pca-的实际应用>#</a></h2><p>PCA 在实际中有广泛的应用，让我们看几个具体的例子。</p><h3 id=应用一数据可视化>应用一：数据可视化<a hidden class=anchor aria-hidden=true href=#应用一数据可视化>#</a></h3><p>高维数据难以直接观察。PCA 可以将高维数据投影到二维或三维，使我们能够直观地看到数据的分布。</p><p><strong>例子</strong>：鸢尾花数据集</p><p>鸢尾花数据集有四个特征：花萼长度、花萼宽度、花瓣长度、花瓣宽度。我们可以用 PCA 将其降维到二维：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>load_iris</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.decomposition</span> <span class=kn>import</span> <span class=n>PCA</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载数据</span>
</span></span><span class=line><span class=cl><span class=n>iris</span> <span class=o>=</span> <span class=n>load_iris</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>iris</span><span class=o>.</span><span class=n>data</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># PCA 降维到 2 维</span>
</span></span><span class=line><span class=cl><span class=n>pca</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>X_pca</span> <span class=o>=</span> <span class=n>pca</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 查看解释方差</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;前两个主成分解释的方差比例: </span><span class=si>{</span><span class=nb>sum</span><span class=p>(</span><span class=n>pca</span><span class=o>.</span><span class=n>explained_variance_ratio_</span><span class=p>)</span><span class=si>:</span><span class=s2>.2%</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>输出结果可能是：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>前两个主成分解释的方差比例: 95.81%
</span></span></code></pre></div><p>这意味着用两个主成分就能保留原始数据 $95.81%$ 的信息！</p><p>可视化后，我们会发现不同品种的鸢尾花在二维平面上的分布有明显差异，这有助于分类。</p><h3 id=应用二图像压缩>应用二：图像压缩<a hidden class=anchor aria-hidden=true href=#应用二图像压缩>#</a></h3><p>数字图像可以看作高维数据。一张 $m \times n$ 的灰度图像是一个 $m \times n$ 维向量。PCA 可以用于图像压缩。</p><p><strong>例子</strong>：人脸图像压缩</p><p>假设我们有 $100$ 张 $100 \times 100$ 的人脸图像，每张图像看作 $10000$ 维向量。PCA 找到的主成分（在人脸识别中称为"特征脸"）反映了人脸的主要变化模式。</p><p>前几个主成分可能对应：整体亮度、水平对比度、面部对称性等。</p><p>压缩过程：</p><ol><li>计算所有人脸图像的 PCA</li><li>选择前 $k$ 个主成分（如 $k = 50$）</li><li>每张人脸图像用 $50$ 个系数表示，而不是 $10000$ 个像素值</li><li>压缩比为 $10000 : 50 = 200 : 1$</li></ol><p>重构时，用这 $50$ 个系数和对应的特征脸重建图像。</p><h3 id=应用三噪声过滤>应用三：噪声过滤<a hidden class=anchor aria-hidden=true href=#应用三噪声过滤>#</a></h3><p>如果数据中包含噪声，且噪声在所有方向上的方差大致相等，而信号只存在于少数主成分方向上，那么 PCA 可以用于噪声过滤。</p><p><strong>方法</strong>：</p><ol><li>对含噪数据进行 PCA</li><li>只保留前 $k$ 个主成分（假设信号主要在前 $k$ 个主成分中）</li><li>用前 $k$ 个主成分重构数据</li><li>重构后的数据会去除噪声</li></ol><p>这个方法等价于低通滤波：保留低频（变化缓慢、方差大）成分，去除高频（快速变化、方差小但包含噪声）成分。</p><h3 id=应用四特征提取与降维>应用四：特征提取与降维<a hidden class=anchor aria-hidden=true href=#应用四特征提取与降维>#</a></h3><p>在机器学习中，高维特征会导致计算复杂度高、过拟合等问题。PCA 可以用于降维，提取最重要的特征。</p><p><strong>例子</strong>：手写数字识别</p><p>MNIST 数据集中的每张数字图像是 $28 \times 28 = 784$ 维。如果直接用原始像素作为特征，维度太高。</p><p>使用 PCA 降维：</p><ul><li>保留 $95%$ 的方差可能只需要 $50$-$100$ 个主成分</li><li>这样可以将 $784$ 维降到 $50$-$100$ 维，大大降低计算复杂度</li><li>同时保留了数字的主要形状信息</li></ul><h3 id=应用五金融风险建模>应用五：金融风险建模<a hidden class=anchor aria-hidden=true href=#应用五金融风险建模>#</a></h3><p>在金融领域，股票收益率之间存在相关性。PCA 可以用于：</p><ol><li><strong>识别市场因子</strong>：第一个主成分通常对应"市场整体走势"，后续主成分可能对应特定行业或因子的风险。</li><li><strong>降维建模</strong>：将数百只股票的收益率降维到少数几个因子，用因子模型建模。</li><li><strong>风险分散</strong>：通过分析主成分的分布，构建多元化的投资组合。</li></ol><h2 id=pca-的优缺点与改进>PCA 的优缺点与改进<a hidden class=anchor aria-hidden=true href=#pca-的优缺点与改进>#</a></h2><h3 id=pca-的优点>PCA 的优点<a hidden class=anchor aria-hidden=true href=#pca-的优点>#</a></h3><ol><li><strong>简单高效</strong>：算法只涉及矩阵运算，计算复杂度主要是特征值分解。</li><li><strong>可解释性</strong>：主成分是原始特征的线性组合，可以通过载荷分析理解。</li><li><strong>无监督</strong>：不需要标签数据，适用于各种场景。</li><li><strong>理论基础完备</strong>：从几何、统计、优化等多个角度都有清晰的解释。</li></ol><h3 id=pca-的缺点>PCA 的缺点<a hidden class=anchor aria-hidden=true href=#pca-的缺点>#</a></h3><ol><li><strong>线性变换</strong>：PCA 只能捕获线性关系。如果数据中有非线性结构，PCA 可能失效。</li><li><strong>方差不等于信息</strong>：在某些情况下，方差小的方向可能包含重要信息（如类别信息）。</li><li><strong>对尺度敏感</strong>：不同特征的量纲会影响主成分。</li><li><strong>可解释性有限</strong>：虽然主成分是线性组合，但高维情况下理解主成分的物理意义仍然困难。</li></ol><h3 id=改进方法>改进方法<a hidden class=anchor aria-hidden=true href=#改进方法>#</a></h3><p>针对 PCA 的局限性，有许多改进方法：</p><p><strong>核 PCA（Kernel PCA）</strong>：
将数据映射到高维空间（使用核函数），在高维空间中做 PCA。这样可以捕获非线性关系。</p><p><strong>t-SNE</strong>：
用于数据可视化，特别擅长保持数据的局部结构。</p><p><strong>UMAP</strong>：
类似 t-SNE，但计算更快，且能更好地保持全局结构。</p><p><strong>独立成分分析（ICA）</strong>：
假设成分是统计独立的（而 PCA 只要求不相关），在某些任务中效果更好。</p><h2 id=总结从数据中提取智慧>总结：从数据中提取智慧<a hidden class=anchor aria-hidden=true href=#总结从数据中提取智慧>#</a></h2><p>PCA 是一个美丽而强大的算法。它用简单的线性代数工具，解决了高维数据分析中的一个核心问题：如何找到数据的"主轴"。</p><p>我们从两个等价的视角理解了 PCA：</p><ol><li><strong>最大化投影方差</strong>：找到数据"伸展"最厉害的方向，使得降维后保留最多的信息。</li><li><strong>最小化重构误差</strong>：找到最优的低维子空间，使得降维再重构的误差最小。</li></ol><p>这两种视角殊途同归，都指向了同一个数学核心：协方差矩阵的特征值分解。</p><p>PCA 的优雅在于它的通用性。从天文学中的恒星分布，到生物学中的基因表达，从金融学中的股票收益，到计算机视觉中的人脸识别，PCA 都有广泛应用。它的核心思想——从复杂的高维数据中提取主要的变化模式——是数据分析的一个永恒主题。</p><p>但 PCA 也有局限性：它是线性的，对尺度敏感，且不能保证方差大的方向包含所有重要信息。因此，在实际应用中，我们需要根据具体问题选择合适的方法，或者将 PCA 与其他技术结合使用。</p><p>从 Pearson 的几何直觉，到 Hotelling 的统计完善，PCA 已经发展了一个多世纪。但它的核心思想依然闪耀着智慧的光芒：<strong>在复杂的混沌中寻找简洁的秩序，在冗余的信息中提取本质的模式</strong>。这不仅是 PCA 的哲学，也是所有数据科学方法的终极目标。</p><p>当我们面对海量数据时，PCA 提醒我们：不要被复杂性所淹没，寻找隐藏在数据背后的主轴，那些主轴将引导我们走向理解和洞察的彼岸。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/pca/>PCA</a></li><li><a href=https://s-ai-unix.github.io/tags/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/>主成分分析</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%89%B9%E5%BE%81%E5%80%BC%E5%88%86%E8%A7%A3/>特征值分解</a></li><li><a href=https://s-ai-unix.github.io/tags/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5/>协方差矩阵</a></li><li><a href=https://s-ai-unix.github.io/tags/%E9%99%8D%E7%BB%B4/>降维</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-24-bayesian-classifier/><span class=title>« Prev</span><br><span>贝叶斯分类器：从条件概率到智能决策的优雅之旅</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-24-gan-comprehensive-guide/><span class=title>Next »</span><br><span>生成对抗网络：从混沌中创造秩序的博弈论</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share PCA 主成分分析：从数据降维的优雅艺术 on x" href="https://x.com/intent/tweet/?text=PCA%20%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90%ef%bc%9a%e4%bb%8e%e6%95%b0%e6%8d%ae%e9%99%8d%e7%bb%b4%e7%9a%84%e4%bc%98%e9%9b%85%e8%89%ba%e6%9c%af&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-24-pca-comprehensive-guide%2f&amp;hashtags=PCA%2c%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90%2c%e7%89%b9%e5%be%81%e5%80%bc%e5%88%86%e8%a7%a3%2c%e5%8d%8f%e6%96%b9%e5%b7%ae%e7%9f%a9%e9%98%b5%2c%e9%99%8d%e7%bb%b4%2c%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share PCA 主成分分析：从数据降维的优雅艺术 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-24-pca-comprehensive-guide%2f&amp;title=PCA%20%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90%ef%bc%9a%e4%bb%8e%e6%95%b0%e6%8d%ae%e9%99%8d%e7%bb%b4%e7%9a%84%e4%bc%98%e9%9b%85%e8%89%ba%e6%9c%af&amp;summary=PCA%20%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90%ef%bc%9a%e4%bb%8e%e6%95%b0%e6%8d%ae%e9%99%8d%e7%bb%b4%e7%9a%84%e4%bc%98%e9%9b%85%e8%89%ba%e6%9c%af&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-24-pca-comprehensive-guide%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share PCA 主成分分析：从数据降维的优雅艺术 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-24-pca-comprehensive-guide%2f&title=PCA%20%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90%ef%bc%9a%e4%bb%8e%e6%95%b0%e6%8d%ae%e9%99%8d%e7%bb%b4%e7%9a%84%e4%bc%98%e9%9b%85%e8%89%ba%e6%9c%af"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share PCA 主成分分析：从数据降维的优雅艺术 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-24-pca-comprehensive-guide%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var n,t=e.innerHTML,s=!1;for(t.indexOf("<em>")!==-1&&(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("</em>")!==-1&&(n=t.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),n=n.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("<em>$")!==-1&&(n=t.replace(/<em>\$/g,"$"),n!==t&&(t=n,s=!0)),t.indexOf("</em>$")!==-1&&(n=t.replace(/<\/em>\$/g,"$"),n!==t&&(t=n,s=!0));t.indexOf("<em>")!==-1&&t.indexOf("</em>")!==-1;){if(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n===t)break;t=n,s=!0}t.indexOf("_{")!==-1&&(n=t.replace(/_{/g,"_{"),n!==t&&(t=n,s=!0)),t.indexOf("}_")!==-1&&(n=t.replace(/}_/g,"}"),n!==t&&(t=n,s=!0)),t.indexOf("\\\\")!==-1&&(n=t.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),n!==t&&(t=n,s=!0)),s&&(e.innerHTML=t)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var n,t=e.innerHTML,s=!1;for(t.indexOf("<em>")!==-1&&(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("</em>")!==-1&&(n=t.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),n=n.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("<em>$")!==-1&&(n=t.replace(/<em>\$/g,"$"),n!==t&&(t=n,s=!0)),t.indexOf("</em>$")!==-1&&(n=t.replace(/<\/em>\$/g,"$"),n!==t&&(t=n,s=!0));t.indexOf("<em>")!==-1&&t.indexOf("</em>")!==-1;){if(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n===t)break;t=n,s=!0}t.indexOf("_{")!==-1&&(n=t.replace(/_{/g,"_{"),n!==t&&(t=n,s=!0)),t.indexOf("}_")!==-1&&(n=t.replace(/}_/g,"}"),n!==t&&(t=n,s=!0)),t.indexOf("\\\\")!==-1&&(n=t.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),n!==t&&(t=n,s=!0)),s&&(e.innerHTML=t)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>