<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>谱定理：线性代数的优雅与机器学习的基石 | s-ai-unix's Blog</title><meta name=keywords content="线性代数,机器学习,谱定理,PCA,算法"><meta name=description content="从对称矩阵到深度学习：系统性介绍谱定理的核心理论及其在机器学习中的应用，包括正交对角化、SVD、PCA、谱聚类和图神经网络"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-25-spectral-theorem/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-25-spectral-theorem/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-25-spectral-theorem/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="谱定理：线性代数的优雅与机器学习的基石"><meta property="og:description" content="从对称矩阵到深度学习：系统性介绍谱定理的核心理论及其在机器学习中的应用，包括正交对角化、SVD、PCA、谱聚类和图神经网络"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-25T18:00:00+08:00"><meta property="article:modified_time" content="2026-01-25T18:00:00+08:00"><meta property="article:tag" content="线性代数"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="谱定理"><meta property="article:tag" content="PCA"><meta property="article:tag" content="算法"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/spectral-theorem.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/spectral-theorem.jpg"><meta name=twitter:title content="谱定理：线性代数的优雅与机器学习的基石"><meta name=twitter:description content="从对称矩阵到深度学习：系统性介绍谱定理的核心理论及其在机器学习中的应用，包括正交对角化、SVD、PCA、谱聚类和图神经网络"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"谱定理：线性代数的优雅与机器学习的基石","item":"https://s-ai-unix.github.io/posts/2026-01-25-spectral-theorem/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"谱定理：线性代数的优雅与机器学习的基石","name":"谱定理：线性代数的优雅与机器学习的基石","description":"从对称矩阵到深度学习：系统性介绍谱定理的核心理论及其在机器学习中的应用，包括正交对角化、SVD、PCA、谱聚类和图神经网络","keywords":["线性代数","机器学习","谱定理","PCA","算法"],"articleBody":"引言：对称性的数学之美 在数学的众多分支中，有一个深刻的原理反复出现：对称性带来简化。在物理学中，空间的对称性意味着守恒量；在群论中，对称结构导致简单的表示；在线性代数中，对称矩阵拥有最优雅的对角化理论——这就是谱定理。\n想象你站在一个椭圆中心。如果你沿任意方向看出去，椭圆的\"宽度\"各不相同。但有两个特殊的方向——椭圆的长轴和短轴——沿这些方向，椭圆的形状最简单，只是一个被拉伸的圆。这两个正交的方向，就是椭圆的\"主轴\"，它们对应的拉伸倍数，就是\"特征值\"。\n这个直观的几何图像，正是谱定理的核心。谱定理告诉我们：任何实对称矩阵都可以通过正交变换对角化。换句话说，在适当的坐标系下，对称矩阵描述的线性变换只是沿某些正交方向的简单拉伸。\n在机器学习和深度学习中，谱定理无处不在。从主成分分析（PCA）到奇异值分解（SVD），从谱聚类到图神经网络，谱定理提供了理解数据和算法的理论基础。\n在这篇文章中，我们将系统性地介绍谱定理的核心理论，从实对称矩阵的正交对角化到一般的奇异值分解，从PCA到谱聚类，深入浅出地推导每一个公式，并通过可视化图形直观理解这些概念。\n第一章：谱定理的基础理论 1.1 特征值与特征向量：不变的方向 给定一个 $n \\times n$ 矩阵 $A$，如果存在非零向量 $v \\in \\mathbb{R}^n$ 和标量 $\\lambda \\in \\mathbb{R}$，使得\n$$ Av = \\lambda v $$\n则称 $\\lambda$ 是 $A$ 的特征值，$v$ 是对应的特征向量。\n几何意义：特征向量 $v$ 是线性变换 $A$ 下的\"不变方向\"——变换后，这个向量只是被拉伸或压缩了 $\\lambda$ 倍，方向保持不变。\n特征多项式：特征值是特征方程的根\n$$ \\det(A - \\lambda I) = 0 $$\n对于 $n \\times n$ 矩阵，这是一个 $n$ 次多项式，在复数域上有 $n$ 个根（计入重数）。\n1.2 对称矩阵的特殊性质 实对称矩阵 $A \\in \\mathbb{R}^{n \\times n}$（即 $A^\\top = A$）拥有三个重要性质：\n性质1：所有特征值都是实数\n证明：设 $\\lambda$ 是 $A$ 的特征值，$v \\neq 0$ 是对应的特征向量（可能是复向量）。则\n$$ Av = \\lambda v $$\n取共轭转置：$\\overline{v}^\\top A = \\overline{\\lambda} \\overline{v}^\\top$（因为 $A$ 是实矩阵）\n右乘 $v$：$\\overline{v}^\\top A v = \\overline{\\lambda} \\overline{v}^\\top v$\n但 $\\overline{v}^\\top A v = \\overline{v}^\\top (\\lambda v) = \\lambda \\overline{v}^\\top v$\n因此 $\\lambda \\overline{v}^\\top v = \\overline{\\lambda} \\overline{v}^\\top v$\n由于 $\\overline{v}^\\top v = \\sum |v_i|^2 \u003e 0$，我们得到 $\\lambda = \\overline{\\lambda}$，即 $\\lambda$ 是实数。\n性质2：不同特征值对应的特征向量正交\n证明：设 $Av_1 = \\lambda_1 v_1$，$Av_2 = \\lambda_2 v_2$，且 $\\lambda_1 \\neq \\lambda_2$。\n计算 $v_2^\\top A v_1$ 两种方式：\n$v_2^\\top A v_1 = v_2^\\top (\\lambda_1 v_1) = \\lambda_1 v_2^\\top v_1$\n$v_2^\\top A v_1 = v_2^\\top A^\\top v_1 = (Av_2)^\\top v_1 = (\\lambda_2 v_2)^\\top v_1 = \\lambda_2 v_2^\\top v_1$\n因此 $\\lambda_1 v_2^\\top v_1 = \\lambda_2 v_2^\\top v_1$，即 $(\\lambda_1 - \\lambda_2) v_2^\\top v_1 = 0$\n由于 $\\lambda_1 \\neq \\lambda_2$，必须有 $v_2^\\top v_1 = 0$，即 $v_1 \\perp v_2$。\n性质3：可正交对角化\n这是谱定理的核心内容，我们在下一节详细讨论。\n1.3 正交矩阵与正交对角化 定义：矩阵 $Q \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，如果\n$$ Q^\\top Q = Q Q^\\top = I $$\n等价地，$Q$ 的列向量构成 $\\mathbb{R}^n$ 的一组标准正交基。\n几何意义：正交矩阵表示旋转或反射变换，保持向量的长度和夹角。\n定理：矩阵 $A$ 可正交对角化当且仅当 $A$ 是实对称矩阵。即存在正交矩阵 $Q$ 和对角矩阵 $\\Lambda$，使得\n$$ A = Q \\Lambda Q^\\top $$\n其中 $\\Lambda = \\operatorname{diag}(\\lambda_1, \\ldots, \\lambda_n)$，$\\lambda_i$ 是 $A$ 的特征值，$Q$ 的列是对应的特征向量。\n图1：对称矩阵的特征向量。橙色线是特征向量方向，蓝色虚线是变换后的特征向量（仍在同一直线上）。椭圆显示单位圆经变换 A 后的形状，长轴和短轴恰好沿特征向量方向。\n第二章：谱定理的证明与深入理解 2.1 谱定理的完整表述 谱定理（实对称矩阵版本）：\n设 $A \\in \\mathbb{R}^{n \\times n}$ 是对称矩阵，则：\n$A$ 有 $n$ 个实特征值 $\\lambda_1, \\ldots, \\lambda_n$（计入重数） 存在 $\\mathbb{R}^n$ 的一组标准正交基 ${q_1, \\ldots, q_n}$，其中每个 $q_i$ 都是 $A$ 的特征向量 $A$ 可以表示为 $A = \\sum_{i=1}^n \\lambda_i q_i q_i^\\top$ 2.2 谱定理的证明 我们使用归纳法证明谱定理。\n基础情况（$n=1$）：平凡成立。\n归纳步骤：假设对 $(n-1) \\times (n-1)$ 对称矩阵成立。设 $A$ 是 $n \\times n$ 对称矩阵。\n步骤1：由于特征多项式在复数域上总有根，取 $A$ 的一个特征值 $\\lambda_1$（由性质1，$\\lambda_1$ 是实数）和对应的单位特征向量 $q_1$。\n步骤2：将 $q_1$ 扩展为 $\\mathbb{R}^n$ 的标准正交基 ${q_1, q_2, \\ldots, q_n}$。令 $Q = [q_1 ; q_2 ; \\cdots ; q_n]$，则 $Q$ 是正交矩阵。\n步骤3：考虑 $Q^\\top A Q$。计算其第一列：\n$(Q^\\top A Q)_{:,1} = Q^\\top A q_1 = Q^\\top (\\lambda_1 q_1) = \\lambda_1 Q^\\top q_1 = \\lambda_1 e_1$\n其中 $e_1 = (1, 0, \\ldots, 0)^\\top$。\n由于 $Q^\\top A Q$ 也是对称矩阵（$(Q^\\top A Q)^\\top = Q^\\top A^\\top Q = Q^\\top A Q$），它的第一行也必须是 $(\\lambda_1, 0, \\ldots, 0)$。\n因此\n$$ Q^\\top A Q = \\begin{pmatrix} \\lambda_1 \u0026 0 \\ 0 \u0026 B \\end{pmatrix} $$\n其中 $B$ 是 $(n-1) \\times (n-1)$ 对称矩阵。\n步骤4：由归纳假设，$B$ 可正交对角化：存在 $(n-1) \\times (n-1)$ 正交矩阵 $Q_B$ 使得 $Q_B^\\top B Q_B = \\Lambda_B$ 是对角矩阵。\n步骤5：令\n$$ \\widetilde{Q} = \\begin{pmatrix} 1 \u0026 0 \\ 0 \u0026 Q_B \\end{pmatrix}, \\quad \\widetilde{Q}^\\top (Q^\\top A Q) \\widetilde{Q} = \\begin{pmatrix} \\lambda_1 \u0026 0 \\ 0 \u0026 \\Lambda_B \\end{pmatrix} $$\n则 $\\widetilde{Q} Q$ 是正交矩阵，且\n$$ (\\widetilde{Q} Q)^\\top A (\\widetilde{Q} Q) = \\operatorname{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n) $$\n证毕。\n2.3 谱分解的谱系解释 谱定理的另一种表达方式是谱分解：\n$$ A = \\sum_{i=1}^n \\lambda_i q_i q_i^\\top = \\sum_{i=1}^n \\lambda_i P_i $$\n其中 $P_i = q_i q_i^\\top$ 是到特征空间 $\\operatorname{span}{q_i}$ 的正交投影算子。\n性质：\n$P_i^2 = P_i$（幂等性） $P_i P_j = 0$（$i \\neq j$，正交性） $\\sum_{i=1}^n P_i = I$（完备性） 直观理解：对称矩阵 $A$ 可以分解为沿各个正交方向的\"拉伸\"的组合，每个方向上的拉伸倍数就是对应的特征值。\n第三章：奇异值分解（SVD）——推广到任意矩阵 3.1 从谱定理到SVD 谱定理适用于对称方阵。但实际应用中，我们经常遇到非方阵（如数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$）。奇异值分解（SVD）是谱定理的自然推广。\n定理（SVD）：任何矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 都可以分解为\n$$ A = U \\Sigma V^\\top $$\n其中：\n$U \\in \\mathbb{R}^{m \\times m}$ 是正交矩阵（左奇异向量） $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵（右奇异向量） $\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是对角矩阵，对角元素 $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r \u003e 0$ 是奇异值，$r = \\operatorname{rank}(A)$ 3.2 SVD的推导 步骤1：考虑 $A^\\top A \\in \\mathbb{R}^{n \\times n}$，这是一个对称半正定矩阵。\n由谱定理，$A^\\top A$ 可正交对角化：\n$$ A^\\top A = V \\Lambda V^\\top $$\n其中 $V$ 的列 $v_1, \\ldots, v_n$ 是特征向量，$\\Lambda = \\operatorname{diag}(\\lambda_1, \\ldots, \\lambda_n)$。\n由于 $A^\\top A$ 半正定，所有 $\\lambda_i \\geq 0$。\n步骤2：定义奇异值 $\\sigma_i = \\sqrt{\\lambda_i}$。\n步骤3：定义 $u_i = \\frac{Av_i}{\\sigma_i}$（当 $\\sigma_i \u003e 0$）。\n验证 $u_i$ 是单位向量：\n$$ \\lVert u_i \\rVert^2 = \\frac{v_i^\\top A^\\top A v_i}{\\sigma_i^2} = \\frac{v_i^\\top (\\lambda_i v_i)}{\\lambda_i} = v_i^\\top v_i = 1 $$\n且 $u_i$ 两两正交：\n$$ u_i^\\top u_j = \\frac{v_i^\\top A^\\top A v_j}{\\sigma_i \\sigma_j} = \\frac{\\lambda_j v_i^\\top v_j}{\\sigma_i \\sigma_j} = 0 \\quad (i \\neq j) $$\n步骤4：将 $u_i$ 扩展为 $\\mathbb{R}^m$ 的标准正交基，得到 $U$。\n步骤5：验证 $A = U \\Sigma V^\\top$。\n对于任意 $v_j$：\n$$ A v_j = \\sigma_j u_j = U (\\Sigma e_j) = U \\Sigma (V^\\top v_j) $$\n由于 ${v_1, \\ldots, v_n}$ 是基，对任意 $x \\in \\mathbb{R}^n$，$A x = U \\Sigma V^\\top x$。\n3.3 SVD的几何意义 SVD告诉我们，任何线性变换 $A : \\mathbb{R}^n \\to \\mathbb{R}^m$ 都可以分解为三个步骤：\n旋转/反射（$V^\\top$）：在 $\\mathbb{R}^n$ 中改变坐标系 伸缩（$\\Sigma$）：沿各坐标轴方向伸缩 旋转/反射（$U$）：在 $\\mathbb{R}^m$ 中改变坐标系 直观理解：$A$ 将单位球映射为一个椭球，奇异值给出椭球的主轴长度，左、右奇异向量给出主轴方向。\n图2：SVD分解的矩阵形式。M被分解为U、Σ、Vᵀ三个矩阵的乘积，其中U和V是正交矩阵，Σ是对角矩阵（非方阵时补零）。\n第四章：主成分分析（PCA）的谱定理视角 4.1 PCA的问题设定 给定中心化数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$（$n$ 个样本，$p$ 个特征，每列均值为零），PCA的目标是找到一组正交方向，使得数据在这些方向上的方差最大化。\n问题：最大化方差\n$$ \\max_{w \\in \\mathbb{R}^p, \\lVert w \\rVert = 1} \\operatorname{Var}(Xw) = \\frac{1}{n} \\sum_{i=1}^n (x_i^\\top w)^2 = \\frac{1}{n} \\lVert Xw \\rVert^2 = \\frac{1}{n} w^\\top X^\\top X w $$\n4.2 PCA的谱定理推导 定义样本协方差矩阵 $C = \\frac{1}{n} X^\\top X$，这是一个 $p \\times p$ 对称半正定矩阵。\n由谱定理，$C$ 可正交对角化：\n$$ C = V \\Lambda V^\\top $$\n其中 $\\Lambda = \\operatorname{diag}(\\lambda_1, \\ldots, \\lambda_p)$，$\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_p \\geq 0$。\n关键观察：PCA问题的解恰好是 $C$ 的特征向量。\n证明：对于单位向量 $w$，令 $v = V^\\top w$。由于 $V$ 正交，$\\lVert v \\rVert = \\lVert w \\rVert = 1$。\n$$ w^\\top C w = w^\\top V \\Lambda V^\\top w = v^\\top \\Lambda v = \\sum_{i=1}^p \\lambda_i v_i^2 \\leq \\lambda_1 \\sum_{i=1}^p v_i^2 = \\lambda_1 $$\n当 $v = e_1$（即 $w = v_1$，$C$ 的第一特征向量）时等号成立。\n4.3 PCA的几何解释 PCA寻找的是数据的\"主轴\"——数据变化最大的方向。这些方向恰好是协方差矩阵的特征向量，特征值大小表示沿该方向的方差大小。\n图3：PCA的几何意义。数据点的椭圆轮廓显示了数据的分布，橙色箭头是第一主成分（最大方差方向），绿色箭头是第二主成分。虚线椭圆是2σ置信椭圆。\n4.4 降维与重建 保留前 $k$ 个主成分，将数据投影到 $k$ 维子空间：\n$$ X_{\\text{projected}} = X V_k $$\n其中 $V_k = [v_1 ; v_2 ; \\cdots ; v_k]$ 包含前 $k$ 个特征向量。\n重建（近似）原始数据：\n$$ X_{\\text{reconstructed}} = X_{\\text{projected}} V_k^\\top = X V_k V_k^\\top $$\n误差分析：重建误差（Frobenius范数）等于被舍弃特征值的和：\n$$ \\lVert X - X V_k V_k^\\top \\rVert_F^2 = n \\sum_{i=k+1}^p \\lambda_i $$\n图4：PCA降维去噪效果。灰色点是原始噪声数据，橙色点是保留第一主成分后重建的数据，蓝色箭头是第一主成分方向。噪声被有效过滤。\n4.5 解释方差比 第 $k$ 主成分的解释方差比为：\n$$ \\frac{\\lambda_k}{\\sum_{i=1}^p \\lambda_i} $$\n前 $k$ 个主成分的累积解释方差比为：\n$$ \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^p \\lambda_i} $$\n图5：特征值衰减与累积解释方差。蓝色条形是各主成分的解释方差比，橙色线是累积解释方差比。可以看到，前5个主成分解释了超过95%的方差。\n第五章：谱聚类与拉普拉斯矩阵 5.1 图的谱理论 给定无向加权图 $G = (V, E, W)$，其中 $W_{ij}$ 是节点 $i$ 和 $j$ 之间的边权重。\n度矩阵：$D = \\operatorname{diag}(d_1, \\ldots, d_n)$，其中 $d_i = \\sum_{j=1}^n W_{ij}$\n图拉普拉斯矩阵：$L = D - W$\n性质：\n$L$ 是对称半正定矩阵 $L$ 的最小特征值是 $0$，对应的特征向量是全 $1$ 向量 第二小特征值称为代数连通度或Fiedler值，对应的特征向量称为Fiedler向量 5.2 谱聚类算法 谱聚类利用拉普拉斯矩阵的特征向量进行聚类：\n算法：\n构建相似度图（如k近邻图、全连接图） 计算拉普拉斯矩阵 $L$ 计算 $L$ 的前 $k$ 个特征向量 $u_1, \\ldots, u_k$ 将节点嵌入到 $\\mathbb{R}^k$：节点 $i$ 映射为 $(u_1(i), \\ldots, u_k(i))$ 在嵌入空间中运行k-means聚类 为什么有效：拉普拉斯矩阵的特征向量捕获了图的\"全局结构\"。Fiedler向量的正负性自然地将图分成两个连接紧密的部分。\n图6：图拉普拉斯矩阵的Fiedler向量可视化。节点颜色代表Fiedler向量的值，蓝色为负，红色为正。可以看到Fiedler向量自然地将图分成了两组。\n5.3 谱聚类的直观例子 考虑两个月牙形数据集：传统的基于距离的聚类（如k-means）无法正确分离，但谱聚类可以。\n关键在于：谱聚类不是在原始空间中聚类，而是在\"谱空间\"中聚类。在谱空间中，原本纠缠的数据点被正确分离。\n图7：谱聚类对两个月牙形数据的聚类结果。蓝色和橙色代表两个不同的簇，谱聚类成功分离了这两个纠缠的月牙形数据。\n5.4 归一化割（Normalized Cut） 谱聚类与归一化割优化问题密切相关：\n$$ \\min_{A \\subset V} \\operatorname{Ncut}(A) = \\frac{\\operatorname{cut}(A)}{\\operatorname{vol}(A)} + \\frac{\\operatorname{cut}(A)}{\\operatorname{vol}(\\overline{A})} $$\n其中 $\\operatorname{cut}(A) = \\sum_{i \\in A, j \\notin A} W_{ij}$ 是分割的代价，$\\operatorname{vol}(A) = \\sum_{i \\in A} d_i$ 是节点集合的\"体积\"。\n定理：归一化割问题的松弛解恰好是拉普拉斯矩阵的第二小特征向量。\n第六章：神经网络中的谱方法 6.1 图神经网络（GNN）中的谱卷积 图卷积网络（GCN）的核心思想是在图上进行卷积操作。经典的卷积定义在欧几里得空间（如图像），但图没有规则的网格结构。\n谱图卷积利用图拉普拉斯矩阵的特征分解定义卷积：\n设 $L = U \\Lambda U^\\top$ 是拉普拉斯矩阵的谱分解，信号 $x \\in \\mathbb{R}^n$ 的傅里叶变换是 $\\hat{x} = U^\\top x$。\n图上的卷积定义为：\n$$ y = g_\\theta * x = U g_\\theta(\\Lambda) U^\\top x $$\n其中 $g_\\theta(\\Lambda) = \\operatorname{diag}(\\theta(\\lambda_1), \\ldots, \\theta(\\lambda_n))$ 是频域滤波器。\nChebNet近似：直接计算特征分解太昂贵（$O(n^3)$），使用切比雪夫多项式近似：\n$$ g_\\theta(\\Lambda) \\approx \\sum_{k=0}^K \\theta_k T_k(\\widetilde{L}) $$\n其中 $T_k$ 是第 $k$ 阶切比雪夫多项式，$\\widetilde{L} = \\frac{2}{\\lambda_{\\max}} L - I$ 是缩放的拉普拉斯矩阵。\n6.2 GCN的一阶近似 Kipf \u0026 Welling (2017) 提出了更简单的一阶近似：\n$$ y = D^{-1/2} \\widetilde{A} D^{-1/2} X \\Theta $$\n其中 $\\widetilde{A} = A + I$ 是加入自环的邻接矩阵。\n这个公式可以理解为：每个节点的表示是其邻居表示的加权平均，权重由图的度决定。\n6.3 注意力机制的谱视角 Transformer中的注意力机制也可以从谱的角度理解。自注意力矩阵 $S \\in \\mathbb{R}^{n \\times n}$ 定义为：\n$$ S_{ij} = \\frac{\\exp(q_i^\\top k_j)}{\\sum_{l=1}^n \\exp(q_i^\\top k_l)} $$\n这可以看作是在动态构建的图上的消息传递，图的权重由注意力得分决定。\n谱归一化：为了稳定训练，可以对注意力矩阵的奇异值进行约束：\n$$ S \\leftarrow \\frac{S}{\\sigma_{\\max}(S)} $$\n6.4 激活函数的谱分析 ReLU激活函数 $f(x) = \\max(0, x)$ 的谱性质很重要。考虑其在正交变换下的行为：\n如果 $W$ 是随机正交矩阵，则 $\\operatorname{E}[\\lVert f(Wx) \\rVert^2] = \\frac{1}{2}\\lVert x \\rVert^2$（假设 $x$ 的各分量独立对称分布）。\n这解释了为什么需要He初始化：$W \\sim \\mathcal{N}(0, \\frac{2}{n_{in}})$，以保持信号在通过ReLU后方差不变。\n第七章：数值方法与应用案例 7.1 特征值计算算法 幂法：计算最大特征值和对应特征向量\n初始化：随机向量 b(0) 重复：b(k+1) = A b(k) / ||A b(k)|| 收敛：b(k) → 主特征向量，||A b(k)|| → 主特征值 收敛速度：取决于 $|\\lambda_2 / \\lambda_1|$（次大特征值与最大特征值的比值之比）。\nQR算法：计算所有特征值\n通过QR迭代将矩阵逐步上三角化：\n$$ A_0 = A \\ A_k = Q_k R_k \\quad \\text{（QR分解）} \\ A_{k+1} = R_k Q_k $$\n对于对称矩阵，$A_k$ 收敛到对角矩阵（特征值在主对角线上）。\n7.2 SVD在图像压缩中的应用 图像可以看作矩阵 $I \\in \\mathbb{R}^{m \\times n}$（像素值）。SVD的低秩近似：\n$$ I \\approx I_k = \\sum_{i=1}^k \\sigma_i u_i v_i^\\top $$\n其中 $k \\ll \\min(m, n)$。\n压缩比：存储 $I_k$ 需要 $k(m + n + 1)$ 个数，而原始图像需要 $mn$ 个数。\n当 $k = \\frac{mn}{m+n}$ 时，压缩比约为 50%。\n7.3 协同过滤中的矩阵分解 推荐系统中的用户-物品评分矩阵 $R \\in \\mathbb{R}^{m \\times n}$ 通常非常稀疏。协同过滤假设 $R$ 可以分解为：\n$$ R \\approx U V^\\top $$\n其中 $U \\in \\mathbb{R}^{m \\times k}$ 是用户隐因子矩阵，$V \\in \\mathbb{R}^{n \\times k}$ 是物品隐因子矩阵。\n这与截断SVD密切相关，但需要处理缺失值（通常通过交替最小二乘或随机梯度下降求解）。\n7.4 PageRank算法 PageRank是谷歌早期的核心算法，用于衡量网页的重要性。可以理解为马尔可夫链的稳态分布。\n设 $A$ 是网页的邻接矩阵（$A_{ij} = 1$ 如果页面 $j$ 链接到页面 $i$），归一化后得到转移矩阵 $P$。\nPageRank向量 $r$ 满足：\n$$ r = (1-d) \\frac{1}{n} \\mathbf{1} + d P^\\top r $$\n其中 $d \\approx 0.85$ 是阻尼因子。\n这等价于求矩阵 $M = d P^\\top + \\frac{1-d}{n} \\mathbf{1} \\mathbf{1}^\\top$ 的主特征向量。\n第八章：总结与展望 8.1 核心要点回顾 谱定理是连接线性代数、几何分析和机器学习的桥梁：\n谱定理：实对称矩阵可正交对角化，特征向量构成标准正交基 SVD：谱定理向任意矩阵的推广，奇异值是\"信息的强度\" PCA：从谱定理视角看，PCA就是协方差矩阵的特征分解 谱聚类：利用拉普拉斯矩阵的特征向量发现图的社区结构 图神经网络：谱卷积是图上卷积的理论基础 8.2 理论与实践的平衡 在理论层面，谱定理提供了优美的数学结构：对称性导致可对角化，特征值编码了变换的本质信息。\n在实践层面，我们需要考虑：\n计算复杂度：特征分解是 $O(n^3)$ 数值稳定性：病态矩阵需要特殊处理 近似方法：随机SVD、Lanczos算法、幂迭代 8.3 前沿方向 深度学习的理论理解：\n为什么深度网络可以学习？可能与特征值衰减有关 梯度消失/爆炸与雅可比矩阵的谱性质密切相关 谱归一化作为正则化手段 图神经网络的新方向：\n超过一阶的图卷积（但容易过平滑） 自适应图结构学习 结合注意力机制 大规模特征值计算：\n随机算法（如Halko等人的随机SVD） 分布式特征值分解 量子计算在谱分析中的应用 8.4 结语 谱定理之所以优雅，是因为它揭示了线性代数的一个核心真理：对称性带来简化。在机器学习和深度学习的复杂算法背后，谱定理提供了坚实的数学基础。\n从PCA降维到谱聚类，从图像压缩到PageRank，从图神经网络到注意力机制，谱定理的身影无处不在。理解谱定理，就是理解了数据结构的\"骨架\"——那些不随坐标系变化而变化的本质特征。\n希望这篇文章能帮助读者建立对谱定理的系统认识，为进一步学习机器学习理论和算法打下坚实基础。记住：在复杂的数据世界中，谱定理是我们的指南针，指引我们找到最本质的结构。\n参考文献 Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Wellesley-Cambridge Press. Trefethen, L. N., \u0026 Bau, D. (1997). Numerical Linear Algebra. SIAM. Boyd, S., \u0026 Vandenberghe, L. (2018). Introduction to Applied Linear Algebra. Cambridge University Press. Von Luxburg, U. (2007). A Tutorial on Spectral Clustering. Statistics and Computing, 17(4), 395-416. Kipf, T. N., \u0026 Welling, M. (2017). Semi-Supervised Classification with Graph Convolutional Networks. ICLR. Hastie, T., Tibshirani, R., \u0026 Friedman, J. (2009). The Elements of Statistical Learning (2nd ed.). Springer. Golub, G. H., \u0026 Van Loan, C. F. (2013). Matrix Computations (4th ed.). Johns Hopkins University Press. Belkin, M., \u0026 Niyogi, P. (2003). Laplacian Eigenmaps for Dimensionality Reduction and Data Representation. Neural Computation, 15(6), 1373-1396. ","wordCount":"1458","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/spectral-theorem.jpg","datePublished":"2026-01-25T18:00:00+08:00","dateModified":"2026-01-25T18:00:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-25-spectral-theorem/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">谱定理：线性代数的优雅与机器学习的基石</h1><div class=post-description>从对称矩阵到深度学习：系统性介绍谱定理的核心理论及其在机器学习中的应用，包括正交对角化、SVD、PCA、谱聚类和图神经网络</div><div class=post-meta><span title='2026-01-25 18:00:00 +0800 CST'>January 25, 2026</span>&nbsp;·&nbsp;<span>7 min</span>&nbsp;·&nbsp;<span>1458 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/spectral-theorem.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/spectral-theorem.jpg alt=谱定理可视化></a><figcaption>谱定理：线性代数的优雅</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e5%af%b9%e7%a7%b0%e6%80%a7%e7%9a%84%e6%95%b0%e5%ad%a6%e4%b9%8b%e7%be%8e aria-label=引言：对称性的数学之美>引言：对称性的数学之美</a></li><li><a href=#%e7%ac%ac%e4%b8%80%e7%ab%a0%e8%b0%b1%e5%ae%9a%e7%90%86%e7%9a%84%e5%9f%ba%e7%a1%80%e7%90%86%e8%ae%ba aria-label=第一章：谱定理的基础理论>第一章：谱定理的基础理论</a><ul><li><a href=#11-%e7%89%b9%e5%be%81%e5%80%bc%e4%b8%8e%e7%89%b9%e5%be%81%e5%90%91%e9%87%8f%e4%b8%8d%e5%8f%98%e7%9a%84%e6%96%b9%e5%90%91 aria-label="1.1 特征值与特征向量：不变的方向">1.1 特征值与特征向量：不变的方向</a></li><li><a href=#12-%e5%af%b9%e7%a7%b0%e7%9f%a9%e9%98%b5%e7%9a%84%e7%89%b9%e6%ae%8a%e6%80%a7%e8%b4%a8 aria-label="1.2 对称矩阵的特殊性质">1.2 对称矩阵的特殊性质</a></li><li><a href=#13-%e6%ad%a3%e4%ba%a4%e7%9f%a9%e9%98%b5%e4%b8%8e%e6%ad%a3%e4%ba%a4%e5%af%b9%e8%a7%92%e5%8c%96 aria-label="1.3 正交矩阵与正交对角化">1.3 正交矩阵与正交对角化</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e7%ab%a0%e8%b0%b1%e5%ae%9a%e7%90%86%e7%9a%84%e8%af%81%e6%98%8e%e4%b8%8e%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3 aria-label=第二章：谱定理的证明与深入理解>第二章：谱定理的证明与深入理解</a><ul><li><a href=#21-%e8%b0%b1%e5%ae%9a%e7%90%86%e7%9a%84%e5%ae%8c%e6%95%b4%e8%a1%a8%e8%bf%b0 aria-label="2.1 谱定理的完整表述">2.1 谱定理的完整表述</a></li><li><a href=#22-%e8%b0%b1%e5%ae%9a%e7%90%86%e7%9a%84%e8%af%81%e6%98%8e aria-label="2.2 谱定理的证明">2.2 谱定理的证明</a></li><li><a href=#23-%e8%b0%b1%e5%88%86%e8%a7%a3%e7%9a%84%e8%b0%b1%e7%b3%bb%e8%a7%a3%e9%87%8a aria-label="2.3 谱分解的谱系解释">2.3 谱分解的谱系解释</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e7%ab%a0%e5%a5%87%e5%bc%82%e5%80%bc%e5%88%86%e8%a7%a3svd%e6%8e%a8%e5%b9%bf%e5%88%b0%e4%bb%bb%e6%84%8f%e7%9f%a9%e9%98%b5 aria-label=第三章：奇异值分解（SVD）——推广到任意矩阵>第三章：奇异值分解（SVD）——推广到任意矩阵</a><ul><li><a href=#31-%e4%bb%8e%e8%b0%b1%e5%ae%9a%e7%90%86%e5%88%b0svd aria-label="3.1 从谱定理到SVD">3.1 从谱定理到SVD</a></li><li><a href=#32-svd%e7%9a%84%e6%8e%a8%e5%af%bc aria-label="3.2 SVD的推导">3.2 SVD的推导</a></li><li><a href=#33-svd%e7%9a%84%e5%87%a0%e4%bd%95%e6%84%8f%e4%b9%89 aria-label="3.3 SVD的几何意义">3.3 SVD的几何意义</a></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e7%ab%a0%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90pca%e7%9a%84%e8%b0%b1%e5%ae%9a%e7%90%86%e8%a7%86%e8%a7%92 aria-label=第四章：主成分分析（PCA）的谱定理视角>第四章：主成分分析（PCA）的谱定理视角</a><ul><li><a href=#41-pca%e7%9a%84%e9%97%ae%e9%a2%98%e8%ae%be%e5%ae%9a aria-label="4.1 PCA的问题设定">4.1 PCA的问题设定</a></li><li><a href=#42-pca%e7%9a%84%e8%b0%b1%e5%ae%9a%e7%90%86%e6%8e%a8%e5%af%bc aria-label="4.2 PCA的谱定理推导">4.2 PCA的谱定理推导</a></li><li><a href=#43-pca%e7%9a%84%e5%87%a0%e4%bd%95%e8%a7%a3%e9%87%8a aria-label="4.3 PCA的几何解释">4.3 PCA的几何解释</a></li><li><a href=#44-%e9%99%8d%e7%bb%b4%e4%b8%8e%e9%87%8d%e5%bb%ba aria-label="4.4 降维与重建">4.4 降维与重建</a></li><li><a href=#45-%e8%a7%a3%e9%87%8a%e6%96%b9%e5%b7%ae%e6%af%94 aria-label="4.5 解释方差比">4.5 解释方差比</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e7%ab%a0%e8%b0%b1%e8%81%9a%e7%b1%bb%e4%b8%8e%e6%8b%89%e6%99%ae%e6%8b%89%e6%96%af%e7%9f%a9%e9%98%b5 aria-label=第五章：谱聚类与拉普拉斯矩阵>第五章：谱聚类与拉普拉斯矩阵</a><ul><li><a href=#51-%e5%9b%be%e7%9a%84%e8%b0%b1%e7%90%86%e8%ae%ba aria-label="5.1 图的谱理论">5.1 图的谱理论</a></li><li><a href=#52-%e8%b0%b1%e8%81%9a%e7%b1%bb%e7%ae%97%e6%b3%95 aria-label="5.2 谱聚类算法">5.2 谱聚类算法</a></li><li><a href=#53-%e8%b0%b1%e8%81%9a%e7%b1%bb%e7%9a%84%e7%9b%b4%e8%a7%82%e4%be%8b%e5%ad%90 aria-label="5.3 谱聚类的直观例子">5.3 谱聚类的直观例子</a></li><li><a href=#54-%e5%bd%92%e4%b8%80%e5%8c%96%e5%89%b2normalized-cut aria-label="5.4 归一化割（Normalized Cut）">5.4 归一化割（Normalized Cut）</a></li></ul></li><li><a href=#%e7%ac%ac%e5%85%ad%e7%ab%a0%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%ad%e7%9a%84%e8%b0%b1%e6%96%b9%e6%b3%95 aria-label=第六章：神经网络中的谱方法>第六章：神经网络中的谱方法</a><ul><li><a href=#61-%e5%9b%be%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9cgnn%e4%b8%ad%e7%9a%84%e8%b0%b1%e5%8d%b7%e7%a7%af aria-label="6.1 图神经网络（GNN）中的谱卷积">6.1 图神经网络（GNN）中的谱卷积</a></li><li><a href=#62-gcn%e7%9a%84%e4%b8%80%e9%98%b6%e8%bf%91%e4%bc%bc aria-label="6.2 GCN的一阶近似">6.2 GCN的一阶近似</a></li><li><a href=#63-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e8%b0%b1%e8%a7%86%e8%a7%92 aria-label="6.3 注意力机制的谱视角">6.3 注意力机制的谱视角</a></li><li><a href=#64-%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0%e7%9a%84%e8%b0%b1%e5%88%86%e6%9e%90 aria-label="6.4 激活函数的谱分析">6.4 激活函数的谱分析</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%83%e7%ab%a0%e6%95%b0%e5%80%bc%e6%96%b9%e6%b3%95%e4%b8%8e%e5%ba%94%e7%94%a8%e6%a1%88%e4%be%8b aria-label=第七章：数值方法与应用案例>第七章：数值方法与应用案例</a><ul><li><a href=#71-%e7%89%b9%e5%be%81%e5%80%bc%e8%ae%a1%e7%ae%97%e7%ae%97%e6%b3%95 aria-label="7.1 特征值计算算法">7.1 特征值计算算法</a></li><li><a href=#72-svd%e5%9c%a8%e5%9b%be%e5%83%8f%e5%8e%8b%e7%bc%a9%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8 aria-label="7.2 SVD在图像压缩中的应用">7.2 SVD在图像压缩中的应用</a></li><li><a href=#73-%e5%8d%8f%e5%90%8c%e8%bf%87%e6%bb%a4%e4%b8%ad%e7%9a%84%e7%9f%a9%e9%98%b5%e5%88%86%e8%a7%a3 aria-label="7.3 协同过滤中的矩阵分解">7.3 协同过滤中的矩阵分解</a></li><li><a href=#74-pagerank%e7%ae%97%e6%b3%95 aria-label="7.4 PageRank算法">7.4 PageRank算法</a></li></ul></li><li><a href=#%e7%ac%ac%e5%85%ab%e7%ab%a0%e6%80%bb%e7%bb%93%e4%b8%8e%e5%b1%95%e6%9c%9b aria-label=第八章：总结与展望>第八章：总结与展望</a><ul><li><a href=#81-%e6%a0%b8%e5%bf%83%e8%a6%81%e7%82%b9%e5%9b%9e%e9%a1%be aria-label="8.1 核心要点回顾">8.1 核心要点回顾</a></li><li><a href=#82-%e7%90%86%e8%ae%ba%e4%b8%8e%e5%ae%9e%e8%b7%b5%e7%9a%84%e5%b9%b3%e8%a1%a1 aria-label="8.2 理论与实践的平衡">8.2 理论与实践的平衡</a></li><li><a href=#83-%e5%89%8d%e6%b2%bf%e6%96%b9%e5%90%91 aria-label="8.3 前沿方向">8.3 前沿方向</a></li><li><a href=#84-%e7%bb%93%e8%af%ad aria-label="8.4 结语">8.4 结语</a></li></ul></li><li><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae aria-label=参考文献>参考文献</a></li></ul></div></details></div><div class=post-content><h2 id=引言对称性的数学之美>引言：对称性的数学之美<a hidden class=anchor aria-hidden=true href=#引言对称性的数学之美>#</a></h2><p>在数学的众多分支中，有一个深刻的原理反复出现：<strong>对称性带来简化</strong>。在物理学中，空间的对称性意味着守恒量；在群论中，对称结构导致简单的表示；在线性代数中，对称矩阵拥有最优雅的对角化理论——这就是<strong>谱定理</strong>。</p><p>想象你站在一个椭圆中心。如果你沿任意方向看出去，椭圆的"宽度"各不相同。但有两个特殊的方向——椭圆的长轴和短轴——沿这些方向，椭圆的形状最简单，只是一个被拉伸的圆。这两个正交的方向，就是椭圆的"主轴"，它们对应的拉伸倍数，就是"特征值"。</p><p>这个直观的几何图像，正是谱定理的核心。谱定理告诉我们：<strong>任何实对称矩阵都可以通过正交变换对角化</strong>。换句话说，在适当的坐标系下，对称矩阵描述的线性变换只是沿某些正交方向的简单拉伸。</p><p>在机器学习和深度学习中，谱定理无处不在。从主成分分析（PCA）到奇异值分解（SVD），从谱聚类到图神经网络，谱定理提供了理解数据和算法的理论基础。</p><p>在这篇文章中，我们将系统性地介绍谱定理的核心理论，从实对称矩阵的正交对角化到一般的奇异值分解，从PCA到谱聚类，深入浅出地推导每一个公式，并通过可视化图形直观理解这些概念。</p><h2 id=第一章谱定理的基础理论>第一章：谱定理的基础理论<a hidden class=anchor aria-hidden=true href=#第一章谱定理的基础理论>#</a></h2><h3 id=11-特征值与特征向量不变的方向>1.1 特征值与特征向量：不变的方向<a hidden class=anchor aria-hidden=true href=#11-特征值与特征向量不变的方向>#</a></h3><p>给定一个 $n \times n$ 矩阵 $A$，如果存在非零向量 $v \in \mathbb{R}^n$ 和标量 $\lambda \in \mathbb{R}$，使得</p><p>$$
Av = \lambda v
$$</p><p>则称 $\lambda$ 是 $A$ 的<strong>特征值</strong>，$v$ 是对应的<strong>特征向量</strong>。</p><p><strong>几何意义</strong>：特征向量 $v$ 是线性变换 $A$ 下的"不变方向"——变换后，这个向量只是被拉伸或压缩了 $\lambda$ 倍，方向保持不变。</p><p><strong>特征多项式</strong>：特征值是特征方程的根</p><p>$$
\det(A - \lambda I) = 0
$$</p><p>对于 $n \times n$ 矩阵，这是一个 $n$ 次多项式，在复数域上有 $n$ 个根（计入重数）。</p><h3 id=12-对称矩阵的特殊性质>1.2 对称矩阵的特殊性质<a hidden class=anchor aria-hidden=true href=#12-对称矩阵的特殊性质>#</a></h3><p>实对称矩阵 $A \in \mathbb{R}^{n \times n}$（即 $A^\top = A$）拥有三个重要性质：</p><p><strong>性质1：所有特征值都是实数</strong></p><p><strong>证明</strong>：设 $\lambda$ 是 $A$ 的特征值，$v \neq 0$ 是对应的特征向量（可能是复向量）。则</p><p>$$
Av = \lambda v
$$</p><p>取共轭转置：$\overline{v}^\top A = \overline{\lambda} \overline{v}^\top$（因为 $A$ 是实矩阵）</p><p>右乘 $v$：$\overline{v}^\top A v = \overline{\lambda} \overline{v}^\top v$</p><p>但 $\overline{v}^\top A v = \overline{v}^\top (\lambda v) = \lambda \overline{v}^\top v$</p><p>因此 $\lambda \overline{v}^\top v = \overline{\lambda} \overline{v}^\top v$</p><p>由于 $\overline{v}^\top v = \sum |v_i|^2 > 0$，我们得到 $\lambda = \overline{\lambda}$，即 $\lambda$ 是实数。</p><p><strong>性质2：不同特征值对应的特征向量正交</strong></p><p><strong>证明</strong>：设 $Av_1 = \lambda_1 v_1$，$Av_2 = \lambda_2 v_2$，且 $\lambda_1 \neq \lambda_2$。</p><p>计算 $v_2^\top A v_1$ 两种方式：</p><p>$v_2^\top A v_1 = v_2^\top (\lambda_1 v_1) = \lambda_1 v_2^\top v_1$</p><p>$v_2^\top A v_1 = v_2^\top A^\top v_1 = (Av_2)^\top v_1 = (\lambda_2 v_2)^\top v_1 = \lambda_2 v_2^\top v_1$</p><p>因此 $\lambda_1 v_2^\top v_1 = \lambda_2 v_2^\top v_1$，即 $(\lambda_1 - \lambda_2) v_2^\top v_1 = 0$</p><p>由于 $\lambda_1 \neq \lambda_2$，必须有 $v_2^\top v_1 = 0$，即 $v_1 \perp v_2$。</p><p><strong>性质3：可正交对角化</strong></p><p>这是谱定理的核心内容，我们在下一节详细讨论。</p><h3 id=13-正交矩阵与正交对角化>1.3 正交矩阵与正交对角化<a hidden class=anchor aria-hidden=true href=#13-正交矩阵与正交对角化>#</a></h3><p><strong>定义</strong>：矩阵 $Q \in \mathbb{R}^{n \times n}$ 是<strong>正交矩阵</strong>，如果</p><p>$$
Q^\top Q = Q Q^\top = I
$$</p><p>等价地，$Q$ 的列向量构成 $\mathbb{R}^n$ 的一组标准正交基。</p><p><strong>几何意义</strong>：正交矩阵表示旋转或反射变换，保持向量的长度和夹角。</p><p><strong>定理</strong>：矩阵 $A$ 可正交对角化当且仅当 $A$ 是实对称矩阵。即存在正交矩阵 $Q$ 和对角矩阵 $\Lambda$，使得</p><p>$$
A = Q \Lambda Q^\top
$$</p><p>其中 $\Lambda = \operatorname{diag}(\lambda_1, \ldots, \lambda_n)$，$\lambda_i$ 是 $A$ 的特征值，$Q$ 的列是对应的特征向量。</p><p><img alt=对称矩阵的特征向量 loading=lazy src=/images/math/spectral-eigenvectors.png></p><p>图1：对称矩阵的特征向量。橙色线是特征向量方向，蓝色虚线是变换后的特征向量（仍在同一直线上）。椭圆显示单位圆经变换 A 后的形状，长轴和短轴恰好沿特征向量方向。</p><h2 id=第二章谱定理的证明与深入理解>第二章：谱定理的证明与深入理解<a hidden class=anchor aria-hidden=true href=#第二章谱定理的证明与深入理解>#</a></h2><h3 id=21-谱定理的完整表述>2.1 谱定理的完整表述<a hidden class=anchor aria-hidden=true href=#21-谱定理的完整表述>#</a></h3><p><strong>谱定理（实对称矩阵版本）</strong>：</p><p>设 $A \in \mathbb{R}^{n \times n}$ 是对称矩阵，则：</p><ol><li>$A$ 有 $n$ 个实特征值 $\lambda_1, \ldots, \lambda_n$（计入重数）</li><li>存在 $\mathbb{R}^n$ 的一组标准正交基 ${q_1, \ldots, q_n}$，其中每个 $q_i$ 都是 $A$ 的特征向量</li><li>$A$ 可以表示为 $A = \sum_{i=1}^n \lambda_i q_i q_i^\top$</li></ol><h3 id=22-谱定理的证明>2.2 谱定理的证明<a hidden class=anchor aria-hidden=true href=#22-谱定理的证明>#</a></h3><p>我们使用<strong>归纳法</strong>证明谱定理。</p><p><strong>基础情况</strong>（$n=1$）：平凡成立。</p><p><strong>归纳步骤</strong>：假设对 $(n-1) \times (n-1)$ 对称矩阵成立。设 $A$ 是 $n \times n$ 对称矩阵。</p><p><strong>步骤1</strong>：由于特征多项式在复数域上总有根，取 $A$ 的一个特征值 $\lambda_1$（由性质1，$\lambda_1$ 是实数）和对应的单位特征向量 $q_1$。</p><p><strong>步骤2</strong>：将 $q_1$ 扩展为 $\mathbb{R}^n$ 的标准正交基 ${q_1, q_2, \ldots, q_n}$。令 $Q = [q_1 ; q_2 ; \cdots ; q_n]$，则 $Q$ 是正交矩阵。</p><p><strong>步骤3</strong>：考虑 $Q^\top A Q$。计算其第一列：</p><p>$(Q^\top A Q)_{:,1} = Q^\top A q_1 = Q^\top (\lambda_1 q_1) = \lambda_1 Q^\top q_1 = \lambda_1 e_1$</p><p>其中 $e_1 = (1, 0, \ldots, 0)^\top$。</p><p>由于 $Q^\top A Q$ 也是对称矩阵（$(Q^\top A Q)^\top = Q^\top A^\top Q = Q^\top A Q$），它的第一行也必须是 $(\lambda_1, 0, \ldots, 0)$。</p><p>因此</p><p>$$
Q^\top A Q = \begin{pmatrix} \lambda_1 & 0 \ 0 & B \end{pmatrix}
$$</p><p>其中 $B$ 是 $(n-1) \times (n-1)$ 对称矩阵。</p><p><strong>步骤4</strong>：由归纳假设，$B$ 可正交对角化：存在 $(n-1) \times (n-1)$ 正交矩阵 $Q_B$ 使得 $Q_B^\top B Q_B = \Lambda_B$ 是对角矩阵。</p><p><strong>步骤5</strong>：令</p><p>$$
\widetilde{Q} = \begin{pmatrix} 1 & 0 \ 0 & Q_B \end{pmatrix}, \quad \widetilde{Q}^\top (Q^\top A Q) \widetilde{Q} = \begin{pmatrix} \lambda_1 & 0 \ 0 & \Lambda_B \end{pmatrix}
$$</p><p>则 $\widetilde{Q} Q$ 是正交矩阵，且</p><p>$$
(\widetilde{Q} Q)^\top A (\widetilde{Q} Q) = \operatorname{diag}(\lambda_1, \lambda_2, \ldots, \lambda_n)
$$</p><p>证毕。</p><h3 id=23-谱分解的谱系解释>2.3 谱分解的谱系解释<a hidden class=anchor aria-hidden=true href=#23-谱分解的谱系解释>#</a></h3><p>谱定理的另一种表达方式是<strong>谱分解</strong>：</p><p>$$
A = \sum_{i=1}^n \lambda_i q_i q_i^\top = \sum_{i=1}^n \lambda_i P_i
$$</p><p>其中 $P_i = q_i q_i^\top$ 是到特征空间 $\operatorname{span}{q_i}$ 的正交投影算子。</p><p><strong>性质</strong>：</p><ul><li>$P_i^2 = P_i$（幂等性）</li><li>$P_i P_j = 0$（$i \neq j$，正交性）</li><li>$\sum_{i=1}^n P_i = I$（完备性）</li></ul><p><strong>直观理解</strong>：对称矩阵 $A$ 可以分解为沿各个正交方向的"拉伸"的组合，每个方向上的拉伸倍数就是对应的特征值。</p><h2 id=第三章奇异值分解svd推广到任意矩阵>第三章：奇异值分解（SVD）——推广到任意矩阵<a hidden class=anchor aria-hidden=true href=#第三章奇异值分解svd推广到任意矩阵>#</a></h2><h3 id=31-从谱定理到svd>3.1 从谱定理到SVD<a hidden class=anchor aria-hidden=true href=#31-从谱定理到svd>#</a></h3><p>谱定理适用于对称方阵。但实际应用中，我们经常遇到非方阵（如数据矩阵 $X \in \mathbb{R}^{n \times p}$）。奇异值分解（SVD）是谱定理的自然推广。</p><p><strong>定理（SVD）</strong>：任何矩阵 $A \in \mathbb{R}^{m \times n}$ 都可以分解为</p><p>$$
A = U \Sigma V^\top
$$</p><p>其中：</p><ul><li>$U \in \mathbb{R}^{m \times m}$ 是正交矩阵（左奇异向量）</li><li>$V \in \mathbb{R}^{n \times n}$ 是正交矩阵（右奇异向量）</li><li>$\Sigma \in \mathbb{R}^{m \times n}$ 是对角矩阵，对角元素 $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$ 是<strong>奇异值</strong>，$r = \operatorname{rank}(A)$</li></ul><h3 id=32-svd的推导>3.2 SVD的推导<a hidden class=anchor aria-hidden=true href=#32-svd的推导>#</a></h3><p><strong>步骤1</strong>：考虑 $A^\top A \in \mathbb{R}^{n \times n}$，这是一个对称半正定矩阵。</p><p>由谱定理，$A^\top A$ 可正交对角化：</p><p>$$
A^\top A = V \Lambda V^\top
$$</p><p>其中 $V$ 的列 $v_1, \ldots, v_n$ 是特征向量，$\Lambda = \operatorname{diag}(\lambda_1, \ldots, \lambda_n)$。</p><p>由于 $A^\top A$ 半正定，所有 $\lambda_i \geq 0$。</p><p><strong>步骤2</strong>：定义<strong>奇异值</strong> $\sigma_i = \sqrt{\lambda_i}$。</p><p><strong>步骤3</strong>：定义 $u_i = \frac{Av_i}{\sigma_i}$（当 $\sigma_i > 0$）。</p><p>验证 $u_i$ 是单位向量：</p><p>$$
\lVert u_i \rVert^2 = \frac{v_i^\top A^\top A v_i}{\sigma_i^2} = \frac{v_i^\top (\lambda_i v_i)}{\lambda_i} = v_i^\top v_i = 1
$$</p><p>且 $u_i$ 两两正交：</p><p>$$
u_i^\top u_j = \frac{v_i^\top A^\top A v_j}{\sigma_i \sigma_j} = \frac{\lambda_j v_i^\top v_j}{\sigma_i \sigma_j} = 0 \quad (i \neq j)
$$</p><p><strong>步骤4</strong>：将 $u_i$ 扩展为 $\mathbb{R}^m$ 的标准正交基，得到 $U$。</p><p><strong>步骤5</strong>：验证 $A = U \Sigma V^\top$。</p><p>对于任意 $v_j$：</p><p>$$
A v_j = \sigma_j u_j = U (\Sigma e_j) = U \Sigma (V^\top v_j)
$$</p><p>由于 ${v_1, \ldots, v_n}$ 是基，对任意 $x \in \mathbb{R}^n$，$A x = U \Sigma V^\top x$。</p><h3 id=33-svd的几何意义>3.3 SVD的几何意义<a hidden class=anchor aria-hidden=true href=#33-svd的几何意义>#</a></h3><p>SVD告诉我们，任何线性变换 $A : \mathbb{R}^n \to \mathbb{R}^m$ 都可以分解为三个步骤：</p><ol><li><strong>旋转/反射</strong>（$V^\top$）：在 $\mathbb{R}^n$ 中改变坐标系</li><li><strong>伸缩</strong>（$\Sigma$）：沿各坐标轴方向伸缩</li><li><strong>旋转/反射</strong>（$U$）：在 $\mathbb{R}^m$ 中改变坐标系</li></ol><p><strong>直观理解</strong>：$A$ 将单位球映射为一个椭球，奇异值给出椭球的主轴长度，左、右奇异向量给出主轴方向。</p><p><img alt=SVD分解示意 loading=lazy src=/images/math/spectral-svd.png></p><p>图2：SVD分解的矩阵形式。M被分解为U、Σ、Vᵀ三个矩阵的乘积，其中U和V是正交矩阵，Σ是对角矩阵（非方阵时补零）。</p><h2 id=第四章主成分分析pca的谱定理视角>第四章：主成分分析（PCA）的谱定理视角<a hidden class=anchor aria-hidden=true href=#第四章主成分分析pca的谱定理视角>#</a></h2><h3 id=41-pca的问题设定>4.1 PCA的问题设定<a hidden class=anchor aria-hidden=true href=#41-pca的问题设定>#</a></h3><p>给定中心化数据矩阵 $X \in \mathbb{R}^{n \times p}$（$n$ 个样本，$p$ 个特征，每列均值为零），PCA的目标是找到一组正交方向，使得数据在这些方向上的方差最大化。</p><p><strong>问题</strong>：最大化方差</p><p>$$
\max_{w \in \mathbb{R}^p, \lVert w \rVert = 1} \operatorname{Var}(Xw) = \frac{1}{n} \sum_{i=1}^n (x_i^\top w)^2 = \frac{1}{n} \lVert Xw \rVert^2 = \frac{1}{n} w^\top X^\top X w
$$</p><h3 id=42-pca的谱定理推导>4.2 PCA的谱定理推导<a hidden class=anchor aria-hidden=true href=#42-pca的谱定理推导>#</a></h3><p>定义样本协方差矩阵 $C = \frac{1}{n} X^\top X$，这是一个 $p \times p$ 对称半正定矩阵。</p><p>由谱定理，$C$ 可正交对角化：</p><p>$$
C = V \Lambda V^\top
$$</p><p>其中 $\Lambda = \operatorname{diag}(\lambda_1, \ldots, \lambda_p)$，$\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p \geq 0$。</p><p><strong>关键观察</strong>：PCA问题的解恰好是 $C$ 的特征向量。</p><p><strong>证明</strong>：对于单位向量 $w$，令 $v = V^\top w$。由于 $V$ 正交，$\lVert v \rVert = \lVert w \rVert = 1$。</p><p>$$
w^\top C w = w^\top V \Lambda V^\top w = v^\top \Lambda v = \sum_{i=1}^p \lambda_i v_i^2 \leq \lambda_1 \sum_{i=1}^p v_i^2 = \lambda_1
$$</p><p>当 $v = e_1$（即 $w = v_1$，$C$ 的第一特征向量）时等号成立。</p><h3 id=43-pca的几何解释>4.3 PCA的几何解释<a hidden class=anchor aria-hidden=true href=#43-pca的几何解释>#</a></h3><p>PCA寻找的是数据的"主轴"——数据变化最大的方向。这些方向恰好是协方差矩阵的特征向量，特征值大小表示沿该方向的方差大小。</p><p><img alt=特征值与数据分布 loading=lazy src=/images/math/spectral-eigenvalues-dist.png></p><p>图3：PCA的几何意义。数据点的椭圆轮廓显示了数据的分布，橙色箭头是第一主成分（最大方差方向），绿色箭头是第二主成分。虚线椭圆是2σ置信椭圆。</p><h3 id=44-降维与重建>4.4 降维与重建<a hidden class=anchor aria-hidden=true href=#44-降维与重建>#</a></h3><p>保留前 $k$ 个主成分，将数据投影到 $k$ 维子空间：</p><p>$$
X_{\text{projected}} = X V_k
$$</p><p>其中 $V_k = [v_1 ; v_2 ; \cdots ; v_k]$ 包含前 $k$ 个特征向量。</p><p>重建（近似）原始数据：</p><p>$$
X_{\text{reconstructed}} = X_{\text{projected}} V_k^\top = X V_k V_k^\top
$$</p><p><strong>误差分析</strong>：重建误差（Frobenius范数）等于被舍弃特征值的和：</p><p>$$
\lVert X - X V_k V_k^\top \rVert_F^2 = n \sum_{i=k+1}^p \lambda_i
$$</p><p><img alt=PCA降维去噪 loading=lazy src=/images/math/spectral-pca-denoising.png></p><p>图4：PCA降维去噪效果。灰色点是原始噪声数据，橙色点是保留第一主成分后重建的数据，蓝色箭头是第一主成分方向。噪声被有效过滤。</p><h3 id=45-解释方差比>4.5 解释方差比<a hidden class=anchor aria-hidden=true href=#45-解释方差比>#</a></h3><p>第 $k$ 主成分的<strong>解释方差比</strong>为：</p><p>$$
\frac{\lambda_k}{\sum_{i=1}^p \lambda_i}
$$</p><p>前 $k$ 个主成分的<strong>累积解释方差比</strong>为：</p><p>$$
\frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^p \lambda_i}
$$</p><p><img alt=特征值衰减 loading=lazy src=/images/math/spectral-eigenvalue-decay.png></p><p>图5：特征值衰减与累积解释方差。蓝色条形是各主成分的解释方差比，橙色线是累积解释方差比。可以看到，前5个主成分解释了超过95%的方差。</p><h2 id=第五章谱聚类与拉普拉斯矩阵>第五章：谱聚类与拉普拉斯矩阵<a hidden class=anchor aria-hidden=true href=#第五章谱聚类与拉普拉斯矩阵>#</a></h2><h3 id=51-图的谱理论>5.1 图的谱理论<a hidden class=anchor aria-hidden=true href=#51-图的谱理论>#</a></h3><p>给定无向加权图 $G = (V, E, W)$，其中 $W_{ij}$ 是节点 $i$ 和 $j$ 之间的边权重。</p><p><strong>度矩阵</strong>：$D = \operatorname{diag}(d_1, \ldots, d_n)$，其中 $d_i = \sum_{j=1}^n W_{ij}$</p><p><strong>图拉普拉斯矩阵</strong>：$L = D - W$</p><p><strong>性质</strong>：</p><ol><li>$L$ 是对称半正定矩阵</li><li>$L$ 的最小特征值是 $0$，对应的特征向量是全 $1$ 向量</li><li>第二小特征值称为<strong>代数连通度</strong>或<strong>Fiedler值</strong>，对应的特征向量称为<strong>Fiedler向量</strong></li></ol><h3 id=52-谱聚类算法>5.2 谱聚类算法<a hidden class=anchor aria-hidden=true href=#52-谱聚类算法>#</a></h3><p>谱聚类利用拉普拉斯矩阵的特征向量进行聚类：</p><p><strong>算法</strong>：</p><ol><li>构建相似度图（如k近邻图、全连接图）</li><li>计算拉普拉斯矩阵 $L$</li><li>计算 $L$ 的前 $k$ 个特征向量 $u_1, \ldots, u_k$</li><li>将节点嵌入到 $\mathbb{R}^k$：节点 $i$ 映射为 $(u_1(i), \ldots, u_k(i))$</li><li>在嵌入空间中运行k-means聚类</li></ol><p><strong>为什么有效</strong>：拉普拉斯矩阵的特征向量捕获了图的"全局结构"。Fiedler向量的正负性自然地将图分成两个连接紧密的部分。</p><p><img alt=图拉普拉斯矩阵的Fiedler向量 loading=lazy src=/images/math/spectral-graph-laplacian.png></p><p>图6：图拉普拉斯矩阵的Fiedler向量可视化。节点颜色代表Fiedler向量的值，蓝色为负，红色为正。可以看到Fiedler向量自然地将图分成了两组。</p><h3 id=53-谱聚类的直观例子>5.3 谱聚类的直观例子<a hidden class=anchor aria-hidden=true href=#53-谱聚类的直观例子>#</a></h3><p>考虑两个月牙形数据集：传统的基于距离的聚类（如k-means）无法正确分离，但谱聚类可以。</p><p>关键在于：谱聚类不是在原始空间中聚类，而是在"谱空间"中聚类。在谱空间中，原本纠缠的数据点被正确分离。</p><p><img alt=谱聚类结果 loading=lazy src=/images/math/spectral-clustering.png></p><p>图7：谱聚类对两个月牙形数据的聚类结果。蓝色和橙色代表两个不同的簇，谱聚类成功分离了这两个纠缠的月牙形数据。</p><h3 id=54-归一化割normalized-cut>5.4 归一化割（Normalized Cut）<a hidden class=anchor aria-hidden=true href=#54-归一化割normalized-cut>#</a></h3><p>谱聚类与<strong>归一化割</strong>优化问题密切相关：</p><p>$$
\min_{A \subset V} \operatorname{Ncut}(A) = \frac{\operatorname{cut}(A)}{\operatorname{vol}(A)} + \frac{\operatorname{cut}(A)}{\operatorname{vol}(\overline{A})}
$$</p><p>其中 $\operatorname{cut}(A) = \sum_{i \in A, j \notin A} W_{ij}$ 是分割的代价，$\operatorname{vol}(A) = \sum_{i \in A} d_i$ 是节点集合的"体积"。</p><p><strong>定理</strong>：归一化割问题的松弛解恰好是拉普拉斯矩阵的第二小特征向量。</p><h2 id=第六章神经网络中的谱方法>第六章：神经网络中的谱方法<a hidden class=anchor aria-hidden=true href=#第六章神经网络中的谱方法>#</a></h2><h3 id=61-图神经网络gnn中的谱卷积>6.1 图神经网络（GNN）中的谱卷积<a hidden class=anchor aria-hidden=true href=#61-图神经网络gnn中的谱卷积>#</a></h3><p>图卷积网络（GCN）的核心思想是在图上进行卷积操作。经典的卷积定义在欧几里得空间（如图像），但图没有规则的网格结构。</p><p><strong>谱图卷积</strong>利用图拉普拉斯矩阵的特征分解定义卷积：</p><p>设 $L = U \Lambda U^\top$ 是拉普拉斯矩阵的谱分解，信号 $x \in \mathbb{R}^n$ 的傅里叶变换是 $\hat{x} = U^\top x$。</p><p>图上的卷积定义为：</p><p>$$
y = g_\theta * x = U g_\theta(\Lambda) U^\top x
$$</p><p>其中 $g_\theta(\Lambda) = \operatorname{diag}(\theta(\lambda_1), \ldots, \theta(\lambda_n))$ 是频域滤波器。</p><p><strong>ChebNet近似</strong>：直接计算特征分解太昂贵（$O(n^3)$），使用切比雪夫多项式近似：</p><p>$$
g_\theta(\Lambda) \approx \sum_{k=0}^K \theta_k T_k(\widetilde{L})
$$</p><p>其中 $T_k$ 是第 $k$ 阶切比雪夫多项式，$\widetilde{L} = \frac{2}{\lambda_{\max}} L - I$ 是缩放的拉普拉斯矩阵。</p><h3 id=62-gcn的一阶近似>6.2 GCN的一阶近似<a hidden class=anchor aria-hidden=true href=#62-gcn的一阶近似>#</a></h3><p>Kipf & Welling (2017) 提出了更简单的一阶近似：</p><p>$$
y = D^{-1/2} \widetilde{A} D^{-1/2} X \Theta
$$</p><p>其中 $\widetilde{A} = A + I$ 是加入自环的邻接矩阵。</p><p>这个公式可以理解为：每个节点的表示是其邻居表示的加权平均，权重由图的度决定。</p><h3 id=63-注意力机制的谱视角>6.3 注意力机制的谱视角<a hidden class=anchor aria-hidden=true href=#63-注意力机制的谱视角>#</a></h3><p>Transformer中的注意力机制也可以从谱的角度理解。自注意力矩阵 $S \in \mathbb{R}^{n \times n}$ 定义为：</p><p>$$
S_{ij} = \frac{\exp(q_i^\top k_j)}{\sum_{l=1}^n \exp(q_i^\top k_l)}
$$</p><p>这可以看作是在动态构建的图上的消息传递，图的权重由注意力得分决定。</p><p><strong>谱归一化</strong>：为了稳定训练，可以对注意力矩阵的奇异值进行约束：</p><p>$$
S \leftarrow \frac{S}{\sigma_{\max}(S)}
$$</p><h3 id=64-激活函数的谱分析>6.4 激活函数的谱分析<a hidden class=anchor aria-hidden=true href=#64-激活函数的谱分析>#</a></h3><p>ReLU激活函数 $f(x) = \max(0, x)$ 的谱性质很重要。考虑其在正交变换下的行为：</p><p>如果 $W$ 是随机正交矩阵，则 $\operatorname{E}[\lVert f(Wx) \rVert^2] = \frac{1}{2}\lVert x \rVert^2$（假设 $x$ 的各分量独立对称分布）。</p><p>这解释了为什么需要<strong>He初始化</strong>：$W \sim \mathcal{N}(0, \frac{2}{n_{in}})$，以保持信号在通过ReLU后方差不变。</p><h2 id=第七章数值方法与应用案例>第七章：数值方法与应用案例<a hidden class=anchor aria-hidden=true href=#第七章数值方法与应用案例>#</a></h2><h3 id=71-特征值计算算法>7.1 特征值计算算法<a hidden class=anchor aria-hidden=true href=#71-特征值计算算法>#</a></h3><p><strong>幂法</strong>：计算最大特征值和对应特征向量</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>初始化：随机向量 b(0)
</span></span><span class=line><span class=cl>重复：b(k+1) = A b(k) / ||A b(k)||
</span></span><span class=line><span class=cl>收敛：b(k) → 主特征向量，||A b(k)|| → 主特征值
</span></span></code></pre></div><p><strong>收敛速度</strong>：取决于 $|\lambda_2 / \lambda_1|$（次大特征值与最大特征值的比值之比）。</p><p><strong>QR算法</strong>：计算所有特征值</p><p>通过QR迭代将矩阵逐步上三角化：</p><p>$$
A_0 = A \
A_k = Q_k R_k \quad \text{（QR分解）} \
A_{k+1} = R_k Q_k
$$</p><p>对于对称矩阵，$A_k$ 收敛到对角矩阵（特征值在主对角线上）。</p><h3 id=72-svd在图像压缩中的应用>7.2 SVD在图像压缩中的应用<a hidden class=anchor aria-hidden=true href=#72-svd在图像压缩中的应用>#</a></h3><p>图像可以看作矩阵 $I \in \mathbb{R}^{m \times n}$（像素值）。SVD的低秩近似：</p><p>$$
I \approx I_k = \sum_{i=1}^k \sigma_i u_i v_i^\top
$$</p><p>其中 $k \ll \min(m, n)$。</p><p><strong>压缩比</strong>：存储 $I_k$ 需要 $k(m + n + 1)$ 个数，而原始图像需要 $mn$ 个数。</p><p>当 $k = \frac{mn}{m+n}$ 时，压缩比约为 50%。</p><h3 id=73-协同过滤中的矩阵分解>7.3 协同过滤中的矩阵分解<a hidden class=anchor aria-hidden=true href=#73-协同过滤中的矩阵分解>#</a></h3><p>推荐系统中的用户-物品评分矩阵 $R \in \mathbb{R}^{m \times n}$ 通常非常稀疏。协同过滤假设 $R$ 可以分解为：</p><p>$$
R \approx U V^\top
$$</p><p>其中 $U \in \mathbb{R}^{m \times k}$ 是用户隐因子矩阵，$V \in \mathbb{R}^{n \times k}$ 是物品隐因子矩阵。</p><p>这与截断SVD密切相关，但需要处理缺失值（通常通过交替最小二乘或随机梯度下降求解）。</p><h3 id=74-pagerank算法>7.4 PageRank算法<a hidden class=anchor aria-hidden=true href=#74-pagerank算法>#</a></h3><p>PageRank是谷歌早期的核心算法，用于衡量网页的重要性。可以理解为马尔可夫链的稳态分布。</p><p>设 $A$ 是网页的邻接矩阵（$A_{ij} = 1$ 如果页面 $j$ 链接到页面 $i$），归一化后得到转移矩阵 $P$。</p><p>PageRank向量 $r$ 满足：</p><p>$$
r = (1-d) \frac{1}{n} \mathbf{1} + d P^\top r
$$</p><p>其中 $d \approx 0.85$ 是阻尼因子。</p><p>这等价于求矩阵 $M = d P^\top + \frac{1-d}{n} \mathbf{1} \mathbf{1}^\top$ 的主特征向量。</p><h2 id=第八章总结与展望>第八章：总结与展望<a hidden class=anchor aria-hidden=true href=#第八章总结与展望>#</a></h2><h3 id=81-核心要点回顾>8.1 核心要点回顾<a hidden class=anchor aria-hidden=true href=#81-核心要点回顾>#</a></h3><p>谱定理是连接线性代数、几何分析和机器学习的桥梁：</p><ol><li><strong>谱定理</strong>：实对称矩阵可正交对角化，特征向量构成标准正交基</li><li><strong>SVD</strong>：谱定理向任意矩阵的推广，奇异值是"信息的强度"</li><li><strong>PCA</strong>：从谱定理视角看，PCA就是协方差矩阵的特征分解</li><li><strong>谱聚类</strong>：利用拉普拉斯矩阵的特征向量发现图的社区结构</li><li><strong>图神经网络</strong>：谱卷积是图上卷积的理论基础</li></ol><h3 id=82-理论与实践的平衡>8.2 理论与实践的平衡<a hidden class=anchor aria-hidden=true href=#82-理论与实践的平衡>#</a></h3><p>在理论层面，谱定理提供了优美的数学结构：对称性导致可对角化，特征值编码了变换的本质信息。</p><p>在实践层面，我们需要考虑：</p><ul><li>计算复杂度：特征分解是 $O(n^3)$</li><li>数值稳定性：病态矩阵需要特殊处理</li><li>近似方法：随机SVD、Lanczos算法、幂迭代</li></ul><h3 id=83-前沿方向>8.3 前沿方向<a hidden class=anchor aria-hidden=true href=#83-前沿方向>#</a></h3><p><strong>深度学习的理论理解</strong>：</p><ul><li>为什么深度网络可以学习？可能与特征值衰减有关</li><li>梯度消失/爆炸与雅可比矩阵的谱性质密切相关</li><li>谱归一化作为正则化手段</li></ul><p><strong>图神经网络的新方向</strong>：</p><ul><li>超过一阶的图卷积（但容易过平滑）</li><li>自适应图结构学习</li><li>结合注意力机制</li></ul><p><strong>大规模特征值计算</strong>：</p><ul><li>随机算法（如Halko等人的随机SVD）</li><li>分布式特征值分解</li><li>量子计算在谱分析中的应用</li></ul><h3 id=84-结语>8.4 结语<a hidden class=anchor aria-hidden=true href=#84-结语>#</a></h3><p>谱定理之所以优雅，是因为它揭示了线性代数的一个核心真理：<strong>对称性带来简化</strong>。在机器学习和深度学习的复杂算法背后，谱定理提供了坚实的数学基础。</p><p>从PCA降维到谱聚类，从图像压缩到PageRank，从图神经网络到注意力机制，谱定理的身影无处不在。理解谱定理，就是理解了数据结构的"骨架"——那些不随坐标系变化而变化的本质特征。</p><p>希望这篇文章能帮助读者建立对谱定理的系统认识，为进一步学习机器学习理论和算法打下坚实基础。记住：在复杂的数据世界中，谱定理是我们的指南针，指引我们找到最本质的结构。</p><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><ol><li>Strang, G. (2016). <em>Introduction to Linear Algebra</em> (5th ed.). Wellesley-Cambridge Press.</li><li>Trefethen, L. N., & Bau, D. (1997). <em>Numerical Linear Algebra</em>. SIAM.</li><li>Boyd, S., & Vandenberghe, L. (2018). <em>Introduction to Applied Linear Algebra</em>. Cambridge University Press.</li><li>Von Luxburg, U. (2007). A Tutorial on Spectral Clustering. <em>Statistics and Computing</em>, 17(4), 395-416.</li><li>Kipf, T. N., & Welling, M. (2017). Semi-Supervised Classification with Graph Convolutional Networks. <em>ICLR</em>.</li><li>Hastie, T., Tibshirani, R., & Friedman, J. (2009). <em>The Elements of Statistical Learning</em> (2nd ed.). Springer.</li><li>Golub, G. H., & Van Loan, C. F. (2013). <em>Matrix Computations</em> (4th ed.). Johns Hopkins University Press.</li><li>Belkin, M., & Niyogi, P. (2003). Laplacian Eigenmaps for Dimensionality Reduction and Data Representation. <em>Neural Computation</em>, 15(6), 1373-1396.</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/>线性代数</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E8%B0%B1%E5%AE%9A%E7%90%86/>谱定理</a></li><li><a href=https://s-ai-unix.github.io/tags/pca/>PCA</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%AE%97%E6%B3%95/>算法</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-25-calculus-ml-systematic-review/><span class=title>« Prev</span><br><span>微积分与机器学习：从变化率到神经网络梯度的完整旅程</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-25-riemann-geometry/><span class=title>Next »</span><br><span>黎曼几何：弯曲空间的优雅语言</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 谱定理：线性代数的优雅与机器学习的基石 on x" href="https://x.com/intent/tweet/?text=%e8%b0%b1%e5%ae%9a%e7%90%86%ef%bc%9a%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%e7%9a%84%e4%bc%98%e9%9b%85%e4%b8%8e%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%9f%ba%e7%9f%b3&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-25-spectral-theorem%2f&amp;hashtags=%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%2c%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%2c%e8%b0%b1%e5%ae%9a%e7%90%86%2cPCA%2c%e7%ae%97%e6%b3%95"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 谱定理：线性代数的优雅与机器学习的基石 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-25-spectral-theorem%2f&amp;title=%e8%b0%b1%e5%ae%9a%e7%90%86%ef%bc%9a%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%e7%9a%84%e4%bc%98%e9%9b%85%e4%b8%8e%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%9f%ba%e7%9f%b3&amp;summary=%e8%b0%b1%e5%ae%9a%e7%90%86%ef%bc%9a%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%e7%9a%84%e4%bc%98%e9%9b%85%e4%b8%8e%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%9f%ba%e7%9f%b3&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-25-spectral-theorem%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 谱定理：线性代数的优雅与机器学习的基石 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-25-spectral-theorem%2f&title=%e8%b0%b1%e5%ae%9a%e7%90%86%ef%bc%9a%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%e7%9a%84%e4%bc%98%e9%9b%85%e4%b8%8e%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%9f%ba%e7%9f%b3"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 谱定理：线性代数的优雅与机器学习的基石 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-25-spectral-theorem%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>