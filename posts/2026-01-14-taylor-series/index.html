<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>泰勒公式：用简单近似复杂的艺术 | s-ai-unix's Blog</title><meta name=keywords content="数学史,机器学习,深度学习,算法"><meta name=description content="从微积分基础到深度学习前沿，探索泰勒公式的强大威力"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-14-taylor-series/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-14-taylor-series/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-14-taylor-series/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="泰勒公式：用简单近似复杂的艺术"><meta property="og:description" content="从微积分基础到深度学习前沿，探索泰勒公式的强大威力"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-14T22:10:00+08:00"><meta property="article:modified_time" content="2026-01-14T22:10:00+08:00"><meta property="article:tag" content="数学史"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="算法"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/unsplash-taylor.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/unsplash-taylor.jpg"><meta name=twitter:title content="泰勒公式：用简单近似复杂的艺术"><meta name=twitter:description content="从微积分基础到深度学习前沿，探索泰勒公式的强大威力"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"泰勒公式：用简单近似复杂的艺术","item":"https://s-ai-unix.github.io/posts/2026-01-14-taylor-series/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"泰勒公式：用简单近似复杂的艺术","name":"泰勒公式：用简单近似复杂的艺术","description":"从微积分基础到深度学习前沿，探索泰勒公式的强大威力","keywords":["数学史","机器学习","深度学习","算法"],"articleBody":"引言：从曲线到直线 想象你站在一座山上，想知道脚下的山坡有多陡。你不需要知道整个山脉的形状，只需要知道你所在位置的局部斜率。这是微积分最基本的思想——用局部信息推断全局行为。\n更进一步，如果山坡弯曲了怎么办？这时不仅需要知道斜率，还需要知道弯曲的程度。这就是泰勒公式的核心思想：用最简单的函数（多项式）来近似复杂的函数，而近似的质量取决于我们使用多少局部信息（导数）。\n泰勒公式被誉为\"数学家最有力的工具之一\"。它不仅连接了离散与连续、局部与整体，更在数值计算、物理建模和现代人工智能中扮演着不可替代的角色。今天，让我们深入探索这个既古老又常新的数学宝藏。\n一、历史回顾：从牛顿到泰勒 泰勒公式的思想可以追溯到牛顿和莱布尼茨创立微积分的时期。牛顿在他的《流数术》中已经隐含了将函数展开为无穷级数的想法。\n布鲁克·泰勒（Brook Taylor，1685-1731）在1715年发表了他的开创性论文《增量法及其逆运算》，首次系统地阐述了用多项式级数逼近函数的方法。有趣的是，泰勒本人并没有意识到他发现的公式的全部潜力，余项的研究（拉格朗日余项、柯西余项等）是后来由拉格朗日等数学家完善的。\n麦克劳林（Colin Maclaurin）发现了泰勒公式在零点展开的特例，即麦克劳林级数。这个形式在实际计算中更为常用，因为计算起来更加方便。\n二、一元函数的泰勒公式 基本形式 假设函数 $f(x)$ 在点 $a$ 处足够光滑（即具有各阶导数），那么我们可以构造一个多项式 $P_n(x)$ 来近似 $f(x)$：\n$$ P_n(x) = f(a) + f’(a)(x-a) + \\frac{f’’(a)}{2!}(x-a)^2 + \\cdots + \\frac{f^{(n)}(a)}{n!}(x-a)^n $$\n泰勒公式告诉我们：\n$$ f(x) = P_n(x) + R_n(x) $$\n其中 $R_n(x)$ 是余项，表示近似误差。\n余项的几种形式 理解余项对于掌握泰勒公式至关重要，因为它告诉我们近似在什么范围内可靠。\n拉格朗日余项：\n$$ R_n(x) = \\frac{f^{(n+1)}(\\xi)}{(n+1)!}(x-a)^{n+1} $$\n其中 $\\xi$ 是 $a$ 和 $x$ 之间的某个值。\n积分余项：\n$$ R_n(x) = \\frac{1}{n!} \\int_a^x f^{(n+1)}(t)(x-t)^n , dt $$\n直观理解 让我们通过一个简单的例子来理解泰勒公式。考虑 $f(x) = e^x$ 在 $a = 0$ 处的泰勒展开（即麦克劳林级数）：\n零阶近似：$e^x \\approx f(0) = 1$ 这是最粗糙的近似，假设函数是常数。\n一阶近似：$e^x \\approx 1 + x$ 这是线性近似，假设函数在局部是一条直线。\n二阶近似：$e^x \\approx 1 + x + \\frac{x^2}{2}$ 这个近似考虑了函数的弯曲程度。\nn阶近似：$e^x \\approx 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots + \\frac{x^n}{n!}$\n当 $n \\to \\infty$ 时，我们得到泰勒级数： $$ e^x = \\sum_{n=0}^{\\infty} \\frac{x^n}{n!} $$\n这个级数对所有实数 $x$ 都收敛。\n几何解释 泰勒公式的几何解释非常优美。每一次增加一项，我们就在更好地拟合曲线：\n零阶项：常数项，确保在 $x=a$ 处函数值正确 一阶项：线性项，确保在 $x=a$ 处切线斜率正确 二阶项：二次项，确保在 $x=a$ 处曲率正确 更高阶项：确保更高阶的变化率正确 可以想象，每增加一项，我们的多项式就像曲线一样，在展开点附近\"缠绕\"得更紧密。\n三、多元函数的泰勒公式 在现代应用中，我们经常需要处理多元函数。泰勒公式自然地推广到多元情况。\n二元函数的泰勒展开 对于二元函数 $f(x, y)$，在点 $(a, b)$ 处的二阶泰勒展开为：\n$$ f(x, y) \\approx f(a, b) + \\frac{\\partial f}{\\partial x}\\bigg|{(a,b)}(x-a) + \\frac{\\partial f}{\\partial y}\\bigg|{(a,b)}(y-b) $$ $$ + \\frac{1}{2}\\left[\\frac{\\partial^2 f}{\\partial x^2}\\bigg|{(a,b)}(x-a)^2 + 2\\frac{\\partial^2 f}{\\partial x \\partial y}\\bigg|{(a,b)}(x-a)(y-b) + \\frac{\\partial^2 f}{\\partial y^2}\\bigg|_{(a,b)}(y-b)^2\\right] $$\n我们可以用更简洁的矩阵形式表示：\n$$ f(\\mathbf{x}) \\approx f(\\mathbf{a}) + \\nabla f(\\mathbf{a})^{\\top}(\\mathbf{x} - \\mathbf{a}) + \\frac{1}{2}(\\mathbf{x} - \\mathbf{a})^{\\top} H(\\mathbf{a})(\\mathbf{x} - \\mathbf{a}) $$\n其中：\n$\\nabla f(\\mathbf{a})$ 是梯度向量 $H(\\mathbf{a})$ 是海森矩阵（Hessian matrix） 海森矩阵的作用 海森矩阵是多元函数二阶偏导数的矩阵： $$ H = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} \u0026 \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \u0026 \\cdots \\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} \u0026 \\frac{\\partial^2 f}{\\partial x_2^2} \u0026 \\cdots \\ \\vdots \u0026 \\vdots \u0026 \\ddots \\end{pmatrix} $$\n海森矩阵包含了函数的二阶导数信息，它告诉我们：\n函数在各个方向上的弯曲程度（对角元素） 不同方向之间的弯曲耦合关系（非对角元素） 在优化问题中，海森矩阵的正定性决定了临界点的性质（极小值、极大值或鞍点）。\n四、常见函数的泰勒级数 让我们列举一些经典函数的泰勒级数展开（在 $x=0$ 处）：\n三角函数 正弦函数： $$ \\sin x = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\cdots = \\sum_{n=0}^{\\infty} (-1)^n \\frac{x^{2n+1}}{(2n+1)!} $$\n余弦函数： $$ \\cos x = 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!} + \\cdots = \\sum_{n=0}^{\\infty} (-1)^n \\frac{x^{2n}}{(2n)!} $$\n这些级数对所有实数 $x$ 都收敛，展示了周期函数如何用非周期函数的多项式来逼近。\n指数函数 $$ e^x = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots = \\sum_{n=0}^{\\infty} \\frac{x^n}{n!} $$\n通过欧拉公式 $e^{ix} = \\cos x + i \\sin x$，我们可以用指数函数的泰勒级数来验证三角函数的泰勒级数，这是复分析中的一个美妙联系。\n对数函数 $$ \\ln(1+x) = x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\frac{x^4}{4} + \\cdots = \\sum_{n=1}^{\\infty} (-1)^{n-1} \\frac{x^n}{n}, \\quad -1 \u003c x \\leq 1 $$\n注意这个级数只在 $x \\in (-1, 1]$ 上收敛，显示了泰勒级数的收敛域的重要性。\n幂函数 $$ (1+x)^\\alpha = 1 + \\alpha x + \\frac{\\alpha(\\alpha-1)}{2!}x^2 + \\frac{\\alpha(\\alpha-1)(\\alpha-2)}{3!}x^3 + \\cdots $$\n这是广义二项式定理的级数形式。当 $\\alpha$ 是正整数时，级数只有有限项（这正是二项式定理）；当 $\\alpha$ 不是正整数时，我们得到无穷级数。\n五、泰勒级数的收敛性 并非所有光滑函数的泰勒级数都收敛到原函数。这是一个深刻且反直觉的事实。\n解析函数与光滑函数的区别 如果一个函数的泰勒级数在某个区间内收敛到该函数本身，我们称这个函数为解析函数。然而，存在光滑（各阶导数都存在）但非解析的函数。\n经典例子：$f(x) = \\begin{cases} e^{-1/x^2}, \u0026 x \\neq 0 \\ 0, \u0026 x = 0 \\end{cases}$\n可以证明，这个函数在 $x=0$ 处的所有导数都是零，因此它的泰勒级数恒为零，但在 $x \\neq 0$ 处函数值不为零。这是光滑但非解析的经典例子。\n收敛半径 对于幂级数 $\\sum_{n=0}^{\\infty} a_n (x-a)^n$，存在一个收敛半径 $R$，使得：\n当 $|x-a| \u003c R$ 时，级数绝对收敛 当 $|x-a| \u003e R$ 时，级数发散 收敛半径可以通过比值法或根值法计算：\n$$ R = \\lim_{n \\to \\infty} \\left|\\frac{a_n}{a_{n+1}}\\right| \\quad \\text{或} \\quad R = \\frac{1}{\\limsup_{n \\to \\infty} \\sqrt[n]{|a_n|}} $$\n六、数值分析中的应用 函数求值 泰勒公式是计算复杂函数值的强大工具。例如，计算 $\\sin(0.1)$：\n$$ \\sin(0.1) \\approx 0.1 - \\frac{0.1^3}{6} = 0.1 - 0.0001667 = 0.0998333 $$\n实际值约为 $0.0998334$，近似非常精确！\n数值积分 在计算定积分 $\\int_a^b f(x) dx$ 时，如果被积函数比较复杂，我们可以用泰勒展开来近似：\n$$ \\int_a^b f(x) dx \\approx \\int_a^b P_n(x) dx $$\n例如，计算 $\\int_0^{0.1} \\sin(x^2) dx$：\n令 $u = x^2$，当 $x$ 很小时： $$ \\sin(x^2) = \\sin u \\approx u - \\frac{u^3}{6} = x^2 - \\frac{x^6}{6} $$\n因此： $$ \\int_0^{0.1} \\sin(x^2) dx \\approx \\int_0^{0.1} \\left(x^2 - \\frac{x^6}{6}\\right) dx = \\left[\\frac{x^3}{3} - \\frac{x^7}{42}\\right]_0^{0.1} = \\frac{0.001}{3} - \\frac{10^{-7}}{42} \\approx 0.0003333 $$\n数值微分 泰勒公式可以用于近似导数。例如，对于小量 $h$：\n$$ f’(x) \\approx \\frac{f(x+h) - f(x)}{h} $$\n这是前向差分公式，其误差为 $O(h)$。通过泰勒展开，我们可以得到更高精度的公式：\n中心差分公式： $$ f’(x) \\approx \\frac{f(x+h) - f(x-h)}{2h} $$\n误差为 $O(h^2)$，比前向差分更精确。\n误差估计 泰勒公式的余项为我们提供了近似误差的严格界限。例如，用 $P_1(x) = 1 + x$ 近似 $e^x$ 在区间 $[0, 0.1]$ 上的误差：\n由拉格朗日余项： $$ |R_1(x)| = \\left|\\frac{e^\\xi}{2!}x^2\\right| \\leq \\frac{e^{0.1}}{2} \\times 0.01 \\approx 0.0055 $$\n这告诉我们近似误差不会超过 $0.0055$。\n七、优化理论中的应用 泰勒公式在优化理论中处于核心地位。\n最优性条件 对于无约束优化问题 $\\min_{\\mathbf{x}} f(\\mathbf{x})$，一阶必要条件是：\n$$ \\nabla f(\\mathbf{x}^{\\ast}) = \\mathbf{0} $$\n这个条件的直观理解可以从泰勒展开中看出。设 $\\mathbf{x}^{\\ast}$ 是局部极小值点，考虑 $f(\\mathbf{x}^{\\ast} + \\mathbf{h})$ 的一阶泰勒展开：\n$$ f(\\mathbf{x}^{\\ast} + \\mathbf{h}) \\approx f(\\mathbf{x}^{\\ast}) + \\nabla f(\\mathbf{x}^{\\ast})^{\\top} \\mathbf{h} $$\n如果 $\\nabla f(\\mathbf{x}^{\\ast}) \\neq \\mathbf{0}$，我们可以选择 $\\mathbf{h} = -\\alpha \\nabla f(\\mathbf{x}^{\\ast})$（$\\alpha \u003e 0$ 很小），使得 $f(\\mathbf{x}^{\\ast} + \\mathbf{h}) \u003c f(\\mathbf{x}^{\\ast})$，这与 $\\mathbf{x}^{\\ast}$ 是局部极小值矛盾。因此，$\\nabla f(\\mathbf{x}^{\\ast})$ 必须为零。\n二阶充分条件 如果 $\\nabla f(\\mathbf{x}^{\\ast}) = \\mathbf{0}$ 且海森矩阵 $H(\\mathbf{x}^{\\ast})$ 是正定的（所有特征值大于零），则 $\\mathbf{x}^{\\ast}$ 是严格局部极小值点。\n从二阶泰勒展开：\n$$ f(\\mathbf{x}^{\\ast} + \\mathbf{h}) \\approx f(\\mathbf{x}^{\\ast}) + \\frac{1}{2} \\mathbf{h}^{\\top} H(\\mathbf{x}^{\\ast}) \\mathbf{h} $$\n如果 $H$ 正定，则 $\\mathbf{h}^{\\top} H \\mathbf{h} \u003e 0$ 对所有非零 $\\mathbf{h}$ 成立，因此 $f(\\mathbf{x}^{\\ast} + \\mathbf{h}) \u003e f(\\mathbf{x}^{\\ast})$。\n牛顿法 牛顿法是求解方程 $f(x) = 0$ 的经典方法，它利用泰勒展开的线性近似。\n假设我们已经有近似解 $x_n$，将 $f(x)$ 在 $x_n$ 处线性展开：\n$$ f(x) \\approx f(x_n) + f’(x_n)(x - x_n) $$\n令这个线性近似为零，解得： $$ x_{n+1} = x_n - \\frac{f(x_n)}{f’(x_n)} $$\n这就是牛顿法的迭代公式。在优化问题中，我们需要求解 $\\nabla f(\\mathbf{x}) = \\mathbf{0}$，对应的牛顿法迭代为：\n$$ \\mathbf{x}_{n+1} = \\mathbf{x}_n - H(\\mathbf{x}_n)^{-1} \\nabla f(\\mathbf{x}_n) $$\n牛顿法具有二次收敛速度（每一步迭代，有效数字大约翻倍），但需要计算海森矩阵的逆，计算成本较高。\n共轭梯度法与拟牛顿法 为了避免直接计算海森矩阵的逆，发展了共轭梯度法和拟牛顿法（如BFGS算法）。这些方法利用泰勒展开的思想，通过梯度信息来近似海森矩阵，在大规模优化问题中非常有效。\n八、机器学习中的应用 特征空间的非线性映射 泰勒公式可以将非线性问题近似为线性问题。例如，在支持向量机（SVM）中，核技巧的某些理解可以从泰勒展开中获得启发。\n考虑径向基函数核 $K(\\mathbf{x}, \\mathbf{y}) = \\exp(-|\\mathbf{x} - \\mathbf{y}|^2/2\\sigma^2)$。当 $\\sigma$ 很大时，我们可以展开为：\n$$ K(\\mathbf{x}, \\mathbf{y}) \\approx 1 - \\frac{|\\mathbf{x} - \\mathbf{y}|^2}{2\\sigma^2} + \\cdots $$\n这揭示了核函数与多项式特征映射之间的联系。\n梯度下降法的分析 梯度下降法是机器学习中最基础的优化算法。利用泰勒展开，我们可以分析其收敛性。\n考虑目标函数 $f(\\mathbf{x})$，在当前点 $\\mathbf{x}_k$ 处的一阶泰勒展开：\n$$ f(\\mathbf{x}_k + \\alpha \\mathbf{d}_k) \\approx f(\\mathbf{x}_k) + \\alpha \\nabla f(\\mathbf{x}_k)^{\\top} \\mathbf{d}_k $$\n其中 $\\alpha$ 是步长，$\\mathbf{d}_k$ 是搜索方向。对于梯度下降法，$\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)$，因此：\n$$ f(\\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k)) \\approx f(\\mathbf{x}_k) - \\alpha |\\nabla f(\\mathbf{x}_k)|^2 $$\n只要 $\\alpha$ 足够小且 $\\nabla f(\\mathbf{x}_k) \\neq \\mathbf{0}$，目标函数值就会下降。这保证了梯度下降法的每一步（在适当步长下）都能改进目标函数。\nAdaGrad算法 AdaGrad是自适应学习率的优化算法。其核心思想是对每个参数使用不同的学习率，学习率与过去梯度的平方和成反比。这可以用泰勒展开来理解。\n考虑在参数空间中，不同方向上的曲率不同。海森矩阵的特征值衡量了各个方向上的曲率。如果我们能估计曲率，就可以在曲率大的方向上使用更小的步长，在曲率小的方向上使用更大的步长。\nAdaGrad用过去梯度的统计信息来近似曲率信息，这在一定程度上等价于二阶优化方法。\n高斯过程 高斯过程是一种非参数贝叶斯模型，它基于泰勒展开的思想来预测函数值及其不确定性。\n高斯过程假设函数是高斯随机过程，因此任意有限点的函数值服从多元高斯分布。训练后，对于新的输入 $\\mathbf{x}_{\\ast}$，预测分布可以通过条件概率计算：\n$$ p(f_{\\ast}|\\mathbf{X}, \\mathbf{y}, \\mathbf{x}{\\ast}) = \\mathcal{N}(\\mu{\\ast}, \\sigma_{\\ast}^2) $$\n预测均值 $\\mu_{\\ast}$ 本质上是对训练数据点的加权平均，权重由核函数（即协方差函数）决定。这与泰勒展开用局部信息推断全局的思想是一致的。\n九、深度学习中的应用 泰勒公式在深度学习中的应用非常广泛和深入。\n反向传播的泰勒展开解释 反向传播算法是深度学习的核心，它高效地计算损失函数对每个参数的梯度。我们可以用泰勒展开来理解反向传播的原理。\n考虑损失函数 $L$ 关于权重 $W$ 的函数。在训练过程中，我们想要最小化 $L$。通过反向传播，我们计算 $\\frac{\\partial L}{\\partial W}$。\n从泰勒展开的角度，反向传播实际上是在计算一阶导数信息。反向传播的高效性来自于链式法则的巧妙组织，它避免了重复计算。\n二阶优化方法 虽然梯度下降和反向传播基于一阶导数，但二阶信息（海森矩阵）可以显著加速训练。\n牛顿法在深度学习中的应用： $$ W_{new} = W_{old} - H^{-1} \\nabla L $$\n其中 $H$ 是损失函数关于参数的海森矩阵。牛顿法考虑了函数的曲率信息，收敛速度更快。\n然而，海森矩阵的维度与参数数量平方成正比，对于现代深度学习模型（可能有数亿个参数），直接存储和求逆海森矩阵是不现实的。\n拟牛顿法和K-FAC 为了在深度学习中利用二阶信息，发展了拟牛顿法的各种变体。K-FAC（Kronecker-Factored Approximate Curvature）是一种重要的近似方法。\nK-FAC的核心思想是利用深度神经网络结构的特殊性，对海森矩阵进行近似分解。对于全连接层，海森矩阵可以近似为Kronecker乘积的形式：\n$$ H \\approx A \\otimes G $$\n其中 $A$ 与激活值相关，$G$ 与梯度相关。这种近似使得海森矩阵的求逆变得可行。\n损失函数景观分析 深度神经网络的损失函数景观非常复杂，有大量的鞍点和局部极小值。泰勒展开可以帮助我们分析这些临界点的性质。\n在临界点 $\\mathbf{W}^{\\ast}$ 处（$\\nabla L(\\mathbf{W}^{\\ast}) = \\mathbf{0}$），损失函数的二阶泰勒展开为：\n$$ L(\\mathbf{W}^{\\ast} + \\mathbf{h}) \\approx L(\\mathbf{W}^{\\ast}) + \\frac{1}{2} \\mathbf{h}^{\\top} H(\\mathbf{W}^{\\ast}) \\mathbf{h} $$\n海森矩阵的特征值分布告诉我们临界点的类型：\n所有特征值大于零：局部极小值 所有特征值小于零：局部极大值 有正有负：鞍点 现代研究表明，高维神经网络的损失函数景观中，鞍点比局部极小值更普遍。这解释了为什么梯度下降法在实践中仍然有效——它更容易逃离鞍点而不是被困在糟糕的局部极小值中。\n扰动敏感性分析 泰勒展开可以用来分析神经网络对输入扰动的敏感性。这对于理解对抗攻击和鲁棒性很重要。\n设原始输入为 $\\mathbf{x}$，扰动后的输入为 $\\mathbf{x} + \\delta$。网络输出 $f(\\mathbf{x})$ 的一阶泰勒展开：\n$$ f(\\mathbf{x} + \\delta) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^{\\top} \\delta $$\n这告诉我们，如果扰动 $\\delta$ 的方向与梯度方向一致，输出的变化最大。对抗攻击正是利用了这一点，通过精心设计的微小扰动来最大化输出变化。\n网络压缩与剪枝 泰勒展开可以用来评估每个神经元或连接的重要性，从而指导网络压缩和剪枝。\n考虑损失函数关于某个参数 $w_i$ 的函数。如果移除这个参数（设 $w_i = 0$），损失的增量可以用泰勒展开估计：\n$$ \\Delta L \\approx \\frac{\\partial L}{\\partial w_i} (-w_i) + \\frac{1}{2} \\frac{\\partial^2 L}{\\partial w_i^2} w_i^2 $$\n在训练好的网络中，$\\frac{\\partial L}{\\partial w_i} \\approx 0$（接近最优），因此：\n$$ \\Delta L \\approx \\frac{1}{2} \\frac{\\partial^2 L}{\\partial w_i^2} w_i^2 $$\n这个量可以作为参数重要性的指标，用于剪枝策略。\n十、总结：近似的智慧 泰勒公式是微积分皇冠上的明珠之一。它用最简单的多项式函数，去逼近任意复杂的光滑函数。这种\"以简驭繁\"的思想，贯穿了整个数学和工程实践。\n从历史上看，泰勒公式连接了离散与连续、局部与整体。牛顿用二项式定理处理函数流数，泰勒将其系统化为一般方法。麦克劳林发现了在零点展开的特例，拉格朗日完善了余项理论。每一位数学家都在前人的基础上，推动着这个工具的完善。\n从应用上看，泰勒公式在数值计算、物理建模、机器学习和深度学习中扮演着不可替代的角色。无论是计算器计算 $\\sin(0.1)$ 的值，还是神经网络优化算法的设计，都能看到泰勒公式的影子。\n从哲学上看，泰勒公式体现了一种深刻的认知方式：局部通向整体。我们通过理解一点的局部行为（导数），推断函数在附近的行为，进而外推到更远的区域。这种从局部到整体、从已知到未知的推理模式，正是科学方法的核心。\n理解泰勒公式，不仅是掌握一个数学工具，更是培养一种思维习惯。在复杂中寻找简单，在变化中寻找不变，在混沌中寻找秩序。这或许就是数学最动人的力量——用最抽象的语言，讲述着最具体的故事。\n参考资料：\nJames Stewart, Calculus: Early Transcendentals Stephen Boyd \u0026 Lieven Vandenberghe, Convex Optimization Ian Goodfellow, Yoshua Bengio \u0026 Aaron Courville, Deep Learning ","wordCount":"947","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/unsplash-taylor.jpg","datePublished":"2026-01-14T22:10:00+08:00","dateModified":"2026-01-14T22:10:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-14-taylor-series/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">泰勒公式：用简单近似复杂的艺术</h1><div class=post-description>从微积分基础到深度学习前沿，探索泰勒公式的强大威力</div><div class=post-meta><span title='2026-01-14 22:10:00 +0800 CST'>January 14, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>947 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/unsplash-taylor.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/unsplash-taylor.jpg alt=抽象几何曲线></a><figcaption>近似之美</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e4%bb%8e%e6%9b%b2%e7%ba%bf%e5%88%b0%e7%9b%b4%e7%ba%bf aria-label=引言：从曲线到直线>引言：从曲线到直线</a></li><li><a href=#%e4%b8%80%e5%8e%86%e5%8f%b2%e5%9b%9e%e9%a1%be%e4%bb%8e%e7%89%9b%e9%a1%bf%e5%88%b0%e6%b3%b0%e5%8b%92 aria-label=一、历史回顾：从牛顿到泰勒>一、历史回顾：从牛顿到泰勒</a></li><li><a href=#%e4%ba%8c%e4%b8%80%e5%85%83%e5%87%bd%e6%95%b0%e7%9a%84%e6%b3%b0%e5%8b%92%e5%85%ac%e5%bc%8f aria-label=二、一元函数的泰勒公式>二、一元函数的泰勒公式</a><ul><li><a href=#%e5%9f%ba%e6%9c%ac%e5%bd%a2%e5%bc%8f aria-label=基本形式>基本形式</a></li><li><a href=#%e4%bd%99%e9%a1%b9%e7%9a%84%e5%87%a0%e7%a7%8d%e5%bd%a2%e5%bc%8f aria-label=余项的几种形式>余项的几种形式</a></li><li><a href=#%e7%9b%b4%e8%a7%82%e7%90%86%e8%a7%a3 aria-label=直观理解>直观理解</a></li><li><a href=#%e5%87%a0%e4%bd%95%e8%a7%a3%e9%87%8a aria-label=几何解释>几何解释</a></li></ul></li><li><a href=#%e4%b8%89%e5%a4%9a%e5%85%83%e5%87%bd%e6%95%b0%e7%9a%84%e6%b3%b0%e5%8b%92%e5%85%ac%e5%bc%8f aria-label=三、多元函数的泰勒公式>三、多元函数的泰勒公式</a><ul><li><a href=#%e4%ba%8c%e5%85%83%e5%87%bd%e6%95%b0%e7%9a%84%e6%b3%b0%e5%8b%92%e5%b1%95%e5%bc%80 aria-label=二元函数的泰勒展开>二元函数的泰勒展开</a></li><li><a href=#%e6%b5%b7%e6%a3%ae%e7%9f%a9%e9%98%b5%e7%9a%84%e4%bd%9c%e7%94%a8 aria-label=海森矩阵的作用>海森矩阵的作用</a></li></ul></li><li><a href=#%e5%9b%9b%e5%b8%b8%e8%a7%81%e5%87%bd%e6%95%b0%e7%9a%84%e6%b3%b0%e5%8b%92%e7%ba%a7%e6%95%b0 aria-label=四、常见函数的泰勒级数>四、常见函数的泰勒级数</a><ul><li><a href=#%e4%b8%89%e8%a7%92%e5%87%bd%e6%95%b0 aria-label=三角函数>三角函数</a></li><li><a href=#%e6%8c%87%e6%95%b0%e5%87%bd%e6%95%b0 aria-label=指数函数>指数函数</a></li><li><a href=#%e5%af%b9%e6%95%b0%e5%87%bd%e6%95%b0 aria-label=对数函数>对数函数</a></li><li><a href=#%e5%b9%82%e5%87%bd%e6%95%b0 aria-label=幂函数>幂函数</a></li></ul></li><li><a href=#%e4%ba%94%e6%b3%b0%e5%8b%92%e7%ba%a7%e6%95%b0%e7%9a%84%e6%94%b6%e6%95%9b%e6%80%a7 aria-label=五、泰勒级数的收敛性>五、泰勒级数的收敛性</a><ul><li><a href=#%e8%a7%a3%e6%9e%90%e5%87%bd%e6%95%b0%e4%b8%8e%e5%85%89%e6%bb%91%e5%87%bd%e6%95%b0%e7%9a%84%e5%8c%ba%e5%88%ab aria-label=解析函数与光滑函数的区别>解析函数与光滑函数的区别</a></li><li><a href=#%e6%94%b6%e6%95%9b%e5%8d%8a%e5%be%84 aria-label=收敛半径>收敛半径</a></li></ul></li><li><a href=#%e5%85%ad%e6%95%b0%e5%80%bc%e5%88%86%e6%9e%90%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8 aria-label=六、数值分析中的应用>六、数值分析中的应用</a><ul><li><a href=#%e5%87%bd%e6%95%b0%e6%b1%82%e5%80%bc aria-label=函数求值>函数求值</a></li><li><a href=#%e6%95%b0%e5%80%bc%e7%a7%af%e5%88%86 aria-label=数值积分>数值积分</a></li><li><a href=#%e6%95%b0%e5%80%bc%e5%be%ae%e5%88%86 aria-label=数值微分>数值微分</a></li><li><a href=#%e8%af%af%e5%b7%ae%e4%bc%b0%e8%ae%a1 aria-label=误差估计>误差估计</a></li></ul></li><li><a href=#%e4%b8%83%e4%bc%98%e5%8c%96%e7%90%86%e8%ae%ba%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8 aria-label=七、优化理论中的应用>七、优化理论中的应用</a><ul><li><a href=#%e6%9c%80%e4%bc%98%e6%80%a7%e6%9d%a1%e4%bb%b6 aria-label=最优性条件>最优性条件</a></li><li><a href=#%e4%ba%8c%e9%98%b6%e5%85%85%e5%88%86%e6%9d%a1%e4%bb%b6 aria-label=二阶充分条件>二阶充分条件</a></li><li><a href=#%e7%89%9b%e9%a1%bf%e6%b3%95 aria-label=牛顿法>牛顿法</a></li><li><a href=#%e5%85%b1%e8%bd%ad%e6%a2%af%e5%ba%a6%e6%b3%95%e4%b8%8e%e6%8b%9f%e7%89%9b%e9%a1%bf%e6%b3%95 aria-label=共轭梯度法与拟牛顿法>共轭梯度法与拟牛顿法</a></li></ul></li><li><a href=#%e5%85%ab%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8 aria-label=八、机器学习中的应用>八、机器学习中的应用</a><ul><li><a href=#%e7%89%b9%e5%be%81%e7%a9%ba%e9%97%b4%e7%9a%84%e9%9d%9e%e7%ba%bf%e6%80%a7%e6%98%a0%e5%b0%84 aria-label=特征空间的非线性映射>特征空间的非线性映射</a></li><li><a href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%e7%9a%84%e5%88%86%e6%9e%90 aria-label=梯度下降法的分析>梯度下降法的分析</a></li><li><a href=#adagrad%e7%ae%97%e6%b3%95 aria-label=AdaGrad算法>AdaGrad算法</a></li><li><a href=#%e9%ab%98%e6%96%af%e8%bf%87%e7%a8%8b aria-label=高斯过程>高斯过程</a></li></ul></li><li><a href=#%e4%b9%9d%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8 aria-label=九、深度学习中的应用>九、深度学习中的应用</a><ul><li><a href=#%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%9a%84%e6%b3%b0%e5%8b%92%e5%b1%95%e5%bc%80%e8%a7%a3%e9%87%8a aria-label=反向传播的泰勒展开解释>反向传播的泰勒展开解释</a></li><li><a href=#%e4%ba%8c%e9%98%b6%e4%bc%98%e5%8c%96%e6%96%b9%e6%b3%95 aria-label=二阶优化方法>二阶优化方法</a></li><li><a href=#%e6%8b%9f%e7%89%9b%e9%a1%bf%e6%b3%95%e5%92%8ck-fac aria-label=拟牛顿法和K-FAC>拟牛顿法和K-FAC</a></li><li><a href=#%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e6%99%af%e8%a7%82%e5%88%86%e6%9e%90 aria-label=损失函数景观分析>损失函数景观分析</a></li><li><a href=#%e6%89%b0%e5%8a%a8%e6%95%8f%e6%84%9f%e6%80%a7%e5%88%86%e6%9e%90 aria-label=扰动敏感性分析>扰动敏感性分析</a></li><li><a href=#%e7%bd%91%e7%bb%9c%e5%8e%8b%e7%bc%a9%e4%b8%8e%e5%89%aa%e6%9e%9d aria-label=网络压缩与剪枝>网络压缩与剪枝</a></li></ul></li><li><a href=#%e5%8d%81%e6%80%bb%e7%bb%93%e8%bf%91%e4%bc%bc%e7%9a%84%e6%99%ba%e6%85%a7 aria-label=十、总结：近似的智慧>十、总结：近似的智慧</a></li></ul></div></details></div><div class=post-content><h2 id=引言从曲线到直线>引言：从曲线到直线<a hidden class=anchor aria-hidden=true href=#引言从曲线到直线>#</a></h2><p>想象你站在一座山上，想知道脚下的山坡有多陡。你不需要知道整个山脉的形状，只需要知道你所在位置的局部斜率。这是微积分最基本的思想——用局部信息推断全局行为。</p><p>更进一步，如果山坡弯曲了怎么办？这时不仅需要知道斜率，还需要知道弯曲的程度。这就是泰勒公式的核心思想：用最简单的函数（多项式）来近似复杂的函数，而近似的质量取决于我们使用多少局部信息（导数）。</p><p>泰勒公式被誉为"数学家最有力的工具之一"。它不仅连接了离散与连续、局部与整体，更在数值计算、物理建模和现代人工智能中扮演着不可替代的角色。今天，让我们深入探索这个既古老又常新的数学宝藏。</p><h2 id=一历史回顾从牛顿到泰勒>一、历史回顾：从牛顿到泰勒<a hidden class=anchor aria-hidden=true href=#一历史回顾从牛顿到泰勒>#</a></h2><p>泰勒公式的思想可以追溯到牛顿和莱布尼茨创立微积分的时期。牛顿在他的《流数术》中已经隐含了将函数展开为无穷级数的想法。</p><p>布鲁克·泰勒（Brook Taylor，1685-1731）在1715年发表了他的开创性论文《增量法及其逆运算》，首次系统地阐述了用多项式级数逼近函数的方法。有趣的是，泰勒本人并没有意识到他发现的公式的全部潜力，余项的研究（拉格朗日余项、柯西余项等）是后来由拉格朗日等数学家完善的。</p><p>麦克劳林（Colin Maclaurin）发现了泰勒公式在零点展开的特例，即麦克劳林级数。这个形式在实际计算中更为常用，因为计算起来更加方便。</p><h2 id=二一元函数的泰勒公式>二、一元函数的泰勒公式<a hidden class=anchor aria-hidden=true href=#二一元函数的泰勒公式>#</a></h2><h3 id=基本形式>基本形式<a hidden class=anchor aria-hidden=true href=#基本形式>#</a></h3><p>假设函数 $f(x)$ 在点 $a$ 处足够光滑（即具有各阶导数），那么我们可以构造一个多项式 $P_n(x)$ 来近似 $f(x)$：</p><p>$$ P_n(x) = f(a) + f&rsquo;(a)(x-a) + \frac{f&rsquo;&rsquo;(a)}{2!}(x-a)^2 + \cdots + \frac{f^{(n)}(a)}{n!}(x-a)^n $$</p><p>泰勒公式告诉我们：</p><p>$$ f(x) = P_n(x) + R_n(x) $$</p><p>其中 $R_n(x)$ 是余项，表示近似误差。</p><h3 id=余项的几种形式>余项的几种形式<a hidden class=anchor aria-hidden=true href=#余项的几种形式>#</a></h3><p>理解余项对于掌握泰勒公式至关重要，因为它告诉我们近似在什么范围内可靠。</p><p><strong>拉格朗日余项</strong>：</p><p>$$ R_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!}(x-a)^{n+1} $$</p><p>其中 $\xi$ 是 $a$ 和 $x$ 之间的某个值。</p><p><strong>积分余项</strong>：</p><p>$$ R_n(x) = \frac{1}{n!} \int_a^x f^{(n+1)}(t)(x-t)^n , dt $$</p><h3 id=直观理解>直观理解<a hidden class=anchor aria-hidden=true href=#直观理解>#</a></h3><p>让我们通过一个简单的例子来理解泰勒公式。考虑 $f(x) = e^x$ 在 $a = 0$ 处的泰勒展开（即麦克劳林级数）：</p><ul><li><p><strong>零阶近似</strong>：$e^x \approx f(0) = 1$
这是最粗糙的近似，假设函数是常数。</p></li><li><p><strong>一阶近似</strong>：$e^x \approx 1 + x$
这是线性近似，假设函数在局部是一条直线。</p></li><li><p><strong>二阶近似</strong>：$e^x \approx 1 + x + \frac{x^2}{2}$
这个近似考虑了函数的弯曲程度。</p></li><li><p><strong>n阶近似</strong>：$e^x \approx 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots + \frac{x^n}{n!}$</p></li></ul><p>当 $n \to \infty$ 时，我们得到泰勒级数：
$$ e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!} $$</p><p>这个级数对所有实数 $x$ 都收敛。</p><h3 id=几何解释>几何解释<a hidden class=anchor aria-hidden=true href=#几何解释>#</a></h3><p>泰勒公式的几何解释非常优美。每一次增加一项，我们就在更好地拟合曲线：</p><ul><li><strong>零阶项</strong>：常数项，确保在 $x=a$ 处函数值正确</li><li><strong>一阶项</strong>：线性项，确保在 $x=a$ 处切线斜率正确</li><li><strong>二阶项</strong>：二次项，确保在 $x=a$ 处曲率正确</li><li><strong>更高阶项</strong>：确保更高阶的变化率正确</li></ul><p>可以想象，每增加一项，我们的多项式就像曲线一样，在展开点附近"缠绕"得更紧密。</p><h2 id=三多元函数的泰勒公式>三、多元函数的泰勒公式<a hidden class=anchor aria-hidden=true href=#三多元函数的泰勒公式>#</a></h2><p>在现代应用中，我们经常需要处理多元函数。泰勒公式自然地推广到多元情况。</p><h3 id=二元函数的泰勒展开>二元函数的泰勒展开<a hidden class=anchor aria-hidden=true href=#二元函数的泰勒展开>#</a></h3><p>对于二元函数 $f(x, y)$，在点 $(a, b)$ 处的二阶泰勒展开为：</p><p>$$ f(x, y) \approx f(a, b) + \frac{\partial f}{\partial x}\bigg|<em>{(a,b)}(x-a) + \frac{\partial f}{\partial y}\bigg|</em>{(a,b)}(y-b) $$
$$ + \frac{1}{2}\left[\frac{\partial^2 f}{\partial x^2}\bigg|<em>{(a,b)}(x-a)^2 + 2\frac{\partial^2 f}{\partial x \partial y}\bigg|</em>{(a,b)}(x-a)(y-b) + \frac{\partial^2 f}{\partial y^2}\bigg|_{(a,b)}(y-b)^2\right] $$</p><p>我们可以用更简洁的矩阵形式表示：</p><p>$$ f(\mathbf{x}) \approx f(\mathbf{a}) + \nabla f(\mathbf{a})^{\top}(\mathbf{x} - \mathbf{a}) + \frac{1}{2}(\mathbf{x} - \mathbf{a})^{\top} H(\mathbf{a})(\mathbf{x} - \mathbf{a}) $$</p><p>其中：</p><ul><li>$\nabla f(\mathbf{a})$ 是梯度向量</li><li>$H(\mathbf{a})$ 是海森矩阵（Hessian matrix）</li></ul><h3 id=海森矩阵的作用>海森矩阵的作用<a hidden class=anchor aria-hidden=true href=#海森矩阵的作用>#</a></h3><p>海森矩阵是多元函数二阶偏导数的矩阵：
$$ H = \begin{pmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots \ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots \ \vdots & \vdots & \ddots \end{pmatrix} $$</p><p>海森矩阵包含了函数的二阶导数信息，它告诉我们：</p><ul><li>函数在各个方向上的弯曲程度（对角元素）</li><li>不同方向之间的弯曲耦合关系（非对角元素）</li></ul><p>在优化问题中，海森矩阵的正定性决定了临界点的性质（极小值、极大值或鞍点）。</p><h2 id=四常见函数的泰勒级数>四、常见函数的泰勒级数<a hidden class=anchor aria-hidden=true href=#四常见函数的泰勒级数>#</a></h2><p>让我们列举一些经典函数的泰勒级数展开（在 $x=0$ 处）：</p><h3 id=三角函数>三角函数<a hidden class=anchor aria-hidden=true href=#三角函数>#</a></h3><p><strong>正弦函数</strong>：
$$ \sin x = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \cdots = \sum_{n=0}^{\infty} (-1)^n \frac{x^{2n+1}}{(2n+1)!} $$</p><p><strong>余弦函数</strong>：
$$ \cos x = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \cdots = \sum_{n=0}^{\infty} (-1)^n \frac{x^{2n}}{(2n)!} $$</p><p>这些级数对所有实数 $x$ 都收敛，展示了周期函数如何用非周期函数的多项式来逼近。</p><h3 id=指数函数>指数函数<a hidden class=anchor aria-hidden=true href=#指数函数>#</a></h3><p>$$ e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots = \sum_{n=0}^{\infty} \frac{x^n}{n!} $$</p><p>通过欧拉公式 $e^{ix} = \cos x + i \sin x$，我们可以用指数函数的泰勒级数来验证三角函数的泰勒级数，这是复分析中的一个美妙联系。</p><h3 id=对数函数>对数函数<a hidden class=anchor aria-hidden=true href=#对数函数>#</a></h3><p>$$ \ln(1+x) = x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \cdots = \sum_{n=1}^{\infty} (-1)^{n-1} \frac{x^n}{n}, \quad -1 &lt; x \leq 1 $$</p><p>注意这个级数只在 $x \in (-1, 1]$ 上收敛，显示了泰勒级数的收敛域的重要性。</p><h3 id=幂函数>幂函数<a hidden class=anchor aria-hidden=true href=#幂函数>#</a></h3><p>$$ (1+x)^\alpha = 1 + \alpha x + \frac{\alpha(\alpha-1)}{2!}x^2 + \frac{\alpha(\alpha-1)(\alpha-2)}{3!}x^3 + \cdots $$</p><p>这是广义二项式定理的级数形式。当 $\alpha$ 是正整数时，级数只有有限项（这正是二项式定理）；当 $\alpha$ 不是正整数时，我们得到无穷级数。</p><h2 id=五泰勒级数的收敛性>五、泰勒级数的收敛性<a hidden class=anchor aria-hidden=true href=#五泰勒级数的收敛性>#</a></h2><p>并非所有光滑函数的泰勒级数都收敛到原函数。这是一个深刻且反直觉的事实。</p><h3 id=解析函数与光滑函数的区别>解析函数与光滑函数的区别<a hidden class=anchor aria-hidden=true href=#解析函数与光滑函数的区别>#</a></h3><p>如果一个函数的泰勒级数在某个区间内收敛到该函数本身，我们称这个函数为<strong>解析函数</strong>。然而，存在光滑（各阶导数都存在）但非解析的函数。</p><p><strong>经典例子</strong>：$f(x) = \begin{cases} e^{-1/x^2}, & x \neq 0 \ 0, & x = 0 \end{cases}$</p><p>可以证明，这个函数在 $x=0$ 处的所有导数都是零，因此它的泰勒级数恒为零，但在 $x \neq 0$ 处函数值不为零。这是光滑但非解析的经典例子。</p><h3 id=收敛半径>收敛半径<a hidden class=anchor aria-hidden=true href=#收敛半径>#</a></h3><p>对于幂级数 $\sum_{n=0}^{\infty} a_n (x-a)^n$，存在一个收敛半径 $R$，使得：</p><ul><li>当 $|x-a| &lt; R$ 时，级数绝对收敛</li><li>当 $|x-a| > R$ 时，级数发散</li></ul><p>收敛半径可以通过比值法或根值法计算：</p><p>$$ R = \lim_{n \to \infty} \left|\frac{a_n}{a_{n+1}}\right| \quad \text{或} \quad R = \frac{1}{\limsup_{n \to \infty} \sqrt[n]{|a_n|}} $$</p><h2 id=六数值分析中的应用>六、数值分析中的应用<a hidden class=anchor aria-hidden=true href=#六数值分析中的应用>#</a></h2><h3 id=函数求值>函数求值<a hidden class=anchor aria-hidden=true href=#函数求值>#</a></h3><p>泰勒公式是计算复杂函数值的强大工具。例如，计算 $\sin(0.1)$：</p><p>$$ \sin(0.1) \approx 0.1 - \frac{0.1^3}{6} = 0.1 - 0.0001667 = 0.0998333 $$</p><p>实际值约为 $0.0998334$，近似非常精确！</p><h3 id=数值积分>数值积分<a hidden class=anchor aria-hidden=true href=#数值积分>#</a></h3><p>在计算定积分 $\int_a^b f(x) dx$ 时，如果被积函数比较复杂，我们可以用泰勒展开来近似：</p><p>$$ \int_a^b f(x) dx \approx \int_a^b P_n(x) dx $$</p><p>例如，计算 $\int_0^{0.1} \sin(x^2) dx$：</p><p>令 $u = x^2$，当 $x$ 很小时：
$$ \sin(x^2) = \sin u \approx u - \frac{u^3}{6} = x^2 - \frac{x^6}{6} $$</p><p>因此：
$$ \int_0^{0.1} \sin(x^2) dx \approx \int_0^{0.1} \left(x^2 - \frac{x^6}{6}\right) dx = \left[\frac{x^3}{3} - \frac{x^7}{42}\right]_0^{0.1} = \frac{0.001}{3} - \frac{10^{-7}}{42} \approx 0.0003333 $$</p><h3 id=数值微分>数值微分<a hidden class=anchor aria-hidden=true href=#数值微分>#</a></h3><p>泰勒公式可以用于近似导数。例如，对于小量 $h$：</p><p>$$ f&rsquo;(x) \approx \frac{f(x+h) - f(x)}{h} $$</p><p>这是前向差分公式，其误差为 $O(h)$。通过泰勒展开，我们可以得到更高精度的公式：</p><p><strong>中心差分公式</strong>：
$$ f&rsquo;(x) \approx \frac{f(x+h) - f(x-h)}{2h} $$</p><p>误差为 $O(h^2)$，比前向差分更精确。</p><h3 id=误差估计>误差估计<a hidden class=anchor aria-hidden=true href=#误差估计>#</a></h3><p>泰勒公式的余项为我们提供了近似误差的严格界限。例如，用 $P_1(x) = 1 + x$ 近似 $e^x$ 在区间 $[0, 0.1]$ 上的误差：</p><p>由拉格朗日余项：
$$ |R_1(x)| = \left|\frac{e^\xi}{2!}x^2\right| \leq \frac{e^{0.1}}{2} \times 0.01 \approx 0.0055 $$</p><p>这告诉我们近似误差不会超过 $0.0055$。</p><h2 id=七优化理论中的应用>七、优化理论中的应用<a hidden class=anchor aria-hidden=true href=#七优化理论中的应用>#</a></h2><p>泰勒公式在优化理论中处于核心地位。</p><h3 id=最优性条件>最优性条件<a hidden class=anchor aria-hidden=true href=#最优性条件>#</a></h3><p>对于无约束优化问题 $\min_{\mathbf{x}} f(\mathbf{x})$，一阶必要条件是：</p><p>$$ \nabla f(\mathbf{x}^{\ast}) = \mathbf{0} $$</p><p>这个条件的直观理解可以从泰勒展开中看出。设 $\mathbf{x}^{\ast}$ 是局部极小值点，考虑 $f(\mathbf{x}^{\ast} + \mathbf{h})$ 的一阶泰勒展开：</p><p>$$ f(\mathbf{x}^{\ast} + \mathbf{h}) \approx f(\mathbf{x}^{\ast}) + \nabla f(\mathbf{x}^{\ast})^{\top} \mathbf{h} $$</p><p>如果 $\nabla f(\mathbf{x}^{\ast}) \neq \mathbf{0}$，我们可以选择 $\mathbf{h} = -\alpha \nabla f(\mathbf{x}^{\ast})$（$\alpha > 0$ 很小），使得 $f(\mathbf{x}^{\ast} + \mathbf{h}) &lt; f(\mathbf{x}^{\ast})$，这与 $\mathbf{x}^{\ast}$ 是局部极小值矛盾。因此，$\nabla f(\mathbf{x}^{\ast})$ 必须为零。</p><h3 id=二阶充分条件>二阶充分条件<a hidden class=anchor aria-hidden=true href=#二阶充分条件>#</a></h3><p>如果 $\nabla f(\mathbf{x}^{\ast}) = \mathbf{0}$ 且海森矩阵 $H(\mathbf{x}^{\ast})$ 是正定的（所有特征值大于零），则 $\mathbf{x}^{\ast}$ 是严格局部极小值点。</p><p>从二阶泰勒展开：</p><p>$$ f(\mathbf{x}^{\ast} + \mathbf{h}) \approx f(\mathbf{x}^{\ast}) + \frac{1}{2} \mathbf{h}^{\top} H(\mathbf{x}^{\ast}) \mathbf{h} $$</p><p>如果 $H$ 正定，则 $\mathbf{h}^{\top} H \mathbf{h} > 0$ 对所有非零 $\mathbf{h}$ 成立，因此 $f(\mathbf{x}^{\ast} + \mathbf{h}) > f(\mathbf{x}^{\ast})$。</p><h3 id=牛顿法>牛顿法<a hidden class=anchor aria-hidden=true href=#牛顿法>#</a></h3><p>牛顿法是求解方程 $f(x) = 0$ 的经典方法，它利用泰勒展开的线性近似。</p><p>假设我们已经有近似解 $x_n$，将 $f(x)$ 在 $x_n$ 处线性展开：</p><p>$$ f(x) \approx f(x_n) + f&rsquo;(x_n)(x - x_n) $$</p><p>令这个线性近似为零，解得：
$$ x_{n+1} = x_n - \frac{f(x_n)}{f&rsquo;(x_n)} $$</p><p>这就是牛顿法的迭代公式。在优化问题中，我们需要求解 $\nabla f(\mathbf{x}) = \mathbf{0}$，对应的牛顿法迭代为：</p><p>$$ \mathbf{x}_{n+1} = \mathbf{x}_n - H(\mathbf{x}_n)^{-1} \nabla f(\mathbf{x}_n) $$</p><p>牛顿法具有二次收敛速度（每一步迭代，有效数字大约翻倍），但需要计算海森矩阵的逆，计算成本较高。</p><h3 id=共轭梯度法与拟牛顿法>共轭梯度法与拟牛顿法<a hidden class=anchor aria-hidden=true href=#共轭梯度法与拟牛顿法>#</a></h3><p>为了避免直接计算海森矩阵的逆，发展了共轭梯度法和拟牛顿法（如BFGS算法）。这些方法利用泰勒展开的思想，通过梯度信息来近似海森矩阵，在大规模优化问题中非常有效。</p><h2 id=八机器学习中的应用>八、机器学习中的应用<a hidden class=anchor aria-hidden=true href=#八机器学习中的应用>#</a></h2><h3 id=特征空间的非线性映射>特征空间的非线性映射<a hidden class=anchor aria-hidden=true href=#特征空间的非线性映射>#</a></h3><p>泰勒公式可以将非线性问题近似为线性问题。例如，在支持向量机（SVM）中，核技巧的某些理解可以从泰勒展开中获得启发。</p><p>考虑径向基函数核 $K(\mathbf{x}, \mathbf{y}) = \exp(-|\mathbf{x} - \mathbf{y}|^2/2\sigma^2)$。当 $\sigma$ 很大时，我们可以展开为：</p><p>$$ K(\mathbf{x}, \mathbf{y}) \approx 1 - \frac{|\mathbf{x} - \mathbf{y}|^2}{2\sigma^2} + \cdots $$</p><p>这揭示了核函数与多项式特征映射之间的联系。</p><h3 id=梯度下降法的分析>梯度下降法的分析<a hidden class=anchor aria-hidden=true href=#梯度下降法的分析>#</a></h3><p>梯度下降法是机器学习中最基础的优化算法。利用泰勒展开，我们可以分析其收敛性。</p><p>考虑目标函数 $f(\mathbf{x})$，在当前点 $\mathbf{x}_k$ 处的一阶泰勒展开：</p><p>$$ f(\mathbf{x}_k + \alpha \mathbf{d}_k) \approx f(\mathbf{x}_k) + \alpha \nabla f(\mathbf{x}_k)^{\top} \mathbf{d}_k $$</p><p>其中 $\alpha$ 是步长，$\mathbf{d}_k$ 是搜索方向。对于梯度下降法，$\mathbf{d}_k = -\nabla f(\mathbf{x}_k)$，因此：</p><p>$$ f(\mathbf{x}_k - \alpha \nabla f(\mathbf{x}_k)) \approx f(\mathbf{x}_k) - \alpha |\nabla f(\mathbf{x}_k)|^2 $$</p><p>只要 $\alpha$ 足够小且 $\nabla f(\mathbf{x}_k) \neq \mathbf{0}$，目标函数值就会下降。这保证了梯度下降法的每一步（在适当步长下）都能改进目标函数。</p><h3 id=adagrad算法>AdaGrad算法<a hidden class=anchor aria-hidden=true href=#adagrad算法>#</a></h3><p>AdaGrad是自适应学习率的优化算法。其核心思想是对每个参数使用不同的学习率，学习率与过去梯度的平方和成反比。这可以用泰勒展开来理解。</p><p>考虑在参数空间中，不同方向上的曲率不同。海森矩阵的特征值衡量了各个方向上的曲率。如果我们能估计曲率，就可以在曲率大的方向上使用更小的步长，在曲率小的方向上使用更大的步长。</p><p>AdaGrad用过去梯度的统计信息来近似曲率信息，这在一定程度上等价于二阶优化方法。</p><h3 id=高斯过程>高斯过程<a hidden class=anchor aria-hidden=true href=#高斯过程>#</a></h3><p>高斯过程是一种非参数贝叶斯模型，它基于泰勒展开的思想来预测函数值及其不确定性。</p><p>高斯过程假设函数是高斯随机过程，因此任意有限点的函数值服从多元高斯分布。训练后，对于新的输入 $\mathbf{x}_{\ast}$，预测分布可以通过条件概率计算：</p><p>$$ p(f_{\ast}|\mathbf{X}, \mathbf{y}, \mathbf{x}<em>{\ast}) = \mathcal{N}(\mu</em>{\ast}, \sigma_{\ast}^2) $$</p><p>预测均值 $\mu_{\ast}$ 本质上是对训练数据点的加权平均，权重由核函数（即协方差函数）决定。这与泰勒展开用局部信息推断全局的思想是一致的。</p><h2 id=九深度学习中的应用>九、深度学习中的应用<a hidden class=anchor aria-hidden=true href=#九深度学习中的应用>#</a></h2><p>泰勒公式在深度学习中的应用非常广泛和深入。</p><h3 id=反向传播的泰勒展开解释>反向传播的泰勒展开解释<a hidden class=anchor aria-hidden=true href=#反向传播的泰勒展开解释>#</a></h3><p>反向传播算法是深度学习的核心，它高效地计算损失函数对每个参数的梯度。我们可以用泰勒展开来理解反向传播的原理。</p><p>考虑损失函数 $L$ 关于权重 $W$ 的函数。在训练过程中，我们想要最小化 $L$。通过反向传播，我们计算 $\frac{\partial L}{\partial W}$。</p><p>从泰勒展开的角度，反向传播实际上是在计算一阶导数信息。反向传播的高效性来自于链式法则的巧妙组织，它避免了重复计算。</p><h3 id=二阶优化方法>二阶优化方法<a hidden class=anchor aria-hidden=true href=#二阶优化方法>#</a></h3><p>虽然梯度下降和反向传播基于一阶导数，但二阶信息（海森矩阵）可以显著加速训练。</p><p><strong>牛顿法在深度学习中的应用</strong>：
$$ W_{new} = W_{old} - H^{-1} \nabla L $$</p><p>其中 $H$ 是损失函数关于参数的海森矩阵。牛顿法考虑了函数的曲率信息，收敛速度更快。</p><p>然而，海森矩阵的维度与参数数量平方成正比，对于现代深度学习模型（可能有数亿个参数），直接存储和求逆海森矩阵是不现实的。</p><h3 id=拟牛顿法和k-fac>拟牛顿法和K-FAC<a hidden class=anchor aria-hidden=true href=#拟牛顿法和k-fac>#</a></h3><p>为了在深度学习中利用二阶信息，发展了拟牛顿法的各种变体。K-FAC（Kronecker-Factored Approximate Curvature）是一种重要的近似方法。</p><p>K-FAC的核心思想是利用深度神经网络结构的特殊性，对海森矩阵进行近似分解。对于全连接层，海森矩阵可以近似为Kronecker乘积的形式：</p><p>$$ H \approx A \otimes G $$</p><p>其中 $A$ 与激活值相关，$G$ 与梯度相关。这种近似使得海森矩阵的求逆变得可行。</p><h3 id=损失函数景观分析>损失函数景观分析<a hidden class=anchor aria-hidden=true href=#损失函数景观分析>#</a></h3><p>深度神经网络的损失函数景观非常复杂，有大量的鞍点和局部极小值。泰勒展开可以帮助我们分析这些临界点的性质。</p><p>在临界点 $\mathbf{W}^{\ast}$ 处（$\nabla L(\mathbf{W}^{\ast}) = \mathbf{0}$），损失函数的二阶泰勒展开为：</p><p>$$ L(\mathbf{W}^{\ast} + \mathbf{h}) \approx L(\mathbf{W}^{\ast}) + \frac{1}{2} \mathbf{h}^{\top} H(\mathbf{W}^{\ast}) \mathbf{h} $$</p><p>海森矩阵的特征值分布告诉我们临界点的类型：</p><ul><li>所有特征值大于零：局部极小值</li><li>所有特征值小于零：局部极大值</li><li>有正有负：鞍点</li></ul><p>现代研究表明，高维神经网络的损失函数景观中，鞍点比局部极小值更普遍。这解释了为什么梯度下降法在实践中仍然有效——它更容易逃离鞍点而不是被困在糟糕的局部极小值中。</p><h3 id=扰动敏感性分析>扰动敏感性分析<a hidden class=anchor aria-hidden=true href=#扰动敏感性分析>#</a></h3><p>泰勒展开可以用来分析神经网络对输入扰动的敏感性。这对于理解对抗攻击和鲁棒性很重要。</p><p>设原始输入为 $\mathbf{x}$，扰动后的输入为 $\mathbf{x} + \delta$。网络输出 $f(\mathbf{x})$ 的一阶泰勒展开：</p><p>$$ f(\mathbf{x} + \delta) \approx f(\mathbf{x}) + \nabla f(\mathbf{x})^{\top} \delta $$</p><p>这告诉我们，如果扰动 $\delta$ 的方向与梯度方向一致，输出的变化最大。对抗攻击正是利用了这一点，通过精心设计的微小扰动来最大化输出变化。</p><h3 id=网络压缩与剪枝>网络压缩与剪枝<a hidden class=anchor aria-hidden=true href=#网络压缩与剪枝>#</a></h3><p>泰勒展开可以用来评估每个神经元或连接的重要性，从而指导网络压缩和剪枝。</p><p>考虑损失函数关于某个参数 $w_i$ 的函数。如果移除这个参数（设 $w_i = 0$），损失的增量可以用泰勒展开估计：</p><p>$$ \Delta L \approx \frac{\partial L}{\partial w_i} (-w_i) + \frac{1}{2} \frac{\partial^2 L}{\partial w_i^2} w_i^2 $$</p><p>在训练好的网络中，$\frac{\partial L}{\partial w_i} \approx 0$（接近最优），因此：</p><p>$$ \Delta L \approx \frac{1}{2} \frac{\partial^2 L}{\partial w_i^2} w_i^2 $$</p><p>这个量可以作为参数重要性的指标，用于剪枝策略。</p><h2 id=十总结近似的智慧>十、总结：近似的智慧<a hidden class=anchor aria-hidden=true href=#十总结近似的智慧>#</a></h2><p>泰勒公式是微积分皇冠上的明珠之一。它用最简单的多项式函数，去逼近任意复杂的光滑函数。这种"以简驭繁"的思想，贯穿了整个数学和工程实践。</p><p>从历史上看，泰勒公式连接了离散与连续、局部与整体。牛顿用二项式定理处理函数流数，泰勒将其系统化为一般方法。麦克劳林发现了在零点展开的特例，拉格朗日完善了余项理论。每一位数学家都在前人的基础上，推动着这个工具的完善。</p><p>从应用上看，泰勒公式在数值计算、物理建模、机器学习和深度学习中扮演着不可替代的角色。无论是计算器计算 $\sin(0.1)$ 的值，还是神经网络优化算法的设计，都能看到泰勒公式的影子。</p><p>从哲学上看，泰勒公式体现了一种深刻的认知方式：<strong>局部通向整体</strong>。我们通过理解一点的局部行为（导数），推断函数在附近的行为，进而外推到更远的区域。这种从局部到整体、从已知到未知的推理模式，正是科学方法的核心。</p><p>理解泰勒公式，不仅是掌握一个数学工具，更是培养一种思维习惯。在复杂中寻找简单，在变化中寻找不变，在混沌中寻找秩序。这或许就是数学最动人的力量——用最抽象的语言，讲述着最具体的故事。</p><hr><p><strong>参考资料：</strong></p><ul><li>James Stewart, <em>Calculus: Early Transcendentals</em></li><li>Stephen Boyd & Lieven Vandenberghe, <em>Convex Optimization</em></li><li>Ian Goodfellow, Yoshua Bengio & Aaron Courville, <em>Deep Learning</em></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E6%95%B0%E5%AD%A6%E5%8F%B2/>数学史</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%AE%97%E6%B3%95/>算法</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-14-greens-gauss-stokes-formulas-guide/><span class=title>« Prev</span><br><span>微积分的三大公式：格林、高斯与斯托克斯定理的统一视角</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-14-laplace-equation/><span class=title>Next »</span><br><span>拉普拉斯方程：数学物理中的优雅平衡</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 泰勒公式：用简单近似复杂的艺术 on x" href="https://x.com/intent/tweet/?text=%e6%b3%b0%e5%8b%92%e5%85%ac%e5%bc%8f%ef%bc%9a%e7%94%a8%e7%ae%80%e5%8d%95%e8%bf%91%e4%bc%bc%e5%a4%8d%e6%9d%82%e7%9a%84%e8%89%ba%e6%9c%af&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-14-taylor-series%2f&amp;hashtags=%e6%95%b0%e5%ad%a6%e5%8f%b2%2c%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%2c%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%2c%e7%ae%97%e6%b3%95"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 泰勒公式：用简单近似复杂的艺术 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-14-taylor-series%2f&amp;title=%e6%b3%b0%e5%8b%92%e5%85%ac%e5%bc%8f%ef%bc%9a%e7%94%a8%e7%ae%80%e5%8d%95%e8%bf%91%e4%bc%bc%e5%a4%8d%e6%9d%82%e7%9a%84%e8%89%ba%e6%9c%af&amp;summary=%e6%b3%b0%e5%8b%92%e5%85%ac%e5%bc%8f%ef%bc%9a%e7%94%a8%e7%ae%80%e5%8d%95%e8%bf%91%e4%bc%bc%e5%a4%8d%e6%9d%82%e7%9a%84%e8%89%ba%e6%9c%af&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-14-taylor-series%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 泰勒公式：用简单近似复杂的艺术 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-14-taylor-series%2f&title=%e6%b3%b0%e5%8b%92%e5%85%ac%e5%bc%8f%ef%bc%9a%e7%94%a8%e7%ae%80%e5%8d%95%e8%bf%91%e4%bc%bc%e5%a4%8d%e6%9d%82%e7%9a%84%e8%89%ba%e6%9c%af"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 泰勒公式：用简单近似复杂的艺术 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-14-taylor-series%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>