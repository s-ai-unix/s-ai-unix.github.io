<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>高斯混合模型：从数据中解构隐藏结构的艺术 | s-ai-unix's Blog</title><meta name=keywords content="GMM,高斯混合模型,EM算法,聚类,高斯分布,期望最大化"><meta name=description content="深入探讨机器学习中的核心无监督学习算法 GMM，从高斯分布回顾到 EM 算法的完整推导，从几何直观到实际应用，娓娓道来。"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-24-gmm-comprehensive-guide/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-24-gmm-comprehensive-guide/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-24-gmm-comprehensive-guide/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="高斯混合模型：从数据中解构隐藏结构的艺术"><meta property="og:description" content="深入探讨机器学习中的核心无监督学习算法 GMM，从高斯分布回顾到 EM 算法的完整推导，从几何直观到实际应用，娓娓道来。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-24T18:00:00+08:00"><meta property="article:modified_time" content="2026-01-24T18:00:00+08:00"><meta property="article:tag" content="GMM"><meta property="article:tag" content="高斯混合模型"><meta property="article:tag" content="EM算法"><meta property="article:tag" content="聚类"><meta property="article:tag" content="高斯分布"><meta property="article:tag" content="期望最大化"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/gmm-probability.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/gmm-probability.jpg"><meta name=twitter:title content="高斯混合模型：从数据中解构隐藏结构的艺术"><meta name=twitter:description content="深入探讨机器学习中的核心无监督学习算法 GMM，从高斯分布回顾到 EM 算法的完整推导，从几何直观到实际应用，娓娓道来。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"高斯混合模型：从数据中解构隐藏结构的艺术","item":"https://s-ai-unix.github.io/posts/2026-01-24-gmm-comprehensive-guide/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"高斯混合模型：从数据中解构隐藏结构的艺术","name":"高斯混合模型：从数据中解构隐藏结构的艺术","description":"深入探讨机器学习中的核心无监督学习算法 GMM，从高斯分布回顾到 EM 算法的完整推导，从几何直观到实际应用，娓娓道来。","keywords":["GMM","高斯混合模型","EM算法","聚类","高斯分布","期望最大化"],"articleBody":"引言：从混沌中发现结构 想象你是一个天文学家，正在观测夜空中的恒星。这些恒星并非均匀分布，而是呈现出明显的\"聚集\"现象：有些恒星形成了紧密的星团，有些则稀疏地散布在广阔的空间中。你的任务是理解这些恒星是如何分布的——它们属于哪些星团，每个星团的形状和位置是什么。\n这就是一个典型的聚类问题：将数据点分组成若干个有意义的组。\n最直观的聚类方法是 K-means：将每个数据点分配到最近的簇中心，然后更新簇中心，迭代直至收敛。但 K-means 有一个致命的限制：它假设每个簇是\"圆形\"的（在二维）或\"球形\"的（在高维）。这意味着它只能捕捉硬边界的簇，无法处理更复杂的形状，也无法表示一个数据点可能\"部分地\"属于多个簇。\n这时，一个更强大的工具出现了：高斯混合模型（Gaussian Mixture Model, GMM）。GMM 不再做非此即彼的硬分类，而是给每个数据点一个\"软\"的归属概率——它有多大可能性属于每个簇。这种软聚类的方法不仅更灵活，而且能捕捉更复杂的数据分布。\n更重要的是，GMM 引入了机器学习中最深刻的算法之一：EM 算法（Expectation-Maximization，期望最大化）。EM 算法是一种优雅的迭代算法，用于解决含有隐变量的概率模型的参数估计问题。\n本文将带你深入 GMM 的世界。我们将从高斯分布的复习开始，理解从 K-means 到 GMM 的自然演进，推导 EM 算法的每一步，探索几何直观，最后了解它在现实世界的应用。准备好了吗？让我们开始这场从数据中发现隐藏结构的旅程。\n高斯分布的回顾：多元正态分布 在深入 GMM 之前，我们需要先熟悉多元高斯分布（Multivariate Gaussian Distribution）的数学形式。\n一元高斯分布 回忆一下，一元高斯分布的概率密度函数是：\n$$ f(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) $$\n其中：\n$\\mu$ 是均值（期望） $\\sigma^2$ 是方差 $\\sigma \u003e 0$ 是标准差 这个分布的形状是经典的\"钟形曲线\"：在 $\\mu$ 处达到峰值，向两侧对称衰减。\n多元高斯分布 多元高斯分布是上述概念的推广。设 $\\mathbf{x} \\in \\mathbb{R}^d$ 是一个 $d$ 维随机向量，$\\mathbf{\\mu} \\in \\mathbb{R}^d$ 是均值向量，$\\mathbf{\\Sigma} \\in \\mathbb{R}^{d \\times d}$ 是协方差矩阵（对称正定）。\n多元高斯分布的概率密度函数是：\n$$ f(\\mathbf{x} | \\mathbf{\\mu}, \\mathbf{\\Sigma}) = \\frac{1}{(2\\pi)^{d/2} |\\mathbf{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\mathbf{\\mu})^\\top \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})\\right) $$\n这里需要解释几个符号：\n行列式 $|\\mathbf{\\Sigma}|$：协方差矩阵的行列式 逆矩阵 $\\mathbf{\\Sigma}^{-1}$：协方差矩阵的逆矩阵 二次型 $(\\mathbf{x} - \\mathbf{\\mu})^\\top \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})$：这给出了点 $\\mathbf{x}$ 到均值 $\\mathbf{\\mu}$ 的\"距离\" 协方差矩阵的意义 协方差矩阵 $\\mathbf{\\Sigma}$ 捕捉了数据的分布形状。对角线元素 $\\Sigma_{ii}$ 是第 $i$ 个特征的方差，非对角线元素 $\\Sigma_{ij}$ 是第 $i$ 个和第 $j$ 个特征的协方差：\n$$ \\Sigma_{ij} = \\text{Cov}(X_i, X_j) = E[(X_i - \\mu_i)(X_j - \\mu_j)] $$\n如果 $\\Sigma_{ij} = 0$，说明第 $i$ 个和第 $j$ 个特征不相关。\n等高线：高斯分布的\"轮廓\" 多元高斯分布的等高线（即 $f(\\mathbf{x})$ 取常数的曲面）是椭圆（在二维）或椭球（在高维）。这些椭球的长轴方向由 $\\mathbf{\\Sigma}$ 的特征向量给出，轴的长度由特征值给出。\n这个几何理解非常重要：每个高斯分布可以看作一个\"数据云\"，其形状由均值（位置）和协方差矩阵（形状）决定。\n从 K-means 到 GMM：硬聚类 vs 软聚类 K-means 的局限性 K-means 算法可以用一个简单的概率模型来解释：假设数据由 $K$ 个点源生成，每个点源以等概率生成数据点，且每个数据点服从以该点源为中心的各向同性高斯分布（方差在所有方向上相等）。\n数学上，这等价于假设每个簇的协方差矩阵是 $\\sigma^2 \\mathbf{I}$（$\\sigma^2$ 乘以单位矩阵），即\"圆形\"或\"球形\"的分布。\n这个假设有两个问题：\n形状限制：现实中的数据簇往往不是球形的。例如，考虑二维数据，如果簇是椭圆形的，K-means 可能会将一个椭圆簇分成两个球形簇。 硬分配：每个数据点只能完全属于一个簇。但很多情况下，数据点确实\"介于\"两个簇之间。 GMM 的核心思想 GMM 的核心思想是：用 $K$ 个高斯分布的线性组合来建模数据。每个数据点以一定的概率来自每个高斯分布，而且这个概率是我们需要学习的。\n数学上，GMM 的概率密度函数是：\n$$ p(\\mathbf{x}) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k) $$\n其中：\n$K$ 是高斯分量（簇）的数量 $\\pi_k$ 是混合系数（mixing coefficient），满足 $\\sum_{k=1}^{K} \\pi_k = 1$ 且 $\\pi_k \\geq 0$ $\\mathbf{\\mu}_k$ 是第 $k$ 个高斯分量的均值 $\\mathbf{\\Sigma}_k$ 是第 $k$ 个高斯分量的协方差矩阵 $\\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)$ 是均值为 $\\mathbf{\\mu}_k$、协方差矩阵为 $\\mathbf{\\Sigma}_k$ 的高斯分布 软聚类：责任（Responsibility） 在 GMM 中，我们引入一个隐变量 $\\mathbf{z}$，表示数据点来自哪个高斯分量。$\\mathbf{z}$ 是一个 $K$ 维的 one-hot 向量，如果 $\\mathbf{z} = \\mathbf{e}_k$（第 $k$ 个元素为 1，其余为 0），则表示 $\\mathbf{x}$ 来自第 $k$ 个高斯分量。\n后验概率（responsibility）$\\gamma_{nk}$ 定义为：\n$$ \\gamma_{nk} = p(z_k = 1 | \\mathbf{x}_n, \\mathbf{\\pi}, \\mathbf{\\mu}, \\mathbf{\\Sigma}) = \\frac{\\pi_k \\mathcal{N}(\\mathbf{x}_n | \\mathbf{\\mu}_k, \\mathbf{\\Sigma}k)}{\\sum{j=1}^{K} \\pi_j \\mathcal{N}(\\mathbf{x}_n | \\mathbf{\\mu}_j, \\mathbf{\\Sigma}_j)} $$\n这个公式的解释是：给定数据点 $\\mathbf{x}_n$，它来自第 $k$ 个高斯分量的后验概率，正比于第 $k$ 个高斯分量的先验概率 $\\pi_k$ 乘以该高斯分量生成 $\\mathbf{x}_n$ 的似然。\n$\\gamma_{nk}$ 可以理解为数据点 $\\mathbf{x}n$ 对第 $k$ 个高斯分量的\"责任\"或\"软分配\"。与 K-means 的硬分配不同，$\\gamma{nk}$ 是一个 $[0, 1]$ 之间的概率值。\n完整的 GMM 模型 GMM 的完整模型包含：\n生成过程：\n从混合分布 $\\text{Categorical}(\\pi_1, \\ldots, \\pi_K)$ 中采样一个分量 $z$ 从 $\\mathcal{N}(\\mathbf{\\mu}_z, \\mathbf{\\Sigma}_z)$ 中采样 $\\mathbf{x}$ 参数集：\n$\\mathbf{\\pi} = (\\pi_1, \\pi_2, \\ldots, \\pi_K)$：混合系数 $\\mathbf{\\mu} = (\\mathbf{\\mu}_1, \\mathbf{\\mu}_2, \\ldots, \\mathbf{\\mu}_K)$：均值向量 $\\mathbf{\\Sigma} = (\\mathbf{\\Sigma}_1, \\mathbf{\\Sigma}_2, \\ldots, \\mathbf{\\Sigma}_K)$：协方差矩阵 隐变量：\n$\\mathbf{z}_1, \\mathbf{z}_2, \\ldots, \\mathbf{z}_N$：每个数据点的分量归属 EM 算法：从随机到最优的优雅迭代 现在，我们面临一个关键问题：给定数据集 ${\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_N}$，如何估计 GMM 的参数 $\\mathbf{\\pi}, \\mathbf{\\mu}, \\mathbf{\\Sigma}$？\n这是一个经典的含有隐变量的参数估计问题。直接使用最大似然估计会得到一个极其复杂的优化问题，无法解析求解。\nEM 算法提供了一种优雅的解决方案：通过迭代地优化下界来逐步改进参数估计。\n下界：对数似然函数的期望 设观测数据为 $\\mathbf{X} = {\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_N}$，隐变量为 $\\mathbf{Z} = {\\mathbf{z}_1, \\mathbf{z}_2, \\ldots, \\mathbf{z}_N}$。\n完全数据（观测+隐变量）的对数似然函数是：\n$$ \\mathcal{L}c(\\mathbf{\\pi}, \\mathbf{\\mu}, \\mathbf{\\Sigma} | \\mathbf{X}, \\mathbf{Z}) = \\sum{n=1}^{N} \\sum_{k=1}^{K} z_{nk} \\left[\\log \\pi_k + \\log \\mathcal{N}(\\mathbf{x}_n | \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)\\right] $$\n但 $\\mathbf{Z}$ 是未知的，我们无法直接优化 $\\mathcal{L}_c$。EM 算法的思路是：在给定当前参数的条件下，计算隐变量的后验期望，然后用这个期望来更新参数。\n定义 $Q$ 函数：\n$$ Q(\\mathbf{\\theta}, \\mathbf{\\theta}^{\\text{old}}) = E_{\\mathbf{Z}|\\mathbf{X}, \\mathbf{\\theta}^{\\text{old}}}[\\log p(\\mathbf{X}, \\mathbf{Z} | \\mathbf{\\theta})] $$\n其中 $\\mathbf{\\theta} = (\\mathbf{\\pi}, \\mathbf{\\mu}, \\mathbf{\\Sigma})$ 是所有参数。\nEM 算法的核心保证是：$Q$ 函数的增加意味着对数似然函数的增加（或至少不减少）。\nE 步：计算后验期望 给定当前参数 $\\mathbf{\\theta}^{\\text{old}}$，计算后验概率 $\\gamma_{nk}^{(t)}$：\n$$ \\gamma_{nk}^{(t)} = \\frac{\\pi_k^{(t)} \\mathcal{N}(\\mathbf{x}_n | \\mathbf{\\mu}_k^{(t)}, \\mathbf{\\Sigma}k^{(t)})}{\\sum{j=1}^{K} \\pi_j^{(t)} \\mathcal{N}(\\mathbf{x}_n | \\mathbf{\\mu}_j^{(t)}, \\mathbf{\\Sigma}_j^{(t)})} $$\n然后，计算 $Q$ 函数的期望。经过一些代数运算（这里我们略去繁琐的推导），$Q$ 函数可以写成：\n$$ Q(\\mathbf{\\theta}, \\mathbf{\\theta}^{(t)}) = \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\gamma_{nk}^{(t)} \\left[\\log \\pi_k + \\log \\mathcal{N}(\\mathbf{x}_n | \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)\\right] $$\nM 步：最大化 $Q$ 函数 现在，我们需要对 $\\mathbf{\\theta}$ 最大化 $Q$ 函数。这可以分成三个独立的问题。\n1. 更新混合系数 $\\pi_k$ 对 $\\pi_k$ 最大化 $Q$ 函数，带有约束 $\\sum_{k=1}^{K} \\pi_k = 1$ 和 $\\pi_k \\geq 0$。\n使用拉格朗日乘数法：\n$$ \\frac{\\partial}{\\partial \\pi_k} \\left[Q + \\lambda \\left(\\sum_{j=1}^{K} \\pi_j - 1\\right)\\right] = \\sum_{n=1}^{N} \\frac{\\gamma_{nk}^{(t)}}{\\pi_k} + \\lambda = 0 $$\n解得：\n$$ \\pi_k^{(t+1)} = \\frac{1}{N} \\sum_{n=1}^{N} \\gamma_{nk}^{(t)} $$\n直观上，新的混合系数是所有数据点对第 $k$ 个分量的平均责任。\n2. 更新均值 $\\mathbf{\\mu}_k$ 对 $\\mathbf{\\mu}_k$ 最大化 $Q$ 函数，我们得到：\n$$ \\mathbf{\\mu}k^{(t+1)} = \\frac{\\sum{n=1}^{N} \\gamma_{nk}^{(t)} \\mathbf{x}n}{\\sum{n=1}^{N} \\gamma_{nk}^{(t)}} $$\n直观上，新的均值是所有数据点的加权平均，权重是数据点对该分量的责任。\n3. 更新协方差矩阵 $\\mathbf{\\Sigma}_k$ 对 $\\mathbf{\\Sigma}_k$ 最大化 $Q$ 函数，我们得到：\n$$ \\mathbf{\\Sigma}k^{(t+1)} = \\frac{\\sum{n=1}^{N} \\gamma_{nk}^{(t)} (\\mathbf{x}_n - \\mathbf{\\mu}_k^{(t+1)})(\\mathbf{x}n - \\mathbf{\\mu}k^{(t+1)})^\\top}{\\sum{n=1}^{N} \\gamma{nk}^{(t)}} $$\n直观上，新的协方差矩阵是加权样本协方差，权重是责任。\nEM 算法的完整流程 综合起来，EM 算法的流程是：\n初始化：\n随机初始化参数 $\\mathbf{\\mu}^{(0)}, \\mathbf{\\Sigma}^{(0)}, \\mathbf{\\pi}^{(0)}$ 或使用 K-means++ 进行更好的初始化 迭代： 对于 $t = 0, 1, 2, \\ldots$：\nE 步：计算后验责任 $$ \\gamma_{nk}^{(t)} = \\frac{\\pi_k^{(t)} \\mathcal{N}(\\mathbf{x}_n | \\mathbf{\\mu}_k^{(t)}, \\mathbf{\\Sigma}k^{(t)})}{\\sum{j=1}^{K} \\pi_j^{(t)} \\mathcal{N}(\\mathbf{x}_n | \\mathbf{\\mu}_j^{(t)}, \\mathbf{\\Sigma}_j^{(t)})} $$\nM 步：更新参数\n$$ \\pi_k^{(t+1)} = \\frac{1}{N} \\sum_{n=1}^{N} \\gamma_{nk}^{(t)} $$\n$$ \\mathbf{\\mu}k^{(t+1)} = \\frac{\\sum{n=1}^{N} \\gamma_{nk}^{(t)} \\mathbf{x}n}{\\sum{n=1}^{N} \\gamma_{nk}^{(t)}} $$\n$$ \\mathbf{\\Sigma}k^{(t+1)} = \\frac{\\sum{n=1}^{N} \\gamma_{nk}^{(t)} (\\mathbf{x}_n - \\mathbf{\\mu}_k^{(t+1)})(\\mathbf{x}n - \\mathbf{\\mu}k^{(t+1)})^\\top}{\\sum{n=1}^{N} \\gamma{nk}^{(t)}} $$\n检查收敛：如果参数变化很小或对数似然函数变化很小，停止迭代\nEM 算法的收敛性：单调递增的保证 EM 算法有一个非常重要的性质：单调性保证。具体来说，每次迭代后，对数似然函数都会增加（或至少不减少）：\n$$ \\mathcal{L}(\\mathbf{\\theta}^{(t+1)}) \\geq \\mathcal{L}(\\mathbf{\\theta}^{(t)}) $$\n这个性质可以通过以下步骤证明：\n下界关系：对于任何参数 $\\mathbf{\\theta}$，有 $$ \\mathcal{L}(\\mathbf{\\theta}) \\geq Q(\\mathbf{\\theta}, \\mathbf{\\theta}^{(t)}) $$ 这是因为 $Q$ 函数是 $\\log p(\\mathbf{X}, \\mathbf{Z} | \\mathbf{\\theta})$ 对 $\\mathbf{Z}$ 的期望，而对数似然函数是 $\\log \\sum_{\\mathbf{Z}} p(\\mathbf{X}, \\mathbf{Z} | \\mathbf{\\theta})$。根据 Jensen 不等式，$\\log E[X] \\geq E[\\log X]$，所以不等式成立。\nE 步保持相等：$Q(\\mathbf{\\theta}^{(t)}, \\mathbf{\\theta}^{(t)}) = \\mathcal{L}(\\mathbf{\\theta}^{(t)})$，因为我们在 E 步中是用后验分布计算期望的。\nM 步最大化 $Q$：$\\mathbf{\\theta}^{(t+1)} = \\arg\\max_{\\mathbf{\\theta}} Q(\\mathbf{\\theta}, \\mathbf{\\theta}^{(t)})$\n综合： $$ \\mathcal{L}(\\mathbf{\\theta}^{(t+1)}) \\geq Q(\\mathbf{\\theta}^{(t+1)}, \\mathbf{\\theta}^{(t)}) \\geq Q(\\mathbf{\\theta}^{(t)}, \\mathbf{\\theta}^{(t)}) = \\mathcal{L}(\\mathbf{\\theta}^{(t)}) $$\n这个单调性保证意味着 EM 算法会收敛到局部最优解，但需要注意：\n收敛速度：开始快，后期慢 局部最优：可能陷入局部最优，取决于初始化 数值稳定性：协方差矩阵可能出现奇异性问题，需要正则化 几何直观：椭圆与等高线 GMM 的几何直观非常优美。每个高斯分量可以看作一个\"数据云\"，其形状是椭球。\n二维 GMM 的可视化 在二维情况下，每个高斯分量的等高线是椭圆。椭圆的长轴方向是协方差矩阵 $\\mathbf{\\Sigma}$ 的特征向量方向，轴的长度与特征值的平方根成正比。\n下图展示了一个简单的二维 GMM：\n图 1：二维 GMM 可视化。红色椭圆表示第一个高斯分量，蓝色椭圆表示第二个高斯分量。数据点用不同的颜色表示它们对每个分量的责任（颜色越深表示责任越大）\n软聚类的直观解释 想象每个数据点是一滴墨水，每个高斯分量是一片吸收墨水的海绵。$\\gamma_{nk}$ 表示墨水滴被第 $k$ 个海绵吸收的比例。\n在 E 步中，我们计算每滴墨水被每个海绵吸收的比例。在 M 步中，我们调整每个海绵的位置和形状，以便更好地吸收分配给它的墨水。\n与 K-means 的对比 K-means 可以看作 GMM 的一个特例：\n每个高斯分量的协方差矩阵是 $\\mathbf{\\Sigma}_k = \\sigma^2 \\mathbf{I}$（球形） 责任 $\\gamma_{nk}$ 硬化为 0 或 1（硬分配） 下图对比了 K-means 和 GMM 的差异：\n图 2：K-means vs GMM。K-means（左）进行硬分配，每个数据点只属于一个簇。GMM（右）进行软分配，每个数据点对所有簇都有责任值，颜色越深表示责任越大\nEM 算法的初始化：避免局部最优 EM 算法的一个关键问题是对数似然函数可能有多个局部最大值，而 EM 算法只能保证收敛到最近的局部最大值。初始化的好坏严重影响最终结果。\n随机初始化 最简单的方法是随机初始化：\n随机选择 $K$ 个数据点作为初始均值 将协方差矩阵初始化为单位矩阵 混合系数初始化为 $\\pi_k = 1/K$ 这种方法简单但可能收敛到次优解。\nK-means++ 初始化 K-means++ 是一种更好的初始化方法：\n随机选择第一个中心 对于 $k = 2$ 到 $K$： 计算每个数据点到最近已选中心的距离平方 $d_k(\\mathbf{x}_n)$ 选择下一个中心的概率与 $d_k(\\mathbf{x}_n)^2$ 成正比 用这 $K$ 个中心初始化 K-means，运行直到收敛 用 K-means 的结果初始化 GMM 的均值 计算每个簇的样本协方差作为 GMM 的初始协方差矩阵 计算每个簇的样本比例作为初始混合系数 这种方法能显著提高初始质量。\n实际应用：从语音到图像 GMM 在许多实际领域有广泛应用。\n应用一：语音识别 在语音识别中，GMM 用于建模声学模型。每个音素（如 /a/, /e/, /m/）可以用多个高斯分量建模，以捕捉不同的发音方式和说话人特征。\n例如，音素 /a/ 可能有 3-5 个高斯分量，分别对应不同说话人、不同口音或不同上下文。\n应用二：异常检测 GMM 可以用于异常检测：如果一个数据点对所有高斯分量都有很低的似然，则可能是异常点。\n方法：\n用正常数据训练 GMM 计算新数据点的似然 $p(\\mathbf{x}) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)$ 如果 $p(\\mathbf{x}) \u003c \\text{threshold}$，标记为异常 这在金融欺诈检测、网络入侵检测等领域有广泛应用。\n应用三：图像分割 在图像分割中，GMM 可以用于将像素聚类到不同的颜色区域。例如：\n将每个像素表示为一个三维向量 $(R, G, B)$ 用 GMM 对所有像素建模，选择 $K$ 个高斯分量 每个像素分配到责任最大的分量 结果是图像被分割成 $K$ 个颜色区域 这种方法简单但有效，常作为复杂图像分割算法的预处理步骤。\n应用四：文本建模 在自然语言处理中，GMM 可以用于建模词向量的分布，或者用于主题模型的变体。例如，可以用 GMM 将文档聚类到不同的主题。\nGMM 的优缺点与扩展 GMM 的优点 灵活性：能建模任意形状的数据分布（多个高斯分量的线性组合） 软聚类：提供概率分配，而非硬分配 理论基础完备：有坚实的统计理论支持 可解释性：每个高斯分量都有明确的物理意义 GMM 的缺点 局部最优：EM 算法可能陷入局部最优 初始化敏感：不同的初始化可能导致不同的结果 模型选择困难：如何选择 $K$（高斯分量数量）是一个开放问题 数值稳定性：协方差矩阵可能出现奇异性 模型选择：如何选择 $K$？ 选择高斯分量数量 $K$ 的常用方法：\nBIC (Bayesian Information Criterion)： $$ \\text{BIC}(K) = -2\\mathcal{L}_{\\text{max}}(K) + \\frac{p}{2} \\log N $$ 其中 $p$ 是参数数量。选择 BIC 最小的 $K$。\nAIC (Akaike Information Criterion)： $$ \\text{AIC}(K) = -2\\mathcal{L}_{\\text{max}}(K) + 2p $$\n交叉验证：将数据分为训练集和验证集，选择在验证集上表现最好的 $K$。\n扩展：贝叶斯 GMM 传统 GMM 的一个问题是过拟合：过多的高斯分量会导致模型过于复杂。贝叶斯 GMM 通过为每个高斯分量放置先验分布来解决过拟合问题。\n贝叶斯 GMM 使用变分推断或 MCMC 来计算后验分布，而非点估计。这提供了更完整的不确定性量化。\n总结：从数据中学习隐藏的艺术 GMM 是一个美丽而强大的算法。它用概率的语言描述了数据的隐藏结构，用 EM 算法优雅地解决了参数估计问题。\n从 K-means 到 GMM，我们看到了从硬聚类到软聚类的自然演进。从单一的球形簇，我们到了灵活的椭圆数据云。从简单的距离度量，我们到了复杂的概率模型。\nEM 算法的优雅之处在于：它不直接优化难以处理的似然函数，而是通过迭代地优化下界来逐步改进。这种方法在机器学习中有广泛应用，不仅在 GMM 中，还在隐马尔可夫模型、潜在狄利克雷分配等模型中。\nGMM 的哲学也值得思考：它假设数据是由简单的概率模型生成的，即使真实的数据生成过程可能更复杂。这种\"简约性假设\"是统计学的核心思想之一——我们用简单的模型来拟合复杂的数据，然后检查模型是否足够好。\n在实际应用中，GMM 与其他技术的结合往往能产生更好的效果。例如，GMM 可以用作更复杂模型的基础，或者在预处理阶段帮助理解数据结构。\n从观测数据中学习隐藏的结构，这是机器学习的终极目标之一。GMM 为我们提供了一个强有力的工具，让我们能够从混沌的数据中发现秩序，从噪声中提取信号。这不仅是数学的胜利，更是理解世界的艺术。\n","wordCount":"913","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/gmm-probability.jpg","datePublished":"2026-01-24T18:00:00+08:00","dateModified":"2026-01-24T18:00:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-24-gmm-comprehensive-guide/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">高斯混合模型：从数据中解构隐藏结构的艺术</h1><div class=post-description>深入探讨机器学习中的核心无监督学习算法 GMM，从高斯分布回顾到 EM 算法的完整推导，从几何直观到实际应用，娓娓道来。</div><div class=post-meta><span title='2026-01-24 18:00:00 +0800 CST'>January 24, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>913 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/gmm-probability.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/gmm-probability.jpg alt=高斯混合模型可视化></a><figcaption>数据中的隐藏结构</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e4%bb%8e%e6%b7%b7%e6%b2%8c%e4%b8%ad%e5%8f%91%e7%8e%b0%e7%bb%93%e6%9e%84 aria-label=引言：从混沌中发现结构>引言：从混沌中发现结构</a></li><li><a href=#%e9%ab%98%e6%96%af%e5%88%86%e5%b8%83%e7%9a%84%e5%9b%9e%e9%a1%be%e5%a4%9a%e5%85%83%e6%ad%a3%e6%80%81%e5%88%86%e5%b8%83 aria-label=高斯分布的回顾：多元正态分布>高斯分布的回顾：多元正态分布</a><ul><li><a href=#%e4%b8%80%e5%85%83%e9%ab%98%e6%96%af%e5%88%86%e5%b8%83 aria-label=一元高斯分布>一元高斯分布</a></li><li><a href=#%e5%a4%9a%e5%85%83%e9%ab%98%e6%96%af%e5%88%86%e5%b8%83 aria-label=多元高斯分布>多元高斯分布</a></li><li><a href=#%e5%8d%8f%e6%96%b9%e5%b7%ae%e7%9f%a9%e9%98%b5%e7%9a%84%e6%84%8f%e4%b9%89 aria-label=协方差矩阵的意义>协方差矩阵的意义</a></li><li><a href=#%e7%ad%89%e9%ab%98%e7%ba%bf%e9%ab%98%e6%96%af%e5%88%86%e5%b8%83%e7%9a%84%e8%bd%ae%e5%bb%93 aria-label='等高线：高斯分布的"轮廓"'>等高线：高斯分布的"轮廓"</a></li></ul></li><li><a href=#%e4%bb%8e-k-means-%e5%88%b0-gmm%e7%a1%ac%e8%81%9a%e7%b1%bb-vs-%e8%bd%af%e8%81%9a%e7%b1%bb aria-label="从 K-means 到 GMM：硬聚类 vs 软聚类">从 K-means 到 GMM：硬聚类 vs 软聚类</a><ul><li><a href=#k-means-%e7%9a%84%e5%b1%80%e9%99%90%e6%80%a7 aria-label="K-means 的局限性">K-means 的局限性</a></li><li><a href=#gmm-%e7%9a%84%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3 aria-label="GMM 的核心思想">GMM 的核心思想</a></li><li><a href=#%e8%bd%af%e8%81%9a%e7%b1%bb%e8%b4%a3%e4%bb%bbresponsibility aria-label=软聚类：责任（Responsibility）>软聚类：责任（Responsibility）</a></li><li><a href=#%e5%ae%8c%e6%95%b4%e7%9a%84-gmm-%e6%a8%a1%e5%9e%8b aria-label="完整的 GMM 模型">完整的 GMM 模型</a></li></ul></li><li><a href=#em-%e7%ae%97%e6%b3%95%e4%bb%8e%e9%9a%8f%e6%9c%ba%e5%88%b0%e6%9c%80%e4%bc%98%e7%9a%84%e4%bc%98%e9%9b%85%e8%bf%ad%e4%bb%a3 aria-label="EM 算法：从随机到最优的优雅迭代">EM 算法：从随机到最优的优雅迭代</a><ul><li><a href=#%e4%b8%8b%e7%95%8c%e5%af%b9%e6%95%b0%e4%bc%bc%e7%84%b6%e5%87%bd%e6%95%b0%e7%9a%84%e6%9c%9f%e6%9c%9b aria-label=下界：对数似然函数的期望>下界：对数似然函数的期望</a></li><li><a href=#e-%e6%ad%a5%e8%ae%a1%e7%ae%97%e5%90%8e%e9%aa%8c%e6%9c%9f%e6%9c%9b aria-label="E 步：计算后验期望">E 步：计算后验期望</a></li><li><a href=#m-%e6%ad%a5%e6%9c%80%e5%a4%a7%e5%8c%96-q-%e5%87%bd%e6%95%b0 aria-label="M 步：最大化 $Q$ 函数">M 步：最大化 $Q$ 函数</a><ul><li><a href=#1-%e6%9b%b4%e6%96%b0%e6%b7%b7%e5%90%88%e7%b3%bb%e6%95%b0-pi_k aria-label="1. 更新混合系数 $\pi_k$">1. 更新混合系数 $\pi_k$</a></li><li><a href=#2-%e6%9b%b4%e6%96%b0%e5%9d%87%e5%80%bc-mathbfmu_k aria-label="2. 更新均值 $\mathbf{\mu}_k$">2. 更新均值 $\mathbf{\mu}_k$</a></li><li><a href=#3-%e6%9b%b4%e6%96%b0%e5%8d%8f%e6%96%b9%e5%b7%ae%e7%9f%a9%e9%98%b5-mathbfsigma_k aria-label="3. 更新协方差矩阵 $\mathbf{\Sigma}_k$">3. 更新协方差矩阵 $\mathbf{\Sigma}_k$</a></li></ul></li><li><a href=#em-%e7%ae%97%e6%b3%95%e7%9a%84%e5%ae%8c%e6%95%b4%e6%b5%81%e7%a8%8b aria-label="EM 算法的完整流程">EM 算法的完整流程</a></li></ul></li><li><a href=#em-%e7%ae%97%e6%b3%95%e7%9a%84%e6%94%b6%e6%95%9b%e6%80%a7%e5%8d%95%e8%b0%83%e9%80%92%e5%a2%9e%e7%9a%84%e4%bf%9d%e8%af%81 aria-label="EM 算法的收敛性：单调递增的保证">EM 算法的收敛性：单调递增的保证</a></li><li><a href=#%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%82%e6%a4%ad%e5%9c%86%e4%b8%8e%e7%ad%89%e9%ab%98%e7%ba%bf aria-label=几何直观：椭圆与等高线>几何直观：椭圆与等高线</a><ul><li><a href=#%e4%ba%8c%e7%bb%b4-gmm-%e7%9a%84%e5%8f%af%e8%a7%86%e5%8c%96 aria-label="二维 GMM 的可视化">二维 GMM 的可视化</a></li><li><a href=#%e8%bd%af%e8%81%9a%e7%b1%bb%e7%9a%84%e7%9b%b4%e8%a7%82%e8%a7%a3%e9%87%8a aria-label=软聚类的直观解释>软聚类的直观解释</a></li><li><a href=#%e4%b8%8e-k-means-%e7%9a%84%e5%af%b9%e6%af%94 aria-label="与 K-means 的对比">与 K-means 的对比</a></li></ul></li><li><a href=#em-%e7%ae%97%e6%b3%95%e7%9a%84%e5%88%9d%e5%a7%8b%e5%8c%96%e9%81%bf%e5%85%8d%e5%b1%80%e9%83%a8%e6%9c%80%e4%bc%98 aria-label="EM 算法的初始化：避免局部最优">EM 算法的初始化：避免局部最优</a><ul><li><a href=#%e9%9a%8f%e6%9c%ba%e5%88%9d%e5%a7%8b%e5%8c%96 aria-label=随机初始化>随机初始化</a></li><li><a href=#k-means-%e5%88%9d%e5%a7%8b%e5%8c%96 aria-label="K-means++ 初始化">K-means++ 初始化</a></li></ul></li><li><a href=#%e5%ae%9e%e9%99%85%e5%ba%94%e7%94%a8%e4%bb%8e%e8%af%ad%e9%9f%b3%e5%88%b0%e5%9b%be%e5%83%8f aria-label=实际应用：从语音到图像>实际应用：从语音到图像</a><ul><li><a href=#%e5%ba%94%e7%94%a8%e4%b8%80%e8%af%ad%e9%9f%b3%e8%af%86%e5%88%ab aria-label=应用一：语音识别>应用一：语音识别</a></li><li><a href=#%e5%ba%94%e7%94%a8%e4%ba%8c%e5%bc%82%e5%b8%b8%e6%a3%80%e6%b5%8b aria-label=应用二：异常检测>应用二：异常检测</a></li><li><a href=#%e5%ba%94%e7%94%a8%e4%b8%89%e5%9b%be%e5%83%8f%e5%88%86%e5%89%b2 aria-label=应用三：图像分割>应用三：图像分割</a></li><li><a href=#%e5%ba%94%e7%94%a8%e5%9b%9b%e6%96%87%e6%9c%ac%e5%bb%ba%e6%a8%a1 aria-label=应用四：文本建模>应用四：文本建模</a></li></ul></li><li><a href=#gmm-%e7%9a%84%e4%bc%98%e7%bc%ba%e7%82%b9%e4%b8%8e%e6%89%a9%e5%b1%95 aria-label="GMM 的优缺点与扩展">GMM 的优缺点与扩展</a><ul><li><a href=#gmm-%e7%9a%84%e4%bc%98%e7%82%b9 aria-label="GMM 的优点">GMM 的优点</a></li><li><a href=#gmm-%e7%9a%84%e7%bc%ba%e7%82%b9 aria-label="GMM 的缺点">GMM 的缺点</a></li><li><a href=#%e6%a8%a1%e5%9e%8b%e9%80%89%e6%8b%a9%e5%a6%82%e4%bd%95%e9%80%89%e6%8b%a9-k aria-label="模型选择：如何选择 $K$？">模型选择：如何选择 $K$？</a></li><li><a href=#%e6%89%a9%e5%b1%95%e8%b4%9d%e5%8f%b6%e6%96%af-gmm aria-label="扩展：贝叶斯 GMM">扩展：贝叶斯 GMM</a></li></ul></li><li><a href=#%e6%80%bb%e7%bb%93%e4%bb%8e%e6%95%b0%e6%8d%ae%e4%b8%ad%e5%ad%a6%e4%b9%a0%e9%9a%90%e8%97%8f%e7%9a%84%e8%89%ba%e6%9c%af aria-label=总结：从数据中学习隐藏的艺术>总结：从数据中学习隐藏的艺术</a></li></ul></div></details></div><div class=post-content><h2 id=引言从混沌中发现结构>引言：从混沌中发现结构<a hidden class=anchor aria-hidden=true href=#引言从混沌中发现结构>#</a></h2><p>想象你是一个天文学家，正在观测夜空中的恒星。这些恒星并非均匀分布，而是呈现出明显的"聚集"现象：有些恒星形成了紧密的星团，有些则稀疏地散布在广阔的空间中。你的任务是理解这些恒星是如何分布的——它们属于哪些星团，每个星团的形状和位置是什么。</p><p>这就是一个典型的聚类问题：将数据点分组成若干个有意义的组。</p><p>最直观的聚类方法是 K-means：将每个数据点分配到最近的簇中心，然后更新簇中心，迭代直至收敛。但 K-means 有一个致命的限制：它假设每个簇是"圆形"的（在二维）或"球形"的（在高维）。这意味着它只能捕捉硬边界的簇，无法处理更复杂的形状，也无法表示一个数据点可能"部分地"属于多个簇。</p><p>这时，一个更强大的工具出现了：高斯混合模型（Gaussian Mixture Model, GMM）。GMM 不再做非此即彼的硬分类，而是给每个数据点一个"软"的归属概率——它有多大可能性属于每个簇。这种软聚类的方法不仅更灵活，而且能捕捉更复杂的数据分布。</p><p>更重要的是，GMM 引入了机器学习中最深刻的算法之一：EM 算法（Expectation-Maximization，期望最大化）。EM 算法是一种优雅的迭代算法，用于解决含有隐变量的概率模型的参数估计问题。</p><p>本文将带你深入 GMM 的世界。我们将从高斯分布的复习开始，理解从 K-means 到 GMM 的自然演进，推导 EM 算法的每一步，探索几何直观，最后了解它在现实世界的应用。准备好了吗？让我们开始这场从数据中发现隐藏结构的旅程。</p><h2 id=高斯分布的回顾多元正态分布>高斯分布的回顾：多元正态分布<a hidden class=anchor aria-hidden=true href=#高斯分布的回顾多元正态分布>#</a></h2><p>在深入 GMM 之前，我们需要先熟悉多元高斯分布（Multivariate Gaussian Distribution）的数学形式。</p><h3 id=一元高斯分布>一元高斯分布<a hidden class=anchor aria-hidden=true href=#一元高斯分布>#</a></h3><p>回忆一下，一元高斯分布的概率密度函数是：</p><p>$$
f(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$</p><p>其中：</p><ul><li>$\mu$ 是均值（期望）</li><li>$\sigma^2$ 是方差</li><li>$\sigma > 0$ 是标准差</li></ul><p>这个分布的形状是经典的"钟形曲线"：在 $\mu$ 处达到峰值，向两侧对称衰减。</p><h3 id=多元高斯分布>多元高斯分布<a hidden class=anchor aria-hidden=true href=#多元高斯分布>#</a></h3><p>多元高斯分布是上述概念的推广。设 $\mathbf{x} \in \mathbb{R}^d$ 是一个 $d$ 维随机向量，$\mathbf{\mu} \in \mathbb{R}^d$ 是均值向量，$\mathbf{\Sigma} \in \mathbb{R}^{d \times d}$ 是协方差矩阵（对称正定）。</p><p>多元高斯分布的概率密度函数是：</p><p>$$
f(\mathbf{x} | \mathbf{\mu}, \mathbf{\Sigma}) = \frac{1}{(2\pi)^{d/2} |\mathbf{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2} (\mathbf{x} - \mathbf{\mu})^\top \mathbf{\Sigma}^{-1} (\mathbf{x} - \mathbf{\mu})\right)
$$</p><p>这里需要解释几个符号：</p><ol><li><strong>行列式</strong> $|\mathbf{\Sigma}|$：协方差矩阵的行列式</li><li><strong>逆矩阵</strong> $\mathbf{\Sigma}^{-1}$：协方差矩阵的逆矩阵</li><li><strong>二次型</strong> $(\mathbf{x} - \mathbf{\mu})^\top \mathbf{\Sigma}^{-1} (\mathbf{x} - \mathbf{\mu})$：这给出了点 $\mathbf{x}$ 到均值 $\mathbf{\mu}$ 的"距离"</li></ol><h3 id=协方差矩阵的意义>协方差矩阵的意义<a hidden class=anchor aria-hidden=true href=#协方差矩阵的意义>#</a></h3><p>协方差矩阵 $\mathbf{\Sigma}$ 捕捉了数据的分布形状。对角线元素 $\Sigma_{ii}$ 是第 $i$ 个特征的方差，非对角线元素 $\Sigma_{ij}$ 是第 $i$ 个和第 $j$ 个特征的协方差：</p><p>$$
\Sigma_{ij} = \text{Cov}(X_i, X_j) = E[(X_i - \mu_i)(X_j - \mu_j)]
$$</p><p>如果 $\Sigma_{ij} = 0$，说明第 $i$ 个和第 $j$ 个特征不相关。</p><h3 id=等高线高斯分布的轮廓>等高线：高斯分布的"轮廓"<a hidden class=anchor aria-hidden=true href=#等高线高斯分布的轮廓>#</a></h3><p>多元高斯分布的等高线（即 $f(\mathbf{x})$ 取常数的曲面）是椭圆（在二维）或椭球（在高维）。这些椭球的长轴方向由 $\mathbf{\Sigma}$ 的特征向量给出，轴的长度由特征值给出。</p><p>这个几何理解非常重要：每个高斯分布可以看作一个"数据云"，其形状由均值（位置）和协方差矩阵（形状）决定。</p><h2 id=从-k-means-到-gmm硬聚类-vs-软聚类>从 K-means 到 GMM：硬聚类 vs 软聚类<a hidden class=anchor aria-hidden=true href=#从-k-means-到-gmm硬聚类-vs-软聚类>#</a></h2><h3 id=k-means-的局限性>K-means 的局限性<a hidden class=anchor aria-hidden=true href=#k-means-的局限性>#</a></h3><p>K-means 算法可以用一个简单的概率模型来解释：假设数据由 $K$ 个点源生成，每个点源以等概率生成数据点，且每个数据点服从以该点源为中心的各向同性高斯分布（方差在所有方向上相等）。</p><p>数学上，这等价于假设每个簇的协方差矩阵是 $\sigma^2 \mathbf{I}$（$\sigma^2$ 乘以单位矩阵），即"圆形"或"球形"的分布。</p><p>这个假设有两个问题：</p><ol><li><strong>形状限制</strong>：现实中的数据簇往往不是球形的。例如，考虑二维数据，如果簇是椭圆形的，K-means 可能会将一个椭圆簇分成两个球形簇。</li><li><strong>硬分配</strong>：每个数据点只能完全属于一个簇。但很多情况下，数据点确实"介于"两个簇之间。</li></ol><h3 id=gmm-的核心思想>GMM 的核心思想<a hidden class=anchor aria-hidden=true href=#gmm-的核心思想>#</a></h3><p>GMM 的核心思想是：用 $K$ 个高斯分布的线性组合来建模数据。每个数据点以一定的概率来自每个高斯分布，而且这个概率是我们需要学习的。</p><p>数学上，GMM 的概率密度函数是：</p><p>$$
p(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x} | \mathbf{\mu}_k, \mathbf{\Sigma}_k)
$$</p><p>其中：</p><ul><li>$K$ 是高斯分量（簇）的数量</li><li>$\pi_k$ 是混合系数（mixing coefficient），满足 $\sum_{k=1}^{K} \pi_k = 1$ 且 $\pi_k \geq 0$</li><li>$\mathbf{\mu}_k$ 是第 $k$ 个高斯分量的均值</li><li>$\mathbf{\Sigma}_k$ 是第 $k$ 个高斯分量的协方差矩阵</li><li>$\mathcal{N}(\mathbf{x} | \mathbf{\mu}_k, \mathbf{\Sigma}_k)$ 是均值为 $\mathbf{\mu}_k$、协方差矩阵为 $\mathbf{\Sigma}_k$ 的高斯分布</li></ul><h3 id=软聚类责任responsibility>软聚类：责任（Responsibility）<a hidden class=anchor aria-hidden=true href=#软聚类责任responsibility>#</a></h3><p>在 GMM 中，我们引入一个隐变量 $\mathbf{z}$，表示数据点来自哪个高斯分量。$\mathbf{z}$ 是一个 $K$ 维的 one-hot 向量，如果 $\mathbf{z} = \mathbf{e}_k$（第 $k$ 个元素为 1，其余为 0），则表示 $\mathbf{x}$ 来自第 $k$ 个高斯分量。</p><p>后验概率（responsibility）$\gamma_{nk}$ 定义为：</p><p>$$
\gamma_{nk} = p(z_k = 1 | \mathbf{x}_n, \mathbf{\pi}, \mathbf{\mu}, \mathbf{\Sigma}) = \frac{\pi_k \mathcal{N}(\mathbf{x}_n | \mathbf{\mu}_k, \mathbf{\Sigma}<em>k)}{\sum</em>{j=1}^{K} \pi_j \mathcal{N}(\mathbf{x}_n | \mathbf{\mu}_j, \mathbf{\Sigma}_j)}
$$</p><p>这个公式的解释是：给定数据点 $\mathbf{x}_n$，它来自第 $k$ 个高斯分量的后验概率，正比于第 $k$ 个高斯分量的先验概率 $\pi_k$ 乘以该高斯分量生成 $\mathbf{x}_n$ 的似然。</p><p>$\gamma_{nk}$ 可以理解为数据点 $\mathbf{x}<em>n$ 对第 $k$ 个高斯分量的"责任"或"软分配"。与 K-means 的硬分配不同，$\gamma</em>{nk}$ 是一个 $[0, 1]$ 之间的概率值。</p><h3 id=完整的-gmm-模型>完整的 GMM 模型<a hidden class=anchor aria-hidden=true href=#完整的-gmm-模型>#</a></h3><p>GMM 的完整模型包含：</p><ol><li><p><strong>生成过程</strong>：</p><ul><li>从混合分布 $\text{Categorical}(\pi_1, \ldots, \pi_K)$ 中采样一个分量 $z$</li><li>从 $\mathcal{N}(\mathbf{\mu}_z, \mathbf{\Sigma}_z)$ 中采样 $\mathbf{x}$</li></ul></li><li><p><strong>参数集</strong>：</p><ul><li>$\mathbf{\pi} = (\pi_1, \pi_2, \ldots, \pi_K)$：混合系数</li><li>$\mathbf{\mu} = (\mathbf{\mu}_1, \mathbf{\mu}_2, \ldots, \mathbf{\mu}_K)$：均值向量</li><li>$\mathbf{\Sigma} = (\mathbf{\Sigma}_1, \mathbf{\Sigma}_2, \ldots, \mathbf{\Sigma}_K)$：协方差矩阵</li></ul></li><li><p><strong>隐变量</strong>：</p><ul><li>$\mathbf{z}_1, \mathbf{z}_2, \ldots, \mathbf{z}_N$：每个数据点的分量归属</li></ul></li></ol><h2 id=em-算法从随机到最优的优雅迭代>EM 算法：从随机到最优的优雅迭代<a hidden class=anchor aria-hidden=true href=#em-算法从随机到最优的优雅迭代>#</a></h2><p>现在，我们面临一个关键问题：给定数据集 ${\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N}$，如何估计 GMM 的参数 $\mathbf{\pi}, \mathbf{\mu}, \mathbf{\Sigma}$？</p><p>这是一个经典的含有隐变量的参数估计问题。直接使用最大似然估计会得到一个极其复杂的优化问题，无法解析求解。</p><p>EM 算法提供了一种优雅的解决方案：通过迭代地优化下界来逐步改进参数估计。</p><h3 id=下界对数似然函数的期望>下界：对数似然函数的期望<a hidden class=anchor aria-hidden=true href=#下界对数似然函数的期望>#</a></h3><p>设观测数据为 $\mathbf{X} = {\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N}$，隐变量为 $\mathbf{Z} = {\mathbf{z}_1, \mathbf{z}_2, \ldots, \mathbf{z}_N}$。</p><p>完全数据（观测+隐变量）的对数似然函数是：</p><p>$$
\mathcal{L}<em>c(\mathbf{\pi}, \mathbf{\mu}, \mathbf{\Sigma} | \mathbf{X}, \mathbf{Z}) = \sum</em>{n=1}^{N} \sum_{k=1}^{K} z_{nk} \left[\log \pi_k + \log \mathcal{N}(\mathbf{x}_n | \mathbf{\mu}_k, \mathbf{\Sigma}_k)\right]
$$</p><p>但 $\mathbf{Z}$ 是未知的，我们无法直接优化 $\mathcal{L}_c$。EM 算法的思路是：在给定当前参数的条件下，计算隐变量的后验期望，然后用这个期望来更新参数。</p><p>定义 $Q$ 函数：</p><p>$$
Q(\mathbf{\theta}, \mathbf{\theta}^{\text{old}}) = E_{\mathbf{Z}|\mathbf{X}, \mathbf{\theta}^{\text{old}}}[\log p(\mathbf{X}, \mathbf{Z} | \mathbf{\theta})]
$$</p><p>其中 $\mathbf{\theta} = (\mathbf{\pi}, \mathbf{\mu}, \mathbf{\Sigma})$ 是所有参数。</p><p>EM 算法的核心保证是：$Q$ 函数的增加意味着对数似然函数的增加（或至少不减少）。</p><h3 id=e-步计算后验期望>E 步：计算后验期望<a hidden class=anchor aria-hidden=true href=#e-步计算后验期望>#</a></h3><p>给定当前参数 $\mathbf{\theta}^{\text{old}}$，计算后验概率 $\gamma_{nk}^{(t)}$：</p><p>$$
\gamma_{nk}^{(t)} = \frac{\pi_k^{(t)} \mathcal{N}(\mathbf{x}_n | \mathbf{\mu}_k^{(t)}, \mathbf{\Sigma}<em>k^{(t)})}{\sum</em>{j=1}^{K} \pi_j^{(t)} \mathcal{N}(\mathbf{x}_n | \mathbf{\mu}_j^{(t)}, \mathbf{\Sigma}_j^{(t)})}
$$</p><p>然后，计算 $Q$ 函数的期望。经过一些代数运算（这里我们略去繁琐的推导），$Q$ 函数可以写成：</p><p>$$
Q(\mathbf{\theta}, \mathbf{\theta}^{(t)}) = \sum_{n=1}^{N} \sum_{k=1}^{K} \gamma_{nk}^{(t)} \left[\log \pi_k + \log \mathcal{N}(\mathbf{x}_n | \mathbf{\mu}_k, \mathbf{\Sigma}_k)\right]
$$</p><h3 id=m-步最大化-q-函数>M 步：最大化 $Q$ 函数<a hidden class=anchor aria-hidden=true href=#m-步最大化-q-函数>#</a></h3><p>现在，我们需要对 $\mathbf{\theta}$ 最大化 $Q$ 函数。这可以分成三个独立的问题。</p><h4 id=1-更新混合系数-pi_k>1. 更新混合系数 $\pi_k$<a hidden class=anchor aria-hidden=true href=#1-更新混合系数-pi_k>#</a></h4><p>对 $\pi_k$ 最大化 $Q$ 函数，带有约束 $\sum_{k=1}^{K} \pi_k = 1$ 和 $\pi_k \geq 0$。</p><p>使用拉格朗日乘数法：</p><p>$$
\frac{\partial}{\partial \pi_k} \left[Q + \lambda \left(\sum_{j=1}^{K} \pi_j - 1\right)\right] = \sum_{n=1}^{N} \frac{\gamma_{nk}^{(t)}}{\pi_k} + \lambda = 0
$$</p><p>解得：</p><p>$$
\pi_k^{(t+1)} = \frac{1}{N} \sum_{n=1}^{N} \gamma_{nk}^{(t)}
$$</p><p>直观上，新的混合系数是所有数据点对第 $k$ 个分量的平均责任。</p><h4 id=2-更新均值-mathbfmu_k>2. 更新均值 $\mathbf{\mu}_k$<a hidden class=anchor aria-hidden=true href=#2-更新均值-mathbfmu_k>#</a></h4><p>对 $\mathbf{\mu}_k$ 最大化 $Q$ 函数，我们得到：</p><p>$$
\mathbf{\mu}<em>k^{(t+1)} = \frac{\sum</em>{n=1}^{N} \gamma_{nk}^{(t)} \mathbf{x}<em>n}{\sum</em>{n=1}^{N} \gamma_{nk}^{(t)}}
$$</p><p>直观上，新的均值是所有数据点的加权平均，权重是数据点对该分量的责任。</p><h4 id=3-更新协方差矩阵-mathbfsigma_k>3. 更新协方差矩阵 $\mathbf{\Sigma}_k$<a hidden class=anchor aria-hidden=true href=#3-更新协方差矩阵-mathbfsigma_k>#</a></h4><p>对 $\mathbf{\Sigma}_k$ 最大化 $Q$ 函数，我们得到：</p><p>$$
\mathbf{\Sigma}<em>k^{(t+1)} = \frac{\sum</em>{n=1}^{N} \gamma_{nk}^{(t)} (\mathbf{x}_n - \mathbf{\mu}_k^{(t+1)})(\mathbf{x}<em>n - \mathbf{\mu}<em>k^{(t+1)})^\top}{\sum</em>{n=1}^{N} \gamma</em>{nk}^{(t)}}
$$</p><p>直观上，新的协方差矩阵是加权样本协方差，权重是责任。</p><h3 id=em-算法的完整流程>EM 算法的完整流程<a hidden class=anchor aria-hidden=true href=#em-算法的完整流程>#</a></h3><p>综合起来，EM 算法的流程是：</p><p><strong>初始化</strong>：</p><ol><li>随机初始化参数 $\mathbf{\mu}^{(0)}, \mathbf{\Sigma}^{(0)}, \mathbf{\pi}^{(0)}$<ul><li>或使用 K-means++ 进行更好的初始化</li></ul></li></ol><p><strong>迭代</strong>：
对于 $t = 0, 1, 2, \ldots$：</p><ol><li><p><strong>E 步</strong>：计算后验责任
$$
\gamma_{nk}^{(t)} = \frac{\pi_k^{(t)} \mathcal{N}(\mathbf{x}_n | \mathbf{\mu}_k^{(t)}, \mathbf{\Sigma}<em>k^{(t)})}{\sum</em>{j=1}^{K} \pi_j^{(t)} \mathcal{N}(\mathbf{x}_n | \mathbf{\mu}_j^{(t)}, \mathbf{\Sigma}_j^{(t)})}
$$</p></li><li><p><strong>M 步</strong>：更新参数</p><p>$$
\pi_k^{(t+1)} = \frac{1}{N} \sum_{n=1}^{N} \gamma_{nk}^{(t)}
$$</p><p>$$
\mathbf{\mu}<em>k^{(t+1)} = \frac{\sum</em>{n=1}^{N} \gamma_{nk}^{(t)} \mathbf{x}<em>n}{\sum</em>{n=1}^{N} \gamma_{nk}^{(t)}}
$$</p><p>$$
\mathbf{\Sigma}<em>k^{(t+1)} = \frac{\sum</em>{n=1}^{N} \gamma_{nk}^{(t)} (\mathbf{x}_n - \mathbf{\mu}_k^{(t+1)})(\mathbf{x}<em>n - \mathbf{\mu}<em>k^{(t+1)})^\top}{\sum</em>{n=1}^{N} \gamma</em>{nk}^{(t)}}
$$</p></li><li><p><strong>检查收敛</strong>：如果参数变化很小或对数似然函数变化很小，停止迭代</p></li></ol><h2 id=em-算法的收敛性单调递增的保证>EM 算法的收敛性：单调递增的保证<a hidden class=anchor aria-hidden=true href=#em-算法的收敛性单调递增的保证>#</a></h2><p>EM 算法有一个非常重要的性质：单调性保证。具体来说，每次迭代后，对数似然函数都会增加（或至少不减少）：</p><p>$$
\mathcal{L}(\mathbf{\theta}^{(t+1)}) \geq \mathcal{L}(\mathbf{\theta}^{(t)})
$$</p><p>这个性质可以通过以下步骤证明：</p><ol><li><p><strong>下界关系</strong>：对于任何参数 $\mathbf{\theta}$，有
$$
\mathcal{L}(\mathbf{\theta}) \geq Q(\mathbf{\theta}, \mathbf{\theta}^{(t)})
$$
这是因为 $Q$ 函数是 $\log p(\mathbf{X}, \mathbf{Z} | \mathbf{\theta})$ 对 $\mathbf{Z}$ 的期望，而对数似然函数是 $\log \sum_{\mathbf{Z}} p(\mathbf{X}, \mathbf{Z} | \mathbf{\theta})$。根据 Jensen 不等式，$\log E[X] \geq E[\log X]$，所以不等式成立。</p></li><li><p><strong>E 步保持相等</strong>：$Q(\mathbf{\theta}^{(t)}, \mathbf{\theta}^{(t)}) = \mathcal{L}(\mathbf{\theta}^{(t)})$，因为我们在 E 步中是用后验分布计算期望的。</p></li><li><p><strong>M 步最大化 $Q$</strong>：$\mathbf{\theta}^{(t+1)} = \arg\max_{\mathbf{\theta}} Q(\mathbf{\theta}, \mathbf{\theta}^{(t)})$</p></li><li><p><strong>综合</strong>：
$$
\mathcal{L}(\mathbf{\theta}^{(t+1)}) \geq Q(\mathbf{\theta}^{(t+1)}, \mathbf{\theta}^{(t)}) \geq Q(\mathbf{\theta}^{(t)}, \mathbf{\theta}^{(t)}) = \mathcal{L}(\mathbf{\theta}^{(t)})
$$</p></li></ol><p>这个单调性保证意味着 EM 算法会收敛到局部最优解，但需要注意：</p><ul><li>收敛速度：开始快，后期慢</li><li>局部最优：可能陷入局部最优，取决于初始化</li><li>数值稳定性：协方差矩阵可能出现奇异性问题，需要正则化</li></ul><h2 id=几何直观椭圆与等高线>几何直观：椭圆与等高线<a hidden class=anchor aria-hidden=true href=#几何直观椭圆与等高线>#</a></h2><p>GMM 的几何直观非常优美。每个高斯分量可以看作一个"数据云"，其形状是椭球。</p><h3 id=二维-gmm-的可视化>二维 GMM 的可视化<a hidden class=anchor aria-hidden=true href=#二维-gmm-的可视化>#</a></h3><p>在二维情况下，每个高斯分量的等高线是椭圆。椭圆的长轴方向是协方差矩阵 $\mathbf{\Sigma}$ 的特征向量方向，轴的长度与特征值的平方根成正比。</p><p>下图展示了一个简单的二维 GMM：</p><p><img alt="GMM 2D 可视化" loading=lazy src=/images/math/gmm-2d-visualization.png></p><p><strong>图 1</strong>：二维 GMM 可视化。红色椭圆表示第一个高斯分量，蓝色椭圆表示第二个高斯分量。数据点用不同的颜色表示它们对每个分量的责任（颜色越深表示责任越大）</p><h3 id=软聚类的直观解释>软聚类的直观解释<a hidden class=anchor aria-hidden=true href=#软聚类的直观解释>#</a></h3><p>想象每个数据点是一滴墨水，每个高斯分量是一片吸收墨水的海绵。$\gamma_{nk}$ 表示墨水滴被第 $k$ 个海绵吸收的比例。</p><p>在 E 步中，我们计算每滴墨水被每个海绵吸收的比例。在 M 步中，我们调整每个海绵的位置和形状，以便更好地吸收分配给它的墨水。</p><h3 id=与-k-means-的对比>与 K-means 的对比<a hidden class=anchor aria-hidden=true href=#与-k-means-的对比>#</a></h3><p>K-means 可以看作 GMM 的一个特例：</p><ul><li>每个高斯分量的协方差矩阵是 $\mathbf{\Sigma}_k = \sigma^2 \mathbf{I}$（球形）</li><li>责任 $\gamma_{nk}$ 硬化为 0 或 1（硬分配）</li></ul><p>下图对比了 K-means 和 GMM 的差异：</p><p><img alt="K-means vs GMM 对比" loading=lazy src=/images/math/kmeans-vs-gmm-comparison.png></p><p><strong>图 2</strong>：K-means vs GMM。K-means（左）进行硬分配，每个数据点只属于一个簇。GMM（右）进行软分配，每个数据点对所有簇都有责任值，颜色越深表示责任越大</p><h2 id=em-算法的初始化避免局部最优>EM 算法的初始化：避免局部最优<a hidden class=anchor aria-hidden=true href=#em-算法的初始化避免局部最优>#</a></h2><p>EM 算法的一个关键问题是对数似然函数可能有多个局部最大值，而 EM 算法只能保证收敛到最近的局部最大值。初始化的好坏严重影响最终结果。</p><h3 id=随机初始化>随机初始化<a hidden class=anchor aria-hidden=true href=#随机初始化>#</a></h3><p>最简单的方法是随机初始化：</p><ul><li>随机选择 $K$ 个数据点作为初始均值</li><li>将协方差矩阵初始化为单位矩阵</li><li>混合系数初始化为 $\pi_k = 1/K$</li></ul><p>这种方法简单但可能收敛到次优解。</p><h3 id=k-means-初始化>K-means++ 初始化<a hidden class=anchor aria-hidden=true href=#k-means-初始化>#</a></h3><p>K-means++ 是一种更好的初始化方法：</p><ol><li>随机选择第一个中心</li><li>对于 $k = 2$ 到 $K$：<ul><li>计算每个数据点到最近已选中心的距离平方 $d_k(\mathbf{x}_n)$</li><li>选择下一个中心的概率与 $d_k(\mathbf{x}_n)^2$ 成正比</li></ul></li><li>用这 $K$ 个中心初始化 K-means，运行直到收敛</li><li>用 K-means 的结果初始化 GMM 的均值</li><li>计算每个簇的样本协方差作为 GMM 的初始协方差矩阵</li><li>计算每个簇的样本比例作为初始混合系数</li></ol><p>这种方法能显著提高初始质量。</p><h2 id=实际应用从语音到图像>实际应用：从语音到图像<a hidden class=anchor aria-hidden=true href=#实际应用从语音到图像>#</a></h2><p>GMM 在许多实际领域有广泛应用。</p><h3 id=应用一语音识别>应用一：语音识别<a hidden class=anchor aria-hidden=true href=#应用一语音识别>#</a></h3><p>在语音识别中，GMM 用于建模声学模型。每个音素（如 /a/, /e/, /m/）可以用多个高斯分量建模，以捕捉不同的发音方式和说话人特征。</p><p>例如，音素 /a/ 可能有 3-5 个高斯分量，分别对应不同说话人、不同口音或不同上下文。</p><h3 id=应用二异常检测>应用二：异常检测<a hidden class=anchor aria-hidden=true href=#应用二异常检测>#</a></h3><p>GMM 可以用于异常检测：如果一个数据点对所有高斯分量都有很低的似然，则可能是异常点。</p><p><strong>方法</strong>：</p><ol><li>用正常数据训练 GMM</li><li>计算新数据点的似然 $p(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x} | \mathbf{\mu}_k, \mathbf{\Sigma}_k)$</li><li>如果 $p(\mathbf{x}) &lt; \text{threshold}$，标记为异常</li></ol><p>这在金融欺诈检测、网络入侵检测等领域有广泛应用。</p><h3 id=应用三图像分割>应用三：图像分割<a hidden class=anchor aria-hidden=true href=#应用三图像分割>#</a></h3><p>在图像分割中，GMM 可以用于将像素聚类到不同的颜色区域。例如：</p><ol><li>将每个像素表示为一个三维向量 $(R, G, B)$</li><li>用 GMM 对所有像素建模，选择 $K$ 个高斯分量</li><li>每个像素分配到责任最大的分量</li><li>结果是图像被分割成 $K$ 个颜色区域</li></ol><p>这种方法简单但有效，常作为复杂图像分割算法的预处理步骤。</p><h3 id=应用四文本建模>应用四：文本建模<a hidden class=anchor aria-hidden=true href=#应用四文本建模>#</a></h3><p>在自然语言处理中，GMM 可以用于建模词向量的分布，或者用于主题模型的变体。例如，可以用 GMM 将文档聚类到不同的主题。</p><h2 id=gmm-的优缺点与扩展>GMM 的优缺点与扩展<a hidden class=anchor aria-hidden=true href=#gmm-的优缺点与扩展>#</a></h2><h3 id=gmm-的优点>GMM 的优点<a hidden class=anchor aria-hidden=true href=#gmm-的优点>#</a></h3><ol><li><strong>灵活性</strong>：能建模任意形状的数据分布（多个高斯分量的线性组合）</li><li><strong>软聚类</strong>：提供概率分配，而非硬分配</li><li><strong>理论基础完备</strong>：有坚实的统计理论支持</li><li><strong>可解释性</strong>：每个高斯分量都有明确的物理意义</li></ol><h3 id=gmm-的缺点>GMM 的缺点<a hidden class=anchor aria-hidden=true href=#gmm-的缺点>#</a></h3><ol><li><strong>局部最优</strong>：EM 算法可能陷入局部最优</li><li><strong>初始化敏感</strong>：不同的初始化可能导致不同的结果</li><li><strong>模型选择困难</strong>：如何选择 $K$（高斯分量数量）是一个开放问题</li><li><strong>数值稳定性</strong>：协方差矩阵可能出现奇异性</li></ol><h3 id=模型选择如何选择-k>模型选择：如何选择 $K$？<a hidden class=anchor aria-hidden=true href=#模型选择如何选择-k>#</a></h3><p>选择高斯分量数量 $K$ 的常用方法：</p><ol><li><p><strong>BIC (Bayesian Information Criterion)</strong>：
$$
\text{BIC}(K) = -2\mathcal{L}_{\text{max}}(K) + \frac{p}{2} \log N
$$
其中 $p$ 是参数数量。选择 BIC 最小的 $K$。</p></li><li><p><strong>AIC (Akaike Information Criterion)</strong>：
$$
\text{AIC}(K) = -2\mathcal{L}_{\text{max}}(K) + 2p
$$</p></li><li><p><strong>交叉验证</strong>：将数据分为训练集和验证集，选择在验证集上表现最好的 $K$。</p></li></ol><h3 id=扩展贝叶斯-gmm>扩展：贝叶斯 GMM<a hidden class=anchor aria-hidden=true href=#扩展贝叶斯-gmm>#</a></h3><p>传统 GMM 的一个问题是过拟合：过多的高斯分量会导致模型过于复杂。贝叶斯 GMM 通过为每个高斯分量放置先验分布来解决过拟合问题。</p><p>贝叶斯 GMM 使用变分推断或 MCMC 来计算后验分布，而非点估计。这提供了更完整的不确定性量化。</p><h2 id=总结从数据中学习隐藏的艺术>总结：从数据中学习隐藏的艺术<a hidden class=anchor aria-hidden=true href=#总结从数据中学习隐藏的艺术>#</a></h2><p>GMM 是一个美丽而强大的算法。它用概率的语言描述了数据的隐藏结构，用 EM 算法优雅地解决了参数估计问题。</p><p>从 K-means 到 GMM，我们看到了从硬聚类到软聚类的自然演进。从单一的球形簇，我们到了灵活的椭圆数据云。从简单的距离度量，我们到了复杂的概率模型。</p><p>EM 算法的优雅之处在于：它不直接优化难以处理的似然函数，而是通过迭代地优化下界来逐步改进。这种方法在机器学习中有广泛应用，不仅在 GMM 中，还在隐马尔可夫模型、潜在狄利克雷分配等模型中。</p><p>GMM 的哲学也值得思考：它假设数据是由简单的概率模型生成的，即使真实的数据生成过程可能更复杂。这种"简约性假设"是统计学的核心思想之一——我们用简单的模型来拟合复杂的数据，然后检查模型是否足够好。</p><p>在实际应用中，GMM 与其他技术的结合往往能产生更好的效果。例如，GMM 可以用作更复杂模型的基础，或者在预处理阶段帮助理解数据结构。</p><p>从观测数据中学习隐藏的结构，这是机器学习的终极目标之一。GMM 为我们提供了一个强有力的工具，让我们能够从混沌的数据中发现秩序，从噪声中提取信号。这不仅是数学的胜利，更是理解世界的艺术。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/gmm/>GMM</a></li><li><a href=https://s-ai-unix.github.io/tags/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/>高斯混合模型</a></li><li><a href=https://s-ai-unix.github.io/tags/em%E7%AE%97%E6%B3%95/>EM算法</a></li><li><a href=https://s-ai-unix.github.io/tags/%E8%81%9A%E7%B1%BB/>聚类</a></li><li><a href=https://s-ai-unix.github.io/tags/%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/>高斯分布</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7%E5%8C%96/>期望最大化</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-24-variational-autoencoder/><span class=title>« Prev</span><br><span>变分自编码器：从概率建模到深度生成的优雅桥梁</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-24-bayesian-classifier/><span class=title>Next »</span><br><span>贝叶斯分类器：从条件概率到智能决策的优雅之旅</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 高斯混合模型：从数据中解构隐藏结构的艺术 on x" href="https://x.com/intent/tweet/?text=%e9%ab%98%e6%96%af%e6%b7%b7%e5%90%88%e6%a8%a1%e5%9e%8b%ef%bc%9a%e4%bb%8e%e6%95%b0%e6%8d%ae%e4%b8%ad%e8%a7%a3%e6%9e%84%e9%9a%90%e8%97%8f%e7%bb%93%e6%9e%84%e7%9a%84%e8%89%ba%e6%9c%af&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-24-gmm-comprehensive-guide%2f&amp;hashtags=GMM%2c%e9%ab%98%e6%96%af%e6%b7%b7%e5%90%88%e6%a8%a1%e5%9e%8b%2cEM%e7%ae%97%e6%b3%95%2c%e8%81%9a%e7%b1%bb%2c%e9%ab%98%e6%96%af%e5%88%86%e5%b8%83%2c%e6%9c%9f%e6%9c%9b%e6%9c%80%e5%a4%a7%e5%8c%96"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 高斯混合模型：从数据中解构隐藏结构的艺术 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-24-gmm-comprehensive-guide%2f&amp;title=%e9%ab%98%e6%96%af%e6%b7%b7%e5%90%88%e6%a8%a1%e5%9e%8b%ef%bc%9a%e4%bb%8e%e6%95%b0%e6%8d%ae%e4%b8%ad%e8%a7%a3%e6%9e%84%e9%9a%90%e8%97%8f%e7%bb%93%e6%9e%84%e7%9a%84%e8%89%ba%e6%9c%af&amp;summary=%e9%ab%98%e6%96%af%e6%b7%b7%e5%90%88%e6%a8%a1%e5%9e%8b%ef%bc%9a%e4%bb%8e%e6%95%b0%e6%8d%ae%e4%b8%ad%e8%a7%a3%e6%9e%84%e9%9a%90%e8%97%8f%e7%bb%93%e6%9e%84%e7%9a%84%e8%89%ba%e6%9c%af&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-24-gmm-comprehensive-guide%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 高斯混合模型：从数据中解构隐藏结构的艺术 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-24-gmm-comprehensive-guide%2f&title=%e9%ab%98%e6%96%af%e6%b7%b7%e5%90%88%e6%a8%a1%e5%9e%8b%ef%bc%9a%e4%bb%8e%e6%95%b0%e6%8d%ae%e4%b8%ad%e8%a7%a3%e6%9e%84%e9%9a%90%e8%97%8f%e7%bb%93%e6%9e%84%e7%9a%84%e8%89%ba%e6%9c%af"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 高斯混合模型：从数据中解构隐藏结构的艺术 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-24-gmm-comprehensive-guide%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>