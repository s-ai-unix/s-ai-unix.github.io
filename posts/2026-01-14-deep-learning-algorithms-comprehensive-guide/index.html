<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>基于神经网络的深度学习算法：从感知机到Transformer的完整指南 | s-ai-unix's Blog</title><meta name=keywords content="深度学习,神经网络,机器学习,数学史,综述"><meta name=description content="本文全面回顾深度学习算法的发展历程、数学原理、架构演进及未来前景，涵盖从基础神经网络到Transformer的完整演进路径。"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-14-deep-learning-algorithms-comprehensive-guide/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-14-deep-learning-algorithms-comprehensive-guide/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-14-deep-learning-algorithms-comprehensive-guide/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="基于神经网络的深度学习算法：从感知机到Transformer的完整指南"><meta property="og:description" content="本文全面回顾深度学习算法的发展历程、数学原理、架构演进及未来前景，涵盖从基础神经网络到Transformer的完整演进路径。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-14T08:30:00+08:00"><meta property="article:modified_time" content="2026-01-14T08:30:00+08:00"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="神经网络"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="数学史"><meta property="article:tag" content="综述"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/photo-1620712943543-bcc4688e7485.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/photo-1620712943543-bcc4688e7485.jpg"><meta name=twitter:title content="基于神经网络的深度学习算法：从感知机到Transformer的完整指南"><meta name=twitter:description content="本文全面回顾深度学习算法的发展历程、数学原理、架构演进及未来前景，涵盖从基础神经网络到Transformer的完整演进路径。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"基于神经网络的深度学习算法：从感知机到Transformer的完整指南","item":"https://s-ai-unix.github.io/posts/2026-01-14-deep-learning-algorithms-comprehensive-guide/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"基于神经网络的深度学习算法：从感知机到Transformer的完整指南","name":"基于神经网络的深度学习算法：从感知机到Transformer的完整指南","description":"本文全面回顾深度学习算法的发展历程、数学原理、架构演进及未来前景，涵盖从基础神经网络到Transformer的完整演进路径。","keywords":["深度学习","神经网络","机器学习","数学史","综述"],"articleBody":"引言：从生物启发到智能革命 1943年，Warren McCulloch和Walter Pitts提出了第一个神经元数学模型。他们用一个简单的数学公式模拟了生物神经元的工作方式：接收输入、加权求和、激活输出。这个看似简单的想法，却孕育了后来改变世界的人工智能技术。\n1958年，Frank Rosenblatt发明了感知机（Perceptron），这是第一个可以学习的神经网络。但1969年，Minsky和Papert在《Perceptrons》一书中证明了单层感知机无法解决异或（XOR）问题，这个致命缺陷导致了神经网络研究的第一次寒冬。\n1986年，David Rumelhart、Geoffrey Hinton和Ronald Williams重新发现了反向传播算法，解决了多层网络的训练问题。神经网络迎来了短暂的春天。\n但在90年代到2000年代初，支持向量机（SVM）等传统机器学习算法统治了学术界。神经网络因为数据量不足、计算能力有限、缺乏有效的训练技巧，再次陷入沉寂。\n2012年，ImageNet竞赛上，Hinton的学生Alex Krizhevsky使用深度卷积神经网络AlexNet，以压倒性优势击败了传统方法，分类错误率从26%降低到15.3%。这一年，深度学习时代正式开启。\n从此，深度学习以惊人的速度发展：2014年的VGG、GoogLeNet，2015年的ResNet解决深度退化问题，2017年的Transformer彻底改变自然语言处理，2022年的ChatGPT让全世界见识到大模型的力量。\n本文将从数学原理出发，系统讲解深度学习的核心算法：从基础神经网络到卷积神经网络（CNN），从循环神经网络（RNN）到Transformer，最后探讨未来发展趋势。\n第一章：神经网络的数学基础 1.1 单神经元：感知机的数学模型 1.1.1 前向传播 感知机是最基础的神经网络单元，模拟生物神经元的工作原理。给定输入向量 $x \\in \\mathbb{R}^d$，权重向量 $w \\in \\mathbb{R}^d$，偏置 $b \\in \\mathbb{R}$：\n$$z = w^Tx + b = \\sum_{i=1}^d w_i x_i + b$$\n激活函数 $\\sigma(z)$ 决定神经元的输出：\n$$a = \\sigma(z)$$\n1.1.2 常用激活函数 Sigmoid函数： $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n导数： $$\\sigma’(z) = \\sigma(z)(1 - \\sigma(z))$$\n性质：\n输出范围：$(0, 1)$ S型曲线，可微 缺点：梯度消失（$| \\sigma’(z) | \\leq 0.25$），输出不以零为中心 Tanh函数： $$\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n导数： $$\\tanh’(z) = 1 - \\tanh^2(z)$$\n性质：\n输出范围：$(-1, 1)$ 以零为中心，比Sigmoid收敛更快 ReLU（Rectified Linear Unit）： $$\\text{ReLU}(z) = \\max(0, z)$$\n导数： $$\\text{ReLU}’(z) = \\begin{cases} 1 \u0026 z \u003e 0 \\ 0 \u0026 z \\leq 0 \\end{cases}$$\n性质：\n计算简单（不涉及指数运算） 缓解梯度消失问题 缺点：神经元\"死亡\"（$z \\leq 0$ 时梯度为0） Leaky ReLU： $$\\text{LeakyReLU}(z) = \\max(\\alpha z, z), \\quad \\alpha \u003c 1$$\n解决神经元死亡问题（$\\alpha$ 是小正数，如0.01）\nSwish： $$\\text{Swish}(z) = z \\cdot \\sigma(z) = \\frac{z}{1 + e^{-z}}$$\n在许多任务中表现优于ReLU\n1.2 多层前馈神经网络 1.2.1 网络结构 多层神经网络由输入层、隐藏层、输出层组成。设网络有 $L$ 层，第 $l$ 层有 $n^{[l]}$ 个神经元。\n记号：\n$W^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times n^{[l-1]}}$：第 $l$ 层的权重矩阵 $b^{[l]} \\in \\mathbb{R}^{n^{[l]}}$：第 $l$ 层的偏置向量 $Z^{[l]} \\in \\mathbb{R}^{n^{[l]}}$：第 $l$ 层的线性变换结果 $A^{[l]} \\in \\mathbb{R}^{n^{[l]}}$：第 $l$ 层的激活输出 1.2.2 前向传播 第1层（输入层到第1个隐藏层）： $$Z^{[1]} = W^{[1]}X + b^{[1]}$$ $$A^{[1]} = \\sigma^{[1]}(Z^{[1]})$$\n第 $l$ 层（$l = 2, \\ldots, L$）： $$Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$$ $$A^{[l]} = \\sigma^{[l]}(Z^{[l]})$$\n其中 $X = A^{[0]}$ 是输入，$\\hat{Y} = A^{[L]}$ 是网络输出。\n1.2.3 向量化实现 给定 $m$ 个样本的训练集 $X \\in \\mathbb{R}^{d \\times m}$，前向传播可以矩阵化：\n$$Z^{[1]} = W^{[1]}X + b^{[1]}$$ $$A^{[1]} = \\sigma^{[1]}(Z^{[1]})$$ $$\\vdots$$ $$Z^{[L]} = W^{[L]}A^{[L-1]} + b^{[L]}$$ $$\\hat{Y} = A^{[L]} = \\sigma^{[L]}(Z^{[L]})$$\n这种实现利用矩阵运算，可以利用GPU加速。\n1.3 损失函数 1.3.1 回归任务 均方误差（MSE）： $$\\mathcal{L} = \\frac{1}{2m}\\sum_{i=1}^m (y^{(i)} - \\hat{y}^{(i)})^2$$\n平均绝对误差（MAE）： $$\\mathcal{L} = \\frac{1}{m}\\sum_{i=1}^m |y^{(i)} - \\hat{y}^{(i)}|$$\n1.3.2 分类任务 交叉熵损失（多分类）： 设 $y \\in {0, 1}^K$ 是one-hot编码，$\\hat{y} = \\text{softmax}(z)$ 是预测概率：\n$$\\mathcal{L} = -\\frac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^K y_k^{(i)} \\log \\hat{y}_k^{(i)}$$\nSoftmax函数： $$\\text{softmax}(z)k = \\frac{e^{z_k}}{\\sum{j=1}^K e^{z_j}}$$\n性质：\n输出是概率分布（$\\sum_k \\hat{y}_k = 1$，$\\hat{y}_k \u003e 0$） 对数空间计算数值稳定 交叉熵损失（二分类）： $$\\mathcal{L} = -\\frac{1}{m}\\sum_{i=1}^m [y^{(i)} \\log \\hat{y}^{(i)} + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})]$$\n1.4 反向传播算法 反向传播是深度学习的核心算法，用于高效计算梯度。\n1.4.1 链式法则 损失函数对参数的梯度可以通过链式法则计算：\n$$\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial W^{[l]}}$$\n$$\\frac{\\partial \\mathcal{L}}{\\partial b^{[l]}} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial b^{[l]}}$$\n1.4.2 反向传播推导 定义误差项（error term）： $$dZ^{[l]} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{[l]}}$$\n输出层： $$dZ^{[L]} = A^{[L]} - Y$$\n（这是使用交叉熵损失和Softmax激活的简化结果）\n隐藏层（从后向前传播）： $$dA^{[l]} = (W^{[l+1]})^T dZ^{[l+1]}$$ $$dZ^{[l]} = dA^{[l]} \\odot \\sigma^{[l]’}(Z^{[l]})$$\n其中 $\\odot$ 是逐元素乘积（Hadamard product）。\n梯度计算： $$dW^{[l]} = \\frac{1}{m} dZ^{[l]} (A^{[l-1]})^T$$ $$db^{[l]} = \\frac{1}{m} \\sum_{i=1}^m dZ^{[l]}_{:, i}$$\n1.4.3 算法复杂度 前向传播：$O(\\sum_{l=1}^L n^{[l]} n^{[l-1]})$\n反向传播：$O(\\sum_{l=1}^L n^{[l]} n^{[l-1]})$\n两者复杂度相同！这是反向传播算法的高效之处。\n1.5 梯度下降与优化算法 1.5.1 批量梯度下降（Batch GD） 每次迭代使用所有样本：\n$$W^{[l]} := W^{[l]} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}}$$\n缺点：数据量大时速度慢\n1.5.2 随机梯度下降（SGD） 每次迭代使用一个样本：\n$$W^{[l]} := W^{[l]} - \\alpha \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial W^{[l]}}$$\n优点：速度快，但梯度方差大\n1.5.3 小批量梯度下降（Mini-batch GD） 每次迭代使用一批样本（常用64、128、256）：\n$$W^{[l]} := W^{[l]} - \\alpha \\frac{1}{m_t} \\sum_{i \\in \\mathcal{B}_t} \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial W^{[l]}}$$\n结合了BGD和SGD的优点\n1.5.4 动量法（Momentum） 引入速度项，加速收敛：\n$$v_{dW^{[l]}} := \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}}$$ $$W^{[l]} := W^{[l]} - \\alpha v_{dW^{[l]}}$$\n$\\beta_1 \\approx 0.9$，控制历史梯度的衰减率\n1.5.5 Adam优化器 结合动量法和RMSprop，自适应学习率：\n计算动量： $$m_{dW^{[l]}} := \\beta_1 m_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}}$$ $$v_{dW^{[l]}} := \\beta_2 v_{dW^{[l]}} + (1 - \\beta_2) \\left(\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}}\\right)^2$$\n偏差修正： $$\\hat{m}{dW^{[l]}} = \\frac{m{dW^{[l]}}}{1 - \\beta_1^t}$$ $$\\hat{v}{dW^{[l]}} = \\frac{v{dW^{[l]}}}{1 - \\beta_2^t}$$\n参数更新： $$W^{[l]} := W^{[l]} - \\alpha \\frac{\\hat{m}{dW^{[l]}}}{\\sqrt{\\hat{v}{dW^{[l]}}} + \\epsilon}$$\n超参数：$\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-8}$\n1.6 正则化技术 1.6.1 L2正则化（权重衰减） $$\\mathcal{L}{\\text{reg}} = \\mathcal{L} + \\frac{\\lambda}{2m} \\sum{l=1}^L |W^{[l]}|_F^2$$\n梯度更新： $$dW^{[l]}_{\\text{reg}} = dW^{[l]} + \\frac{\\lambda}{m} W^{[l]}$$\n权重被\"拉向\"零\n1.6.2 Dropout 训练时以概率 $p$ 随机丢弃神经元：\n$$\\tilde{a}^{[l]} = a^{[l]} \\odot r^{[l]}, \\quad r^{[l]}_i \\sim \\text{Bernoulli}(1-p)$$\n$$\\hat{a}^{[l]} = \\frac{\\tilde{a}^{[l]}}{1-p}$$（缩放保持期望）\n测试时使用所有神经元，权重乘以 $(1-p)$\n作用：防止过拟合，相当于训练了多个子网络的集成\n1.6.3 批量归一化（Batch Normalization） 标准化每层的激活值：\n训练时： $$\\mu^{[l]} = \\frac{1}{m} \\sum_{i=1}^m Z^{[l]}{:, i}$$ $$\\sigma^2^{[l]} = \\frac{1}{m} \\sum{i=1}^m (Z^{[l]}{:, i} - \\mu^{[l]})^2$$ $$\\hat{Z}^{[l]} = \\frac{Z^{[l]} - \\mu^{[l]}}{\\sqrt{\\sigma^2^{[l]} + \\epsilon}}$$ $$Z{\\text{BN}}^{[l]} = \\gamma^{[l]} \\odot \\hat{Z}^{[l]} + \\beta^{[l]}$$\n可学习参数：$\\gamma^{[l]}$（缩放）、$\\beta^{[l]}$（平移）\n测试时：使用移动平均的均值和方差\n作用：\n加速训练 允许使用更大学习率 减少对初始化的敏感度 1.6.4 早停法（Early Stopping） 在验证集上监控性能，性能不再提升时停止训练\n第二章：卷积神经网络（CNN） 2.1 卷积操作 2.1.1 2D卷积 给定输入特征图 $X \\in \\mathbb{R}^{H \\times W \\times C}$，卷积核 $K \\in \\mathbb{R}^{k_h \\times k_w \\times C}$：\n$$Z_{i,j} = \\sum_{c=1}^C \\sum_{u=0}^{k_h-1} \\sum_{v=0}^{k_w-1} X_{i+u, j+v, c} \\cdot K_{u,v,c} + b$$\n其中 $b \\in \\mathbb{R}$ 是偏置。\n2.1.2 步长（Stride） 步长 $s$ 控制卷积核滑动的步长，输出尺寸：\n$$H’ = \\left\\lfloor \\frac{H - k_h}{s} \\right\\rfloor + 1$$ $$W’ = \\left\\lfloor \\frac{W - k_w}{s} \\right\\rfloor + 1$$\n2.1.3 填充（Padding） 填充 $p$ 在输入周围补零，输出尺寸：\n$$H’ = \\left\\lfloor \\frac{H + 2p - k_h}{s} \\right\\rfloor + 1$$ $$W’ = \\left\\lfloor \\frac{W + 2p - k_w}{s} \\right\\rfloor + 1$$\nValid padding（不填充）：$p = 0$ Same padding（保持尺寸）：$p = \\lfloor \\frac{k-1}{2} \\rfloor$\n2.1.4 通道 多个卷积核产生多个输出通道：\n输入 $X \\in \\mathbb{R}^{H \\times W \\times C_{in}}$，卷积核组 $K \\in \\mathbb{R}^{k_h \\times k_w \\times C_{in} \\times C_{out}}$\n输出 $Z \\in \\mathbb{R}^{H’ \\times W’ \\times C_{out}}$\n2.1.5 卷积 vs 全连接 卷积层是稀疏连接的权重共享：\n参数数量：$k_h \\times k_w \\times C_{in} \\times C_{out}$（与输入尺寸无关） 平移不变性：相同模式在不同位置识别 全连接层：\n参数数量：$H \\times W \\times C_{in} \\times n_{out}$（与输入尺寸成正比） 2.2 池化层 2.2.1 最大池化（Max Pooling） $$Z_{i,j} = \\max_{0 \\leq u \u003c k_h, 0 \\leq v \u003c k_w} X_{i \\cdot s + u, j \\cdot s + v}$$\n作用：\n降采样，减少计算量 平移不变性 防止过拟合 2.2.2 平均池化（Average Pooling） $$Z_{i,j} = \\frac{1}{k_h k_w} \\sum_{u=0}^{k_h-1} \\sum_{v=0}^{k_w-1} X_{i \\cdot s + u, j \\cdot s + v}$$\n2.2.3 全局平均池化（Global Average Pooling） 对每个通道取平均，输出 $1 \\times 1 \\times C$：\n$$Z_c = \\frac{1}{HW} \\sum_{i=1}^H \\sum_{j=1}^W X_{i,j,c}$$\n常用于替代全连接层，减少参数\n2.3 经典CNN架构 2.3.1 LeNet-5（1998） Yann LeCun设计的手写数字识别网络（MNIST）\n结构：\n卷积层（6个5×5卷积核，步长1，padding0）→ 激活（tanh） 平均池化（2×2，步长2） 卷积层（16个5×5卷积核）→ 激活 平均池化（2×2，步长2） 卷积层（120个5×5卷积核）→ 激活 全连接层（84个神经元） 输出层（10个神经元，softmax） 参数量：约6万个\n2.3.2 AlexNet（2012） 深度学习革命的起点，ImageNet冠军\n结构：\n卷积层（96个11×11卷积核，步长4）→ ReLU → 最大池化（3×3，步长2）→ 局部响应归一化（LRN） 卷积层（256个5×5卷积核，padding2）→ ReLU → 最大池化 → LRN 卷积层（384个3×3卷积核，padding1）→ ReLU 卷积层（384个3×3卷积核，padding1）→ ReLU 卷积层（256个3×3卷积核，padding1）→ ReLU → 最大池化 全连接层（4096个神经元）→ Dropout（0.5） 全连接层（4096个神经元）→ Dropout（0.5） 输出层（1000个神经元，softmax） 参数量：约6000万个\n创新点：\n使用ReLU激活（加速训练） Dropout防止过拟合 数据增强（平移、翻转、颜色变化） GPU加速训练（两块GTX 580） 2.3.3 VGG（2014） “简单但有效\"的设计理念\n核心思想：使用小卷积核（3×3）堆叠代替大卷积核\n两个3×3卷积的感受野相当于一个5×5卷积： $$3 \\times 3 \\to 3 \\times 3 \\to \\text{感受野} = 5 \\times 5$$\n优点：\n参数更少：$2 \\times 3^2 = 18 \u003c 5^2 = 25$ 更多非线性层（每个3×3后都有ReLU） 深度可以更深 VGG-16结构：\nConv3-64（2个3×3卷积，64通道）→ MaxPool Conv3-128（2个3×3卷积，128通道）→ MaxPool Conv3-256（3个3×3卷积，256通道）→ MaxPool Conv3-512（3个3×3卷积，512通道）→ MaxPool Conv3-512（3个3×3卷积，512通道）→ MaxPool FC-4096 → FC-4096 → FC-1000 参数量：约1.38亿个\n2.3.4 GoogLeNet（Inception v1）（2014） 引入Inception模块，多尺度特征提取\nInception模块： 并行多个不同大小的卷积核（1×1, 3×3, 5×5），并拼接结果\n问题：计算量大\n优化：使用1×1卷积降维（bottleneck）\n在3×3和5×5卷积前加1×1卷积减少通道数 在池化后加1×1卷积减少通道数 GoogLeNet结构：\n9个Inception模块堆叠 使用全局平均池化替代全连接层 辅助分类器（中间层）加速训练 参数量：约600万个（远少于AlexNet）\n2.3.5 ResNet（2015） 解决深度网络的退化问题（degradation）\n残差连接： $$y = F(x, {W}) + x$$\n其中 $F(x, {W})$ 是残差函数（至少2层）\n为什么有效？\n传统网络学习 $H(x)$，ResNet学习残差 $F(x) = H(x) - x$\n如果最优是恒等映射 $H(x) = x$，则只需让 $F(x) = 0$（更容易） 梯度可以通过恒等连接直接传播，缓解梯度消失 ResNet-50结构：\n使用bottleneck设计：1×1降维 → 3×3卷积 → 1×1升维 4个stage，通道数逐步增加（64, 128, 256, 512） 每个stage有多个残差块（3, 4, 6, 3） 参数量：约2560万个\n深度表现：\nResNet-18, 34：基本残差块 ResNet-50, 101, 152：bottleneck残差块 ResNet-152在ImageNet上达到3.57%的top-5错误率（低于人类5.1%） 2.3.6 DenseNet（2017） 密集连接网络，每一层都与前面的所有层连接\n密集块（Dense Block）： $$x_l = H_l([x_0, x_1, \\ldots, x_{l-1}])$$\n其中 $[x_0, x_1, \\ldots, x_{l-1}]$ 是前面所有层输出的拼接\n优点：\n特征复用，参数更少 梯度流动更顺畅 缓解梯度消失 过渡层（Transition Layer）： 在密集块之间，进行降维和降采样\n1×1卷积降维 2×2平均池化 DenseNet-121结构：\n4个密集块（增长率 $k = 32$） 每个块有 6, 12, 24, 16 层 参数量：约800万个 2.4 CNN的应用 2.4.1 图像分类 任务：给定图像，预测类别标签\n数据集：\nImageNet（1000类，140万张图像） CIFAR-10/100（10/100类，6万张小图像） MNIST（10类手写数字） 2.4.2 目标检测 任务：定位图像中的物体并分类\n算法：\nR-CNN系列（R-CNN, Fast R-CNN, Faster R-CNN）：两阶段检测 YOLO（You Only Look Once）：单阶段检测 SSD（Single Shot MultiBox Detector）：多尺度单阶段检测 2.4.3 语义分割 任务：为图像中的每个像素分类\n算法：\nFCN（Fully Convolutional Network）：将全连接层替换为卷积层 U-Net：编码器-解码器结构，跳跃连接 DeepLab：空洞卷积扩大感受野 2.4.4 人脸识别 任务：验证或识别人脸\n算法：\nFaceNet：使用triplet loss学习人脸嵌入 ArcFace：添加角度间隔margin CosFace：余弦间隔 第三章：循环神经网络（RNN） 3.1 RNN基础 3.1.1 序列建模问题 传统神经网络处理固定尺寸输入，无法处理变长序列。\nRNN（Recurrent Neural Network）通过隐藏状态传递历史信息。\n3.1.2 RNN前向传播 给定序列 $x = (x_1, x_2, \\ldots, x_T)$\n初始化：$h_0 = \\mathbf{0}$\n对于 $t = 1, 2, \\ldots, T$： $$h_t = \\sigma_h(W_h h_{t-1} + W_x x_t + b_h)$$ $$\\hat{y}_t = \\sigma_y(W_y h_t + b_y)$$\n其中：\n$W_h \\in \\mathbb{R}^{n_h \\times n_h}$：隐藏状态到隐藏状态的权重 $W_x \\in \\mathbb{R}^{n_h \\times n_x}$：输入到隐藏状态的权重 $W_y \\in \\mathbb{R}^{n_y \\times n_h}$：隐藏状态到输出的权重 $n_h$：隐藏状态维度 $\\sigma_h, \\sigma_y$：激活函数 3.1.3 时间展开（Unrolling） RNN在时间步上展开，等价于深度网络：\n同样的参数 $W_h, W_x, W_y$ 在不同时间步共享 深度 = 序列长度 $T$ 3.1.4 梯度消失与梯度爆炸 反向传播时（BPTT），梯度通过时间反向传播：\n$$\\frac{\\partial \\mathcal{L}}{\\partial h_t} = \\prod_{k=t}^{T} \\frac{\\partial h_{k+1}}{\\partial h_k} \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_{k+1}}$$\n其中 $\\frac{\\partial h_{k+1}}{\\partial h_k} = \\text{diag}(\\sigma_h’(z_k)) W_h$\n梯度消失：如果 $|\\text{diag}(\\sigma_h’(z_k)) W_h| \u003c 1$，长程依赖无法学习\n梯度爆炸：如果 $|\\text{diag}(\\sigma_h’(z_k)) W_h| \u003e 1$，梯度指数增长，训练不稳定\n解决方法：\n梯度裁剪（Gradient Clipping）：$|g| \\leftarrow \\min(|g|, \\theta)$ 使用LSTM/GRU门控结构 3.2 LSTM（Long Short-Term Memory） 3.2.1 核心思想 LSTM通过门控机制（gating）控制信息的流动，选择性记忆和遗忘。\n3.2.2 LSTM单元 给定输入 $x_t$ 和前一隐藏状态 $h_{t-1}$、细胞状态 $c_{t-1}$\n遗忘门（Forget Gate）：决定丢弃什么信息 $$f_t = \\sigma(W_f [h_{t-1}, x_t] + b_f)$$\n输入门（Input Gate）：决定存储什么新信息 $$i_t = \\sigma(W_i [h_{t-1}, x_t] + b_i)$$ $$\\tilde{c}t = \\tanh(W_c [h{t-1}, x_t] + b_c)$$\n细胞状态更新： $$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$$\n输出门（Output Gate）：决定输出什么 $$o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o)$$ $$h_t = o_t \\odot \\tanh(c_t)$$\n其中 $\\odot$ 是逐元素乘积。\n3.2.3 为什么LSTM有效？ 细胞状态 $c_t$ 作为\"高速公路”：梯度可以直接传播，缓解梯度消失 门控机制：动态控制信息流 遗忘门 $f_t \\to 0$：遗忘长期信息 遗忘门 $f_t \\to 1$：保持长期信息 输入门 $i_t \\to 0$：忽略当前输入 输入门 $i_t \\to 1$：记录当前输入 3.3 GRU（Gated Recurrent Unit） 3.3.1 简化的LSTM GRU是LSTM的简化版本，参数更少。\n3.3.2 GRU单元 $$z_t = \\sigma(W_z [h_{t-1}, x_t] + b_z)$$（更新门） $$r_t = \\sigma(W_r [h_{t-1}, x_t] + b_r)$$（重置门） $$\\tilde{h}t = \\tanh(W_h [r_t \\odot h{t-1}, x_t] + b_h)$$ $$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n更新门 $z_t$：控制前一隐藏状态的保留程度\n$z_t \\to 0$：保留 $h_{t-1}$ $z_t \\to 1$：更新为 $\\tilde{h}_t$ 重置门 $r_t$：控制前一隐藏状态在候选新状态中的参与度\n$r_t \\to 0$：忽略 $h_{t-1}$，类似\"重置\" $r_t \\to 1$：考虑 $h_{t-1}$ 3.4 双向RNN（BiRNN） 有些任务需要同时考虑过去和未来信息。\n双向RNN：\n前向RNN：$ \\overrightarrow{h}t = RNN{\\text{forward}}(x_t, \\overrightarrow{h}_{t-1}) $ 后向RNN：$ \\overleftarrow{h}t = RNN{\\text{backward}}(x_t, \\overleftarrow{h}_{t+1}) $ 组合：$h_t = [\\overrightarrow{h}_t; \\overleftarrow{h}_t]$ 应用：机器翻译、命名实体识别、语音识别\n3.5 RNN的应用 3.5.1 语言模型 任务：给定前面的词，预测下一个词\n$$P(w_t | w_{","wordCount":"2188","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/photo-1620712943543-bcc4688e7485.jpg","datePublished":"2026-01-14T08:30:00+08:00","dateModified":"2026-01-14T08:30:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-14-deep-learning-algorithms-comprehensive-guide/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">基于神经网络的深度学习算法：从感知机到Transformer的完整指南</h1><div class=post-description>本文全面回顾深度学习算法的发展历程、数学原理、架构演进及未来前景，涵盖从基础神经网络到Transformer的完整演进路径。</div><div class=post-meta><span title='2026-01-14 08:30:00 +0800 CST'>January 14, 2026</span>&nbsp;·&nbsp;<span>11 min</span>&nbsp;·&nbsp;<span>2188 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/photo-1620712943543-bcc4688e7485.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/photo-1620712943543-bcc4688e7485.jpg alt=神经网络连接></a><figcaption>深度之美</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e4%bb%8e%e7%94%9f%e7%89%a9%e5%90%af%e5%8f%91%e5%88%b0%e6%99%ba%e8%83%bd%e9%9d%a9%e5%91%bd aria-label=引言：从生物启发到智能革命>引言：从生物启发到智能革命</a></li><li><a href=#%e7%ac%ac%e4%b8%80%e7%ab%a0%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e6%95%b0%e5%ad%a6%e5%9f%ba%e7%a1%80 aria-label=第一章：神经网络的数学基础>第一章：神经网络的数学基础</a><ul><li><a href=#11-%e5%8d%95%e7%a5%9e%e7%bb%8f%e5%85%83%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e6%95%b0%e5%ad%a6%e6%a8%a1%e5%9e%8b aria-label="1.1 单神经元：感知机的数学模型">1.1 单神经元：感知机的数学模型</a><ul><li><a href=#111-%e5%89%8d%e5%90%91%e4%bc%a0%e6%92%ad aria-label="1.1.1 前向传播">1.1.1 前向传播</a></li><li><a href=#112-%e5%b8%b8%e7%94%a8%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0 aria-label="1.1.2 常用激活函数">1.1.2 常用激活函数</a></li></ul></li><li><a href=#12-%e5%a4%9a%e5%b1%82%e5%89%8d%e9%a6%88%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="1.2 多层前馈神经网络">1.2 多层前馈神经网络</a><ul><li><a href=#121-%e7%bd%91%e7%bb%9c%e7%bb%93%e6%9e%84 aria-label="1.2.1 网络结构">1.2.1 网络结构</a></li><li><a href=#122-%e5%89%8d%e5%90%91%e4%bc%a0%e6%92%ad aria-label="1.2.2 前向传播">1.2.2 前向传播</a></li><li><a href=#123-%e5%90%91%e9%87%8f%e5%8c%96%e5%ae%9e%e7%8e%b0 aria-label="1.2.3 向量化实现">1.2.3 向量化实现</a></li></ul></li><li><a href=#13-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0 aria-label="1.3 损失函数">1.3 损失函数</a><ul><li><a href=#131-%e5%9b%9e%e5%bd%92%e4%bb%bb%e5%8a%a1 aria-label="1.3.1 回归任务">1.3.1 回归任务</a></li><li><a href=#132-%e5%88%86%e7%b1%bb%e4%bb%bb%e5%8a%a1 aria-label="1.3.2 分类任务">1.3.2 分类任务</a></li></ul></li><li><a href=#14-%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95 aria-label="1.4 反向传播算法">1.4 反向传播算法</a><ul><li><a href=#141-%e9%93%be%e5%bc%8f%e6%b3%95%e5%88%99 aria-label="1.4.1 链式法则">1.4.1 链式法则</a></li><li><a href=#142-%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e6%8e%a8%e5%af%bc aria-label="1.4.2 反向传播推导">1.4.2 反向传播推导</a></li><li><a href=#143-%e7%ae%97%e6%b3%95%e5%a4%8d%e6%9d%82%e5%ba%a6 aria-label="1.4.3 算法复杂度">1.4.3 算法复杂度</a></li></ul></li><li><a href=#15-%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e4%b8%8e%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95 aria-label="1.5 梯度下降与优化算法">1.5 梯度下降与优化算法</a><ul><li><a href=#151-%e6%89%b9%e9%87%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dbatch-gd aria-label="1.5.1 批量梯度下降（Batch GD）">1.5.1 批量梯度下降（Batch GD）</a></li><li><a href=#152-%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dsgd aria-label="1.5.2 随机梯度下降（SGD）">1.5.2 随机梯度下降（SGD）</a></li><li><a href=#153-%e5%b0%8f%e6%89%b9%e9%87%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dmini-batch-gd aria-label="1.5.3 小批量梯度下降（Mini-batch GD）">1.5.3 小批量梯度下降（Mini-batch GD）</a></li><li><a href=#154-%e5%8a%a8%e9%87%8f%e6%b3%95momentum aria-label="1.5.4 动量法（Momentum）">1.5.4 动量法（Momentum）</a></li><li><a href=#155-adam%e4%bc%98%e5%8c%96%e5%99%a8 aria-label="1.5.5 Adam优化器">1.5.5 Adam优化器</a></li></ul></li><li><a href=#16-%e6%ad%a3%e5%88%99%e5%8c%96%e6%8a%80%e6%9c%af aria-label="1.6 正则化技术">1.6 正则化技术</a><ul><li><a href=#161-l2%e6%ad%a3%e5%88%99%e5%8c%96%e6%9d%83%e9%87%8d%e8%a1%b0%e5%87%8f aria-label="1.6.1 L2正则化（权重衰减）">1.6.1 L2正则化（权重衰减）</a></li><li><a href=#162-dropout aria-label="1.6.2 Dropout">1.6.2 Dropout</a></li><li><a href=#163-%e6%89%b9%e9%87%8f%e5%bd%92%e4%b8%80%e5%8c%96batch-normalization aria-label="1.6.3 批量归一化（Batch Normalization）">1.6.3 批量归一化（Batch Normalization）</a></li><li><a href=#164-%e6%97%a9%e5%81%9c%e6%b3%95early-stopping aria-label="1.6.4 早停法（Early Stopping）">1.6.4 早停法（Early Stopping）</a></li></ul></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e7%ab%a0%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9ccnn aria-label=第二章：卷积神经网络（CNN）>第二章：卷积神经网络（CNN）</a><ul><li><a href=#21-%e5%8d%b7%e7%a7%af%e6%93%8d%e4%bd%9c aria-label="2.1 卷积操作">2.1 卷积操作</a><ul><li><a href=#211-2d%e5%8d%b7%e7%a7%af aria-label="2.1.1 2D卷积">2.1.1 2D卷积</a></li><li><a href=#212-%e6%ad%a5%e9%95%bfstride aria-label="2.1.2 步长（Stride）">2.1.2 步长（Stride）</a></li><li><a href=#213-%e5%a1%ab%e5%85%85padding aria-label="2.1.3 填充（Padding）">2.1.3 填充（Padding）</a></li><li><a href=#214-%e9%80%9a%e9%81%93 aria-label="2.1.4 通道">2.1.4 通道</a></li><li><a href=#215-%e5%8d%b7%e7%a7%af-vs-%e5%85%a8%e8%bf%9e%e6%8e%a5 aria-label="2.1.5 卷积 vs 全连接">2.1.5 卷积 vs 全连接</a></li></ul></li><li><a href=#22-%e6%b1%a0%e5%8c%96%e5%b1%82 aria-label="2.2 池化层">2.2 池化层</a><ul><li><a href=#221-%e6%9c%80%e5%a4%a7%e6%b1%a0%e5%8c%96max-pooling aria-label="2.2.1 最大池化（Max Pooling）">2.2.1 最大池化（Max Pooling）</a></li><li><a href=#222-%e5%b9%b3%e5%9d%87%e6%b1%a0%e5%8c%96average-pooling aria-label="2.2.2 平均池化（Average Pooling）">2.2.2 平均池化（Average Pooling）</a></li><li><a href=#223-%e5%85%a8%e5%b1%80%e5%b9%b3%e5%9d%87%e6%b1%a0%e5%8c%96global-average-pooling aria-label="2.2.3 全局平均池化（Global Average Pooling）">2.2.3 全局平均池化（Global Average Pooling）</a></li></ul></li><li><a href=#23-%e7%bb%8f%e5%85%b8cnn%e6%9e%b6%e6%9e%84 aria-label="2.3 经典CNN架构">2.3 经典CNN架构</a><ul><li><a href=#231-lenet-51998 aria-label="2.3.1 LeNet-5（1998）">2.3.1 LeNet-5（1998）</a></li><li><a href=#232-alexnet2012 aria-label="2.3.2 AlexNet（2012）">2.3.2 AlexNet（2012）</a></li><li><a href=#233-vgg2014 aria-label="2.3.3 VGG（2014）">2.3.3 VGG（2014）</a></li><li><a href=#234-googlenetinception-v12014 aria-label="2.3.4 GoogLeNet（Inception v1）（2014）">2.3.4 GoogLeNet（Inception v1）（2014）</a></li><li><a href=#235-resnet2015 aria-label="2.3.5 ResNet（2015）">2.3.5 ResNet（2015）</a></li><li><a href=#236-densenet2017 aria-label="2.3.6 DenseNet（2017）">2.3.6 DenseNet（2017）</a></li></ul></li><li><a href=#24-cnn%e7%9a%84%e5%ba%94%e7%94%a8 aria-label="2.4 CNN的应用">2.4 CNN的应用</a><ul><li><a href=#241-%e5%9b%be%e5%83%8f%e5%88%86%e7%b1%bb aria-label="2.4.1 图像分类">2.4.1 图像分类</a></li><li><a href=#242-%e7%9b%ae%e6%a0%87%e6%a3%80%e6%b5%8b aria-label="2.4.2 目标检测">2.4.2 目标检测</a></li><li><a href=#243-%e8%af%ad%e4%b9%89%e5%88%86%e5%89%b2 aria-label="2.4.3 语义分割">2.4.3 语义分割</a></li><li><a href=#244-%e4%ba%ba%e8%84%b8%e8%af%86%e5%88%ab aria-label="2.4.4 人脸识别">2.4.4 人脸识别</a></li></ul></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e7%ab%a0%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9crnn aria-label=第三章：循环神经网络（RNN）>第三章：循环神经网络（RNN）</a><ul><li><a href=#31-rnn%e5%9f%ba%e7%a1%80 aria-label="3.1 RNN基础">3.1 RNN基础</a><ul><li><a href=#311-%e5%ba%8f%e5%88%97%e5%bb%ba%e6%a8%a1%e9%97%ae%e9%a2%98 aria-label="3.1.1 序列建模问题">3.1.1 序列建模问题</a></li><li><a href=#312-rnn%e5%89%8d%e5%90%91%e4%bc%a0%e6%92%ad aria-label="3.1.2 RNN前向传播">3.1.2 RNN前向传播</a></li><li><a href=#313-%e6%97%b6%e9%97%b4%e5%b1%95%e5%bc%80unrolling aria-label="3.1.3 时间展开（Unrolling）">3.1.3 时间展开（Unrolling）</a></li><li><a href=#314-%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1%e4%b8%8e%e6%a2%af%e5%ba%a6%e7%88%86%e7%82%b8 aria-label="3.1.4 梯度消失与梯度爆炸">3.1.4 梯度消失与梯度爆炸</a></li></ul></li><li><a href=#32-lstmlong-short-term-memory aria-label="3.2 LSTM（Long Short-Term Memory）">3.2 LSTM（Long Short-Term Memory）</a><ul><li><a href=#321-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3 aria-label="3.2.1 核心思想">3.2.1 核心思想</a></li><li><a href=#322-lstm%e5%8d%95%e5%85%83 aria-label="3.2.2 LSTM单元">3.2.2 LSTM单元</a></li><li><a href=#323-%e4%b8%ba%e4%bb%80%e4%b9%88lstm%e6%9c%89%e6%95%88 aria-label="3.2.3 为什么LSTM有效？">3.2.3 为什么LSTM有效？</a></li></ul></li><li><a href=#33-grugated-recurrent-unit aria-label="3.3 GRU（Gated Recurrent Unit）">3.3 GRU（Gated Recurrent Unit）</a><ul><li><a href=#331-%e7%ae%80%e5%8c%96%e7%9a%84lstm aria-label="3.3.1 简化的LSTM">3.3.1 简化的LSTM</a></li><li><a href=#332-gru%e5%8d%95%e5%85%83 aria-label="3.3.2 GRU单元">3.3.2 GRU单元</a></li></ul></li><li><a href=#34-%e5%8f%8c%e5%90%91rnnbirnn aria-label="3.4 双向RNN（BiRNN）">3.4 双向RNN（BiRNN）</a></li><li><a href=#35-rnn%e7%9a%84%e5%ba%94%e7%94%a8 aria-label="3.5 RNN的应用">3.5 RNN的应用</a><ul><li><a href=#351-%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b aria-label="3.5.1 语言模型">3.5.1 语言模型</a></li><li><a href=#352-%e6%9c%ba%e5%99%a8%e7%bf%bb%e8%af%91 aria-label="3.5.2 机器翻译">3.5.2 机器翻译</a></li><li><a href=#353-%e8%af%ad%e9%9f%b3%e8%af%86%e5%88%ab aria-label="3.5.3 语音识别">3.5.3 语音识别</a></li><li><a href=#354-%e9%97%ae%e7%ad%94%e7%b3%bb%e7%bb%9f aria-label="3.5.4 问答系统">3.5.4 问答系统</a></li></ul></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e7%ab%a0%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e4%b8%8etransformer aria-label=第四章：注意力机制与Transformer>第四章：注意力机制与Transformer</a><ul><li><a href=#41-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6 aria-label="4.1 注意力机制">4.1 注意力机制</a><ul><li><a href=#411-%e5%8a%a8%e6%9c%ba aria-label="4.1.1 动机">4.1.1 动机</a></li><li><a href=#412-bahdanau%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%8a%a0%e6%80%a7%e6%b3%a8%e6%84%8f%e5%8a%9b aria-label="4.1.2 Bahdanau注意力（加性注意力）">4.1.2 Bahdanau注意力（加性注意力）</a></li><li><a href=#413-luong%e6%b3%a8%e6%84%8f%e5%8a%9b%e4%b9%98%e6%80%a7%e6%b3%a8%e6%84%8f%e5%8a%9b aria-label="4.1.3 Luong注意力（乘性注意力）">4.1.3 Luong注意力（乘性注意力）</a></li></ul></li><li><a href=#42-%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6 aria-label="4.2 自注意力机制">4.2 自注意力机制</a><ul><li><a href=#421-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3 aria-label="4.2.1 核心思想">4.2.1 核心思想</a></li><li><a href=#422-%e7%bc%a9%e6%94%be%e7%82%b9%e7%a7%af%e6%b3%a8%e6%84%8f%e5%8a%9b aria-label="4.2.2 缩放点积注意力">4.2.2 缩放点积注意力</a></li></ul></li><li><a href=#43-transformer%e6%9e%b6%e6%9e%84 aria-label="4.3 Transformer架构">4.3 Transformer架构</a><ul><li><a href=#431-%e6%95%b4%e4%bd%93%e6%9e%b6%e6%9e%84 aria-label="4.3.1 整体架构">4.3.1 整体架构</a></li><li><a href=#432-%e7%bc%96%e7%a0%81%e5%99%a8%e5%b1%82 aria-label="4.3.2 编码器层">4.3.2 编码器层</a></li><li><a href=#433-%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9bmulti-head-attention aria-label="4.3.3 多头注意力（Multi-Head Attention）">4.3.3 多头注意力（Multi-Head Attention）</a></li><li><a href=#434-%e8%a7%a3%e7%a0%81%e5%99%a8%e5%b1%82 aria-label="4.3.4 解码器层">4.3.4 解码器层</a></li><li><a href=#435-%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81 aria-label="4.3.5 位置编码">4.3.5 位置编码</a></li><li><a href=#436-%e8%be%93%e5%85%a5%e5%b5%8c%e5%85%a5 aria-label="4.3.6 输入嵌入">4.3.6 输入嵌入</a></li></ul></li><li><a href=#44-transformer%e5%8f%98%e4%bd%93 aria-label="4.4 Transformer变体">4.4 Transformer变体</a><ul><li><a href=#441-bertbidirectional-encoder-representations-from-transformers aria-label="4.4.1 BERT（Bidirectional Encoder Representations from Transformers）">4.4.1 BERT（Bidirectional Encoder Representations from Transformers）</a></li><li><a href=#442-gptgenerative-pre-trained-transformer aria-label="4.4.2 GPT（Generative Pre-trained Transformer）">4.4.2 GPT（Generative Pre-trained Transformer）</a></li><li><a href=#443-t5text-to-text-transfer-transformer aria-label="4.4.3 T5（Text-to-Text Transfer Transformer）">4.4.3 T5（Text-to-Text Transfer Transformer）</a></li><li><a href=#444-vision-transformervit aria-label="4.4.4 Vision Transformer（ViT）">4.4.4 Vision Transformer（ViT）</a></li><li><a href=#445-chatgptinstructgptgpt-35 aria-label="4.4.5 ChatGPT（InstructGPT/GPT-3.5）">4.4.5 ChatGPT（InstructGPT/GPT-3.5）</a></li></ul></li><li><a href=#45-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e5%ba%94%e7%94%a8 aria-label="4.5 注意力机制的应用">4.5 注意力机制的应用</a><ul><li><a href=#451-%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%e5%a4%84%e7%90%86 aria-label="4.5.1 自然语言处理">4.5.1 自然语言处理</a></li><li><a href=#452-%e8%ae%a1%e7%ae%97%e6%9c%ba%e8%a7%86%e8%a7%89 aria-label="4.5.2 计算机视觉">4.5.2 计算机视觉</a></li><li><a href=#453-%e5%a4%9a%e6%a8%a1%e6%80%81 aria-label="4.5.3 多模态">4.5.3 多模态</a></li></ul></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e7%ab%a0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af aria-label=第五章：深度学习的应用场景>第五章：深度学习的应用场景</a><ul><li><a href=#51-%e8%ae%a1%e7%ae%97%e6%9c%ba%e8%a7%86%e8%a7%89 aria-label="5.1 计算机视觉">5.1 计算机视觉</a><ul><li><a href=#511-%e5%9b%be%e5%83%8f%e5%88%86%e7%b1%bb aria-label="5.1.1 图像分类">5.1.1 图像分类</a></li><li><a href=#512-%e7%9b%ae%e6%a0%87%e6%a3%80%e6%b5%8b aria-label="5.1.2 目标检测">5.1.2 目标检测</a></li><li><a href=#513-%e8%af%ad%e4%b9%89%e5%88%86%e5%89%b2 aria-label="5.1.3 语义分割">5.1.3 语义分割</a></li><li><a href=#514-%e4%ba%ba%e8%84%b8%e8%af%86%e5%88%ab aria-label="5.1.4 人脸识别">5.1.4 人脸识别</a></li></ul></li><li><a href=#52-%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%e5%a4%84%e7%90%86 aria-label="5.2 自然语言处理">5.2 自然语言处理</a><ul><li><a href=#521-%e6%96%87%e6%9c%ac%e7%94%9f%e6%88%90 aria-label="5.2.1 文本生成">5.2.1 文本生成</a></li><li><a href=#522-%e9%97%ae%e7%ad%94%e7%b3%bb%e7%bb%9f aria-label="5.2.2 问答系统">5.2.2 问答系统</a></li><li><a href=#523-%e6%83%85%e6%84%9f%e5%88%86%e6%9e%90 aria-label="5.2.3 情感分析">5.2.3 情感分析</a></li><li><a href=#524-%e5%91%bd%e5%90%8d%e5%ae%9e%e4%bd%93%e8%af%86%e5%88%ab aria-label="5.2.4 命名实体识别">5.2.4 命名实体识别</a></li></ul></li><li><a href=#53-%e8%af%ad%e9%9f%b3%e5%a4%84%e7%90%86 aria-label="5.3 语音处理">5.3 语音处理</a><ul><li><a href=#531-%e8%af%ad%e9%9f%b3%e8%af%86%e5%88%abasr aria-label="5.3.1 语音识别（ASR）">5.3.1 语音识别（ASR）</a></li><li><a href=#532-%e8%af%ad%e9%9f%b3%e5%90%88%e6%88%90tts aria-label="5.3.2 语音合成（TTS）">5.3.2 语音合成（TTS）</a></li><li><a href=#533-%e8%af%b4%e8%af%9d%e4%ba%ba%e8%af%86%e5%88%ab aria-label="5.3.3 说话人识别">5.3.3 说话人识别</a></li></ul></li><li><a href=#54-%e6%8e%a8%e8%8d%90%e7%b3%bb%e7%bb%9f aria-label="5.4 推荐系统">5.4 推荐系统</a><ul><li><a href=#541-%e5%8d%8f%e5%90%8c%e8%bf%87%e6%bb%a4--%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0 aria-label="5.4.1 协同过滤 + 深度学习">5.4.1 协同过滤 + 深度学习</a></li><li><a href=#542-%e5%ba%8f%e5%88%97%e6%8e%a8%e8%8d%90 aria-label="5.4.2 序列推荐">5.4.2 序列推荐</a></li></ul></li><li><a href=#55-%e6%b8%b8%e6%88%8f aria-label="5.5 游戏">5.5 游戏</a><ul><li><a href=#551-alphago2016 aria-label="5.5.1 AlphaGo（2016）">5.5.1 AlphaGo（2016）</a></li><li><a href=#552-alphastar%e6%98%9f%e9%99%85%e4%ba%89%e9%9c%b8ii aria-label="5.5.2 AlphaStar（星际争霸II）">5.5.2 AlphaStar（星际争霸II）</a></li></ul></li><li><a href=#56-%e7%94%9f%e7%89%a9%e4%bf%a1%e6%81%af%e5%ad%a6 aria-label="5.6 生物信息学">5.6 生物信息学</a><ul><li><a href=#561-%e8%9b%8b%e7%99%bd%e8%b4%a8%e7%bb%93%e6%9e%84%e9%a2%84%e6%b5%8balphafold2 aria-label="5.6.1 蛋白质结构预测（AlphaFold2）">5.6.1 蛋白质结构预测（AlphaFold2）</a></li><li><a href=#562-%e5%9f%ba%e5%9b%a0%e5%ba%8f%e5%88%97%e5%88%86%e6%9e%90 aria-label="5.6.2 基因序列分析">5.6.2 基因序列分析</a></li></ul></li></ul></li><li><a href=#%e7%ac%ac%e5%85%ad%e7%ab%a0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%8c%91%e6%88%98%e4%b8%8e%e6%9c%aa%e6%9d%a5 aria-label=第六章：深度学习的挑战与未来>第六章：深度学习的挑战与未来</a><ul><li><a href=#61-%e5%bd%93%e5%89%8d%e6%8c%91%e6%88%98 aria-label="6.1 当前挑战">6.1 当前挑战</a><ul><li><a href=#611-%e6%95%b0%e6%8d%ae%e9%a5%a5%e6%b8%b4 aria-label="6.1.1 数据饥渴">6.1.1 数据饥渴</a></li><li><a href=#612-%e8%ae%a1%e7%ae%97%e8%b5%84%e6%ba%90%e9%9c%80%e6%b1%82 aria-label="6.1.2 计算资源需求">6.1.2 计算资源需求</a></li><li><a href=#613-%e5%8f%af%e8%a7%a3%e9%87%8a%e6%80%a7 aria-label="6.1.3 可解释性">6.1.3 可解释性</a></li><li><a href=#614-%e6%b3%9b%e5%8c%96%e8%83%bd%e5%8a%9b aria-label="6.1.4 泛化能力">6.1.4 泛化能力</a></li><li><a href=#615-%e5%ae%89%e5%85%a8%e6%80%a7 aria-label="6.1.5 安全性">6.1.5 安全性</a></li><li><a href=#616-%e5%85%ac%e5%b9%b3%e6%80%a7%e4%b8%8e%e5%81%8f%e8%a7%81 aria-label="6.1.6 公平性与偏见">6.1.6 公平性与偏见</a></li></ul></li><li><a href=#62-%e6%9c%aa%e6%9d%a5%e5%8f%91%e5%b1%95%e8%b6%8b%e5%8a%bf aria-label="6.2 未来发展趋势">6.2 未来发展趋势</a><ul><li><a href=#621-%e5%a4%a7%e6%a8%a1%e5%9e%8bfoundation-models aria-label="6.2.1 大模型（Foundation Models）">6.2.1 大模型（Foundation Models）</a></li><li><a href=#622-%e5%a4%9a%e6%a8%a1%e6%80%81%e5%ad%a6%e4%b9%a0 aria-label="6.2.2 多模态学习">6.2.2 多模态学习</a></li><li><a href=#623-%e8%87%aa%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0 aria-label="6.2.3 自监督学习">6.2.3 自监督学习</a></li><li><a href=#624-%e5%b0%8f%e6%a0%b7%e6%9c%ac%e4%b8%8e%e9%9b%b6%e6%a0%b7%e6%9c%ac%e5%ad%a6%e4%b9%a0 aria-label="6.2.4 小样本与零样本学习">6.2.4 小样本与零样本学习</a></li><li><a href=#625-%e7%a5%9e%e7%bb%8f%e7%ac%a6%e5%8f%b7ai aria-label="6.2.5 神经符号AI">6.2.5 神经符号AI</a></li><li><a href=#626-%e7%a5%9e%e7%bb%8f%e5%bd%a2%e6%80%81%e8%ae%a1%e7%ae%97 aria-label="6.2.6 神经形态计算">6.2.6 神经形态计算</a></li><li><a href=#627-%e7%94%9f%e6%88%90%e5%bc%8fai aria-label="6.2.7 生成式AI">6.2.7 生成式AI</a></li><li><a href=#628-%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd aria-label="6.2.8 具身智能">6.2.8 具身智能</a></li><li><a href=#629-ai-for-science aria-label="6.2.9 AI for Science">6.2.9 AI for Science</a></li><li><a href=#6210-%e5%8f%af%e4%bf%a1%e8%b5%96aitrustworthy-ai aria-label="6.2.10 可信赖AI（Trustworthy AI）">6.2.10 可信赖AI（Trustworthy AI）</a></li></ul></li><li><a href=#63-%e7%bb%93%e8%af%ad aria-label="6.3 结语">6.3 结语</a></li></ul></li><li><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae aria-label=参考文献>参考文献</a></li></ul></div></details></div><div class=post-content><h2 id=引言从生物启发到智能革命>引言：从生物启发到智能革命<a hidden class=anchor aria-hidden=true href=#引言从生物启发到智能革命>#</a></h2><p>1943年，Warren McCulloch和Walter Pitts提出了第一个神经元数学模型。他们用一个简单的数学公式模拟了生物神经元的工作方式：接收输入、加权求和、激活输出。这个看似简单的想法，却孕育了后来改变世界的人工智能技术。</p><p>1958年，Frank Rosenblatt发明了感知机（Perceptron），这是第一个可以学习的神经网络。但1969年，Minsky和Papert在《Perceptrons》一书中证明了单层感知机无法解决异或（XOR）问题，这个致命缺陷导致了神经网络研究的第一次寒冬。</p><p>1986年，David Rumelhart、Geoffrey Hinton和Ronald Williams重新发现了反向传播算法，解决了多层网络的训练问题。神经网络迎来了短暂的春天。</p><p>但在90年代到2000年代初，支持向量机（SVM）等传统机器学习算法统治了学术界。神经网络因为数据量不足、计算能力有限、缺乏有效的训练技巧，再次陷入沉寂。</p><p>2012年，ImageNet竞赛上，Hinton的学生Alex Krizhevsky使用深度卷积神经网络AlexNet，以压倒性优势击败了传统方法，分类错误率从26%降低到15.3%。这一年，深度学习时代正式开启。</p><p>从此，深度学习以惊人的速度发展：2014年的VGG、GoogLeNet，2015年的ResNet解决深度退化问题，2017年的Transformer彻底改变自然语言处理，2022年的ChatGPT让全世界见识到大模型的力量。</p><p>本文将从数学原理出发，系统讲解深度学习的核心算法：从基础神经网络到卷积神经网络（CNN），从循环神经网络（RNN）到Transformer，最后探讨未来发展趋势。</p><h2 id=第一章神经网络的数学基础>第一章：神经网络的数学基础<a hidden class=anchor aria-hidden=true href=#第一章神经网络的数学基础>#</a></h2><h3 id=11-单神经元感知机的数学模型>1.1 单神经元：感知机的数学模型<a hidden class=anchor aria-hidden=true href=#11-单神经元感知机的数学模型>#</a></h3><h4 id=111-前向传播>1.1.1 前向传播<a hidden class=anchor aria-hidden=true href=#111-前向传播>#</a></h4><p>感知机是最基础的神经网络单元，模拟生物神经元的工作原理。给定输入向量 $x \in \mathbb{R}^d$，权重向量 $w \in \mathbb{R}^d$，偏置 $b \in \mathbb{R}$：</p><p>$$z = w^Tx + b = \sum_{i=1}^d w_i x_i + b$$</p><p>激活函数 $\sigma(z)$ 决定神经元的输出：</p><p>$$a = \sigma(z)$$</p><h4 id=112-常用激活函数>1.1.2 常用激活函数<a hidden class=anchor aria-hidden=true href=#112-常用激活函数>#</a></h4><p><strong>Sigmoid函数</strong>：
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$</p><p>导数：
$$\sigma&rsquo;(z) = \sigma(z)(1 - \sigma(z))$$</p><p>性质：</p><ul><li>输出范围：$(0, 1)$</li><li>S型曲线，可微</li><li>缺点：梯度消失（$| \sigma&rsquo;(z) | \leq 0.25$），输出不以零为中心</li></ul><p><strong>Tanh函数</strong>：
$$\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$</p><p>导数：
$$\tanh&rsquo;(z) = 1 - \tanh^2(z)$$</p><p>性质：</p><ul><li>输出范围：$(-1, 1)$</li><li>以零为中心，比Sigmoid收敛更快</li></ul><p><strong>ReLU（Rectified Linear Unit）</strong>：
$$\text{ReLU}(z) = \max(0, z)$$</p><p>导数：
$$\text{ReLU}&rsquo;(z) = \begin{cases} 1 & z > 0 \ 0 & z \leq 0 \end{cases}$$</p><p>性质：</p><ul><li>计算简单（不涉及指数运算）</li><li>缓解梯度消失问题</li><li>缺点：神经元"死亡"（$z \leq 0$ 时梯度为0）</li></ul><p><strong>Leaky ReLU</strong>：
$$\text{LeakyReLU}(z) = \max(\alpha z, z), \quad \alpha &lt; 1$$</p><p>解决神经元死亡问题（$\alpha$ 是小正数，如0.01）</p><p><strong>Swish</strong>：
$$\text{Swish}(z) = z \cdot \sigma(z) = \frac{z}{1 + e^{-z}}$$</p><p>在许多任务中表现优于ReLU</p><h3 id=12-多层前馈神经网络>1.2 多层前馈神经网络<a hidden class=anchor aria-hidden=true href=#12-多层前馈神经网络>#</a></h3><h4 id=121-网络结构>1.2.1 网络结构<a hidden class=anchor aria-hidden=true href=#121-网络结构>#</a></h4><p>多层神经网络由输入层、隐藏层、输出层组成。设网络有 $L$ 层，第 $l$ 层有 $n^{[l]}$ 个神经元。</p><p>记号：</p><ul><li>$W^{[l]} \in \mathbb{R}^{n^{[l]} \times n^{[l-1]}}$：第 $l$ 层的权重矩阵</li><li>$b^{[l]} \in \mathbb{R}^{n^{[l]}}$：第 $l$ 层的偏置向量</li><li>$Z^{[l]} \in \mathbb{R}^{n^{[l]}}$：第 $l$ 层的线性变换结果</li><li>$A^{[l]} \in \mathbb{R}^{n^{[l]}}$：第 $l$ 层的激活输出</li></ul><h4 id=122-前向传播>1.2.2 前向传播<a hidden class=anchor aria-hidden=true href=#122-前向传播>#</a></h4><p><strong>第1层</strong>（输入层到第1个隐藏层）：
$$Z^{[1]} = W^{[1]}X + b^{[1]}$$
$$A^{[1]} = \sigma^{[1]}(Z^{[1]})$$</p><p><strong>第 $l$ 层</strong>（$l = 2, \ldots, L$）：
$$Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$$
$$A^{[l]} = \sigma^{[l]}(Z^{[l]})$$</p><p>其中 $X = A^{[0]}$ 是输入，$\hat{Y} = A^{[L]}$ 是网络输出。</p><h4 id=123-向量化实现>1.2.3 向量化实现<a hidden class=anchor aria-hidden=true href=#123-向量化实现>#</a></h4><p>给定 $m$ 个样本的训练集 $X \in \mathbb{R}^{d \times m}$，前向传播可以矩阵化：</p><p>$$Z^{[1]} = W^{[1]}X + b^{[1]}$$
$$A^{[1]} = \sigma^{[1]}(Z^{[1]})$$
$$\vdots$$
$$Z^{[L]} = W^{[L]}A^{[L-1]} + b^{[L]}$$
$$\hat{Y} = A^{[L]} = \sigma^{[L]}(Z^{[L]})$$</p><p>这种实现利用矩阵运算，可以利用GPU加速。</p><h3 id=13-损失函数>1.3 损失函数<a hidden class=anchor aria-hidden=true href=#13-损失函数>#</a></h3><h4 id=131-回归任务>1.3.1 回归任务<a hidden class=anchor aria-hidden=true href=#131-回归任务>#</a></h4><p><strong>均方误差（MSE）</strong>：
$$\mathcal{L} = \frac{1}{2m}\sum_{i=1}^m (y^{(i)} - \hat{y}^{(i)})^2$$</p><p><strong>平均绝对误差（MAE）</strong>：
$$\mathcal{L} = \frac{1}{m}\sum_{i=1}^m |y^{(i)} - \hat{y}^{(i)}|$$</p><h4 id=132-分类任务>1.3.2 分类任务<a hidden class=anchor aria-hidden=true href=#132-分类任务>#</a></h4><p><strong>交叉熵损失（多分类）</strong>：
设 $y \in {0, 1}^K$ 是one-hot编码，$\hat{y} = \text{softmax}(z)$ 是预测概率：</p><p>$$\mathcal{L} = -\frac{1}{m}\sum_{i=1}^m \sum_{k=1}^K y_k^{(i)} \log \hat{y}_k^{(i)}$$</p><p><strong>Softmax函数</strong>：
$$\text{softmax}(z)<em>k = \frac{e^{z_k}}{\sum</em>{j=1}^K e^{z_j}}$$</p><p>性质：</p><ul><li>输出是概率分布（$\sum_k \hat{y}_k = 1$，$\hat{y}_k > 0$）</li><li>对数空间计算数值稳定</li></ul><p><strong>交叉熵损失（二分类）</strong>：
$$\mathcal{L} = -\frac{1}{m}\sum_{i=1}^m [y^{(i)} \log \hat{y}^{(i)} + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)})]$$</p><h3 id=14-反向传播算法>1.4 反向传播算法<a hidden class=anchor aria-hidden=true href=#14-反向传播算法>#</a></h3><p>反向传播是深度学习的核心算法，用于高效计算梯度。</p><h4 id=141-链式法则>1.4.1 链式法则<a hidden class=anchor aria-hidden=true href=#141-链式法则>#</a></h4><p>损失函数对参数的梯度可以通过链式法则计算：</p><p>$$\frac{\partial \mathcal{L}}{\partial W^{[l]}} = \frac{\partial \mathcal{L}}{\partial Z^{[l]}} \frac{\partial Z^{[l]}}{\partial W^{[l]}}$$</p><p>$$\frac{\partial \mathcal{L}}{\partial b^{[l]}} = \frac{\partial \mathcal{L}}{\partial Z^{[l]}} \frac{\partial Z^{[l]}}{\partial b^{[l]}}$$</p><h4 id=142-反向传播推导>1.4.2 反向传播推导<a hidden class=anchor aria-hidden=true href=#142-反向传播推导>#</a></h4><p>定义误差项（error term）：
$$dZ^{[l]} = \frac{\partial \mathcal{L}}{\partial Z^{[l]}}$$</p><p><strong>输出层</strong>：
$$dZ^{[L]} = A^{[L]} - Y$$</p><p>（这是使用交叉熵损失和Softmax激活的简化结果）</p><p><strong>隐藏层</strong>（从后向前传播）：
$$dA^{[l]} = (W^{[l+1]})^T dZ^{[l+1]}$$
$$dZ^{[l]} = dA^{[l]} \odot \sigma^{[l]&rsquo;}(Z^{[l]})$$</p><p>其中 $\odot$ 是逐元素乘积（Hadamard product）。</p><p><strong>梯度计算</strong>：
$$dW^{[l]} = \frac{1}{m} dZ^{[l]} (A^{[l-1]})^T$$
$$db^{[l]} = \frac{1}{m} \sum_{i=1}^m dZ^{[l]}_{:, i}$$</p><h4 id=143-算法复杂度>1.4.3 算法复杂度<a hidden class=anchor aria-hidden=true href=#143-算法复杂度>#</a></h4><p>前向传播：$O(\sum_{l=1}^L n^{[l]} n^{[l-1]})$</p><p>反向传播：$O(\sum_{l=1}^L n^{[l]} n^{[l-1]})$</p><p>两者复杂度相同！这是反向传播算法的高效之处。</p><h3 id=15-梯度下降与优化算法>1.5 梯度下降与优化算法<a hidden class=anchor aria-hidden=true href=#15-梯度下降与优化算法>#</a></h3><h4 id=151-批量梯度下降batch-gd>1.5.1 批量梯度下降（Batch GD）<a hidden class=anchor aria-hidden=true href=#151-批量梯度下降batch-gd>#</a></h4><p>每次迭代使用所有样本：</p><p>$$W^{[l]} := W^{[l]} - \alpha \frac{\partial \mathcal{L}}{\partial W^{[l]}}$$</p><p>缺点：数据量大时速度慢</p><h4 id=152-随机梯度下降sgd>1.5.2 随机梯度下降（SGD）<a hidden class=anchor aria-hidden=true href=#152-随机梯度下降sgd>#</a></h4><p>每次迭代使用一个样本：</p><p>$$W^{[l]} := W^{[l]} - \alpha \frac{\partial \mathcal{L}^{(i)}}{\partial W^{[l]}}$$</p><p>优点：速度快，但梯度方差大</p><h4 id=153-小批量梯度下降mini-batch-gd>1.5.3 小批量梯度下降（Mini-batch GD）<a hidden class=anchor aria-hidden=true href=#153-小批量梯度下降mini-batch-gd>#</a></h4><p>每次迭代使用一批样本（常用64、128、256）：</p><p>$$W^{[l]} := W^{[l]} - \alpha \frac{1}{m_t} \sum_{i \in \mathcal{B}_t} \frac{\partial \mathcal{L}^{(i)}}{\partial W^{[l]}}$$</p><p>结合了BGD和SGD的优点</p><h4 id=154-动量法momentum>1.5.4 动量法（Momentum）<a hidden class=anchor aria-hidden=true href=#154-动量法momentum>#</a></h4><p>引入速度项，加速收敛：</p><p>$$v_{dW^{[l]}} := \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial \mathcal{L}}{\partial W^{[l]}}$$
$$W^{[l]} := W^{[l]} - \alpha v_{dW^{[l]}}$$</p><p>$\beta_1 \approx 0.9$，控制历史梯度的衰减率</p><h4 id=155-adam优化器>1.5.5 Adam优化器<a hidden class=anchor aria-hidden=true href=#155-adam优化器>#</a></h4><p>结合动量法和RMSprop，自适应学习率：</p><p><strong>计算动量</strong>：
$$m_{dW^{[l]}} := \beta_1 m_{dW^{[l]}} + (1 - \beta_1) \frac{\partial \mathcal{L}}{\partial W^{[l]}}$$
$$v_{dW^{[l]}} := \beta_2 v_{dW^{[l]}} + (1 - \beta_2) \left(\frac{\partial \mathcal{L}}{\partial W^{[l]}}\right)^2$$</p><p><strong>偏差修正</strong>：
$$\hat{m}<em>{dW^{[l]}} = \frac{m</em>{dW^{[l]}}}{1 - \beta_1^t}$$
$$\hat{v}<em>{dW^{[l]}} = \frac{v</em>{dW^{[l]}}}{1 - \beta_2^t}$$</p><p><strong>参数更新</strong>：
$$W^{[l]} := W^{[l]} - \alpha \frac{\hat{m}<em>{dW^{[l]}}}{\sqrt{\hat{v}</em>{dW^{[l]}}} + \epsilon}$$</p><p>超参数：$\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 10^{-8}$</p><h3 id=16-正则化技术>1.6 正则化技术<a hidden class=anchor aria-hidden=true href=#16-正则化技术>#</a></h3><h4 id=161-l2正则化权重衰减>1.6.1 L2正则化（权重衰减）<a hidden class=anchor aria-hidden=true href=#161-l2正则化权重衰减>#</a></h4><p>$$\mathcal{L}<em>{\text{reg}} = \mathcal{L} + \frac{\lambda}{2m} \sum</em>{l=1}^L |W^{[l]}|_F^2$$</p><p>梯度更新：
$$dW^{[l]}_{\text{reg}} = dW^{[l]} + \frac{\lambda}{m} W^{[l]}$$</p><p>权重被"拉向"零</p><h4 id=162-dropout>1.6.2 Dropout<a hidden class=anchor aria-hidden=true href=#162-dropout>#</a></h4><p>训练时以概率 $p$ 随机丢弃神经元：</p><p>$$\tilde{a}^{[l]} = a^{[l]} \odot r^{[l]}, \quad r^{[l]}_i \sim \text{Bernoulli}(1-p)$$</p><p>$$\hat{a}^{[l]} = \frac{\tilde{a}^{[l]}}{1-p}$$（缩放保持期望）</p><p>测试时使用所有神经元，权重乘以 $(1-p)$</p><p><strong>作用</strong>：防止过拟合，相当于训练了多个子网络的集成</p><h4 id=163-批量归一化batch-normalization>1.6.3 批量归一化（Batch Normalization）<a hidden class=anchor aria-hidden=true href=#163-批量归一化batch-normalization>#</a></h4><p>标准化每层的激活值：</p><p><strong>训练时</strong>：
$$\mu^{[l]} = \frac{1}{m} \sum_{i=1}^m Z^{[l]}<em>{:, i}$$
$$\sigma^2^{[l]} = \frac{1}{m} \sum</em>{i=1}^m (Z^{[l]}<em>{:, i} - \mu^{[l]})^2$$
$$\hat{Z}^{[l]} = \frac{Z^{[l]} - \mu^{[l]}}{\sqrt{\sigma^2^{[l]} + \epsilon}}$$
$$Z</em>{\text{BN}}^{[l]} = \gamma^{[l]} \odot \hat{Z}^{[l]} + \beta^{[l]}$$</p><p>可学习参数：$\gamma^{[l]}$（缩放）、$\beta^{[l]}$（平移）</p><p><strong>测试时</strong>：使用移动平均的均值和方差</p><p><strong>作用</strong>：</p><ul><li>加速训练</li><li>允许使用更大学习率</li><li>减少对初始化的敏感度</li></ul><h4 id=164-早停法early-stopping>1.6.4 早停法（Early Stopping）<a hidden class=anchor aria-hidden=true href=#164-早停法early-stopping>#</a></h4><p>在验证集上监控性能，性能不再提升时停止训练</p><h2 id=第二章卷积神经网络cnn>第二章：卷积神经网络（CNN）<a hidden class=anchor aria-hidden=true href=#第二章卷积神经网络cnn>#</a></h2><h3 id=21-卷积操作>2.1 卷积操作<a hidden class=anchor aria-hidden=true href=#21-卷积操作>#</a></h3><h4 id=211-2d卷积>2.1.1 2D卷积<a hidden class=anchor aria-hidden=true href=#211-2d卷积>#</a></h4><p>给定输入特征图 $X \in \mathbb{R}^{H \times W \times C}$，卷积核 $K \in \mathbb{R}^{k_h \times k_w \times C}$：</p><p>$$Z_{i,j} = \sum_{c=1}^C \sum_{u=0}^{k_h-1} \sum_{v=0}^{k_w-1} X_{i+u, j+v, c} \cdot K_{u,v,c} + b$$</p><p>其中 $b \in \mathbb{R}$ 是偏置。</p><h4 id=212-步长stride>2.1.2 步长（Stride）<a hidden class=anchor aria-hidden=true href=#212-步长stride>#</a></h4><p>步长 $s$ 控制卷积核滑动的步长，输出尺寸：</p><p>$$H&rsquo; = \left\lfloor \frac{H - k_h}{s} \right\rfloor + 1$$
$$W&rsquo; = \left\lfloor \frac{W - k_w}{s} \right\rfloor + 1$$</p><h4 id=213-填充padding>2.1.3 填充（Padding）<a hidden class=anchor aria-hidden=true href=#213-填充padding>#</a></h4><p>填充 $p$ 在输入周围补零，输出尺寸：</p><p>$$H&rsquo; = \left\lfloor \frac{H + 2p - k_h}{s} \right\rfloor + 1$$
$$W&rsquo; = \left\lfloor \frac{W + 2p - k_w}{s} \right\rfloor + 1$$</p><p><strong>Valid padding</strong>（不填充）：$p = 0$
<strong>Same padding</strong>（保持尺寸）：$p = \lfloor \frac{k-1}{2} \rfloor$</p><h4 id=214-通道>2.1.4 通道<a hidden class=anchor aria-hidden=true href=#214-通道>#</a></h4><p>多个卷积核产生多个输出通道：</p><p>输入 $X \in \mathbb{R}^{H \times W \times C_{in}}$，卷积核组 $K \in \mathbb{R}^{k_h \times k_w \times C_{in} \times C_{out}}$</p><p>输出 $Z \in \mathbb{R}^{H&rsquo; \times W&rsquo; \times C_{out}}$</p><h4 id=215-卷积-vs-全连接>2.1.5 卷积 vs 全连接<a hidden class=anchor aria-hidden=true href=#215-卷积-vs-全连接>#</a></h4><p>卷积层是稀疏连接的权重共享：</p><ul><li>参数数量：$k_h \times k_w \times C_{in} \times C_{out}$（与输入尺寸无关）</li><li>平移不变性：相同模式在不同位置识别</li></ul><p>全连接层：</p><ul><li>参数数量：$H \times W \times C_{in} \times n_{out}$（与输入尺寸成正比）</li></ul><h3 id=22-池化层>2.2 池化层<a hidden class=anchor aria-hidden=true href=#22-池化层>#</a></h3><h4 id=221-最大池化max-pooling>2.2.1 最大池化（Max Pooling）<a hidden class=anchor aria-hidden=true href=#221-最大池化max-pooling>#</a></h4><p>$$Z_{i,j} = \max_{0 \leq u &lt; k_h, 0 \leq v &lt; k_w} X_{i \cdot s + u, j \cdot s + v}$$</p><p><strong>作用</strong>：</p><ul><li>降采样，减少计算量</li><li>平移不变性</li><li>防止过拟合</li></ul><h4 id=222-平均池化average-pooling>2.2.2 平均池化（Average Pooling）<a hidden class=anchor aria-hidden=true href=#222-平均池化average-pooling>#</a></h4><p>$$Z_{i,j} = \frac{1}{k_h k_w} \sum_{u=0}^{k_h-1} \sum_{v=0}^{k_w-1} X_{i \cdot s + u, j \cdot s + v}$$</p><h4 id=223-全局平均池化global-average-pooling>2.2.3 全局平均池化（Global Average Pooling）<a hidden class=anchor aria-hidden=true href=#223-全局平均池化global-average-pooling>#</a></h4><p>对每个通道取平均，输出 $1 \times 1 \times C$：</p><p>$$Z_c = \frac{1}{HW} \sum_{i=1}^H \sum_{j=1}^W X_{i,j,c}$$</p><p>常用于替代全连接层，减少参数</p><h3 id=23-经典cnn架构>2.3 经典CNN架构<a hidden class=anchor aria-hidden=true href=#23-经典cnn架构>#</a></h3><h4 id=231-lenet-51998>2.3.1 LeNet-5（1998）<a hidden class=anchor aria-hidden=true href=#231-lenet-51998>#</a></h4><p>Yann LeCun设计的手写数字识别网络（MNIST）</p><p>结构：</p><ol><li>卷积层（6个5×5卷积核，步长1，padding0）→ 激活（tanh）</li><li>平均池化（2×2，步长2）</li><li>卷积层（16个5×5卷积核）→ 激活</li><li>平均池化（2×2，步长2）</li><li>卷积层（120个5×5卷积核）→ 激活</li><li>全连接层（84个神经元）</li><li>输出层（10个神经元，softmax）</li></ol><p>参数量：约6万个</p><h4 id=232-alexnet2012>2.3.2 AlexNet（2012）<a hidden class=anchor aria-hidden=true href=#232-alexnet2012>#</a></h4><p>深度学习革命的起点，ImageNet冠军</p><p>结构：</p><ol><li>卷积层（96个11×11卷积核，步长4）→ ReLU → 最大池化（3×3，步长2）→ 局部响应归一化（LRN）</li><li>卷积层（256个5×5卷积核，padding2）→ ReLU → 最大池化 → LRN</li><li>卷积层（384个3×3卷积核，padding1）→ ReLU</li><li>卷积层（384个3×3卷积核，padding1）→ ReLU</li><li>卷积层（256个3×3卷积核，padding1）→ ReLU → 最大池化</li><li>全连接层（4096个神经元）→ Dropout（0.5）</li><li>全连接层（4096个神经元）→ Dropout（0.5）</li><li>输出层（1000个神经元，softmax）</li></ol><p>参数量：约6000万个</p><p><strong>创新点</strong>：</p><ul><li>使用ReLU激活（加速训练）</li><li>Dropout防止过拟合</li><li>数据增强（平移、翻转、颜色变化）</li><li>GPU加速训练（两块GTX 580）</li></ul><h4 id=233-vgg2014>2.3.3 VGG（2014）<a hidden class=anchor aria-hidden=true href=#233-vgg2014>#</a></h4><p>&ldquo;简单但有效"的设计理念</p><p><strong>核心思想</strong>：使用小卷积核（3×3）堆叠代替大卷积核</p><p>两个3×3卷积的感受野相当于一个5×5卷积：
$$3 \times 3 \to 3 \times 3 \to \text{感受野} = 5 \times 5$$</p><p>优点：</p><ul><li>参数更少：$2 \times 3^2 = 18 &lt; 5^2 = 25$</li><li>更多非线性层（每个3×3后都有ReLU）</li><li>深度可以更深</li></ul><p><strong>VGG-16结构</strong>：</p><ol><li>Conv3-64（2个3×3卷积，64通道）→ MaxPool</li><li>Conv3-128（2个3×3卷积，128通道）→ MaxPool</li><li>Conv3-256（3个3×3卷积，256通道）→ MaxPool</li><li>Conv3-512（3个3×3卷积，512通道）→ MaxPool</li><li>Conv3-512（3个3×3卷积，512通道）→ MaxPool</li><li>FC-4096 → FC-4096 → FC-1000</li></ol><p>参数量：约1.38亿个</p><h4 id=234-googlenetinception-v12014>2.3.4 GoogLeNet（Inception v1）（2014）<a hidden class=anchor aria-hidden=true href=#234-googlenetinception-v12014>#</a></h4><p>引入Inception模块，多尺度特征提取</p><p><strong>Inception模块</strong>：
并行多个不同大小的卷积核（1×1, 3×3, 5×5），并拼接结果</p><p>问题：计算量大</p><p><strong>优化</strong>：使用1×1卷积降维（bottleneck）</p><ul><li>在3×3和5×5卷积前加1×1卷积减少通道数</li><li>在池化后加1×1卷积减少通道数</li></ul><p><strong>GoogLeNet结构</strong>：</p><ul><li>9个Inception模块堆叠</li><li>使用全局平均池化替代全连接层</li><li>辅助分类器（中间层）加速训练</li></ul><p>参数量：约600万个（远少于AlexNet）</p><h4 id=235-resnet2015>2.3.5 ResNet（2015）<a hidden class=anchor aria-hidden=true href=#235-resnet2015>#</a></h4><p>解决深度网络的退化问题（degradation）</p><p><strong>残差连接</strong>：
$$y = F(x, {W}) + x$$</p><p>其中 $F(x, {W})$ 是残差函数（至少2层）</p><p><strong>为什么有效？</strong></p><p>传统网络学习 $H(x)$，ResNet学习残差 $F(x) = H(x) - x$</p><ul><li>如果最优是恒等映射 $H(x) = x$，则只需让 $F(x) = 0$（更容易）</li><li>梯度可以通过恒等连接直接传播，缓解梯度消失</li></ul><p><strong>ResNet-50结构</strong>：</p><ul><li>使用bottleneck设计：1×1降维 → 3×3卷积 → 1×1升维</li><li>4个stage，通道数逐步增加（64, 128, 256, 512）</li><li>每个stage有多个残差块（3, 4, 6, 3）</li></ul><p>参数量：约2560万个</p><p><strong>深度表现</strong>：</p><ul><li>ResNet-18, 34：基本残差块</li><li>ResNet-50, 101, 152：bottleneck残差块</li><li>ResNet-152在ImageNet上达到3.57%的top-5错误率（低于人类5.1%）</li></ul><h4 id=236-densenet2017>2.3.6 DenseNet（2017）<a hidden class=anchor aria-hidden=true href=#236-densenet2017>#</a></h4><p>密集连接网络，每一层都与前面的所有层连接</p><p><strong>密集块（Dense Block）</strong>：
$$x_l = H_l([x_0, x_1, \ldots, x_{l-1}])$$</p><p>其中 $[x_0, x_1, \ldots, x_{l-1}]$ 是前面所有层输出的拼接</p><p><strong>优点</strong>：</p><ul><li>特征复用，参数更少</li><li>梯度流动更顺畅</li><li>缓解梯度消失</li></ul><p><strong>过渡层（Transition Layer）</strong>：
在密集块之间，进行降维和降采样</p><ul><li>1×1卷积降维</li><li>2×2平均池化</li></ul><p><strong>DenseNet-121结构</strong>：</p><ul><li>4个密集块（增长率 $k = 32$）</li><li>每个块有 6, 12, 24, 16 层</li><li>参数量：约800万个</li></ul><h3 id=24-cnn的应用>2.4 CNN的应用<a hidden class=anchor aria-hidden=true href=#24-cnn的应用>#</a></h3><h4 id=241-图像分类>2.4.1 图像分类<a hidden class=anchor aria-hidden=true href=#241-图像分类>#</a></h4><p><strong>任务</strong>：给定图像，预测类别标签</p><p><strong>数据集</strong>：</p><ul><li>ImageNet（1000类，140万张图像）</li><li>CIFAR-10/100（10/100类，6万张小图像）</li><li>MNIST（10类手写数字）</li></ul><h4 id=242-目标检测>2.4.2 目标检测<a hidden class=anchor aria-hidden=true href=#242-目标检测>#</a></h4><p><strong>任务</strong>：定位图像中的物体并分类</p><p><strong>算法</strong>：</p><ul><li>R-CNN系列（R-CNN, Fast R-CNN, Faster R-CNN）：两阶段检测</li><li>YOLO（You Only Look Once）：单阶段检测</li><li>SSD（Single Shot MultiBox Detector）：多尺度单阶段检测</li></ul><h4 id=243-语义分割>2.4.3 语义分割<a hidden class=anchor aria-hidden=true href=#243-语义分割>#</a></h4><p><strong>任务</strong>：为图像中的每个像素分类</p><p><strong>算法</strong>：</p><ul><li>FCN（Fully Convolutional Network）：将全连接层替换为卷积层</li><li>U-Net：编码器-解码器结构，跳跃连接</li><li>DeepLab：空洞卷积扩大感受野</li></ul><h4 id=244-人脸识别>2.4.4 人脸识别<a hidden class=anchor aria-hidden=true href=#244-人脸识别>#</a></h4><p><strong>任务</strong>：验证或识别人脸</p><p><strong>算法</strong>：</p><ul><li>FaceNet：使用triplet loss学习人脸嵌入</li><li>ArcFace：添加角度间隔margin</li><li>CosFace：余弦间隔</li></ul><h2 id=第三章循环神经网络rnn>第三章：循环神经网络（RNN）<a hidden class=anchor aria-hidden=true href=#第三章循环神经网络rnn>#</a></h2><h3 id=31-rnn基础>3.1 RNN基础<a hidden class=anchor aria-hidden=true href=#31-rnn基础>#</a></h3><h4 id=311-序列建模问题>3.1.1 序列建模问题<a hidden class=anchor aria-hidden=true href=#311-序列建模问题>#</a></h4><p>传统神经网络处理固定尺寸输入，无法处理变长序列。</p><p>RNN（Recurrent Neural Network）通过隐藏状态传递历史信息。</p><h4 id=312-rnn前向传播>3.1.2 RNN前向传播<a hidden class=anchor aria-hidden=true href=#312-rnn前向传播>#</a></h4><p>给定序列 $x = (x_1, x_2, \ldots, x_T)$</p><p>初始化：$h_0 = \mathbf{0}$</p><p>对于 $t = 1, 2, \ldots, T$：
$$h_t = \sigma_h(W_h h_{t-1} + W_x x_t + b_h)$$
$$\hat{y}_t = \sigma_y(W_y h_t + b_y)$$</p><p>其中：</p><ul><li>$W_h \in \mathbb{R}^{n_h \times n_h}$：隐藏状态到隐藏状态的权重</li><li>$W_x \in \mathbb{R}^{n_h \times n_x}$：输入到隐藏状态的权重</li><li>$W_y \in \mathbb{R}^{n_y \times n_h}$：隐藏状态到输出的权重</li><li>$n_h$：隐藏状态维度</li><li>$\sigma_h, \sigma_y$：激活函数</li></ul><h4 id=313-时间展开unrolling>3.1.3 时间展开（Unrolling）<a hidden class=anchor aria-hidden=true href=#313-时间展开unrolling>#</a></h4><p>RNN在时间步上展开，等价于深度网络：</p><ul><li>同样的参数 $W_h, W_x, W_y$ 在不同时间步共享</li><li>深度 = 序列长度 $T$</li></ul><h4 id=314-梯度消失与梯度爆炸>3.1.4 梯度消失与梯度爆炸<a hidden class=anchor aria-hidden=true href=#314-梯度消失与梯度爆炸>#</a></h4><p>反向传播时（BPTT），梯度通过时间反向传播：</p><p>$$\frac{\partial \mathcal{L}}{\partial h_t} = \prod_{k=t}^{T} \frac{\partial h_{k+1}}{\partial h_k} \cdot \frac{\partial \mathcal{L}}{\partial \hat{y}_{k+1}}$$</p><p>其中 $\frac{\partial h_{k+1}}{\partial h_k} = \text{diag}(\sigma_h&rsquo;(z_k)) W_h$</p><p><strong>梯度消失</strong>：如果 $|\text{diag}(\sigma_h&rsquo;(z_k)) W_h| &lt; 1$，长程依赖无法学习</p><p><strong>梯度爆炸</strong>：如果 $|\text{diag}(\sigma_h&rsquo;(z_k)) W_h| > 1$，梯度指数增长，训练不稳定</p><p><strong>解决方法</strong>：</p><ul><li>梯度裁剪（Gradient Clipping）：$|g| \leftarrow \min(|g|, \theta)$</li><li>使用LSTM/GRU门控结构</li></ul><h3 id=32-lstmlong-short-term-memory>3.2 LSTM（Long Short-Term Memory）<a hidden class=anchor aria-hidden=true href=#32-lstmlong-short-term-memory>#</a></h3><h4 id=321-核心思想>3.2.1 核心思想<a hidden class=anchor aria-hidden=true href=#321-核心思想>#</a></h4><p>LSTM通过门控机制（gating）控制信息的流动，选择性记忆和遗忘。</p><h4 id=322-lstm单元>3.2.2 LSTM单元<a hidden class=anchor aria-hidden=true href=#322-lstm单元>#</a></h4><p>给定输入 $x_t$ 和前一隐藏状态 $h_{t-1}$、细胞状态 $c_{t-1}$</p><p><strong>遗忘门（Forget Gate）</strong>：决定丢弃什么信息
$$f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)$$</p><p><strong>输入门（Input Gate）</strong>：决定存储什么新信息
$$i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)$$
$$\tilde{c}<em>t = \tanh(W_c [h</em>{t-1}, x_t] + b_c)$$</p><p><strong>细胞状态更新</strong>：
$$c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t$$</p><p><strong>输出门（Output Gate）</strong>：决定输出什么
$$o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)$$
$$h_t = o_t \odot \tanh(c_t)$$</p><p>其中 $\odot$ 是逐元素乘积。</p><h4 id=323-为什么lstm有效>3.2.3 为什么LSTM有效？<a hidden class=anchor aria-hidden=true href=#323-为什么lstm有效>#</a></h4><ol><li><strong>细胞状态 $c_t$ 作为"高速公路&rdquo;</strong>：梯度可以直接传播，缓解梯度消失</li><li><strong>门控机制</strong>：动态控制信息流<ul><li>遗忘门 $f_t \to 0$：遗忘长期信息</li><li>遗忘门 $f_t \to 1$：保持长期信息</li><li>输入门 $i_t \to 0$：忽略当前输入</li><li>输入门 $i_t \to 1$：记录当前输入</li></ul></li></ol><h3 id=33-grugated-recurrent-unit>3.3 GRU（Gated Recurrent Unit）<a hidden class=anchor aria-hidden=true href=#33-grugated-recurrent-unit>#</a></h3><h4 id=331-简化的lstm>3.3.1 简化的LSTM<a hidden class=anchor aria-hidden=true href=#331-简化的lstm>#</a></h4><p>GRU是LSTM的简化版本，参数更少。</p><h4 id=332-gru单元>3.3.2 GRU单元<a hidden class=anchor aria-hidden=true href=#332-gru单元>#</a></h4><p>$$z_t = \sigma(W_z [h_{t-1}, x_t] + b_z)$$（更新门）
$$r_t = \sigma(W_r [h_{t-1}, x_t] + b_r)$$（重置门）
$$\tilde{h}<em>t = \tanh(W_h [r_t \odot h</em>{t-1}, x_t] + b_h)$$
$$h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$$</p><p><strong>更新门 $z_t$</strong>：控制前一隐藏状态的保留程度</p><ul><li>$z_t \to 0$：保留 $h_{t-1}$</li><li>$z_t \to 1$：更新为 $\tilde{h}_t$</li></ul><p><strong>重置门 $r_t$</strong>：控制前一隐藏状态在候选新状态中的参与度</p><ul><li>$r_t \to 0$：忽略 $h_{t-1}$，类似"重置"</li><li>$r_t \to 1$：考虑 $h_{t-1}$</li></ul><h3 id=34-双向rnnbirnn>3.4 双向RNN（BiRNN）<a hidden class=anchor aria-hidden=true href=#34-双向rnnbirnn>#</a></h3><p>有些任务需要同时考虑过去和未来信息。</p><p><strong>双向RNN</strong>：</p><ul><li>前向RNN：$ \overrightarrow{h}<em>t = RNN</em>{\text{forward}}(x_t, \overrightarrow{h}_{t-1}) $</li><li>后向RNN：$ \overleftarrow{h}<em>t = RNN</em>{\text{backward}}(x_t, \overleftarrow{h}_{t+1}) $</li><li>组合：$h_t = [\overrightarrow{h}_t; \overleftarrow{h}_t]$</li></ul><p>应用：机器翻译、命名实体识别、语音识别</p><h3 id=35-rnn的应用>3.5 RNN的应用<a hidden class=anchor aria-hidden=true href=#35-rnn的应用>#</a></h3><h4 id=351-语言模型>3.5.1 语言模型<a hidden class=anchor aria-hidden=true href=#351-语言模型>#</a></h4><p><strong>任务</strong>：给定前面的词，预测下一个词</p><p>$$P(w_t | w_{&lt;t}) = \text{softmax}(W_y h_t + b_y)$$</p><p><strong>应用</strong>：</p><ul><li>文本生成</li><li>拼写纠正</li><li>语音识别（语言模型用于约束）</li></ul><h4 id=352-机器翻译>3.5.2 机器翻译<a hidden class=anchor aria-hidden=true href=#352-机器翻译>#</a></h4><p><strong>序列到序列（Seq2Seq）模型</strong>：</p><p>编码器-解码器结构：</p><ul><li>编码器：将源序列编码为固定向量</li><li>解码器：从编码向量生成目标序列</li></ul><p>问题：固定长度的编码向量是信息瓶颈</p><p><strong>改进</strong>：注意力机制（Attention）</p><h4 id=353-语音识别>3.5.3 语音识别<a hidden class=anchor aria-hidden=true href=#353-语音识别>#</a></h4><p><strong>声学模型</strong>：将声学特征序列映射到音素或字符序列</p><p><strong>应用</strong>：Siri、Google语音识别、智能音箱</p><h4 id=354-问答系统>3.5.4 问答系统<a hidden class=anchor aria-hidden=true href=#354-问答系统>#</a></h4><p><strong>任务</strong>：给定问题和段落，抽取答案</p><p><strong>方法</strong>：</p><ul><li>将问题和段落编码为向量</li><li>计算相关性</li><li>抽取答案片段</li></ul><h2 id=第四章注意力机制与transformer>第四章：注意力机制与Transformer<a hidden class=anchor aria-hidden=true href=#第四章注意力机制与transformer>#</a></h2><h3 id=41-注意力机制>4.1 注意力机制<a hidden class=anchor aria-hidden=true href=#41-注意力机制>#</a></h3><h4 id=411-动机>4.1.1 动机<a hidden class=anchor aria-hidden=true href=#411-动机>#</a></h4><p>Seq2Seq模型使用固定长度的编码向量，信息瓶颈。</p><p>注意力机制允许解码器在每一步动态关注源序列的不同部分。</p><h4 id=412-bahdanau注意力加性注意力>4.1.2 Bahdanau注意力（加性注意力）<a hidden class=anchor aria-hidden=true href=#412-bahdanau注意力加性注意力>#</a></h4><p><strong>上下文向量</strong>：
$$c_i = \sum_{j=1}^{T_x} \alpha_{ij} h_j$$</p><p>其中 $\alpha_{ij}$ 是注意力权重：
$$\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}$$</p><p>注意力分数：
$$e_{ij} = v_a^T \tanh(W_a h_j + U_a s_{i-1})$$</p><p>$h_j$：编码器隐藏状态，$s_{i-1}$：解码器前一状态</p><h4 id=413-luong注意力乘性注意力>4.1.3 Luong注意力（乘性注意力）<a hidden class=anchor aria-hidden=true href=#413-luong注意力乘性注意力>#</a></h4><p><strong>全局注意力</strong>：
$$c_i = \sum_{j} \alpha_{ij} h_j$$</p><p>注意力分数：
$$e_{ij} = s_{i-1}^T W_a h_j$$</p><p><strong>局部注意力</strong>：只在窗口内计算注意力，减少计算量</p><h3 id=42-自注意力机制>4.2 自注意力机制<a hidden class=anchor aria-hidden=true href=#42-自注意力机制>#</a></h3><h4 id=421-核心思想>4.2.1 核心思想<a hidden class=anchor aria-hidden=true href=#421-核心思想>#</a></h4><p>自注意力允许序列中的每个位置关注其他所有位置。</p><h4 id=422-缩放点积注意力>4.2.2 缩放点积注意力<a hidden class=anchor aria-hidden=true href=#422-缩放点积注意力>#</a></h4><p>给定查询 $Q$、键 $K$、值 $V$：</p><p>$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$</p><p>其中：</p><ul><li>$Q \in \mathbb{R}^{n \times d_k}$</li><li>$K \in \mathbb{R}^{m \times d_k}$</li><li>$V \in \mathbb{R}^{m \times d_v}$</li></ul><p><strong>缩放因子 $\frac{1}{\sqrt{d_k}}$</strong>：防止点积过大导致softmax梯度消失</p><p><strong>直观理解</strong>：</p><ul><li>$Q$：我想找什么</li><li>$K$：我有什么</li><li>注意力权重 = $Q$ 和 $K$ 的相似度（点积）</li><li>输出 = 加权的 $V$</li></ul><h3 id=43-transformer架构>4.3 Transformer架构<a hidden class=anchor aria-hidden=true href=#43-transformer架构>#</a></h3><h4 id=431-整体架构>4.3.1 整体架构<a hidden class=anchor aria-hidden=true href=#431-整体架构>#</a></h4><p>Transformer完全基于注意力机制，摒弃了RNN的循环结构。</p><p><strong>编码器-解码器结构</strong>：</p><ul><li>编码器：$N$ 个相同的层堆叠（通常 $N=6$）</li><li>解码器：$N$ 个相同的层堆叠（通常 $N=6$）</li></ul><h4 id=432-编码器层>4.3.2 编码器层<a hidden class=anchor aria-hidden=true href=#432-编码器层>#</a></h4><p>每层包含两个子层：</p><ol><li>多头自注意力</li><li>前馈神经网络（Feed-Forward Network）</li></ol><p>每个子层后跟残差连接和层归一化：
$$\text{LayerNorm}(x + \text{Sublayer}(x))$$</p><p><strong>前馈神经网络</strong>：
$$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$$</p><p>维度：$d_{\text{model}} \to d_{\text{ff}} \to d_{\text{model}}$
通常 $d_{\text{ff}} = 4d_{\text{model}}$</p><h4 id=433-多头注意力multi-head-attention>4.3.3 多头注意力（Multi-Head Attention）<a hidden class=anchor aria-hidden=true href=#433-多头注意力multi-head-attention>#</a></h4><p><strong>问题</strong>：单头注意力难以捕捉多种关系</p><p><strong>多头注意力</strong>：
$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O$$</p><p>其中：
$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$</p><p>$W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$</p><p>$W^O \in \mathbb{R}^{h d_v \times d_{\text{model}}}$</p><p>通常 $h = 8$ 个头，$d_k = d_v = d_{\text{model}} / h$</p><p><strong>直观理解</strong>：多个头关注不同的方面（语法、语义、指代等）</p><h4 id=434-解码器层>4.3.4 解码器层<a hidden class=anchor aria-hidden=true href=#434-解码器层>#</a></h4><p>每层包含三个子层：</p><ol><li><p><strong>带掩码的多头自注意力</strong>：</p><ul><li>只能关注已生成的部分（不能看未来）</li><li>掩码：将未来位置的注意力分数设为 $-\infty$</li></ul></li><li><p><strong>编码器-解码器注意力（Cross-Attention）</strong>：</p><ul><li>查询来自解码器</li><li>键和值来自编码器输出</li><li>允许解码器关注源序列的不同部分</li></ul></li><li><p><strong>前馈神经网络</strong></p></li></ol><h4 id=435-位置编码>4.3.5 位置编码<a hidden class=anchor aria-hidden=true href=#435-位置编码>#</a></h4><p><strong>问题</strong>：自注意力没有时序信息</p><p><strong>位置编码</strong>：给每个位置添加固定或可学习的编码</p><p><strong>正弦余弦位置编码</strong>：
$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)$$
$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)$$</p><p>性质：</p><ul><li>每个维度对应不同的频率</li><li>允许模型学习相对位置（$PE_{pos+k}$ 可以表示为 $PE_{pos}$ 的线性函数）</li></ul><h4 id=436-输入嵌入>4.3.6 输入嵌入<a hidden class=anchor aria-hidden=true href=#436-输入嵌入>#</a></h4><p><strong>词嵌入</strong>：将词索引映射到 $d_{\text{model}}$ 维向量</p><p>$$x = \text{Embedding}(\text{token_id}) \cdot \sqrt{d_{\text{model}}}$$</p><p>缩放 $\sqrt{d_{\text{model}}}$ 是为了与位置编码保持一致</p><p><strong>最终输入</strong>：$X = \text{Embedding} + \text{PositionalEncoding}$</p><h3 id=44-transformer变体>4.4 Transformer变体<a hidden class=anchor aria-hidden=true href=#44-transformer变体>#</a></h3><h4 id=441-bertbidirectional-encoder-representations-from-transformers>4.4.1 BERT（Bidirectional Encoder Representations from Transformers）<a hidden class=anchor aria-hidden=true href=#441-bertbidirectional-encoder-representations-from-transformers>#</a></h4><p><strong>架构</strong>：仅Transformer编码器</p><p><strong>预训练任务</strong>：</p><ol><li><p><strong>掩码语言模型（MLM）</strong>：随机掩盖15%的token，预测它们</p><ul><li>80%替换为[MASK]</li><li>10%替换为随机词</li><li>10%保持不变</li></ul></li><li><p><strong>下一句预测（NSP）</strong>：给定句子对，预测第二个句子是否是第一个句子的下一句</p></li></ol><p><strong>应用</strong>：</p><ul><li>GLUE基准测试（句子对分类、相似度、推理等）</li><li>文本分类、命名实体识别、问答</li></ul><h4 id=442-gptgenerative-pre-trained-transformer>4.4.2 GPT（Generative Pre-trained Transformer）<a hidden class=anchor aria-hidden=true href=#442-gptgenerative-pre-trained-transformer>#</a></h4><p><strong>架构</strong>：仅Transformer解码器（带掩码的自注意力）</p><p><strong>预训练任务</strong>：标准语言模型（预测下一个词）</p><p><strong>微调</strong>：在下游任务上继续训练（带标签数据）</p><p><strong>GPT-3（2020）</strong>：</p><ul><li>1750亿参数</li><li>零样本/少样本学习能力强</li><li>展示了大模型的强大能力</li></ul><h4 id=443-t5text-to-text-transfer-transformer>4.4.3 T5（Text-to-Text Transfer Transformer）<a hidden class=anchor aria-hidden=true href=#443-t5text-to-text-transfer-transformer>#</a></h4><p><strong>统一框架</strong>：所有NLP任务都转换为文本到文本问题</p><p><strong>示例</strong>：</p><ul><li>翻译：<code>translate English to German: That is good.</code> → <code>Das ist gut.</code></li><li>摘要：<code>summarize: ...</code> → 摘要文本</li><li>分类：<code>cola sentence: ...</code> → <code>acceptable</code>/<code>unacceptable</code></li></ul><h4 id=444-vision-transformervit>4.4.4 Vision Transformer（ViT）<a hidden class=anchor aria-hidden=true href=#444-vision-transformervit>#</a></h4><p><strong>思想</strong>：将Transformer应用于图像</p><p><strong>方法</strong>：</p><ol><li>将图像分割为 $16 \times 16$ 的patches</li><li>将每个patch展平并线性映射为向量</li><li>添加可学习的[CLS] token</li><li>添加位置编码</li><li>输入Transformer编码器</li><li>[CLS] token的输出作为图像表示</li></ol><p><strong>优势</strong>：</p><ul><li>全局感受野（CNN是局部的）</li><li>大规模预训练后表现优异</li></ul><h4 id=445-chatgptinstructgptgpt-35>4.4.5 ChatGPT（InstructGPT/GPT-3.5）<a hidden class=anchor aria-hidden=true href=#445-chatgptinstructgptgpt-35>#</a></h4><p><strong>三阶段训练</strong>：</p><ol><li><p><strong>预训练</strong>：大规模语言模型预训练（自监督）</p></li><li><p><strong>有监督微调（SFT）</strong>：</p><ul><li>人类编写高质量的问答对</li><li>微调模型使其遵循指令</li></ul></li><li><p><strong>人类反馈强化学习（RLHF）</strong>：</p><ul><li><strong>奖励模型（RM）</strong>：训练模型预测人类对回答的偏好（排序）</li><li><strong>强化学习（PPO）</strong>：优化策略（回答生成）以最大化奖励</li></ul></li></ol><p><strong>成功因素</strong>：</p><ul><li>大规模（数十亿参数）</li><li>高质量人类数据（指令、对话）</li><li>对齐人类意图</li></ul><h3 id=45-注意力机制的应用>4.5 注意力机制的应用<a hidden class=anchor aria-hidden=true href=#45-注意力机制的应用>#</a></h3><h4 id=451-自然语言处理>4.5.1 自然语言处理<a hidden class=anchor aria-hidden=true href=#451-自然语言处理>#</a></h4><ul><li>机器翻译</li><li>文本摘要</li><li>问答系统</li><li>情感分析</li><li>命名实体识别</li></ul><h4 id=452-计算机视觉>4.5.2 计算机视觉<a hidden class=anchor aria-hidden=true href=#452-计算机视觉>#</a></h4><ul><li>目标检测（DETR）</li><li>图像分类</li><li>视频理解</li></ul><h4 id=453-多模态>4.5.3 多模态<a hidden class=anchor aria-hidden=true href=#453-多模态>#</a></h4><ul><li>CLIP（图文对比学习）</li><li>DALL·E（文本生成图像）</li><li>视频描述生成</li></ul><h2 id=第五章深度学习的应用场景>第五章：深度学习的应用场景<a hidden class=anchor aria-hidden=true href=#第五章深度学习的应用场景>#</a></h2><h3 id=51-计算机视觉>5.1 计算机视觉<a hidden class=anchor aria-hidden=true href=#51-计算机视觉>#</a></h3><h4 id=511-图像分类>5.1.1 图像分类<a hidden class=anchor aria-hidden=true href=#511-图像分类>#</a></h4><p><strong>应用</strong>：</p><ul><li>ImageNet挑战赛（1000类）</li><li>医学影像诊断（肺结节检测、视网膜病变识别）</li><li>电商商品分类</li></ul><h4 id=512-目标检测>5.1.2 目标检测<a hidden class=anchor aria-hidden=true href=#512-目标检测>#</a></h4><p><strong>应用</strong>：</p><ul><li>自动驾驶（车辆、行人、交通标志检测）</li><li>安防监控（异常行为检测）</li><li>工业质检（缺陷检测）</li></ul><h4 id=513-语义分割>5.1.3 语义分割<a hidden class=anchor aria-hidden=true href=#513-语义分割>#</a></h4><p><strong>应用</strong>：</p><ul><li>自动驾驶（道路、车道线分割）</li><li>医学影像（器官、肿瘤分割）</li><li>遥感图像（土地覆盖分类）</li></ul><h4 id=514-人脸识别>5.1.4 人脸识别<a hidden class=anchor aria-hidden=true href=#514-人脸识别>#</a></h4><p><strong>应用</strong>：</p><ul><li>手机解锁（Face ID）</li><li>支付验证</li><li>安防监控</li></ul><h3 id=52-自然语言处理>5.2 自然语言处理<a hidden class=anchor aria-hidden=true href=#52-自然语言处理>#</a></h3><h4 id=521-文本生成>5.2.1 文本生成<a hidden class=anchor aria-hidden=true href=#521-文本生成>#</a></h4><p><strong>应用</strong>：</p><ul><li>机器翻译</li><li>文本摘要</li><li>创意写作</li></ul><h4 id=522-问答系统>5.2.2 问答系统<a hidden class=anchor aria-hidden=true href=#522-问答系统>#</a></h4><p><strong>应用</strong>：</p><ul><li>搜索引擎</li><li>智能客服</li><li>知识图谱问答</li></ul><h4 id=523-情感分析>5.2.3 情感分析<a hidden class=anchor aria-hidden=true href=#523-情感分析>#</a></h4><p><strong>应用</strong>：</p><ul><li>社交媒体舆情分析</li><li>产品评论分析</li><li>客户满意度调查</li></ul><h4 id=524-命名实体识别>5.2.4 命名实体识别<a hidden class=anchor aria-hidden=true href=#524-命名实体识别>#</a></h4><p><strong>应用</strong>：</p><ul><li>信息抽取（人名、地名、机构名）</li><li>知识图谱构建</li><li>文档结构化</li></ul><h3 id=53-语音处理>5.3 语音处理<a hidden class=anchor aria-hidden=true href=#53-语音处理>#</a></h3><h4 id=531-语音识别asr>5.3.1 语音识别（ASR）<a hidden class=anchor aria-hidden=true href=#531-语音识别asr>#</a></h4><p><strong>应用</strong>：</p><ul><li>语音助手（Siri、小爱同学）</li><li>会议记录</li><li>实时字幕</li></ul><p><strong>技术</strong>：</p><ul><li>声学模型：端到端架构（LAS、CTC、RNN-Transducer）</li><li>语言模型：Transformer</li></ul><h4 id=532-语音合成tts>5.3.2 语音合成（TTS）<a hidden class=anchor aria-hidden=true href=#532-语音合成tts>#</a></h4><p><strong>应用</strong>：</p><ul><li>导航语音</li><li>有声书</li><li>语音助手</li></ul><p><strong>技术</strong>：</p><ul><li>Tacotron（Seq2Seq + 注意力）</li><li>WaveNet（自回归波形生成）</li><li>VITS（变分推断）</li></ul><h4 id=533-说话人识别>5.3.3 说话人识别<a hidden class=anchor aria-hidden=true href=#533-说话人识别>#</a></h4><p><strong>应用</strong>：</p><ul><li>声纹认证</li><li>多说话人分离</li></ul><h3 id=54-推荐系统>5.4 推荐系统<a hidden class=anchor aria-hidden=true href=#54-推荐系统>#</a></h3><h4 id=541-协同过滤--深度学习>5.4.1 协同过滤 + 深度学习<a hidden class=anchor aria-hidden=true href=#541-协同过滤--深度学习>#</a></h4><p><strong>DeepFM</strong>：结合因子分解机和深度学习</p><p><strong>DIN（Deep Interest Network）</strong>：自适应关注历史兴趣</p><h4 id=542-序列推荐>5.4.2 序列推荐<a hidden class=anchor aria-hidden=true href=#542-序列推荐>#</a></h4><p><strong>应用</strong>：</p><ul><li>电商推荐（根据点击历史推荐）</li><li>视频推荐（根据观看历史推荐）</li></ul><p><strong>技术</strong>：Transformer、BERT4Rec</p><h3 id=55-游戏>5.5 游戏<a hidden class=anchor aria-hidden=true href=#55-游戏>#</a></h3><h4 id=551-alphago2016>5.5.1 AlphaGo（2016）<a hidden class=anchor aria-hidden=true href=#551-alphago2016>#</a></h4><p><strong>技术</strong>：</p><ul><li>策略网络（Policy Network）：预测下一步棋</li><li>价值网络（Value Network）：评估局面胜率</li><li>蒙特卡洛树搜索（MCTS）</li></ul><h4 id=552-alphastar星际争霸ii>5.5.2 AlphaStar（星际争霸II）<a hidden class=anchor aria-hidden=true href=#552-alphastar星际争霸ii>#</a></h4><p><strong>技术</strong>：</p><ul><li>多智能体强化学习</li><li>历史自对弈</li><li>延迟（Latency）处理</li></ul><h3 id=56-生物信息学>5.6 生物信息学<a hidden class=anchor aria-hidden=true href=#56-生物信息学>#</a></h3><h4 id=561-蛋白质结构预测alphafold2>5.6.1 蛋白质结构预测（AlphaFold2）<a hidden class=anchor aria-hidden=true href=#561-蛋白质结构预测alphafold2>#</a></h4><p><strong>技术</strong>：</p><ul><li>注意力机制</li><li>残差网络</li><li>端到端学习</li></ul><p><strong>成就</strong>：预测精度接近实验方法（CASP14竞赛冠军）</p><h4 id=562-基因序列分析>5.6.2 基因序列分析<a hidden class=anchor aria-hidden=true href=#562-基因序列分析>#</a></h4><p><strong>应用</strong>：</p><ul><li>基因表达预测</li><li>突变影响预测</li></ul><h2 id=第六章深度学习的挑战与未来>第六章：深度学习的挑战与未来<a hidden class=anchor aria-hidden=true href=#第六章深度学习的挑战与未来>#</a></h2><h3 id=61-当前挑战>6.1 当前挑战<a hidden class=anchor aria-hidden=true href=#61-当前挑战>#</a></h3><h4 id=611-数据饥渴>6.1.1 数据饥渴<a hidden class=anchor aria-hidden=true href=#611-数据饥渴>#</a></h4><p>深度学习需要大量标注数据，但标注成本高。</p><p><strong>解决方法</strong>：</p><ul><li>自监督学习（从无标注数据学习）</li><li>半监督学习</li><li>数据增强</li><li>合成数据</li></ul><h4 id=612-计算资源需求>6.1.2 计算资源需求<a hidden class=anchor aria-hidden=true href=#612-计算资源需求>#</a></h4><p>大模型训练成本极高（GPT-3训练成本约460万美元）。</p><p><strong>解决方法</strong>：</p><ul><li>模型压缩（蒸馏、量化、剪枝）</li><li>高效架构（MobileNet、EfficientNet）</li><li>分布式训练</li></ul><h4 id=613-可解释性>6.1.3 可解释性<a hidden class=anchor aria-hidden=true href=#613-可解释性>#</a></h4><p>深度学习是黑箱，难以解释决策过程。</p><p><strong>解决方法</strong>：</p><ul><li>注意力可视化</li><li>LIME、SHAP（局部解释）</li><li>归因方法（Integrated Gradients）</li><li>可解释架构（ProtoPNet）</li></ul><h4 id=614-泛化能力>6.1.4 泛化能力<a hidden class=anchor aria-hidden=true href=#614-泛化能力>#</a></h4><p>在训练分布上表现好，但分布外（OOD）泛化差。</p><p><strong>解决方法</strong>：</p><ul><li>数据多样性</li><li>域自适应</li><li>元学习</li><li>因果推断</li></ul><h4 id=615-安全性>6.1.5 安全性<a hidden class=anchor aria-hidden=true href=#615-安全性>#</a></h4><p>对抗攻击（Adversarial Attack）：微小扰动导致模型误判。</p><p><strong>示例</strong>：给熊猫图片添加不可见的噪声，模型识别为长臂猿</p><p><strong>解决方法</strong>：</p><ul><li>对抗训练</li><li>鲁棒优化</li><li>检测对抗样本</li></ul><h4 id=616-公平性与偏见>6.1.6 公平性与偏见<a hidden class=anchor aria-hidden=true href=#616-公平性与偏见>#</a></h4><p>模型可能学习并放大数据中的偏见（种族、性别等）。</p><p><strong>解决方法</strong>：</p><ul><li>公平性约束优化</li><li>数据去偏见</li><li>模型审计</li></ul><h3 id=62-未来发展趋势>6.2 未来发展趋势<a hidden class=anchor aria-hidden=true href=#62-未来发展趋势>#</a></h3><h4 id=621-大模型foundation-models>6.2.1 大模型（Foundation Models）<a hidden class=anchor aria-hidden=true href=#621-大模型foundation-models>#</a></h4><p><strong>趋势</strong>：预训练大模型在多个任务上微调</p><p><strong>代表性工作</strong>：</p><ul><li>GPT系列、BERT系列（语言）</li><li>CLIP、DALL·E（视觉）</li><li>Flamingo、BLIP（多模态）</li></ul><p><strong>挑战</strong>：</p><ul><li>部署成本高</li><li>幻觉问题（Hallucination）</li><li>对齐人类价值观</li></ul><h4 id=622-多模态学习>6.2.2 多模态学习<a hidden class=anchor aria-hidden=true href=#622-多模态学习>#</a></h4><p><strong>目标</strong>：融合文本、图像、视频、音频等多模态信息</p><p><strong>应用</strong>：</p><ul><li>图文检索（CLIP）</li><li>视频理解</li><li>视觉问答（VQA）</li><li>图文生成（DALL·E、Stable Diffusion）</li></ul><h4 id=623-自监督学习>6.2.3 自监督学习<a hidden class=anchor aria-hidden=true href=#623-自监督学习>#</a></h4><p><strong>思想</strong>：从无标注数据中学习表示，无需人工标签</p><p><strong>方法</strong>：</p><ul><li>对比学习（SimCLR、MoCo）</li><li>掩码建模（MAE、BERT-style）</li><li>自回归建模（GPT-style）</li></ul><p><strong>优势</strong>：</p><ul><li>利用大规模无标注数据</li><li>学习通用表示，下游任务微调效果好</li></ul><h4 id=624-小样本与零样本学习>6.2.4 小样本与零样本学习<a hidden class=anchor aria-hidden=true href=#624-小样本与零样本学习>#</a></h4><p><strong>目标</strong>：用极少样本甚至无样本学习新任务</p><p><strong>方法</strong>：</p><ul><li>元学习（Learning to Learn）</li><li>原型网络（Prototypical Networks）</li><li>度量学习</li><li>大模型的零样本/少样本能力</li></ul><h4 id=625-神经符号ai>6.2.5 神经符号AI<a hidden class=anchor aria-hidden=true href=#625-神经符号ai>#</a></h4><p><strong>目标</strong>：结合深度学习的感知能力和符号AI的推理能力</p><p><strong>方法</strong>：</p><ul><li>神经网络 + 知识图谱</li><li>程序生成/执行</li><li>逻辑约束学习</li></ul><h4 id=626-神经形态计算>6.2.6 神经形态计算<a hidden class=anchor aria-hidden=true href=#626-神经形态计算>#</a></h4><p><strong>目标</strong>：模拟生物神经元的计算方式，功耗更低</p><p><strong>技术</strong>：</p><ul><li>脉冲神经网络（SNN）</li><li>神经形态芯片（Intel Loihi、IBM TrueNorth）</li></ul><h4 id=627-生成式ai>6.2.7 生成式AI<a hidden class=anchor aria-hidden=true href=#627-生成式ai>#</a></h4><p><strong>趋势</strong>：从判别式模型转向生成式模型</p><p><strong>应用</strong>：</p><ul><li>文本生成（GPT、ChatGPT）</li><li>图像生成（Stable Diffusion、Midjourney）</li><li>视频生成（Sora、Runway）</li><li>音乐生成（Suno、MusicLM）</li></ul><p><strong>挑战</strong>：</p><ul><li>生成质量与多样性权衡</li><li>伦理与版权问题</li><li>深度伪造（Deepfake）</li></ul><h4 id=628-具身智能>6.2.8 具身智能<a hidden class=anchor aria-hidden=true href=#628-具身智能>#</a></h4><p><strong>目标</strong>：AI与物理世界交互（机器人、自动驾驶）</p><p><strong>方法</strong>：</p><ul><li>强化学习</li><li>模拟到真实迁移（Sim2Real）</li><li>世界模型（World Model）</li></ul><h4 id=629-ai-for-science>6.2.9 AI for Science<a hidden class=anchor aria-hidden=true href=#629-ai-for-science>#</a></h4><p><strong>目标</strong>：用AI加速科学发现</p><p><strong>应用</strong>：</p><ul><li>AlphaFold（蛋白质结构预测）</li><li>材料设计</li><li>天气预报</li><li>数学研究（Lean、Coq + AI）</li></ul><h4 id=6210-可信赖aitrustworthy-ai>6.2.10 可信赖AI（Trustworthy AI）<a hidden class=anchor aria-hidden=true href=#6210-可信赖aitrustworthy-ai>#</a></h4><p><strong>目标</strong>：AI安全、可靠、可控</p><p><strong>方向</strong>：</p><ul><li>可解释性（XAI）</li><li>鲁棒性（对抗攻击防御）</li><li>公平性（消除偏见）</li><li>隐私保护（联邦学习、差分隐私）</li><li>对齐（Alignment）：确保AI目标与人类价值观一致</li></ul><h3 id=63-结语>6.3 结语<a hidden class=anchor aria-hidden=true href=#63-结语>#</a></h3><p>深度学习在过去十年取得了惊人的成就，从AlexNet到GPT-4，从图像分类到通用对话AI。但我们也必须清醒地认识到当前的局限性：数据饥渴、计算昂贵、黑箱决策、安全问题。</p><p>未来的发展方向不是简单地堆叠参数，而是：</p><ol><li><strong>更高效的学习</strong>：自监督、小样本、终身学习</li><li><strong>更广泛的应用</strong>：多模态、具身、科学发现</li><li><strong>更可靠的技术</strong>：可解释、鲁棒、对齐</li></ol><p>正如深度学习的先驱Hinton所说：&ldquo;The question of whether computers can think is no more interesting than the question of whether submarines can swim."（计算机能否思考的问题，就像潜水艇能否游泳的问题一样无趣。）</p><p>重要的不是模仿人脑，而是构建真正有用的智能系统。深度学习正在朝着这个方向前进。</p><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><ol><li>LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. <em>Nature</em>, 521(7553), 436-444.</li><li>Goodfellow, I., Bengio, Y., & Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</li><li>Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. <em>NeurIPS</em>.</li><li>Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. <em>arXiv preprint arXiv:1409.1556</em>.</li><li>He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. <em>CVPR</em>.</li><li>Szegedy, C., et al. (2015). Going deeper with convolutions. <em>CVPR</em>.</li><li>Vaswani, A., et al. (2017). Attention is all you need. <em>NeurIPS</em>.</li><li>Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. <em>NAACL</em>.</li><li>Brown, T., et al. (2020). Language models are few-shot learners. <em>NeurIPS</em>.</li><li>Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. <em>Neural Computation</em>, 9(8), 1735-1780.</li><li>Jumper, J., et al. (2021). Highly accurate protein structure prediction with AlphaFold. <em>Nature</em>, 596(7873), 583-589.</li><li>OpenAI. (2023). GPT-4 Technical Report. <em>arXiv preprint arXiv:2303.08774</em>.</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%95%B0%E5%AD%A6%E5%8F%B2/>数学史</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%BB%BC%E8%BF%B0/>综述</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-14-gradient-descent-backpropagation-overview/><span class=title>« Prev</span><br><span>梯度、梯度下降与反向传播：从最优化到深度学习的数学引擎</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-14-reinforcement-learning-comprehensive-guide/><span class=title>Next »</span><br><span>强化学习：从试错到智能的数学之旅</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 基于神经网络的深度学习算法：从感知机到Transformer的完整指南 on x" href="https://x.com/intent/tweet/?text=%e5%9f%ba%e4%ba%8e%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%ef%bc%9a%e4%bb%8e%e6%84%9f%e7%9f%a5%e6%9c%ba%e5%88%b0Transformer%e7%9a%84%e5%ae%8c%e6%95%b4%e6%8c%87%e5%8d%97&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-14-deep-learning-algorithms-comprehensive-guide%2f&amp;hashtags=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%2c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%2c%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%2c%e6%95%b0%e5%ad%a6%e5%8f%b2%2c%e7%bb%bc%e8%bf%b0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基于神经网络的深度学习算法：从感知机到Transformer的完整指南 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-14-deep-learning-algorithms-comprehensive-guide%2f&amp;title=%e5%9f%ba%e4%ba%8e%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%ef%bc%9a%e4%bb%8e%e6%84%9f%e7%9f%a5%e6%9c%ba%e5%88%b0Transformer%e7%9a%84%e5%ae%8c%e6%95%b4%e6%8c%87%e5%8d%97&amp;summary=%e5%9f%ba%e4%ba%8e%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%ef%bc%9a%e4%bb%8e%e6%84%9f%e7%9f%a5%e6%9c%ba%e5%88%b0Transformer%e7%9a%84%e5%ae%8c%e6%95%b4%e6%8c%87%e5%8d%97&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-14-deep-learning-algorithms-comprehensive-guide%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基于神经网络的深度学习算法：从感知机到Transformer的完整指南 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-14-deep-learning-algorithms-comprehensive-guide%2f&title=%e5%9f%ba%e4%ba%8e%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%ef%bc%9a%e4%bb%8e%e6%84%9f%e7%9f%a5%e6%9c%ba%e5%88%b0Transformer%e7%9a%84%e5%ae%8c%e6%95%b4%e6%8c%87%e5%8d%97"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基于神经网络的深度学习算法：从感知机到Transformer的完整指南 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-14-deep-learning-algorithms-comprehensive-guide%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var n,t=e.innerHTML,s=!1;for(t.indexOf("<em>")!==-1&&(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("</em>")!==-1&&(n=t.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),n=n.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("<em>$")!==-1&&(n=t.replace(/<em>\$/g,"$"),n!==t&&(t=n,s=!0)),t.indexOf("</em>$")!==-1&&(n=t.replace(/<\/em>\$/g,"$"),n!==t&&(t=n,s=!0));t.indexOf("<em>")!==-1&&t.indexOf("</em>")!==-1;){if(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n===t)break;t=n,s=!0}t.indexOf("_{")!==-1&&(n=t.replace(/_{/g,"_{"),n!==t&&(t=n,s=!0)),t.indexOf("}_")!==-1&&(n=t.replace(/}_/g,"}"),n!==t&&(t=n,s=!0)),t.indexOf("\\\\")!==-1&&(n=t.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),n!==t&&(t=n,s=!0)),s&&(e.innerHTML=t)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var n,t=e.innerHTML,s=!1;for(t.indexOf("<em>")!==-1&&(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("</em>")!==-1&&(n=t.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),n=n.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("<em>$")!==-1&&(n=t.replace(/<em>\$/g,"$"),n!==t&&(t=n,s=!0)),t.indexOf("</em>$")!==-1&&(n=t.replace(/<\/em>\$/g,"$"),n!==t&&(t=n,s=!0));t.indexOf("<em>")!==-1&&t.indexOf("</em>")!==-1;){if(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n===t)break;t=n,s=!0}t.indexOf("_{")!==-1&&(n=t.replace(/_{/g,"_{"),n!==t&&(t=n,s=!0)),t.indexOf("}_")!==-1&&(n=t.replace(/}_/g,"}"),n!==t&&(t=n,s=!0)),t.indexOf("\\\\")!==-1&&(n=t.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),n!==t&&(t=n,s=!0)),s&&(e.innerHTML=t)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>