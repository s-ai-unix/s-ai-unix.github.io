<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AI 论文解读系列：Vision Transformer 视觉Transformer | s-ai-unix's Blog</title><meta name=keywords content="深度学习,Transformer,计算机视觉,机器学习"><meta name=description content="深入解读 Google Research 的 Vision Transformer 论文，从注意力机制的原理出发，剖析图像块嵌入、位置编码、Transformer Encoder 的完整架构，揭示 Transformer 如何在计算机视觉领域挑战 CNN 的统治地位。"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97vision-transformer-%E8%A7%86%E8%A7%89transformer/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97vision-transformer-%E8%A7%86%E8%A7%89transformer/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97vision-transformer-%E8%A7%86%E8%A7%89transformer/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="AI 论文解读系列：Vision Transformer 视觉Transformer"><meta property="og:description" content="深入解读 Google Research 的 Vision Transformer 论文，从注意力机制的原理出发，剖析图像块嵌入、位置编码、Transformer Encoder 的完整架构，揭示 Transformer 如何在计算机视觉领域挑战 CNN 的统治地位。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-30T08:46:42+08:00"><meta property="article:modified_time" content="2026-01-30T08:46:42+08:00"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="计算机视觉"><meta property="article:tag" content="机器学习"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/vit-cover.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/vit-cover.jpg"><meta name=twitter:title content="AI 论文解读系列：Vision Transformer 视觉Transformer"><meta name=twitter:description content="深入解读 Google Research 的 Vision Transformer 论文，从注意力机制的原理出发，剖析图像块嵌入、位置编码、Transformer Encoder 的完整架构，揭示 Transformer 如何在计算机视觉领域挑战 CNN 的统治地位。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"AI 论文解读系列：Vision Transformer 视觉Transformer","item":"https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97vision-transformer-%E8%A7%86%E8%A7%89transformer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"AI 论文解读系列：Vision Transformer 视觉Transformer","name":"AI 论文解读系列：Vision Transformer 视觉Transformer","description":"深入解读 Google Research 的 Vision Transformer 论文，从注意力机制的原理出发，剖析图像块嵌入、位置编码、Transformer Encoder 的完整架构，揭示 Transformer 如何在计算机视觉领域挑战 CNN 的统治地位。","keywords":["深度学习","Transformer","计算机视觉","机器学习"],"articleBody":"AI 论文解读系列：Vision Transformer 视觉 Transformer 引言 2020 年，Google Research 发表了一篇极具颠覆性的论文《An Image is Worth 16$\\times$16 Words: Transformers for Image Recognition at Scale》。这篇论文提出了 Vision Transformer（ViT），一个纯粹基于 Transformer 架构的视觉模型，在 ImageNet 分类任务上取得了与最先进的卷积神经网络（CNN）相媲美甚至超越的成绩。\n这个成果的震撼之处在于：在计算机视觉领域统治了整整十年的卷积神经网络，终于遇到了真正的挑战者。CNN 凭借其归纳偏置（局部性、平移等变性）在视觉任务中表现出色，而 Transformer 原本是为自然语言处理设计的序列模型。ViT 的成功证明，只要有足够的数据和计算资源，纯粹的注意力机制同样可以在视觉任务中大放异彩。\n本文将从注意力机制的基础出发，循序渐进地剖析 ViT 的架构设计、数学原理和训练策略，揭示为何\"一张图片相当于 16$\\times$16 个单词\"这一简单想法能够改变计算机视觉的格局。\n第一章：从 CNN 到 Transformer 的范式转移 1.1 卷积神经网络的统治时代 自 2012 年 AlexNet 在 ImageNet 竞赛中取得突破性成果以来，卷积神经网络（CNN）一直是计算机视觉领域的主流架构。CNN 的成功建立在几个关键设计之上：\n局部感受野（Local Receptive Fields）：每个神经元只与输入的局部区域连接，捕捉局部特征如边缘、纹理。\n权重共享（Weight Sharing）：同一个卷积核在整个输入上滑动，检测相同特征的不同位置。\n平移等变性（Translation Equivariance）：输入图像平移，特征图也相应平移，保持空间关系。\n这些归纳偏置（Inductive Bias）使 CNN 非常适合处理图像数据，但也带来了一些限制：\n感受野有限，需要堆叠多层才能获取全局信息 对长距离依赖的建模能力较弱 难以直接捕捉空间上相距较远的像素之间的关系 1.2 Transformer 在自然语言处理中的成功 2017 年，Google 在论文《Attention Is All You Need》中提出了 Transformer 架构，彻底改变了自然语言处理（NLP）领域。Transformer 完全基于自注意力机制（Self-Attention），摒弃了循环和卷积结构。\nTransformer 的核心优势：\n全局上下文建模：每个位置都可以直接关注序列中的任意其他位置，不受距离限制。\n并行计算：不像 RNN 需要顺序处理，Transformer 可以并行处理整个序列。\n可扩展性：随着数据量和计算资源的增加，Transformer 的性能持续提升。\n在 NLP 领域，从 BERT 到 GPT 系列，Transformer 架构不断刷新各项任务的基准。一个自然的问题浮现：能否将这一成功迁移到计算机视觉领域？\n1.3 将 Transformer 应用于图像的挑战 直接将 NLP 中的 Transformer 应用于图像面临几个挑战：\n尺度问题：在 NLP 中，输入是离散的单词或子词单元，序列长度通常为几百到几千。而图像是连续的像素网格，即使是 $224 \\times 224$ 的小图像也有 50,176 个像素。\n如果直接将每个像素作为一个 token，自注意力的计算复杂度是 $O(n^2)$，其中 $n$ 是序列长度。对于 $224 \\times 224$ 的图像：\n$$n = 224 \\times 224 = 50,176$$\n自注意力矩阵的大小将是 $50,176 \\times 50,176 \\approx 25$ 亿个元素，内存和计算开销都是不可接受的。\n归纳偏置的缺失：CNN 的局部性和平移等变性是处理图像的强大先验。纯粹的 Transformer 缺乏这些归纳偏置，需要从数据中从头学习空间关系。\nViT 的解决方案既优雅又简单：将图像分割成固定大小的块（patches），将每个块视为一个\"视觉单词\"。\n第二章：Vision Transformer 的核心思想 2.1 图像块嵌入：从像素到序列 ViT 的第一步是将二维图像转换为序列形式。具体做法如下：\n给定一张图像 $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$，其中 $H$ 和 $W$ 是高和宽，$C$ 是通道数（RGB 图像中 $C=3$）。\n将图像划分为 $N$ 个固定大小的块（patches），每个块的大小为 $P \\times P$：\n$$N = \\frac{H \\times W}{P^2}$$\n对于标准的 ViT 配置，输入图像为 $224 \\times 224$，块大小 $P = 16$，则：\n$$N = \\frac{224 \\times 224}{16 \\times 16} = \\frac{50176}{256} = 196$$\n这就是论文标题\"An Image is Worth 16$\\times$16 Words\"的由来——一张图像被转换为 196 个\"视觉单词\"的序列。\n每个图像块被展平并通过一个可训练的线性投影层映射到维度 $D$：\n$$\\mathbf{z}0 = [\\mathbf{x}{class}; \\mathbf{x}_p^1\\mathbf{E}; \\mathbf{x}_p^2\\mathbf{E}; \\cdots; \\mathbf{x}p^N\\mathbf{E}] + \\mathbf{E}{pos}$$\n其中：\n$\\mathbf{x}_p^i \\in \\mathbb{R}^{P^2 \\cdot C}$ 是第 $i$ 个展平的图像块 $\\mathbf{E} \\in \\mathbb{R}^{(P^2 \\cdot C) \\times D}$ 是块嵌入矩阵（Patch Embedding Matrix） $\\mathbf{E}_{pos} \\in \\mathbb{R}^{(N+1) \\times D}$ 是位置嵌入（Position Embedding） $\\mathbf{x}_{class}$ 是可学习的类别 token 2.2 类别 Token 与位置编码 类别 Token（Class Token）：\nViT 借鉴了 BERT 的做法，在序列开头添加一个特殊的可学习嵌入 $\\mathbf{x}_{class}$。这个 token 的输出状态将被用作图像的聚合表示，输入到分类头进行预测：\n$$y = \\text{LN}(\\mathbf{z}_L^0)$$\n其中 $\\mathbf{z}_L^0$ 是 Transformer 最后一层输出的第一个位置（类别 token）的状态。\n位置编码（Position Embedding）：\n由于 Transformer 本身不具有序列顺序的概念，需要添加位置信息。ViT 使用标准的可学习 1D 位置编码：\n$$\\mathbf{E}{pos} = [\\mathbf{e}{pos}^0; \\mathbf{e}{pos}^1; \\cdots; \\mathbf{e}{pos}^N]$$\n实验表明，使用 2D 感知的位置编码或相对位置编码并没有显著提升性能，说明 Transformer 可以从数据中学习空间关系。\n2.3 Transformer Encoder 架构 ViT 使用标准的 Transformer Encoder，由交替的多头自注意力（MSA）和多层感知机（MLP）块组成，每个块之前应用 Layer Normalization（LN）：\n$$\\begin{aligned} \\mathbf{z}'_\\ell \u0026= \\text{MSA}(\\text{LN}(\\mathbf{z}_{\\ell-1})) + \\mathbf{z}_{\\ell-1} \\\\ \\mathbf{z}_\\ell \u0026= \\text{MLP}(\\text{LN}(\\mathbf{z}'_\\ell)) + \\mathbf{z}'_\\ell \\end{aligned}$$ 其中 $\\ell = 1, \\ldots, L$，$L$ 是 Transformer 层的数量。\n多头自注意力（Multi-Head Self-Attention, MSA）：\n对于输入 $\\mathbf{Z} \\in \\mathbb{R}^{N \\times D}$，首先通过三个线性投影得到查询（Query）、键（Key）和值（Value）：\n$$\\begin{aligned} \\mathbf{Q} \u0026= \\mathbf{Z}\\mathbf{W}^Q \\\\ \\mathbf{K} \u0026= \\mathbf{Z}\\mathbf{W}^K \\\\ \\mathbf{V} \u0026= \\mathbf{Z}\\mathbf{W}^V \\end{aligned}$$ 其中 $\\mathbf{W}^Q, \\mathbf{W}^K, \\mathbf{W}^V \\in \\mathbb{R}^{D \\times d_k}$，通常 $d_k = D/h$，$h$ 是注意力头的数量。\n缩放点积注意力定义为：\n$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}$$\n除以 $\\sqrt{d_k}$ 是为了防止点积过大导致 softmax 梯度消失。\n多头注意力将输入投影到多个子空间，并行计算注意力：\n$$\\begin{aligned} \\text{MSA}(\\mathbf{Z}) \u0026= [\\text{head}_1; \\cdots; \\text{head}_h]\\mathbf{W}^O \\\\ \\text{head}_i \u0026= \\text{Attention}(\\mathbf{Z}\\mathbf{W}_i^Q, \\mathbf{Z}\\mathbf{W}_i^K, \\mathbf{Z}\\mathbf{W}_i^V) \\end{aligned}$$ 多层感知机（MLP）：\n每个 Transformer 块包含一个两层的 MLP，使用 GELU 激活函数：\n$$\\text{MLP}(\\mathbf{z}) = \\text{GELU}(\\mathbf{z}\\mathbf{W}_1 + \\mathbf{b}_1)\\mathbf{W}_2 + \\mathbf{b}_2$$\n其中 $\\mathbf{W}_1 \\in \\mathbb{R}^{D \\times 4D}$，$\\mathbf{W}_2 \\in \\mathbb{R}^{4D \\times D}$，中间维度通常扩展为输入的 4 倍。\n第三章：ViT 的变体与架构细节 3.1 不同规模的 ViT 模型 ViT 论文提出了多个不同规模的模型，从 Base 到 Huge：\n模型 层数 $L$ 隐藏维度 $D$ MLP 维度 注意力头数 参数量 ViT-Base 12 768 3072 12 86M ViT-Large 24 1024 4096 16 307M ViT-Huge 32 1280 5120 16 632M 此外，还有针对较小输入设计的变体：\nViT-Tiny/16：$L=12, D=192$，参数量约 5.7M ViT-Small/16：$L=12, D=384$，参数量约 22M 3.2 混合架构：CNN + Transformer 除了纯粹的 ViT，论文还探索了混合架构：使用 CNN 提取特征图，然后将特征图块输入 Transformer。\n具体做法是：使用 ResNet 的中间特征图（如 ResNet-50 的最后一个阶段输出 $14 \\times 14$）代替原始图像块。特征图的每个\"像素\"对应原始图像的一个区域，可以直接作为序列输入 Transformer。\n混合架构的优势：\n利用 CNN 的局部特征提取能力 保持 Transformer 的全局建模优势 在中小数据集上表现更好 3.3 高分辨率微调策略 ViT 在预训练时通常使用较低分辨率（如 $224 \\times 224$），但在微调时可以使用更高分辨率（如 $384 \\times 384$ 或 $512 \\times 512$）。\n当分辨率改变时，图像块数量 $N$ 发生变化，但位置编码需要保持一致。ViT 采用双线性插值（Bilinear Interpolation）调整预训练的位置编码：\n$$\\mathbf{E}{pos}^{new} = \\text{Interpolate}(\\mathbf{E}{pos}^{pretrain}, (N_{new}, D))$$\n这使得模型可以迁移到不同分辨率的任务上，无需从头训练。\n第四章：训练策略与规模化 4.1 大规模预训练的重要性 ViT 的一个关键发现是：Transformer 在视觉任务中需要比 CNN 更多的数据才能发挥优势。\n上图展示了不同规模数据集上的性能对比：\nImageNet-1k（130 万张图像）：ResNet 表现优于 ViT ImageNet-21k（1400 万张图像）：ViT 与 ResNet 性能相当 JFT-300M（3 亿张图像）：ViT 显著超越 ResNet 这一现象的原因在于归纳偏置的差异：\nCNN 具有局部性和平移等变性等强归纳偏置，在数据量较小时可以利用这些先验知识 Transformer 的归纳偏置较弱，更依赖数据来学习空间关系，但在大规模数据上可以学到更通用的表示 4.2 训练超参数与正则化 ViT 使用以下训练策略：\n优化器：AdamW（Adam 的权重衰减修正版本）\n学习率调度：\n热身阶段（Warmup）：前 10k 步线性增加学习率 余弦退火（Cosine Decay）：之后按余弦曲线衰减 数据增强：\nRandAugment：随机组合多种图像变换 Mixup：将两张图像按比例混合 Cutmix：将一张图像的裁剪区域粘贴到另一张 Dropout：注意力 dropout 和 MLP dropout 随机深度（Stochastic Depth）：以一定概率随机丢弃整个 Transformer 块，作为正则化手段。\n4.3 知识蒸馏：DeiT 的改进 由于 ViT 需要大量数据才能发挥优势，Facebook Research 提出了 Data-efficient Image Transformer（DeiT），通过知识蒸馏（Knowledge Distillation）减少数据依赖。\nDeiT 在 ViT 的基础上添加了一个蒸馏 token（Distillation Token），与类别 token 并行：\n$$\\mathbf{z}0 = [\\mathbf{x}{class}; \\mathbf{x}_{distill}; \\mathbf{x}_p^1\\mathbf{E}; \\cdots; \\mathbf{x}p^N\\mathbf{E}] + \\mathbf{E}{pos}$$\n教师网络（通常是预训练的 CNN）的软标签用于训练蒸馏 token，使学生网络学习教师的知识。\n第五章：注意力可视化与可解释性 5.1 自注意力的可视化 ViT 的一个优势是其可解释性。通过可视化注意力权重，可以观察模型关注图像的哪些区域。\n上图展示了 ViT 最后一层的注意力图。可以看到，尽管没有显式的卷积结构，Transformer 依然能够关注到与分类任务相关的语义区域（如狗的面部特征）。\n注意力 rollout 是一种聚合多层注意力的技术，可以追踪信息如何在网络中流动：\n$$\\mathbf{A}^{rollout} = \\prod_{\\ell=1}^{L} \\mathbf{A}^{\\ell}$$\n其中 $\\mathbf{A}^{\\ell}$ 是第 $\\ell$ 层的平均注意力矩阵。\n5.2 位置编码学到的内容 可视化位置编码的相似性矩阵，可以观察到模型学到了 2D 的空间关系：\n$$\\text{Similarity}(i, j) = \\mathbf{e}{pos}^i \\cdot \\mathbf{e}{pos}^j$$\n靠近的图像块具有较高的相似度，远离的图像块相似度较低，说明模型自发学到了位置概念。\n5.3 不同层的注意力模式 浅层和深层的注意力模式有所不同：\n浅层：注意力较为分散，关注局部纹理和边缘 中层：开始关注物体部分和语义区域 深层：高度集中在判别性特征上，如物体关键部位 第六章：ViT 的拓展与应用 6.1 目标检测与分割 ViT 的成功催生了基于 Transformer 的视觉模型在检测和分割任务中的应用。\nDETR（Detection Transformer）：将目标检测视为集合预测问题，使用 Transformer Encoder-Decoder 架构直接输出边界框集合，无需锚框（Anchor）和非极大值抑制（NMS）。\nSegmenter：将 ViT 拓展到语义分割，使用 Transformer Decoder 或线性投影从 patch 特征恢复像素级预测。\nMask2Former：统一了语义分割、实例分割和全景分割的 Transformer 架构。\n6.2 高效 Transformer 变体 标准 ViT 的 $O(N^2)$ 自注意力复杂度在高分率图像上开销较大，研究者提出了多种高效变体：\nSwin Transformer：使用窗口注意力（Window Attention）和移位窗口（Shifted Window）机制，将复杂度降至线性，同时保持跨窗口连接。\nPVT（Pyramid Vision Transformer）：引入金字塔结构，逐步下采样特征图，适应密集预测任务。\nDeformable Attention：借鉴可变形卷积思想，只关注参考点周围的关键采样点，减少计算量。\n6.3 自监督学习与掩码建模 ViT 也推动了视觉领域的自监督学习发展：\nMAE（Masked Autoencoder）：随机掩码 75% 的图像块，让模型根据可见块重建被掩码的部分。这种简单的掩码自编码器预训练在下游任务上取得了优异性能。\nBEiT：借鉴 BERT 的掩码语言建模，使用离散变分自编码器（dVAE）将图像块转换为视觉 token，然后进行掩码预测。\nDINO：通过自蒸馏（Self-Distillation）学习视觉特征，无需标签即可学到可迁移的表示。\n第七章：理论分析与深度理解 7.1 归纳偏置的权衡 ViT 引发了对深度学习模型归纳偏置的重新思考：\nCNN 的强归纳偏置：\n局部性：特征只与邻近区域有关 平移等变性：特征检测器在空间上共享 优点：样本效率高，小数据集表现好 缺点：可能限制模型的表达能力 Transformer 的弱归纳偏置：\n全局注意力：任意位置可直接交互 内容自适应：注意力权重取决于输入内容 优点：表达能力强，大数据集潜力大 缺点：需要更多数据学习空间先验 现代架构（如 ConvNeXt、Swin）尝试融合两者的优点，在保持效率的同时提升表达能力。\n7.2 从核方法看注意力 注意力机制可以从核方法的角度理解。自注意力实际上定义了一个数据相关的核函数：\n$$\\kappa(\\mathbf{q}, \\mathbf{k}) = \\exp\\left(\\frac{\\mathbf{q}^T\\mathbf{k}}{\\sqrt{d_k}}\\right)$$\n输出是值向量的核加权平均：\n$$\\mathbf{o} = \\frac{\\sum_i \\kappa(\\mathbf{q}, \\mathbf{k}_i)\\mathbf{v}_i}{\\sum_j \\kappa(\\mathbf{q}, \\mathbf{k}_j)}$$\n这与核 PCA、高斯过程等方法有深刻联系，说明 Transformer 可以学习复杂的非线性映射。\n7.3 表达能力与优化景观 研究表明，Transformer 的表达能力随着深度和宽度指数增长。与 CNN 相比，Transformer 更容易优化深层网络，因为残差连接和层归一化提供了更好的梯度流。\n此外，自注意力的排列等变性（Permutation Equivariance）使得 Transformer 对输入顺序敏感但结构灵活，适合处理不规则数据结构。\n结语 Vision Transformer 的提出标志着计算机视觉领域的一个重要转折点。它证明了 Transformer 架构不仅适用于自然语言处理，在视觉任务上同样可以达到甚至超越 CNN 的性能。\n回顾 ViT 的核心贡献：\n简单的图像分块策略：将图像划分为 16$\\times$16 的块，转换为序列输入 Transformer 大规模预训练的重要性：揭示了 Transformer 需要更多数据才能发挥优势 纯粹注意力架构的可行性：证明无需卷积，仅靠注意力即可实现强大的视觉理解 ViT 的影响远远超出了图像分类任务。它催生了 DETR、Segmenter、Swin Transformer 等一系列后续工作，推动了目标检测、语义分割、自监督学习等领域的进步。\n更重要的是，ViT 统一了 NLP 和 CV 的架构范式。现在，无论是处理文本、图像还是多模态数据，Transformer 都成为了首选架构。这种统一不仅简化了研究和开发，也为多模态学习（如 CLIP、DALL-E）铺平了道路。\n正如论文标题所言——“一张图片相当于 16$\\times$16 个单词”——ViT 用最简单的方式回答了视觉与语言的统一表示问题，开启了一个全新的时代。\n参考文献 Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., … \u0026 Houlsby, N. (2020). “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” International Conference on Learning Representations (ICLR).\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … \u0026 Polosukhin, I. (2017). “Attention Is All You Need.” Advances in Neural Information Processing Systems (NeurIPS), 30.\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., \u0026 Jegou, H. (2021). “Training Data-efficient Image Transformers \u0026 Distillation through Attention.” International Conference on Machine Learning (ICML), 10347-10357.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., \u0026 Zagoruyko, S. (2020). “End-to-End Object Detection with Transformers.” European Conference on Computer Vision (ECCV), 213-229.\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., … \u0026 Guo, B. (2021). “Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows.” Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 10012-10022.\nHe, K., Chen, X., Xie, S., Li, Y., Dollár, P., \u0026 Girshick, R. (2022). “Masked Autoencoders Are Scalable Vision Learners.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 16000-16009.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … \u0026 Sutskever, I. (2021). “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning (ICML), 8748-8763.\n","wordCount":"986","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/vit-cover.jpg","datePublished":"2026-01-30T08:46:42+08:00","dateModified":"2026-01-30T08:46:42+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97vision-transformer-%E8%A7%86%E8%A7%89transformer/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">AI 论文解读系列：Vision Transformer 视觉Transformer</h1><div class=post-description>深入解读 Google Research 的 Vision Transformer 论文，从注意力机制的原理出发，剖析图像块嵌入、位置编码、Transformer Encoder 的完整架构，揭示 Transformer 如何在计算机视觉领域挑战 CNN 的统治地位。</div><div class=post-meta><span title='2026-01-30 08:46:42 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>986 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/vit-cover.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/vit-cover.jpg alt="AI 论文解读系列 Vision Transformer cover image"></a><figcaption>AI 论文解读系列 Vision Transformer - Cover Image</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#ai-%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97vision-transformer-%e8%a7%86%e8%a7%89-transformer aria-label="AI 论文解读系列：Vision Transformer 视觉 Transformer">AI 论文解读系列：Vision Transformer 视觉 Transformer</a><ul><li><a href=#%e5%bc%95%e8%a8%80 aria-label=引言>引言</a></li><li><a href=#%e7%ac%ac%e4%b8%80%e7%ab%a0%e4%bb%8e-cnn-%e5%88%b0-transformer-%e7%9a%84%e8%8c%83%e5%bc%8f%e8%bd%ac%e7%a7%bb aria-label="第一章：从 CNN 到 Transformer 的范式转移">第一章：从 CNN 到 Transformer 的范式转移</a><ul><li><a href=#11-%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e7%bb%9f%e6%b2%bb%e6%97%b6%e4%bb%a3 aria-label="1.1 卷积神经网络的统治时代">1.1 卷积神经网络的统治时代</a></li><li><a href=#12-transformer-%e5%9c%a8%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%e5%a4%84%e7%90%86%e4%b8%ad%e7%9a%84%e6%88%90%e5%8a%9f aria-label="1.2 Transformer 在自然语言处理中的成功">1.2 Transformer 在自然语言处理中的成功</a></li><li><a href=#13-%e5%b0%86-transformer-%e5%ba%94%e7%94%a8%e4%ba%8e%e5%9b%be%e5%83%8f%e7%9a%84%e6%8c%91%e6%88%98 aria-label="1.3 将 Transformer 应用于图像的挑战">1.3 将 Transformer 应用于图像的挑战</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e7%ab%a0vision-transformer-%e7%9a%84%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3 aria-label="第二章：Vision Transformer 的核心思想">第二章：Vision Transformer 的核心思想</a><ul><li><a href=#21-%e5%9b%be%e5%83%8f%e5%9d%97%e5%b5%8c%e5%85%a5%e4%bb%8e%e5%83%8f%e7%b4%a0%e5%88%b0%e5%ba%8f%e5%88%97 aria-label="2.1 图像块嵌入：从像素到序列">2.1 图像块嵌入：从像素到序列</a></li><li><a href=#22-%e7%b1%bb%e5%88%ab-token-%e4%b8%8e%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81 aria-label="2.2 类别 Token 与位置编码">2.2 类别 Token 与位置编码</a></li><li><a href=#23-transformer-encoder-%e6%9e%b6%e6%9e%84 aria-label="2.3 Transformer Encoder 架构">2.3 Transformer Encoder 架构</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e7%ab%a0vit-%e7%9a%84%e5%8f%98%e4%bd%93%e4%b8%8e%e6%9e%b6%e6%9e%84%e7%bb%86%e8%8a%82 aria-label="第三章：ViT 的变体与架构细节">第三章：ViT 的变体与架构细节</a><ul><li><a href=#31-%e4%b8%8d%e5%90%8c%e8%a7%84%e6%a8%a1%e7%9a%84-vit-%e6%a8%a1%e5%9e%8b aria-label="3.1 不同规模的 ViT 模型">3.1 不同规模的 ViT 模型</a></li><li><a href=#32-%e6%b7%b7%e5%90%88%e6%9e%b6%e6%9e%84cnn--transformer aria-label="3.2 混合架构：CNN + Transformer">3.2 混合架构：CNN + Transformer</a></li><li><a href=#33-%e9%ab%98%e5%88%86%e8%be%a8%e7%8e%87%e5%be%ae%e8%b0%83%e7%ad%96%e7%95%a5 aria-label="3.3 高分辨率微调策略">3.3 高分辨率微调策略</a></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e7%ab%a0%e8%ae%ad%e7%bb%83%e7%ad%96%e7%95%a5%e4%b8%8e%e8%a7%84%e6%a8%a1%e5%8c%96 aria-label=第四章：训练策略与规模化>第四章：训练策略与规模化</a><ul><li><a href=#41-%e5%a4%a7%e8%a7%84%e6%a8%a1%e9%a2%84%e8%ae%ad%e7%bb%83%e7%9a%84%e9%87%8d%e8%a6%81%e6%80%a7 aria-label="4.1 大规模预训练的重要性">4.1 大规模预训练的重要性</a></li><li><a href=#42-%e8%ae%ad%e7%bb%83%e8%b6%85%e5%8f%82%e6%95%b0%e4%b8%8e%e6%ad%a3%e5%88%99%e5%8c%96 aria-label="4.2 训练超参数与正则化">4.2 训练超参数与正则化</a></li><li><a href=#43-%e7%9f%a5%e8%af%86%e8%92%b8%e9%a6%8fdeit-%e7%9a%84%e6%94%b9%e8%bf%9b aria-label="4.3 知识蒸馏：DeiT 的改进">4.3 知识蒸馏：DeiT 的改进</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e7%ab%a0%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%8f%af%e8%a7%86%e5%8c%96%e4%b8%8e%e5%8f%af%e8%a7%a3%e9%87%8a%e6%80%a7 aria-label=第五章：注意力可视化与可解释性>第五章：注意力可视化与可解释性</a><ul><li><a href=#51-%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%9a%84%e5%8f%af%e8%a7%86%e5%8c%96 aria-label="5.1 自注意力的可视化">5.1 自注意力的可视化</a></li><li><a href=#52-%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81%e5%ad%a6%e5%88%b0%e7%9a%84%e5%86%85%e5%ae%b9 aria-label="5.2 位置编码学到的内容">5.2 位置编码学到的内容</a></li><li><a href=#53-%e4%b8%8d%e5%90%8c%e5%b1%82%e7%9a%84%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%a8%a1%e5%bc%8f aria-label="5.3 不同层的注意力模式">5.3 不同层的注意力模式</a></li></ul></li><li><a href=#%e7%ac%ac%e5%85%ad%e7%ab%a0vit-%e7%9a%84%e6%8b%93%e5%b1%95%e4%b8%8e%e5%ba%94%e7%94%a8 aria-label="第六章：ViT 的拓展与应用">第六章：ViT 的拓展与应用</a><ul><li><a href=#61-%e7%9b%ae%e6%a0%87%e6%a3%80%e6%b5%8b%e4%b8%8e%e5%88%86%e5%89%b2 aria-label="6.1 目标检测与分割">6.1 目标检测与分割</a></li><li><a href=#62-%e9%ab%98%e6%95%88-transformer-%e5%8f%98%e4%bd%93 aria-label="6.2 高效 Transformer 变体">6.2 高效 Transformer 变体</a></li><li><a href=#63-%e8%87%aa%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0%e4%b8%8e%e6%8e%a9%e7%a0%81%e5%bb%ba%e6%a8%a1 aria-label="6.3 自监督学习与掩码建模">6.3 自监督学习与掩码建模</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%83%e7%ab%a0%e7%90%86%e8%ae%ba%e5%88%86%e6%9e%90%e4%b8%8e%e6%b7%b1%e5%ba%a6%e7%90%86%e8%a7%a3 aria-label=第七章：理论分析与深度理解>第七章：理论分析与深度理解</a><ul><li><a href=#71-%e5%bd%92%e7%ba%b3%e5%81%8f%e7%bd%ae%e7%9a%84%e6%9d%83%e8%a1%a1 aria-label="7.1 归纳偏置的权衡">7.1 归纳偏置的权衡</a></li><li><a href=#72-%e4%bb%8e%e6%a0%b8%e6%96%b9%e6%b3%95%e7%9c%8b%e6%b3%a8%e6%84%8f%e5%8a%9b aria-label="7.2 从核方法看注意力">7.2 从核方法看注意力</a></li><li><a href=#73-%e8%a1%a8%e8%be%be%e8%83%bd%e5%8a%9b%e4%b8%8e%e4%bc%98%e5%8c%96%e6%99%af%e8%a7%82 aria-label="7.3 表达能力与优化景观">7.3 表达能力与优化景观</a></li></ul></li><li><a href=#%e7%bb%93%e8%af%ad aria-label=结语>结语</a></li><li><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae aria-label=参考文献>参考文献</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=ai-论文解读系列vision-transformer-视觉-transformer>AI 论文解读系列：Vision Transformer 视觉 Transformer<a hidden class=anchor aria-hidden=true href=#ai-论文解读系列vision-transformer-视觉-transformer>#</a></h1><h2 id=引言>引言<a hidden class=anchor aria-hidden=true href=#引言>#</a></h2><p>2020 年，Google Research 发表了一篇极具颠覆性的论文《An Image is Worth 16$\times$16 Words: Transformers for Image Recognition at Scale》。这篇论文提出了 Vision Transformer（ViT），一个纯粹基于 Transformer 架构的视觉模型，在 ImageNet 分类任务上取得了与最先进的卷积神经网络（CNN）相媲美甚至超越的成绩。</p><p>这个成果的震撼之处在于：在计算机视觉领域统治了整整十年的卷积神经网络，终于遇到了真正的挑战者。CNN 凭借其归纳偏置（局部性、平移等变性）在视觉任务中表现出色，而 Transformer 原本是为自然语言处理设计的序列模型。ViT 的成功证明，只要有足够的数据和计算资源，纯粹的注意力机制同样可以在视觉任务中大放异彩。</p><p>本文将从注意力机制的基础出发，循序渐进地剖析 ViT 的架构设计、数学原理和训练策略，揭示为何"一张图片相当于 16$\times$16 个单词"这一简单想法能够改变计算机视觉的格局。</p><h2 id=第一章从-cnn-到-transformer-的范式转移>第一章：从 CNN 到 Transformer 的范式转移<a hidden class=anchor aria-hidden=true href=#第一章从-cnn-到-transformer-的范式转移>#</a></h2><h3 id=11-卷积神经网络的统治时代>1.1 卷积神经网络的统治时代<a hidden class=anchor aria-hidden=true href=#11-卷积神经网络的统治时代>#</a></h3><p>自 2012 年 AlexNet 在 ImageNet 竞赛中取得突破性成果以来，卷积神经网络（CNN）一直是计算机视觉领域的主流架构。CNN 的成功建立在几个关键设计之上：</p><p><strong>局部感受野</strong>（Local Receptive Fields）：每个神经元只与输入的局部区域连接，捕捉局部特征如边缘、纹理。</p><p><strong>权重共享</strong>（Weight Sharing）：同一个卷积核在整个输入上滑动，检测相同特征的不同位置。</p><p><strong>平移等变性</strong>（Translation Equivariance）：输入图像平移，特征图也相应平移，保持空间关系。</p><p>这些归纳偏置（Inductive Bias）使 CNN 非常适合处理图像数据，但也带来了一些限制：</p><ul><li>感受野有限，需要堆叠多层才能获取全局信息</li><li>对长距离依赖的建模能力较弱</li><li>难以直接捕捉空间上相距较远的像素之间的关系</li></ul><h3 id=12-transformer-在自然语言处理中的成功>1.2 Transformer 在自然语言处理中的成功<a hidden class=anchor aria-hidden=true href=#12-transformer-在自然语言处理中的成功>#</a></h3><p>2017 年，Google 在论文《Attention Is All You Need》中提出了 Transformer 架构，彻底改变了自然语言处理（NLP）领域。Transformer 完全基于<strong>自注意力机制</strong>（Self-Attention），摒弃了循环和卷积结构。</p><p>Transformer 的核心优势：</p><p><strong>全局上下文建模</strong>：每个位置都可以直接关注序列中的任意其他位置，不受距离限制。</p><p><strong>并行计算</strong>：不像 RNN 需要顺序处理，Transformer 可以并行处理整个序列。</p><p><strong>可扩展性</strong>：随着数据量和计算资源的增加，Transformer 的性能持续提升。</p><p>在 NLP 领域，从 BERT 到 GPT 系列，Transformer 架构不断刷新各项任务的基准。一个自然的问题浮现：能否将这一成功迁移到计算机视觉领域？</p><h3 id=13-将-transformer-应用于图像的挑战>1.3 将 Transformer 应用于图像的挑战<a hidden class=anchor aria-hidden=true href=#13-将-transformer-应用于图像的挑战>#</a></h3><p>直接将 NLP 中的 Transformer 应用于图像面临几个挑战：</p><p><strong>尺度问题</strong>：在 NLP 中，输入是离散的单词或子词单元，序列长度通常为几百到几千。而图像是连续的像素网格，即使是 $224 \times 224$ 的小图像也有 50,176 个像素。</p><p>如果直接将每个像素作为一个 token，自注意力的计算复杂度是 $O(n^2)$，其中 $n$ 是序列长度。对于 $224 \times 224$ 的图像：</p><p>$$n = 224 \times 224 = 50,176$$</p><p>自注意力矩阵的大小将是 $50,176 \times 50,176 \approx 25$ 亿个元素，内存和计算开销都是不可接受的。</p><p><strong>归纳偏置的缺失</strong>：CNN 的局部性和平移等变性是处理图像的强大先验。纯粹的 Transformer 缺乏这些归纳偏置，需要从数据中从头学习空间关系。</p><p>ViT 的解决方案既优雅又简单：<strong>将图像分割成固定大小的块（patches），将每个块视为一个"视觉单词"</strong>。</p><h2 id=第二章vision-transformer-的核心思想>第二章：Vision Transformer 的核心思想<a hidden class=anchor aria-hidden=true href=#第二章vision-transformer-的核心思想>#</a></h2><h3 id=21-图像块嵌入从像素到序列>2.1 图像块嵌入：从像素到序列<a hidden class=anchor aria-hidden=true href=#21-图像块嵌入从像素到序列>#</a></h3><p>ViT 的第一步是将二维图像转换为序列形式。具体做法如下：</p><p>给定一张图像 $\mathbf{x} \in \mathbb{R}^{H \times W \times C}$，其中 $H$ 和 $W$ 是高和宽，$C$ 是通道数（RGB 图像中 $C=3$）。</p><p>将图像划分为 $N$ 个固定大小的块（patches），每个块的大小为 $P \times P$：</p><p>$$N = \frac{H \times W}{P^2}$$</p><p>对于标准的 ViT 配置，输入图像为 $224 \times 224$，块大小 $P = 16$，则：</p><p>$$N = \frac{224 \times 224}{16 \times 16} = \frac{50176}{256} = 196$$</p><p>这就是论文标题"An Image is Worth 16$\times$16 Words"的由来——一张图像被转换为 196 个"视觉单词"的序列。</p><p><img alt=图像分块 loading=lazy src=/images/plots/vit-patch-embedding.png></p><p>每个图像块被展平并通过一个可训练的线性投影层映射到维度 $D$：</p><p>$$\mathbf{z}<em>0 = [\mathbf{x}</em>{class}; \mathbf{x}_p^1\mathbf{E}; \mathbf{x}_p^2\mathbf{E}; \cdots; \mathbf{x}<em>p^N\mathbf{E}] + \mathbf{E}</em>{pos}$$</p><p>其中：</p><ul><li>$\mathbf{x}_p^i \in \mathbb{R}^{P^2 \cdot C}$ 是第 $i$ 个展平的图像块</li><li>$\mathbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D}$ 是块嵌入矩阵（Patch Embedding Matrix）</li><li>$\mathbf{E}_{pos} \in \mathbb{R}^{(N+1) \times D}$ 是位置嵌入（Position Embedding）</li><li>$\mathbf{x}_{class}$ 是可学习的类别 token</li></ul><h3 id=22-类别-token-与位置编码>2.2 类别 Token 与位置编码<a hidden class=anchor aria-hidden=true href=#22-类别-token-与位置编码>#</a></h3><p><strong>类别 Token</strong>（Class Token）：</p><p>ViT 借鉴了 BERT 的做法，在序列开头添加一个特殊的可学习嵌入 $\mathbf{x}_{class}$。这个 token 的输出状态将被用作图像的聚合表示，输入到分类头进行预测：</p><p>$$y = \text{LN}(\mathbf{z}_L^0)$$</p><p>其中 $\mathbf{z}_L^0$ 是 Transformer 最后一层输出的第一个位置（类别 token）的状态。</p><p><strong>位置编码</strong>（Position Embedding）：</p><p>由于 Transformer 本身不具有序列顺序的概念，需要添加位置信息。ViT 使用标准的可学习 1D 位置编码：</p><p>$$\mathbf{E}<em>{pos} = [\mathbf{e}</em>{pos}^0; \mathbf{e}<em>{pos}^1; \cdots; \mathbf{e}</em>{pos}^N]$$</p><p>实验表明，使用 2D 感知的位置编码或相对位置编码并没有显著提升性能，说明 Transformer 可以从数据中学习空间关系。</p><h3 id=23-transformer-encoder-架构>2.3 Transformer Encoder 架构<a hidden class=anchor aria-hidden=true href=#23-transformer-encoder-架构>#</a></h3><p>ViT 使用标准的 Transformer Encoder，由交替的多头自注意力（MSA）和多层感知机（MLP）块组成，每个块之前应用 Layer Normalization（LN）：</p><div class=math>$$\begin{aligned}
\mathbf{z}'_\ell &= \text{MSA}(\text{LN}(\mathbf{z}_{\ell-1})) + \mathbf{z}_{\ell-1} \\
\mathbf{z}_\ell &= \text{MLP}(\text{LN}(\mathbf{z}'_\ell)) + \mathbf{z}'_\ell
\end{aligned}$$</div><p>其中 $\ell = 1, \ldots, L$，$L$ 是 Transformer 层的数量。</p><p><img alt="ViT 架构" loading=lazy src=/images/plots/vit-architecture.png></p><p><strong>多头自注意力</strong>（Multi-Head Self-Attention, MSA）：</p><p>对于输入 $\mathbf{Z} \in \mathbb{R}^{N \times D}$，首先通过三个线性投影得到查询（Query）、键（Key）和值（Value）：</p><div class=math>$$\begin{aligned}
\mathbf{Q} &= \mathbf{Z}\mathbf{W}^Q \\
\mathbf{K} &= \mathbf{Z}\mathbf{W}^K \\
\mathbf{V} &= \mathbf{Z}\mathbf{W}^V
\end{aligned}$$</div><p>其中 $\mathbf{W}^Q, \mathbf{W}^K, \mathbf{W}^V \in \mathbb{R}^{D \times d_k}$，通常 $d_k = D/h$，$h$ 是注意力头的数量。</p><p>缩放点积注意力定义为：</p><p>$$\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}$$</p><p>除以 $\sqrt{d_k}$ 是为了防止点积过大导致 softmax 梯度消失。</p><p>多头注意力将输入投影到多个子空间，并行计算注意力：</p><div class=math>$$\begin{aligned}
\text{MSA}(\mathbf{Z}) &= [\text{head}_1; \cdots; \text{head}_h]\mathbf{W}^O \\
\text{head}_i &= \text{Attention}(\mathbf{Z}\mathbf{W}_i^Q, \mathbf{Z}\mathbf{W}_i^K, \mathbf{Z}\mathbf{W}_i^V)
\end{aligned}$$</div><p><strong>多层感知机</strong>（MLP）：</p><p>每个 Transformer 块包含一个两层的 MLP，使用 GELU 激活函数：</p><p>$$\text{MLP}(\mathbf{z}) = \text{GELU}(\mathbf{z}\mathbf{W}_1 + \mathbf{b}_1)\mathbf{W}_2 + \mathbf{b}_2$$</p><p>其中 $\mathbf{W}_1 \in \mathbb{R}^{D \times 4D}$，$\mathbf{W}_2 \in \mathbb{R}^{4D \times D}$，中间维度通常扩展为输入的 4 倍。</p><h2 id=第三章vit-的变体与架构细节>第三章：ViT 的变体与架构细节<a hidden class=anchor aria-hidden=true href=#第三章vit-的变体与架构细节>#</a></h2><h3 id=31-不同规模的-vit-模型>3.1 不同规模的 ViT 模型<a hidden class=anchor aria-hidden=true href=#31-不同规模的-vit-模型>#</a></h3><p>ViT 论文提出了多个不同规模的模型，从 Base 到 Huge：</p><table><thead><tr><th>模型</th><th>层数 $L$</th><th>隐藏维度 $D$</th><th>MLP 维度</th><th>注意力头数</th><th>参数量</th></tr></thead><tbody><tr><td>ViT-Base</td><td>12</td><td>768</td><td>3072</td><td>12</td><td>86M</td></tr><tr><td>ViT-Large</td><td>24</td><td>1024</td><td>4096</td><td>16</td><td>307M</td></tr><tr><td>ViT-Huge</td><td>32</td><td>1280</td><td>5120</td><td>16</td><td>632M</td></tr></tbody></table><p>此外，还有针对较小输入设计的变体：</p><ul><li>ViT-Tiny/16：$L=12, D=192$，参数量约 5.7M</li><li>ViT-Small/16：$L=12, D=384$，参数量约 22M</li></ul><h3 id=32-混合架构cnn--transformer>3.2 混合架构：CNN + Transformer<a hidden class=anchor aria-hidden=true href=#32-混合架构cnn--transformer>#</a></h3><p>除了纯粹的 ViT，论文还探索了混合架构：使用 CNN 提取特征图，然后将特征图块输入 Transformer。</p><p>具体做法是：使用 ResNet 的中间特征图（如 ResNet-50 的最后一个阶段输出 $14 \times 14$）代替原始图像块。特征图的每个"像素"对应原始图像的一个区域，可以直接作为序列输入 Transformer。</p><p>混合架构的优势：</p><ul><li>利用 CNN 的局部特征提取能力</li><li>保持 Transformer 的全局建模优势</li><li>在中小数据集上表现更好</li></ul><h3 id=33-高分辨率微调策略>3.3 高分辨率微调策略<a hidden class=anchor aria-hidden=true href=#33-高分辨率微调策略>#</a></h3><p>ViT 在预训练时通常使用较低分辨率（如 $224 \times 224$），但在微调时可以使用更高分辨率（如 $384 \times 384$ 或 $512 \times 512$）。</p><p>当分辨率改变时，图像块数量 $N$ 发生变化，但位置编码需要保持一致。ViT 采用<strong>双线性插值</strong>（Bilinear Interpolation）调整预训练的位置编码：</p><p>$$\mathbf{E}<em>{pos}^{new} = \text{Interpolate}(\mathbf{E}</em>{pos}^{pretrain}, (N_{new}, D))$$</p><p>这使得模型可以迁移到不同分辨率的任务上，无需从头训练。</p><h2 id=第四章训练策略与规模化>第四章：训练策略与规模化<a hidden class=anchor aria-hidden=true href=#第四章训练策略与规模化>#</a></h2><h3 id=41-大规模预训练的重要性>4.1 大规模预训练的重要性<a hidden class=anchor aria-hidden=true href=#41-大规模预训练的重要性>#</a></h3><p>ViT 的一个关键发现是：<strong>Transformer 在视觉任务中需要比 CNN 更多的数据才能发挥优势</strong>。</p><p><img alt=数据规模影响 loading=lazy src=/images/plots/vit-data-scaling.png></p><p>上图展示了不同规模数据集上的性能对比：</p><ul><li><strong>ImageNet-1k</strong>（130 万张图像）：ResNet 表现优于 ViT</li><li><strong>ImageNet-21k</strong>（1400 万张图像）：ViT 与 ResNet 性能相当</li><li><strong>JFT-300M</strong>（3 亿张图像）：ViT 显著超越 ResNet</li></ul><p>这一现象的原因在于归纳偏置的差异：</p><ul><li>CNN 具有局部性和平移等变性等强归纳偏置，在数据量较小时可以利用这些先验知识</li><li>Transformer 的归纳偏置较弱，更依赖数据来学习空间关系，但在大规模数据上可以学到更通用的表示</li></ul><h3 id=42-训练超参数与正则化>4.2 训练超参数与正则化<a hidden class=anchor aria-hidden=true href=#42-训练超参数与正则化>#</a></h3><p>ViT 使用以下训练策略：</p><p><strong>优化器</strong>：AdamW（Adam 的权重衰减修正版本）</p><p><strong>学习率调度</strong>：</p><ul><li>热身阶段（Warmup）：前 10k 步线性增加学习率</li><li>余弦退火（Cosine Decay）：之后按余弦曲线衰减</li></ul><p><strong>数据增强</strong>：</p><ul><li>RandAugment：随机组合多种图像变换</li><li>Mixup：将两张图像按比例混合</li><li>Cutmix：将一张图像的裁剪区域粘贴到另一张</li><li>Dropout：注意力 dropout 和 MLP dropout</li></ul><p><strong>随机深度</strong>（Stochastic Depth）：以一定概率随机丢弃整个 Transformer 块，作为正则化手段。</p><h3 id=43-知识蒸馏deit-的改进>4.3 知识蒸馏：DeiT 的改进<a hidden class=anchor aria-hidden=true href=#43-知识蒸馏deit-的改进>#</a></h3><p>由于 ViT 需要大量数据才能发挥优势，Facebook Research 提出了 Data-efficient Image Transformer（DeiT），通过<strong>知识蒸馏</strong>（Knowledge Distillation）减少数据依赖。</p><p>DeiT 在 ViT 的基础上添加了一个蒸馏 token（Distillation Token），与类别 token 并行：</p><p>$$\mathbf{z}<em>0 = [\mathbf{x}</em>{class}; \mathbf{x}_{distill}; \mathbf{x}_p^1\mathbf{E}; \cdots; \mathbf{x}<em>p^N\mathbf{E}] + \mathbf{E}</em>{pos}$$</p><p>教师网络（通常是预训练的 CNN）的软标签用于训练蒸馏 token，使学生网络学习教师的知识。</p><h2 id=第五章注意力可视化与可解释性>第五章：注意力可视化与可解释性<a hidden class=anchor aria-hidden=true href=#第五章注意力可视化与可解释性>#</a></h2><h3 id=51-自注意力的可视化>5.1 自注意力的可视化<a hidden class=anchor aria-hidden=true href=#51-自注意力的可视化>#</a></h3><p>ViT 的一个优势是其可解释性。通过可视化注意力权重，可以观察模型关注图像的哪些区域。</p><p><img alt=注意力可视化 loading=lazy src=/images/plots/vit-attention-visualization.png></p><p>上图展示了 ViT 最后一层的注意力图。可以看到，尽管没有显式的卷积结构，Transformer 依然能够关注到与分类任务相关的语义区域（如狗的面部特征）。</p><p><strong>注意力 rollout</strong> 是一种聚合多层注意力的技术，可以追踪信息如何在网络中流动：</p><p>$$\mathbf{A}^{rollout} = \prod_{\ell=1}^{L} \mathbf{A}^{\ell}$$</p><p>其中 $\mathbf{A}^{\ell}$ 是第 $\ell$ 层的平均注意力矩阵。</p><h3 id=52-位置编码学到的内容>5.2 位置编码学到的内容<a hidden class=anchor aria-hidden=true href=#52-位置编码学到的内容>#</a></h3><p>可视化位置编码的相似性矩阵，可以观察到模型学到了 2D 的空间关系：</p><p>$$\text{Similarity}(i, j) = \mathbf{e}<em>{pos}^i \cdot \mathbf{e}</em>{pos}^j$$</p><p>靠近的图像块具有较高的相似度，远离的图像块相似度较低，说明模型自发学到了位置概念。</p><h3 id=53-不同层的注意力模式>5.3 不同层的注意力模式<a hidden class=anchor aria-hidden=true href=#53-不同层的注意力模式>#</a></h3><p>浅层和深层的注意力模式有所不同：</p><ul><li><strong>浅层</strong>：注意力较为分散，关注局部纹理和边缘</li><li><strong>中层</strong>：开始关注物体部分和语义区域</li><li><strong>深层</strong>：高度集中在判别性特征上，如物体关键部位</li></ul><h2 id=第六章vit-的拓展与应用>第六章：ViT 的拓展与应用<a hidden class=anchor aria-hidden=true href=#第六章vit-的拓展与应用>#</a></h2><h3 id=61-目标检测与分割>6.1 目标检测与分割<a hidden class=anchor aria-hidden=true href=#61-目标检测与分割>#</a></h3><p>ViT 的成功催生了基于 Transformer 的视觉模型在检测和分割任务中的应用。</p><p><strong>DETR</strong>（Detection Transformer）：将目标检测视为集合预测问题，使用 Transformer Encoder-Decoder 架构直接输出边界框集合，无需锚框（Anchor）和非极大值抑制（NMS）。</p><p><strong>Segmenter</strong>：将 ViT 拓展到语义分割，使用 Transformer Decoder 或线性投影从 patch 特征恢复像素级预测。</p><p><strong>Mask2Former</strong>：统一了语义分割、实例分割和全景分割的 Transformer 架构。</p><h3 id=62-高效-transformer-变体>6.2 高效 Transformer 变体<a hidden class=anchor aria-hidden=true href=#62-高效-transformer-变体>#</a></h3><p>标准 ViT 的 $O(N^2)$ 自注意力复杂度在高分率图像上开销较大，研究者提出了多种高效变体：</p><p><strong>Swin Transformer</strong>：使用窗口注意力（Window Attention）和移位窗口（Shifted Window）机制，将复杂度降至线性，同时保持跨窗口连接。</p><p><strong>PVT</strong>（Pyramid Vision Transformer）：引入金字塔结构，逐步下采样特征图，适应密集预测任务。</p><p><strong>Deformable Attention</strong>：借鉴可变形卷积思想，只关注参考点周围的关键采样点，减少计算量。</p><h3 id=63-自监督学习与掩码建模>6.3 自监督学习与掩码建模<a hidden class=anchor aria-hidden=true href=#63-自监督学习与掩码建模>#</a></h3><p>ViT 也推动了视觉领域的自监督学习发展：</p><p><strong>MAE</strong>（Masked Autoencoder）：随机掩码 75% 的图像块，让模型根据可见块重建被掩码的部分。这种简单的掩码自编码器预训练在下游任务上取得了优异性能。</p><p><strong>BEiT</strong>：借鉴 BERT 的掩码语言建模，使用离散变分自编码器（dVAE）将图像块转换为视觉 token，然后进行掩码预测。</p><p><strong>DINO</strong>：通过自蒸馏（Self-Distillation）学习视觉特征，无需标签即可学到可迁移的表示。</p><h2 id=第七章理论分析与深度理解>第七章：理论分析与深度理解<a hidden class=anchor aria-hidden=true href=#第七章理论分析与深度理解>#</a></h2><h3 id=71-归纳偏置的权衡>7.1 归纳偏置的权衡<a hidden class=anchor aria-hidden=true href=#71-归纳偏置的权衡>#</a></h3><p>ViT 引发了对深度学习模型归纳偏置的重新思考：</p><p><strong>CNN 的强归纳偏置</strong>：</p><ul><li>局部性：特征只与邻近区域有关</li><li>平移等变性：特征检测器在空间上共享</li><li>优点：样本效率高，小数据集表现好</li><li>缺点：可能限制模型的表达能力</li></ul><p><strong>Transformer 的弱归纳偏置</strong>：</p><ul><li>全局注意力：任意位置可直接交互</li><li>内容自适应：注意力权重取决于输入内容</li><li>优点：表达能力强，大数据集潜力大</li><li>缺点：需要更多数据学习空间先验</li></ul><p>现代架构（如 ConvNeXt、Swin）尝试融合两者的优点，在保持效率的同时提升表达能力。</p><h3 id=72-从核方法看注意力>7.2 从核方法看注意力<a hidden class=anchor aria-hidden=true href=#72-从核方法看注意力>#</a></h3><p>注意力机制可以从核方法的角度理解。自注意力实际上定义了一个数据相关的核函数：</p><p>$$\kappa(\mathbf{q}, \mathbf{k}) = \exp\left(\frac{\mathbf{q}^T\mathbf{k}}{\sqrt{d_k}}\right)$$</p><p>输出是值向量的核加权平均：</p><p>$$\mathbf{o} = \frac{\sum_i \kappa(\mathbf{q}, \mathbf{k}_i)\mathbf{v}_i}{\sum_j \kappa(\mathbf{q}, \mathbf{k}_j)}$$</p><p>这与核 PCA、高斯过程等方法有深刻联系，说明 Transformer 可以学习复杂的非线性映射。</p><h3 id=73-表达能力与优化景观>7.3 表达能力与优化景观<a hidden class=anchor aria-hidden=true href=#73-表达能力与优化景观>#</a></h3><p>研究表明，Transformer 的表达能力随着深度和宽度指数增长。与 CNN 相比，Transformer 更容易优化深层网络，因为残差连接和层归一化提供了更好的梯度流。</p><p>此外，自注意力的排列等变性（Permutation Equivariance）使得 Transformer 对输入顺序敏感但结构灵活，适合处理不规则数据结构。</p><h2 id=结语>结语<a hidden class=anchor aria-hidden=true href=#结语>#</a></h2><p>Vision Transformer 的提出标志着计算机视觉领域的一个重要转折点。它证明了 Transformer 架构不仅适用于自然语言处理，在视觉任务上同样可以达到甚至超越 CNN 的性能。</p><p>回顾 ViT 的核心贡献：</p><ol><li><strong>简单的图像分块策略</strong>：将图像划分为 16$\times$16 的块，转换为序列输入 Transformer</li><li><strong>大规模预训练的重要性</strong>：揭示了 Transformer 需要更多数据才能发挥优势</li><li><strong>纯粹注意力架构的可行性</strong>：证明无需卷积，仅靠注意力即可实现强大的视觉理解</li></ol><p>ViT 的影响远远超出了图像分类任务。它催生了 DETR、Segmenter、Swin Transformer 等一系列后续工作，推动了目标检测、语义分割、自监督学习等领域的进步。</p><p>更重要的是，ViT 统一了 NLP 和 CV 的架构范式。现在，无论是处理文本、图像还是多模态数据，Transformer 都成为了首选架构。这种统一不仅简化了研究和开发，也为多模态学习（如 CLIP、DALL-E）铺平了道路。</p><p>正如论文标题所言——&ldquo;一张图片相当于 16$\times$16 个单词&rdquo;——ViT 用最简单的方式回答了视觉与语言的统一表示问题，开启了一个全新的时代。</p><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><ol><li><p>Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., &mldr; & Houlsby, N. (2020). &ldquo;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.&rdquo; <em>International Conference on Learning Representations (ICLR)</em>.</p></li><li><p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., &mldr; & Polosukhin, I. (2017). &ldquo;Attention Is All You Need.&rdquo; <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 30.</p></li><li><p>Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., & Jegou, H. (2021). &ldquo;Training Data-efficient Image Transformers & Distillation through Attention.&rdquo; <em>International Conference on Machine Learning (ICML)</em>, 10347-10357.</p></li><li><p>Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S. (2020). &ldquo;End-to-End Object Detection with Transformers.&rdquo; <em>European Conference on Computer Vision (ECCV)</em>, 213-229.</p></li><li><p>Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., &mldr; & Guo, B. (2021). &ldquo;Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows.&rdquo; <em>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 10012-10022.</p></li><li><p>He, K., Chen, X., Xie, S., Li, Y., Dollár, P., & Girshick, R. (2022). &ldquo;Masked Autoencoders Are Scalable Vision Learners.&rdquo; <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 16000-16009.</p></li><li><p>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., &mldr; & Sutskever, I. (2021). &ldquo;Learning Transferable Visual Models From Natural Language Supervision.&rdquo; <em>International Conference on Machine Learning (ICML)</em>, 8748-8763.</p></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li><li><a href=https://s-ai-unix.github.io/tags/transformer/>Transformer</a></li><li><a href=https://s-ai-unix.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/>计算机视觉</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-30-gpt3-few-shot-learners-paper/><span class=title>« Prev</span><br><span>AI 论文解读系列：GPT-3——当语言模型学会举一反三</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97%E4%B9%8Bresnet-%E6%B7%B1%E5%BA%A6%E6%AE%8B%E5%B7%AE%E5%AD%A6%E4%B9%A0/><span class=title>Next »</span><br><span>AI 论文解读系列：ResNet 深度残差学习</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：Vision Transformer 视觉Transformer on x" href="https://x.com/intent/tweet/?text=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aVision%20Transformer%20%e8%a7%86%e8%a7%89Transformer&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-ai-%25E8%25AE%25BA%25E6%2596%2587%25E8%25A7%25A3%25E8%25AF%25BB%25E7%25B3%25BB%25E5%2588%2597vision-transformer-%25E8%25A7%2586%25E8%25A7%2589transformer%2f&amp;hashtags=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%2cTransformer%2c%e8%ae%a1%e7%ae%97%e6%9c%ba%e8%a7%86%e8%a7%89%2c%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：Vision Transformer 视觉Transformer on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-ai-%25E8%25AE%25BA%25E6%2596%2587%25E8%25A7%25A3%25E8%25AF%25BB%25E7%25B3%25BB%25E5%2588%2597vision-transformer-%25E8%25A7%2586%25E8%25A7%2589transformer%2f&amp;title=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aVision%20Transformer%20%e8%a7%86%e8%a7%89Transformer&amp;summary=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aVision%20Transformer%20%e8%a7%86%e8%a7%89Transformer&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-ai-%25E8%25AE%25BA%25E6%2596%2587%25E8%25A7%25A3%25E8%25AF%25BB%25E7%25B3%25BB%25E5%2588%2597vision-transformer-%25E8%25A7%2586%25E8%25A7%2589transformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：Vision Transformer 视觉Transformer on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-ai-%25E8%25AE%25BA%25E6%2596%2587%25E8%25A7%25A3%25E8%25AF%25BB%25E7%25B3%25BB%25E5%2588%2597vision-transformer-%25E8%25A7%2586%25E8%25A7%2589transformer%2f&title=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aVision%20Transformer%20%e8%a7%86%e8%a7%89Transformer"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：Vision Transformer 视觉Transformer on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-ai-%25E8%25AE%25BA%25E6%2596%2587%25E8%25A7%25A3%25E8%25AF%25BB%25E7%25B3%25BB%25E5%2588%2597vision-transformer-%25E8%25A7%2586%25E8%25A7%2589transformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>