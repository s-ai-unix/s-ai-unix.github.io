<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AlexNet：开启深度学习革命的里程碑 | s-ai-unix's Blog</title><meta name=keywords content="神经网络,深度学习,综述,AlexNet"><meta name=description content="深入浅出解析 AlexNet 的架构原理、关键技术创新和历史意义，从 ImageNet 挑战到深度学习革命，完整推导其数学原理"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-29-alexnet-deep-learning-revolution/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-29-alexnet-deep-learning-revolution/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-29-alexnet-deep-learning-revolution/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="AlexNet：开启深度学习革命的里程碑"><meta property="og:description" content="深入浅出解析 AlexNet 的架构原理、关键技术创新和历史意义，从 ImageNet 挑战到深度学习革命，完整推导其数学原理"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-29T06:00:00+08:00"><meta property="article:modified_time" content="2026-01-29T06:00:00+08:00"><meta property="article:tag" content="神经网络"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="综述"><meta property="article:tag" content="AlexNet"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/alexnet-cover.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/alexnet-cover.jpg"><meta name=twitter:title content="AlexNet：开启深度学习革命的里程碑"><meta name=twitter:description content="深入浅出解析 AlexNet 的架构原理、关键技术创新和历史意义，从 ImageNet 挑战到深度学习革命，完整推导其数学原理"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"AlexNet：开启深度学习革命的里程碑","item":"https://s-ai-unix.github.io/posts/2026-01-29-alexnet-deep-learning-revolution/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"AlexNet：开启深度学习革命的里程碑","name":"AlexNet：开启深度学习革命的里程碑","description":"深入浅出解析 AlexNet 的架构原理、关键技术创新和历史意义，从 ImageNet 挑战到深度学习革命，完整推导其数学原理","keywords":["神经网络","深度学习","综述","AlexNet"],"articleBody":"引言：一个时代的分水岭 $2012$ 年 $9$ 月 $30$ 日，多伦多大学的研究团队在 ImageNet 大规模视觉识别挑战赛（ILSVRC）上提交了一个卷积神经网络模型。当时，没有人意识到这将是一个历史性的时刻。\n这个模型叫做 AlexNet，以第一作者 Alex Krizhevsky 的名字命名。它在图像分类任务上将 Top-5 错误率从上一年的 $25.8%$ 骤降至 $16.4%$——降幅接近 $10$ 个百分点，远超第二名近 $10%$。\n这不是一次普通的进步，这是一次范式革命。\n在此之前，深度学习经历了漫长的\"寒冬\"。尽管 $1986$ 年反向传播算法已被提出，$1998$ 年 LeCun 的 LeNet 已经证明了卷积神经网络的潜力，但深层网络的训练一直受困于梯度消失、计算资源匮乏和数据不足等问题。\nAlexNet 的突破不仅在于它赢得了比赛，更在于它证明了：深度神经网络可以在大规模数据集上有效训练，并且性能远超传统方法。\n这一证明，开启了人工智能的新纪元。\n第一章：黎明前的黑暗——深度学习的寒冬 1.1 感知机的兴衰 要理解 AlexNet 的意义，我们需要回溯到神经网络的起源。\n$1958$ 年，Frank Rosenblatt 提出了感知机（Perceptron），这是第一个能够学习的神经网络模型。Rosenblatt 乐观地宣称：“感知机最终将能够学习、做出决策和翻译语言。”\n然而，$1969$ 年，Marvin Minsky 和 Seymour Papert 在《Perceptrons》一书中证明了感知机的局限性：它无法解决非线性可分问题，比如简单的异或（XOR）问题。\n这个打击是致命的。神经网络研究陷入了第一次寒冬。\n1.2 反向传播的曙光与困境 $1986$ 年，Rumelhart、Hinton 和 Williams 重新发现了反向传播算法（Backpropagation），为训练多层神经网络提供了理论基础。\n反向传播的核心思想：\n给定损失函数 $L$，网络参数 $\\mathbf{W}$，反向传播通过链式法则计算梯度：\n$$\\frac{\\partial L}{\\partial w_{ij}^{(l)}} = \\frac{\\partial L}{\\partial z_i^{(l)}} \\cdot \\frac{\\partial z_i^{(l)}}{\\partial w_{ij}^{(l)}} = \\delta_i^{(l)} \\cdot a_j^{(l-1)}$$\n其中 $\\delta_i^{(l)}$ 是第 $l$ 层第 $i$ 个神经元的误差信号。\n然而，尽管有了理论工具，实际应用仍然受限：\n梯度消失问题：使用 Sigmoid 或 Tanh 激活函数时，深层网络的梯度会指数级衰减 计算资源：$1980$ 年代的计算机无法处理大规模数据 数据集太小：没有足够的标注数据来训练深层网络 1.3 LeNet-5：先驱者的尝试 $1998$ 年，Yann LeCun 等人提出了 LeNet-5，这是一个 $5$ 层的卷积神经网络，成功应用于手写数字识别（MNIST 数据集）。\nLeNet-5 的架构已经包含了现代 CNN 的核心要素：\n卷积层（Convolution） 池化层（Pooling） 全连接层（Fully Connected） 但由于上述限制，LeNet-5 之后，深度学习并没有立即起飞。相反，支持向量机（SVM）和随机森林等传统机器学习方法在 $2000$ 年代占据了主导地位。\n第二章：ImageNet——大数据的觉醒 2.1 数据集的重要性 任何机器学习方法的效果都受限于三个因素：\n算法：模型的表达能力 计算：训练和推理的速度 数据：训练样本的数量和质量 在 $2010$ 年之前，计算机视觉领域的主流数据集规模很小：\nMNIST：$60,000$ 张 $28 \\times 28$ 的手写数字 Caltech-101：$9,144$ 张图片，$101$ 个类别 PASCAL VOC：$20$ 个类别，每年约 $10,000$ 张图片 这些数据集对于浅层模型足够，但无法支撑深层网络的训练。\n2.2 ImageNet 的诞生 $2009$ 年，斯坦福大学的李飞飞教授团队发布了 ImageNet 数据集。这是一个具有划时代意义的项目：\n规模：超过 $1,400$ 万张图片 类别：$21,841$ 个类别（WordNet 层次结构） 标注：每张图片都经过人工标注验证 更重要的是，从 $2010$ 年开始，ImageNet 举办了年度挑战赛（ILSVRC），使用 $1,000$ 个类别的子集，每类约 $1,000$ 张训练图片。\n2.3 传统方法的瓶颈 在 AlexNet 出现之前，ILSVRC 的优胜者都使用传统方法：\nSIFT（尺度不变特征变换）提取局部特征 HOG（方向梯度直方图）描述形状 Bag of Visual Words 编码 SVM 或 随机森林 分类 这些方法在 $2010$ 年和 $2011$ 年的 Top-5 错误率分别为 $28.2%$ 和 $25.8%$，改进幅度很小。\n图1：ImageNet 分类错误率演进（2010-2017）。AlexNet 在 2012 年实现了历史性的突破，将错误率从 25.8% 降至 16.4%，开启了深度学习革命。\n第三章：AlexNet 架构详解 3.1 网络架构概览 AlexNet 包含 $8$ 层可学习层：$5$ 个卷积层和 $3$ 个全连接层。输入是 $224 \\times 224 \\times 3$ 的 RGB 图像，输出是 $1,000$ 个类别的 Softmax 概率。\n图2：AlexNet 网络架构。包含 5 个卷积层（蓝色）和 3 个全连接层（橙色），使用 GPU 并行训练。\n详细架构：\n层 类型 核大小 步长 输出尺寸 参数数量 1 Conv + ReLU + LRN $11 \\times 11$ 4 $55 \\times 55 \\times 96$ $35,000$ 2 MaxPool $3 \\times 3$ 2 $27 \\times 27 \\times 96$ $0$ 3 Conv + ReLU + LRN $5 \\times 5$ 1 $27 \\times 27 \\times 256$ $307,000$ 4 MaxPool $3 \\times 3$ 2 $13 \\times 13 \\times 256$ $0$ 5 Conv + ReLU $3 \\times 3$ 1 $13 \\times 13 \\times 384$ $885,000$ 6 Conv + ReLU $3 \\times 3$ 1 $13 \\times 13 \\times 384$ $1,327,000$ 7 Conv + ReLU $3 \\times 3$ 1 $13 \\times 13 \\times 256$ $885,000$ 8 MaxPool $3 \\times 3$ 2 $6 \\times 6 \\times 256$ $0$ 9 FC + ReLU + Dropout - - $4096$ $37,748,000$ 10 FC + ReLU + Dropout - - $4096$ $16,777,000$ 11 FC + Softmax - - $1000$ $4,097,000$ 总参数量：约 $6,000$ 万个参数。\n3.2 卷积层的数学原理 卷积操作是 CNN 的核心。给定输入特征图 $\\mathbf{X}$ 和卷积核 $\\mathbf{K}$，输出特征图 $\\mathbf{Y}$ 的计算为：\n$$Y_{i,j} = \\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} K_{m,n} \\cdot X_{i+m, j+n} + b$$\n其中 $k$ 是核大小，$b$ 是偏置项。\n在矩阵形式下，这可以表示为：\n$$\\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}$$\nAlexNet 使用较大的初始卷积核（$11 \\times 11$，步长 $4$）来快速降低特征图尺寸，后续使用较小的核（$3 \\times 3$ 和 $5 \\times 5$）提取更精细的特征。\n3.3 GPU 并行架构 AlexNet 的创新之一是使用了两块 GTX 580 GPU 进行并行训练。网络被分成两部分：\nGPU 1 处理下层特征（颜色、纹理） GPU 2 处理上层特征（形状、语义） 这种架构设计使得在当时硬件条件下能够训练更大的网络。\n第四章：三大技术创新 4.1 ReLU：打破梯度消失的枷锁 传统的激活函数如 Sigmoid 和 Tanh 存在一个致命问题：梯度消失。\nSigmoid 函数定义为：\n$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n其导数为：\n$$\\sigma’(z) = \\sigma(z)(1 - \\sigma(z))$$\n当 $|z|$ 较大时，$\\sigma(z)$ 接近 $0$ 或 $1$，导数趋近于 $0$。在反向传播中，这导致梯度逐层衰减，深层网络难以训练。\nReLU（Rectified Linear Unit）的定义非常简单：\n$$f(z) = \\max(0, z)$$\n其导数为：\n$$f’(z) = \\begin{cases} 1 \u0026 \\text{if } z \u003e 0 \\ 0 \u0026 \\text{if } z \u003c 0 \\ \\text{undefined} \u0026 \\text{if } z = 0 \\end{cases}$$\n图3：Sigmoid 与 ReLU 激活函数对比。Sigmoid 在两端出现梯度消失（红色区域），而 ReLU 在正区间保持恒定的梯度，有效缓解了梯度消失问题。\nReLU 的优势：\n计算简单：只需比较和取最大值，无需指数运算 缓解梯度消失：正区间梯度恒为 $1$ 稀疏激活：约 $50%$ 的神经元输出为 $0$，提高计算效率 生物学合理性：与生物神经元的激活机制类似 AlexNet 的实验表明，使用 ReLU 可以使训练速度提升约 $6$ 倍！\n4.2 Dropout：防止过拟合的武器 深层神经网络面临严重的过拟合风险。AlexNet 有 $6,000$ 万个参数，而训练数据只有 $120$ 万张图片，参数数量远超数据量。\nDropout 是一种简单而有效的正则化技术。在训练过程中，以概率 $p$（通常 $p = 0.5$）随机\"丢弃\"（设为 $0$）一部分神经元的输出：\n$$\\tilde{\\mathbf{a}}^{(l)} = \\mathbf{m}^{(l)} \\odot \\mathbf{a}^{(l)}$$\n其中 $\\mathbf{m}^{(l)}$ 是服从 Bernoulli$(p)$ 分布的掩码向量。\n图4：Dropout 正则化示意。在训练时随机丢弃部分隐藏层神经元（灰色），测试时使用所有神经元但调整权重。\n为什么 Dropout 有效？\n集成学习视角：每次训练迭代相当于训练一个不同的子网络，最终结果是多个子网络的平均 打破共适应：防止神经元过度依赖特定其他神经元 稀疏表示：鼓励网络学习更鲁棒的特征 测试时的调整：\n训练时丢弃比例为 $p$，测试时需要将权重乘以 $p$ 来补偿：\n$$\\mathbf{W}{\\text{test}} = p \\cdot \\mathbf{W}{\\text{train}}$$\n或使用inverted dropout：训练时将保留的神经元输出除以 $p$，测试时无需调整。\nAlexNet 在前两个全连接层使用 Dropout，丢弃率设为 $0.5$，显著降低了过拟合。\n4.3 数据增强：扩大数据集的艺术 $120$ 万张训练图片对于 $6,000$ 万个参数来说仍然不够。AlexNet 使用了一系列数据增强技术来人工扩大训练集：\n随机裁剪与翻转：\n从 $256 \\times 256$ 图片随机裁剪 $224 \\times 224$ 区域 水平随机翻转 每张图片可以生成 $2048$ 个不同的训练样本 PCA 颜色增强：\n对 RGB 通道进行主成分分析，然后添加随机扰动：\n$$\\begin{pmatrix} I_{xy}^R \\ I_{xy}^G \\ I_{xy}^B \\end{pmatrix} = \\begin{pmatrix} I_{xy}^R \\ I_{xy}^G \\ I_{xy}^B \\end{pmatrix} + \\begin{pmatrix} \\mathbf{p}_1 \u0026 \\mathbf{p}_2 \u0026 \\mathbf{p}_3 \\end{pmatrix} \\begin{pmatrix} \\alpha_1 \\lambda_1 \\ \\alpha_2 \\lambda_2 \\ \\alpha_3 \\lambda_3 \\end{pmatrix}$$\n其中 $\\mathbf{p}_i$ 是特征向量，$\\lambda_i$ 是特征值，$\\alpha_i$ 是服从 $\\mathcal{N}(0, 0.1)$ 的随机变量。\n图5：数据增强对 AlexNet 性能的影响。从无增强到完整的增强策略，Top-1 和 Top-5 准确率都有显著提升。\n第五章：训练过程与优化 5.1 损失函数 AlexNet 使用 Softmax 交叉熵损失（Cross-Entropy Loss）。对于 $K$ 类分类问题：\n$$L = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{i,k} \\log(\\hat{y}_{i,k})$$\n其中 $\\hat{y}{i,k} = \\frac{e^{z{i,k}}}{\\sum_{j} e^{z_{i,j}}}$ 是 Softmax 输出。\n5.2 随机梯度下降 AlexNet 使用带动量的 SGD（Stochastic Gradient Descent）：\n$$\\mathbf{v}_{t+1} = \\mu \\mathbf{v}_t - \\epsilon \\nabla L(\\mathbf{W}t)$$ $$\\mathbf{W}{t+1} = \\mathbf{W}t + \\mathbf{v}{t+1}$$\n参数设置：\n批量大小（batch size）：$128$ 动量（momentum）：$0.9$ 权重衰减（L2 正则化）：$0.0005$ 初始学习率：$0.01$ 学习率调整策略：\n每当验证误差停止下降时，将学习率除以 $10$。总共降低了 $3$ 次，学习率从 $0.01$ 降到 $0.0001$。\n图6：不同激活函数的训练收敛速度对比。ReLU 的收敛速度明显快于 Sigmoid 和 Tanh，这对于训练深层网络至关重要。\n5.3 权重初始化 深层网络的权重初始化至关重要。AlexNet 使用：\n$$W_{ij} \\sim \\mathcal{N}(0, 0.01^2)$$\n偏置初始化为：\n第 $2$、$4$、$5$ 卷积层和全连接层：初始化为 $1$（加速早期学习） 其他层：初始化为 $0$ 5.4 批量归一化的先驱——LRN 虽然 AlexNet 使用了 LRN（Local Response Normalization，局部响应归一化），但后来被批量归一化（Batch Normalization）取代。LRN 的公式为：\n$$b_{x,y}^i = \\frac{a_{x,y}^i}{\\left( k + \\alpha \\sum_{j=\\max(0,i-n/2)}^{\\min(N-1,i+n/2)} (a_{x,y}^j)^2 \\right)^\\beta}$$\n参数设置：$k = 2$，$n = 5$，$\\alpha = 10^{-4}$，$\\beta = 0.75$。\n第六章：实验结果与历史影响 6.1 ImageNet 2012 结果 AlexNet 在 ILSVRC-2012 上的表现：\n指标 AlexNet 第二名（传统方法） 提升 Top-1 错误率 $37.5%$ $45.7%$ $8.2%$ Top-5 错误率 $16.4%$ $26.2%$ $9.8%$ 这个差距是压倒性的。更重要的是，错误率的下降趋势表明，深度学习还有巨大的提升空间。\n6.2 特征可视化 AlexNet 的一个重要贡献是特征可视化。通过可视化第一层的卷积核，可以看到网络学到了什么：\n第 $1$ 层：检测边缘、颜色、纹理等低级特征 第 $3$、$4$ 层：检测形状、模式等中级特征 第 $5$ 层：检测物体部件、语义等高级特征 这验证了深度学习的核心假设：层次化特征提取。\n6.3 迁移学习的证明 AlexNet 还展示了迁移学习的潜力。在 ImageNet 上预训练的模型，通过微调（fine-tuning）可以在其他任务上取得优异表现：\nCaltech-101：$91.5%$（之前 $86.5%$） Oxford Flowers：$89.5%$（之前 $72.8%$） PASCAL VOC：$77.8%$（之前 $59.3%$） 这证明了深度学习学到的特征是通用的、可迁移的。\n第七章：后续发展与深度学习浪潮 7.1 紧随其后：ZFNet 和 VGG ZFNet（2013）：\n调整 AlexNet 的超参数 使用反卷积可视化特征 错误率降至 $11.7%$ VGGNet（2014）：\n使用更小的 $3 \\times 3$ 卷积核 网络深度达到 $16$-$19$ 层 证明了\"深度\"的重要性 7.2 ResNet：更深的选择 $2015$ 年，ResNet 将网络深度推向 $152$ 层甚至 $1000$ 层以上，错误率降至 $3.6%$（首次超越人类水平 $5.1%$）。\n残差学习的核心思想：\n$$\\mathbf{y} = \\mathcal{F}(\\mathbf{x}, {W_i}) + \\mathbf{x}$$\n通过跳跃连接（skip connection），网络只需要学习残差 $\\mathcal{F}(\\mathbf{x})$，而非完整映射。\n7.3 AlexNet 的遗产 AlexNet 引入的技术至今仍被使用：\nReLU：所有现代 CNN 的标准激活函数 Dropout：正则化的标准技术之一 GPU 训练：深度学习的标准配置 数据增强：数据预处理的标准流程 更重要的是，AlexNet 证明了深度学习是可行的，从而引发了：\n学术界对深度学习的研究热潮 工业界对 AI 的大规模投资 计算机视觉领域的革命性进展 结语：从寒冬到春天 回顾 AlexNet 的历史，我们看到的是科学发展中常见的模式：\n理论基础早已存在（反向传播 $1986$ 年，卷积网络 $1998$ 年） 技术条件的成熟（GPU 计算能力，ImageNet 数据集） 关键创新（ReLU、Dropout、数据增强） 一次突破性的演示（ImageNet 2012） 范式的转变（从传统方法到深度学习） AlexNet 不是一夜之间的奇迹，而是几十年研究积累的结果，加上恰到好处的时机和关键的技术创新。\n对于今天的我们，AlexNet 的故事有几点启示：\n理论不会自动转化为应用：需要工程师的智慧来解决实际问题。\n硬件和数据同样重要：好的算法需要合适的土壤才能生长。\n简单的想法往往最有效：ReLU 和 Dropout 的原理都很简单，但效果惊人。\n科学是累积的：LeCun 的 LeNet、Hinton 的坚持、李飞飞的 ImageNet，都是 AlexNet 成功的基石。\n今天，当我们使用 GPT 进行对话、用 Stable Diffusion 生成图像、让自动驾驶汽车识别路况时，都不应该忘记 $2012$ 年那个秋天，一个 $8$ 层的神经网络在 ImageNet 上投下的那颗石子，激起了人工智能的滔天巨浪。\n附录：关键公式汇总 卷积操作 $$Y_{i,j} = \\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} K_{m,n} \\cdot X_{i+m, j+n} + b$$\nReLU 激活函数 $$f(z) = \\max(0, z)$$\nSoftmax 与交叉熵损失 $$\\hat{y}k = \\frac{e^{z_k}}{\\sum{j=1}^{K} e^{z_j}}$$\n$$L = -\\sum_{k=1}^{K} y_k \\log(\\hat{y}_k)$$\nSGD with Momentum $$\\mathbf{v}_{t+1} = \\mu \\mathbf{v}_t - \\epsilon \\nabla L(\\mathbf{W}t)$$ $$\\mathbf{W}{t+1} = \\mathbf{W}t + \\mathbf{v}{t+1}$$\nDropout 训练：$\\tilde{\\mathbf{a}} = \\mathbf{m} \\odot \\mathbf{a}$，其中 $m_i \\sim \\text{Bernoulli}(p)$\n测试：$\\mathbf{a}_{\\text{test}} = p \\cdot \\mathbf{a}$\n延伸阅读：\nKrizhevsky et al. “ImageNet Classification with Deep Convolutional Neural Networks.” NIPS 2012. LeCun et al. “Gradient-Based Learning Applied to Document Recognition.” 1998. Simonyan \u0026 Zisserman. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” ICLR 2015. He et al. “Deep Residual Learning for Image Recognition.” CVPR 2016. 愿你在深度学习的世界里，找到属于自己的那颗石子。\n","wordCount":"1021","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/alexnet-cover.jpg","datePublished":"2026-01-29T06:00:00+08:00","dateModified":"2026-01-29T06:00:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-29-alexnet-deep-learning-revolution/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">AlexNet：开启深度学习革命的里程碑</h1><div class=post-description>深入浅出解析 AlexNet 的架构原理、关键技术创新和历史意义，从 ImageNet 挑战到深度学习革命，完整推导其数学原理</div><div class=post-meta><span title='2026-01-29 06:00:00 +0800 CST'>January 29, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>1021 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/alexnet-cover.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/alexnet-cover.jpg alt=神经网络与深度学习></a><figcaption>AlexNet：深度学习时代的开端</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e4%b8%80%e4%b8%aa%e6%97%b6%e4%bb%a3%e7%9a%84%e5%88%86%e6%b0%b4%e5%b2%ad aria-label=引言：一个时代的分水岭>引言：一个时代的分水岭</a></li><li><a href=#%e7%ac%ac%e4%b8%80%e7%ab%a0%e9%bb%8e%e6%98%8e%e5%89%8d%e7%9a%84%e9%bb%91%e6%9a%97%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%af%92%e5%86%ac aria-label=第一章：黎明前的黑暗——深度学习的寒冬>第一章：黎明前的黑暗——深度学习的寒冬</a><ul><li><a href=#11-%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e5%85%b4%e8%a1%b0 aria-label="1.1 感知机的兴衰">1.1 感知机的兴衰</a></li><li><a href=#12-%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%9a%84%e6%9b%99%e5%85%89%e4%b8%8e%e5%9b%b0%e5%a2%83 aria-label="1.2 反向传播的曙光与困境">1.2 反向传播的曙光与困境</a></li><li><a href=#13-lenet-5%e5%85%88%e9%a9%b1%e8%80%85%e7%9a%84%e5%b0%9d%e8%af%95 aria-label="1.3 LeNet-5：先驱者的尝试">1.3 LeNet-5：先驱者的尝试</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e7%ab%a0imagenet%e5%a4%a7%e6%95%b0%e6%8d%ae%e7%9a%84%e8%a7%89%e9%86%92 aria-label=第二章：ImageNet——大数据的觉醒>第二章：ImageNet——大数据的觉醒</a><ul><li><a href=#21-%e6%95%b0%e6%8d%ae%e9%9b%86%e7%9a%84%e9%87%8d%e8%a6%81%e6%80%a7 aria-label="2.1 数据集的重要性">2.1 数据集的重要性</a></li><li><a href=#22-imagenet-%e7%9a%84%e8%af%9e%e7%94%9f aria-label="2.2 ImageNet 的诞生">2.2 ImageNet 的诞生</a></li><li><a href=#23-%e4%bc%a0%e7%bb%9f%e6%96%b9%e6%b3%95%e7%9a%84%e7%93%b6%e9%a2%88 aria-label="2.3 传统方法的瓶颈">2.3 传统方法的瓶颈</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e7%ab%a0alexnet-%e6%9e%b6%e6%9e%84%e8%af%a6%e8%a7%a3 aria-label="第三章：AlexNet 架构详解">第三章：AlexNet 架构详解</a><ul><li><a href=#31-%e7%bd%91%e7%bb%9c%e6%9e%b6%e6%9e%84%e6%a6%82%e8%a7%88 aria-label="3.1 网络架构概览">3.1 网络架构概览</a></li><li><a href=#32-%e5%8d%b7%e7%a7%af%e5%b1%82%e7%9a%84%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86 aria-label="3.2 卷积层的数学原理">3.2 卷积层的数学原理</a></li><li><a href=#33-gpu-%e5%b9%b6%e8%a1%8c%e6%9e%b6%e6%9e%84 aria-label="3.3 GPU 并行架构">3.3 GPU 并行架构</a></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e7%ab%a0%e4%b8%89%e5%a4%a7%e6%8a%80%e6%9c%af%e5%88%9b%e6%96%b0 aria-label=第四章：三大技术创新>第四章：三大技术创新</a><ul><li><a href=#41-relu%e6%89%93%e7%a0%b4%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1%e7%9a%84%e6%9e%b7%e9%94%81 aria-label="4.1 ReLU：打破梯度消失的枷锁">4.1 ReLU：打破梯度消失的枷锁</a></li><li><a href=#42-dropout%e9%98%b2%e6%ad%a2%e8%bf%87%e6%8b%9f%e5%90%88%e7%9a%84%e6%ad%a6%e5%99%a8 aria-label="4.2 Dropout：防止过拟合的武器">4.2 Dropout：防止过拟合的武器</a></li><li><a href=#43-%e6%95%b0%e6%8d%ae%e5%a2%9e%e5%bc%ba%e6%89%a9%e5%a4%a7%e6%95%b0%e6%8d%ae%e9%9b%86%e7%9a%84%e8%89%ba%e6%9c%af aria-label="4.3 数据增强：扩大数据集的艺术">4.3 数据增强：扩大数据集的艺术</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e7%ab%a0%e8%ae%ad%e7%bb%83%e8%bf%87%e7%a8%8b%e4%b8%8e%e4%bc%98%e5%8c%96 aria-label=第五章：训练过程与优化>第五章：训练过程与优化</a><ul><li><a href=#51-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0 aria-label="5.1 损失函数">5.1 损失函数</a></li><li><a href=#52-%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d aria-label="5.2 随机梯度下降">5.2 随机梯度下降</a></li><li><a href=#53-%e6%9d%83%e9%87%8d%e5%88%9d%e5%a7%8b%e5%8c%96 aria-label="5.3 权重初始化">5.3 权重初始化</a></li><li><a href=#54-%e6%89%b9%e9%87%8f%e5%bd%92%e4%b8%80%e5%8c%96%e7%9a%84%e5%85%88%e9%a9%b1lrn aria-label="5.4 批量归一化的先驱——LRN">5.4 批量归一化的先驱——LRN</a></li></ul></li><li><a href=#%e7%ac%ac%e5%85%ad%e7%ab%a0%e5%ae%9e%e9%aa%8c%e7%bb%93%e6%9e%9c%e4%b8%8e%e5%8e%86%e5%8f%b2%e5%bd%b1%e5%93%8d aria-label=第六章：实验结果与历史影响>第六章：实验结果与历史影响</a><ul><li><a href=#61-imagenet-2012-%e7%bb%93%e6%9e%9c aria-label="6.1 ImageNet 2012 结果">6.1 ImageNet 2012 结果</a></li><li><a href=#62-%e7%89%b9%e5%be%81%e5%8f%af%e8%a7%86%e5%8c%96 aria-label="6.2 特征可视化">6.2 特征可视化</a></li><li><a href=#63-%e8%bf%81%e7%a7%bb%e5%ad%a6%e4%b9%a0%e7%9a%84%e8%af%81%e6%98%8e aria-label="6.3 迁移学习的证明">6.3 迁移学习的证明</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%83%e7%ab%a0%e5%90%8e%e7%bb%ad%e5%8f%91%e5%b1%95%e4%b8%8e%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%b5%aa%e6%bd%ae aria-label=第七章：后续发展与深度学习浪潮>第七章：后续发展与深度学习浪潮</a><ul><li><a href=#71-%e7%b4%a7%e9%9a%8f%e5%85%b6%e5%90%8ezfnet-%e5%92%8c-vgg aria-label="7.1 紧随其后：ZFNet 和 VGG">7.1 紧随其后：ZFNet 和 VGG</a></li><li><a href=#72-resnet%e6%9b%b4%e6%b7%b1%e7%9a%84%e9%80%89%e6%8b%a9 aria-label="7.2 ResNet：更深的选择">7.2 ResNet：更深的选择</a></li><li><a href=#73-alexnet-%e7%9a%84%e9%81%97%e4%ba%a7 aria-label="7.3 AlexNet 的遗产">7.3 AlexNet 的遗产</a></li></ul></li><li><a href=#%e7%bb%93%e8%af%ad%e4%bb%8e%e5%af%92%e5%86%ac%e5%88%b0%e6%98%a5%e5%a4%a9 aria-label=结语：从寒冬到春天>结语：从寒冬到春天</a></li><li><a href=#%e9%99%84%e5%bd%95%e5%85%b3%e9%94%ae%e5%85%ac%e5%bc%8f%e6%b1%87%e6%80%bb aria-label=附录：关键公式汇总>附录：关键公式汇总</a><ul><li><a href=#%e5%8d%b7%e7%a7%af%e6%93%8d%e4%bd%9c aria-label=卷积操作>卷积操作</a></li><li><a href=#relu-%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0 aria-label="ReLU 激活函数">ReLU 激活函数</a></li><li><a href=#softmax-%e4%b8%8e%e4%ba%a4%e5%8f%89%e7%86%b5%e6%8d%9f%e5%a4%b1 aria-label="Softmax 与交叉熵损失">Softmax 与交叉熵损失</a></li><li><a href=#sgd-with-momentum aria-label="SGD with Momentum">SGD with Momentum</a></li><li><a href=#dropout aria-label=Dropout>Dropout</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=引言一个时代的分水岭>引言：一个时代的分水岭<a hidden class=anchor aria-hidden=true href=#引言一个时代的分水岭>#</a></h2><p>$2012$ 年 $9$ 月 $30$ 日，多伦多大学的研究团队在 ImageNet 大规模视觉识别挑战赛（ILSVRC）上提交了一个卷积神经网络模型。当时，没有人意识到这将是一个历史性的时刻。</p><p>这个模型叫做 <strong>AlexNet</strong>，以第一作者 Alex Krizhevsky 的名字命名。它在图像分类任务上将 Top-5 错误率从上一年的 $25.8%$ 骤降至 $16.4%$——降幅接近 $10$ 个百分点，远超第二名近 $10%$。</p><p>这不是一次普通的进步，这是一次<strong>范式革命</strong>。</p><p>在此之前，深度学习经历了漫长的"寒冬"。尽管 $1986$ 年反向传播算法已被提出，$1998$ 年 LeCun 的 LeNet 已经证明了卷积神经网络的潜力，但深层网络的训练一直受困于梯度消失、计算资源匮乏和数据不足等问题。</p><p>AlexNet 的突破不仅在于它赢得了比赛，更在于它证明了：<strong>深度神经网络可以在大规模数据集上有效训练，并且性能远超传统方法</strong>。</p><p>这一证明，开启了人工智能的新纪元。</p><hr><h2 id=第一章黎明前的黑暗深度学习的寒冬>第一章：黎明前的黑暗——深度学习的寒冬<a hidden class=anchor aria-hidden=true href=#第一章黎明前的黑暗深度学习的寒冬>#</a></h2><h3 id=11-感知机的兴衰>1.1 感知机的兴衰<a hidden class=anchor aria-hidden=true href=#11-感知机的兴衰>#</a></h3><p>要理解 AlexNet 的意义，我们需要回溯到神经网络的起源。</p><p>$1958$ 年，Frank Rosenblatt 提出了<strong>感知机</strong>（Perceptron），这是第一个能够学习的神经网络模型。Rosenblatt 乐观地宣称：&ldquo;感知机最终将能够学习、做出决策和翻译语言。&rdquo;</p><p>然而，$1969$ 年，Marvin Minsky 和 Seymour Papert 在《Perceptrons》一书中证明了感知机的局限性：它无法解决非线性可分问题，比如简单的异或（XOR）问题。</p><p>这个打击是致命的。神经网络研究陷入了第一次寒冬。</p><h3 id=12-反向传播的曙光与困境>1.2 反向传播的曙光与困境<a hidden class=anchor aria-hidden=true href=#12-反向传播的曙光与困境>#</a></h3><p>$1986$ 年，Rumelhart、Hinton 和 Williams 重新发现了<strong>反向传播算法</strong>（Backpropagation），为训练多层神经网络提供了理论基础。</p><p><strong>反向传播的核心思想</strong>：</p><p>给定损失函数 $L$，网络参数 $\mathbf{W}$，反向传播通过链式法则计算梯度：</p><p>$$\frac{\partial L}{\partial w_{ij}^{(l)}} = \frac{\partial L}{\partial z_i^{(l)}} \cdot \frac{\partial z_i^{(l)}}{\partial w_{ij}^{(l)}} = \delta_i^{(l)} \cdot a_j^{(l-1)}$$</p><p>其中 $\delta_i^{(l)}$ 是第 $l$ 层第 $i$ 个神经元的误差信号。</p><p>然而，尽管有了理论工具，实际应用仍然受限：</p><ol><li><strong>梯度消失问题</strong>：使用 Sigmoid 或 Tanh 激活函数时，深层网络的梯度会指数级衰减</li><li><strong>计算资源</strong>：$1980$ 年代的计算机无法处理大规模数据</li><li><strong>数据集太小</strong>：没有足够的标注数据来训练深层网络</li></ol><h3 id=13-lenet-5先驱者的尝试>1.3 LeNet-5：先驱者的尝试<a hidden class=anchor aria-hidden=true href=#13-lenet-5先驱者的尝试>#</a></h3><p>$1998$ 年，Yann LeCun 等人提出了 <strong>LeNet-5</strong>，这是一个 $5$ 层的卷积神经网络，成功应用于手写数字识别（MNIST 数据集）。</p><p>LeNet-5 的架构已经包含了现代 CNN 的核心要素：</p><ul><li>卷积层（Convolution）</li><li>池化层（Pooling）</li><li>全连接层（Fully Connected）</li></ul><p>但由于上述限制，LeNet-5 之后，深度学习并没有立即起飞。相反，支持向量机（SVM）和随机森林等传统机器学习方法在 $2000$ 年代占据了主导地位。</p><hr><h2 id=第二章imagenet大数据的觉醒>第二章：ImageNet——大数据的觉醒<a hidden class=anchor aria-hidden=true href=#第二章imagenet大数据的觉醒>#</a></h2><h3 id=21-数据集的重要性>2.1 数据集的重要性<a hidden class=anchor aria-hidden=true href=#21-数据集的重要性>#</a></h3><p>任何机器学习方法的效果都受限于三个因素：</p><ol><li><strong>算法</strong>：模型的表达能力</li><li><strong>计算</strong>：训练和推理的速度</li><li><strong>数据</strong>：训练样本的数量和质量</li></ol><p>在 $2010$ 年之前，计算机视觉领域的主流数据集规模很小：</p><ul><li>MNIST：$60,000$ 张 $28 \times 28$ 的手写数字</li><li>Caltech-101：$9,144$ 张图片，$101$ 个类别</li><li>PASCAL VOC：$20$ 个类别，每年约 $10,000$ 张图片</li></ul><p>这些数据集对于浅层模型足够，但无法支撑深层网络的训练。</p><h3 id=22-imagenet-的诞生>2.2 ImageNet 的诞生<a hidden class=anchor aria-hidden=true href=#22-imagenet-的诞生>#</a></h3><p>$2009$ 年，斯坦福大学的李飞飞教授团队发布了 <strong>ImageNet</strong> 数据集。这是一个具有划时代意义的项目：</p><ul><li><strong>规模</strong>：超过 $1,400$ 万张图片</li><li><strong>类别</strong>：$21,841$ 个类别（WordNet 层次结构）</li><li><strong>标注</strong>：每张图片都经过人工标注验证</li></ul><p>更重要的是，从 $2010$ 年开始，ImageNet 举办了年度挑战赛（ILSVRC），使用 $1,000$ 个类别的子集，每类约 $1,000$ 张训练图片。</p><h3 id=23-传统方法的瓶颈>2.3 传统方法的瓶颈<a hidden class=anchor aria-hidden=true href=#23-传统方法的瓶颈>#</a></h3><p>在 AlexNet 出现之前，ILSVRC 的优胜者都使用传统方法：</p><ul><li><strong>SIFT</strong>（尺度不变特征变换）提取局部特征</li><li><strong>HOG</strong>（方向梯度直方图）描述形状</li><li><strong>Bag of Visual Words</strong> 编码</li><li><strong>SVM</strong> 或 <strong>随机森林</strong> 分类</li></ul><p>这些方法在 $2010$ 年和 $2011$ 年的 Top-5 错误率分别为 $28.2%$ 和 $25.8%$，改进幅度很小。</p><p><img alt="ImageNet 错误率演进" loading=lazy src=/images/plots/alexnet-imagenet-error.png></p><p><em>图1：ImageNet 分类错误率演进（2010-2017）。AlexNet 在 2012 年实现了历史性的突破，将错误率从 25.8% 降至 16.4%，开启了深度学习革命。</em></p><hr><h2 id=第三章alexnet-架构详解>第三章：AlexNet 架构详解<a hidden class=anchor aria-hidden=true href=#第三章alexnet-架构详解>#</a></h2><h3 id=31-网络架构概览>3.1 网络架构概览<a hidden class=anchor aria-hidden=true href=#31-网络架构概览>#</a></h3><p>AlexNet 包含 $8$ 层可学习层：$5$ 个卷积层和 $3$ 个全连接层。输入是 $224 \times 224 \times 3$ 的 RGB 图像，输出是 $1,000$ 个类别的 Softmax 概率。</p><p><img alt="AlexNet 架构" loading=lazy src=/images/plots/alexnet-architecture.png></p><p><em>图2：AlexNet 网络架构。包含 5 个卷积层（蓝色）和 3 个全连接层（橙色），使用 GPU 并行训练。</em></p><p><strong>详细架构</strong>：</p><table><thead><tr><th>层</th><th>类型</th><th>核大小</th><th>步长</th><th>输出尺寸</th><th>参数数量</th></tr></thead><tbody><tr><td>1</td><td>Conv + ReLU + LRN</td><td>$11 \times 11$</td><td>4</td><td>$55 \times 55 \times 96$</td><td>$35,000$</td></tr><tr><td>2</td><td>MaxPool</td><td>$3 \times 3$</td><td>2</td><td>$27 \times 27 \times 96$</td><td>$0$</td></tr><tr><td>3</td><td>Conv + ReLU + LRN</td><td>$5 \times 5$</td><td>1</td><td>$27 \times 27 \times 256$</td><td>$307,000$</td></tr><tr><td>4</td><td>MaxPool</td><td>$3 \times 3$</td><td>2</td><td>$13 \times 13 \times 256$</td><td>$0$</td></tr><tr><td>5</td><td>Conv + ReLU</td><td>$3 \times 3$</td><td>1</td><td>$13 \times 13 \times 384$</td><td>$885,000$</td></tr><tr><td>6</td><td>Conv + ReLU</td><td>$3 \times 3$</td><td>1</td><td>$13 \times 13 \times 384$</td><td>$1,327,000$</td></tr><tr><td>7</td><td>Conv + ReLU</td><td>$3 \times 3$</td><td>1</td><td>$13 \times 13 \times 256$</td><td>$885,000$</td></tr><tr><td>8</td><td>MaxPool</td><td>$3 \times 3$</td><td>2</td><td>$6 \times 6 \times 256$</td><td>$0$</td></tr><tr><td>9</td><td>FC + ReLU + Dropout</td><td>-</td><td>-</td><td>$4096$</td><td>$37,748,000$</td></tr><tr><td>10</td><td>FC + ReLU + Dropout</td><td>-</td><td>-</td><td>$4096$</td><td>$16,777,000$</td></tr><tr><td>11</td><td>FC + Softmax</td><td>-</td><td>-</td><td>$1000$</td><td>$4,097,000$</td></tr></tbody></table><p><strong>总参数量</strong>：约 $6,000$ 万个参数。</p><h3 id=32-卷积层的数学原理>3.2 卷积层的数学原理<a hidden class=anchor aria-hidden=true href=#32-卷积层的数学原理>#</a></h3><p>卷积操作是 CNN 的核心。给定输入特征图 $\mathbf{X}$ 和卷积核 $\mathbf{K}$，输出特征图 $\mathbf{Y}$ 的计算为：</p><p>$$Y_{i,j} = \sum_{m=0}^{k-1} \sum_{n=0}^{k-1} K_{m,n} \cdot X_{i+m, j+n} + b$$</p><p>其中 $k$ 是核大小，$b$ 是偏置项。</p><p>在矩阵形式下，这可以表示为：</p><p>$$\mathbf{y} = \mathbf{W} \mathbf{x} + \mathbf{b}$$</p><p>AlexNet 使用较大的初始卷积核（$11 \times 11$，步长 $4$）来快速降低特征图尺寸，后续使用较小的核（$3 \times 3$ 和 $5 \times 5$）提取更精细的特征。</p><h3 id=33-gpu-并行架构>3.3 GPU 并行架构<a hidden class=anchor aria-hidden=true href=#33-gpu-并行架构>#</a></h3><p>AlexNet 的创新之一是使用了两块 GTX 580 GPU 进行并行训练。网络被分成两部分：</p><ul><li>GPU 1 处理下层特征（颜色、纹理）</li><li>GPU 2 处理上层特征（形状、语义）</li></ul><p>这种架构设计使得在当时硬件条件下能够训练更大的网络。</p><hr><h2 id=第四章三大技术创新>第四章：三大技术创新<a hidden class=anchor aria-hidden=true href=#第四章三大技术创新>#</a></h2><h3 id=41-relu打破梯度消失的枷锁>4.1 ReLU：打破梯度消失的枷锁<a hidden class=anchor aria-hidden=true href=#41-relu打破梯度消失的枷锁>#</a></h3><p>传统的激活函数如 Sigmoid 和 Tanh 存在一个致命问题：<strong>梯度消失</strong>。</p><p>Sigmoid 函数定义为：</p><p>$$\sigma(z) = \frac{1}{1 + e^{-z}}$$</p><p>其导数为：</p><p>$$\sigma&rsquo;(z) = \sigma(z)(1 - \sigma(z))$$</p><p>当 $|z|$ 较大时，$\sigma(z)$ 接近 $0$ 或 $1$，导数趋近于 $0$。在反向传播中，这导致梯度逐层衰减，深层网络难以训练。</p><p><strong>ReLU</strong>（Rectified Linear Unit）的定义非常简单：</p><p>$$f(z) = \max(0, z)$$</p><p>其导数为：</p><p>$$f&rsquo;(z) = \begin{cases} 1 & \text{if } z > 0 \ 0 & \text{if } z &lt; 0 \ \text{undefined} & \text{if } z = 0 \end{cases}$$</p><p><img alt=激活函数对比 loading=lazy src=/images/plots/alexnet-activations.png></p><p><em>图3：Sigmoid 与 ReLU 激活函数对比。Sigmoid 在两端出现梯度消失（红色区域），而 ReLU 在正区间保持恒定的梯度，有效缓解了梯度消失问题。</em></p><p><strong>ReLU 的优势</strong>：</p><ol><li><strong>计算简单</strong>：只需比较和取最大值，无需指数运算</li><li><strong>缓解梯度消失</strong>：正区间梯度恒为 $1$</li><li><strong>稀疏激活</strong>：约 $50%$ 的神经元输出为 $0$，提高计算效率</li><li><strong>生物学合理性</strong>：与生物神经元的激活机制类似</li></ol><p>AlexNet 的实验表明，使用 ReLU 可以使训练速度提升约 $6$ 倍！</p><h3 id=42-dropout防止过拟合的武器>4.2 Dropout：防止过拟合的武器<a hidden class=anchor aria-hidden=true href=#42-dropout防止过拟合的武器>#</a></h3><p>深层神经网络面临严重的<strong>过拟合</strong>风险。AlexNet 有 $6,000$ 万个参数，而训练数据只有 $120$ 万张图片，参数数量远超数据量。</p><p><strong>Dropout</strong> 是一种简单而有效的正则化技术。在训练过程中，以概率 $p$（通常 $p = 0.5$）随机"丢弃"（设为 $0$）一部分神经元的输出：</p><p>$$\tilde{\mathbf{a}}^{(l)} = \mathbf{m}^{(l)} \odot \mathbf{a}^{(l)}$$</p><p>其中 $\mathbf{m}^{(l)}$ 是服从 Bernoulli$(p)$ 分布的掩码向量。</p><p><img alt="Dropout 示意" loading=lazy src=/images/plots/alexnet-dropout.png></p><p><em>图4：Dropout 正则化示意。在训练时随机丢弃部分隐藏层神经元（灰色），测试时使用所有神经元但调整权重。</em></p><p><strong>为什么 Dropout 有效？</strong></p><ol><li><strong>集成学习视角</strong>：每次训练迭代相当于训练一个不同的子网络，最终结果是多个子网络的平均</li><li><strong>打破共适应</strong>：防止神经元过度依赖特定其他神经元</li><li><strong>稀疏表示</strong>：鼓励网络学习更鲁棒的特征</li></ol><p><strong>测试时的调整</strong>：</p><p>训练时丢弃比例为 $p$，测试时需要将权重乘以 $p$ 来补偿：</p><p>$$\mathbf{W}<em>{\text{test}} = p \cdot \mathbf{W}</em>{\text{train}}$$</p><p>或使用<strong>inverted dropout</strong>：训练时将保留的神经元输出除以 $p$，测试时无需调整。</p><p>AlexNet 在前两个全连接层使用 Dropout，丢弃率设为 $0.5$，显著降低了过拟合。</p><h3 id=43-数据增强扩大数据集的艺术>4.3 数据增强：扩大数据集的艺术<a hidden class=anchor aria-hidden=true href=#43-数据增强扩大数据集的艺术>#</a></h3><p>$120$ 万张训练图片对于 $6,000$ 万个参数来说仍然不够。AlexNet 使用了一系列<strong>数据增强</strong>技术来人工扩大训练集：</p><p><strong>随机裁剪与翻转</strong>：</p><ul><li>从 $256 \times 256$ 图片随机裁剪 $224 \times 224$ 区域</li><li>水平随机翻转</li><li>每张图片可以生成 $2048$ 个不同的训练样本</li></ul><p><strong>PCA 颜色增强</strong>：</p><p>对 RGB 通道进行主成分分析，然后添加随机扰动：</p><p>$$\begin{pmatrix} I_{xy}^R \ I_{xy}^G \ I_{xy}^B \end{pmatrix} = \begin{pmatrix} I_{xy}^R \ I_{xy}^G \ I_{xy}^B \end{pmatrix} + \begin{pmatrix} \mathbf{p}_1 & \mathbf{p}_2 & \mathbf{p}_3 \end{pmatrix} \begin{pmatrix} \alpha_1 \lambda_1 \ \alpha_2 \lambda_2 \ \alpha_3 \lambda_3 \end{pmatrix}$$</p><p>其中 $\mathbf{p}_i$ 是特征向量，$\lambda_i$ 是特征值，$\alpha_i$ 是服从 $\mathcal{N}(0, 0.1)$ 的随机变量。</p><p><img alt=数据增强效果 loading=lazy src=/images/plots/alexnet-data-augmentation.png></p><p><em>图5：数据增强对 AlexNet 性能的影响。从无增强到完整的增强策略，Top-1 和 Top-5 准确率都有显著提升。</em></p><hr><h2 id=第五章训练过程与优化>第五章：训练过程与优化<a hidden class=anchor aria-hidden=true href=#第五章训练过程与优化>#</a></h2><h3 id=51-损失函数>5.1 损失函数<a hidden class=anchor aria-hidden=true href=#51-损失函数>#</a></h3><p>AlexNet 使用 <strong>Softmax 交叉熵损失</strong>（Cross-Entropy Loss）。对于 $K$ 类分类问题：</p><p>$$L = -\frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K} y_{i,k} \log(\hat{y}_{i,k})$$</p><p>其中 $\hat{y}<em>{i,k} = \frac{e^{z</em>{i,k}}}{\sum_{j} e^{z_{i,j}}}$ 是 Softmax 输出。</p><h3 id=52-随机梯度下降>5.2 随机梯度下降<a hidden class=anchor aria-hidden=true href=#52-随机梯度下降>#</a></h3><p>AlexNet 使用带动量的 <strong>SGD</strong>（Stochastic Gradient Descent）：</p><p>$$\mathbf{v}_{t+1} = \mu \mathbf{v}_t - \epsilon \nabla L(\mathbf{W}<em>t)$$
$$\mathbf{W}</em>{t+1} = \mathbf{W}<em>t + \mathbf{v}</em>{t+1}$$</p><p>参数设置：</p><ul><li>批量大小（batch size）：$128$</li><li>动量（momentum）：$0.9$</li><li>权重衰减（L2 正则化）：$0.0005$</li><li>初始学习率：$0.01$</li></ul><p><strong>学习率调整策略</strong>：</p><p>每当验证误差停止下降时，将学习率除以 $10$。总共降低了 $3$ 次，学习率从 $0.01$ 降到 $0.0001$。</p><p><img alt=训练收敛速度 loading=lazy src=/images/plots/alexnet-training-convergence.png></p><p><em>图6：不同激活函数的训练收敛速度对比。ReLU 的收敛速度明显快于 Sigmoid 和 Tanh，这对于训练深层网络至关重要。</em></p><h3 id=53-权重初始化>5.3 权重初始化<a hidden class=anchor aria-hidden=true href=#53-权重初始化>#</a></h3><p>深层网络的权重初始化至关重要。AlexNet 使用：</p><p>$$W_{ij} \sim \mathcal{N}(0, 0.01^2)$$</p><p>偏置初始化为：</p><ul><li>第 $2$、$4$、$5$ 卷积层和全连接层：初始化为 $1$（加速早期学习）</li><li>其他层：初始化为 $0$</li></ul><h3 id=54-批量归一化的先驱lrn>5.4 批量归一化的先驱——LRN<a hidden class=anchor aria-hidden=true href=#54-批量归一化的先驱lrn>#</a></h3><p>虽然 AlexNet 使用了 <strong>LRN</strong>（Local Response Normalization，局部响应归一化），但后来被批量归一化（Batch Normalization）取代。LRN 的公式为：</p><p>$$b_{x,y}^i = \frac{a_{x,y}^i}{\left( k + \alpha \sum_{j=\max(0,i-n/2)}^{\min(N-1,i+n/2)} (a_{x,y}^j)^2 \right)^\beta}$$</p><p>参数设置：$k = 2$，$n = 5$，$\alpha = 10^{-4}$，$\beta = 0.75$。</p><hr><h2 id=第六章实验结果与历史影响>第六章：实验结果与历史影响<a hidden class=anchor aria-hidden=true href=#第六章实验结果与历史影响>#</a></h2><h3 id=61-imagenet-2012-结果>6.1 ImageNet 2012 结果<a hidden class=anchor aria-hidden=true href=#61-imagenet-2012-结果>#</a></h3><p>AlexNet 在 ILSVRC-2012 上的表现：</p><table><thead><tr><th>指标</th><th>AlexNet</th><th>第二名（传统方法）</th><th>提升</th></tr></thead><tbody><tr><td>Top-1 错误率</td><td>$37.5%$</td><td>$45.7%$</td><td>$8.2%$</td></tr><tr><td>Top-5 错误率</td><td>$16.4%$</td><td>$26.2%$</td><td>$9.8%$</td></tr></tbody></table><p>这个差距是压倒性的。更重要的是，错误率的下降趋势表明，深度学习还有巨大的提升空间。</p><h3 id=62-特征可视化>6.2 特征可视化<a hidden class=anchor aria-hidden=true href=#62-特征可视化>#</a></h3><p>AlexNet 的一个重要贡献是<strong>特征可视化</strong>。通过可视化第一层的卷积核，可以看到网络学到了什么：</p><ul><li>第 $1$ 层：检测边缘、颜色、纹理等低级特征</li><li>第 $3$、$4$ 层：检测形状、模式等中级特征</li><li>第 $5$ 层：检测物体部件、语义等高级特征</li></ul><p>这验证了深度学习的核心假设：<strong>层次化特征提取</strong>。</p><h3 id=63-迁移学习的证明>6.3 迁移学习的证明<a hidden class=anchor aria-hidden=true href=#63-迁移学习的证明>#</a></h3><p>AlexNet 还展示了<strong>迁移学习</strong>的潜力。在 ImageNet 上预训练的模型，通过微调（fine-tuning）可以在其他任务上取得优异表现：</p><ul><li><strong>Caltech-101</strong>：$91.5%$（之前 $86.5%$）</li><li><strong>Oxford Flowers</strong>：$89.5%$（之前 $72.8%$）</li><li><strong>PASCAL VOC</strong>：$77.8%$（之前 $59.3%$）</li></ul><p>这证明了深度学习学到的特征是通用的、可迁移的。</p><hr><h2 id=第七章后续发展与深度学习浪潮>第七章：后续发展与深度学习浪潮<a hidden class=anchor aria-hidden=true href=#第七章后续发展与深度学习浪潮>#</a></h2><h3 id=71-紧随其后zfnet-和-vgg>7.1 紧随其后：ZFNet 和 VGG<a hidden class=anchor aria-hidden=true href=#71-紧随其后zfnet-和-vgg>#</a></h3><p><strong>ZFNet</strong>（2013）：</p><ul><li>调整 AlexNet 的超参数</li><li>使用反卷积可视化特征</li><li>错误率降至 $11.7%$</li></ul><p><strong>VGGNet</strong>（2014）：</p><ul><li>使用更小的 $3 \times 3$ 卷积核</li><li>网络深度达到 $16$-$19$ 层</li><li>证明了"深度"的重要性</li></ul><h3 id=72-resnet更深的选择>7.2 ResNet：更深的选择<a hidden class=anchor aria-hidden=true href=#72-resnet更深的选择>#</a></h3><p>$2015$ 年，ResNet 将网络深度推向 $152$ 层甚至 $1000$ 层以上，错误率降至 $3.6%$（首次超越人类水平 $5.1%$）。</p><p><strong>残差学习</strong>的核心思想：</p><p>$$\mathbf{y} = \mathcal{F}(\mathbf{x}, {W_i}) + \mathbf{x}$$</p><p>通过跳跃连接（skip connection），网络只需要学习残差 $\mathcal{F}(\mathbf{x})$，而非完整映射。</p><h3 id=73-alexnet-的遗产>7.3 AlexNet 的遗产<a hidden class=anchor aria-hidden=true href=#73-alexnet-的遗产>#</a></h3><p>AlexNet 引入的技术至今仍被使用：</p><ol><li><strong>ReLU</strong>：所有现代 CNN 的标准激活函数</li><li><strong>Dropout</strong>：正则化的标准技术之一</li><li><strong>GPU 训练</strong>：深度学习的标准配置</li><li><strong>数据增强</strong>：数据预处理的标准流程</li></ol><p>更重要的是，AlexNet 证明了<strong>深度学习是可行的</strong>，从而引发了：</p><ul><li>学术界对深度学习的研究热潮</li><li>工业界对 AI 的大规模投资</li><li>计算机视觉领域的革命性进展</li></ul><hr><h2 id=结语从寒冬到春天>结语：从寒冬到春天<a hidden class=anchor aria-hidden=true href=#结语从寒冬到春天>#</a></h2><p>回顾 AlexNet 的历史，我们看到的是科学发展中常见的模式：</p><ol><li><strong>理论基础早已存在</strong>（反向传播 $1986$ 年，卷积网络 $1998$ 年）</li><li><strong>技术条件的成熟</strong>（GPU 计算能力，ImageNet 数据集）</li><li><strong>关键创新</strong>（ReLU、Dropout、数据增强）</li><li><strong>一次突破性的演示</strong>（ImageNet 2012）</li><li><strong>范式的转变</strong>（从传统方法到深度学习）</li></ol><p>AlexNet 不是一夜之间的奇迹，而是几十年研究积累的结果，加上恰到好处的时机和关键的技术创新。</p><p>对于今天的我们，AlexNet 的故事有几点启示：</p><p><strong>理论不会自动转化为应用</strong>：需要工程师的智慧来解决实际问题。</p><p><strong>硬件和数据同样重要</strong>：好的算法需要合适的土壤才能生长。</p><p><strong>简单的想法往往最有效</strong>：ReLU 和 Dropout 的原理都很简单，但效果惊人。</p><p><strong>科学是累积的</strong>：LeCun 的 LeNet、Hinton 的坚持、李飞飞的 ImageNet，都是 AlexNet 成功的基石。</p><p>今天，当我们使用 GPT 进行对话、用 Stable Diffusion 生成图像、让自动驾驶汽车识别路况时，都不应该忘记 $2012$ 年那个秋天，一个 $8$ 层的神经网络在 ImageNet 上投下的那颗石子，激起了人工智能的滔天巨浪。</p><hr><h2 id=附录关键公式汇总>附录：关键公式汇总<a hidden class=anchor aria-hidden=true href=#附录关键公式汇总>#</a></h2><h3 id=卷积操作>卷积操作<a hidden class=anchor aria-hidden=true href=#卷积操作>#</a></h3><p>$$Y_{i,j} = \sum_{m=0}^{k-1} \sum_{n=0}^{k-1} K_{m,n} \cdot X_{i+m, j+n} + b$$</p><h3 id=relu-激活函数>ReLU 激活函数<a hidden class=anchor aria-hidden=true href=#relu-激活函数>#</a></h3><p>$$f(z) = \max(0, z)$$</p><h3 id=softmax-与交叉熵损失>Softmax 与交叉熵损失<a hidden class=anchor aria-hidden=true href=#softmax-与交叉熵损失>#</a></h3><p>$$\hat{y}<em>k = \frac{e^{z_k}}{\sum</em>{j=1}^{K} e^{z_j}}$$</p><p>$$L = -\sum_{k=1}^{K} y_k \log(\hat{y}_k)$$</p><h3 id=sgd-with-momentum>SGD with Momentum<a hidden class=anchor aria-hidden=true href=#sgd-with-momentum>#</a></h3><p>$$\mathbf{v}_{t+1} = \mu \mathbf{v}_t - \epsilon \nabla L(\mathbf{W}<em>t)$$
$$\mathbf{W}</em>{t+1} = \mathbf{W}<em>t + \mathbf{v}</em>{t+1}$$</p><h3 id=dropout>Dropout<a hidden class=anchor aria-hidden=true href=#dropout>#</a></h3><p>训练：$\tilde{\mathbf{a}} = \mathbf{m} \odot \mathbf{a}$，其中 $m_i \sim \text{Bernoulli}(p)$</p><p>测试：$\mathbf{a}_{\text{test}} = p \cdot \mathbf{a}$</p><hr><p><strong>延伸阅读</strong>：</p><ul><li>Krizhevsky et al. &ldquo;ImageNet Classification with Deep Convolutional Neural Networks.&rdquo; NIPS 2012.</li><li>LeCun et al. &ldquo;Gradient-Based Learning Applied to Document Recognition.&rdquo; 1998.</li><li>Simonyan & Zisserman. &ldquo;Very Deep Convolutional Networks for Large-Scale Image Recognition.&rdquo; ICLR 2015.</li><li>He et al. &ldquo;Deep Residual Learning for Image Recognition.&rdquo; CVPR 2016.</li></ul><p><em>愿你在深度学习的世界里，找到属于自己的那颗石子。</em></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%BB%BC%E8%BF%B0/>综述</a></li><li><a href=https://s-ai-unix.github.io/tags/alexnet/>AlexNet</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-29-tensor-comprehensive-guide/><span class=title>« Prev</span><br><span>张量：从数学抽象到深度学习核心的系统综述</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-28-differential-geometry-deep-learning/><span class=title>Next »</span><br><span>微分几何与深度学习：从流形假设到几何深度学习</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share AlexNet：开启深度学习革命的里程碑 on x" href="https://x.com/intent/tweet/?text=AlexNet%ef%bc%9a%e5%bc%80%e5%90%af%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e9%9d%a9%e5%91%bd%e7%9a%84%e9%87%8c%e7%a8%8b%e7%a2%91&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-29-alexnet-deep-learning-revolution%2f&amp;hashtags=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%2c%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%2c%e7%bb%bc%e8%bf%b0%2cAlexNet"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AlexNet：开启深度学习革命的里程碑 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-29-alexnet-deep-learning-revolution%2f&amp;title=AlexNet%ef%bc%9a%e5%bc%80%e5%90%af%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e9%9d%a9%e5%91%bd%e7%9a%84%e9%87%8c%e7%a8%8b%e7%a2%91&amp;summary=AlexNet%ef%bc%9a%e5%bc%80%e5%90%af%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e9%9d%a9%e5%91%bd%e7%9a%84%e9%87%8c%e7%a8%8b%e7%a2%91&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-29-alexnet-deep-learning-revolution%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AlexNet：开启深度学习革命的里程碑 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-29-alexnet-deep-learning-revolution%2f&title=AlexNet%ef%bc%9a%e5%bc%80%e5%90%af%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e9%9d%a9%e5%91%bd%e7%9a%84%e9%87%8c%e7%a8%8b%e7%a2%91"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AlexNet：开启深度学习革命的里程碑 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-29-alexnet-deep-learning-revolution%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>