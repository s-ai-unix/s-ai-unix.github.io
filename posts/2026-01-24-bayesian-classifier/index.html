<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>贝叶斯分类器：从条件概率到智能决策的优雅之旅 | s-ai-unix's Blog</title><meta name=keywords content="机器学习,算法,综述"><meta name=description content="深入解析贝叶斯分类器的数学本质与应用价值，从贝叶斯定理到朴素贝叶斯，从理论推导到垃圾邮件过滤的实践应用"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-24-bayesian-classifier/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-24-bayesian-classifier/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-24-bayesian-classifier/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="贝叶斯分类器：从条件概率到智能决策的优雅之旅"><meta property="og:description" content="深入解析贝叶斯分类器的数学本质与应用价值，从贝叶斯定理到朴素贝叶斯，从理论推导到垃圾邮件过滤的实践应用"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-24T17:58:30+08:00"><meta property="article:modified_time" content="2026-01-24T17:58:30+08:00"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="算法"><meta property="article:tag" content="综述"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/bayesian-classifier.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/bayesian-classifier.jpg"><meta name=twitter:title content="贝叶斯分类器：从条件概率到智能决策的优雅之旅"><meta name=twitter:description content="深入解析贝叶斯分类器的数学本质与应用价值，从贝叶斯定理到朴素贝叶斯，从理论推导到垃圾邮件过滤的实践应用"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"贝叶斯分类器：从条件概率到智能决策的优雅之旅","item":"https://s-ai-unix.github.io/posts/2026-01-24-bayesian-classifier/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"贝叶斯分类器：从条件概率到智能决策的优雅之旅","name":"贝叶斯分类器：从条件概率到智能决策的优雅之旅","description":"深入解析贝叶斯分类器的数学本质与应用价值，从贝叶斯定理到朴素贝叶斯，从理论推导到垃圾邮件过滤的实践应用","keywords":["机器学习","算法","综述"],"articleBody":"引言：不确定世界中的决策智慧 想象你在一家医院工作，面对一位病人。医生告诉你，这位病人有两种可能的疾病：疾病 A 和疾病 B。通过检查，你发现病人出现了某种症状 S。现在的关键问题是：这种症状的出现，是更倾向于指向疾病 A，还是疾病 B？\n这就是分类问题的本质——根据观察到的特征，将样本划分到不同的类别中。而在众多分类算法中，贝叶斯分类器以其优美的数学形式和深刻的思想基础，始终占据着不可替代的位置。\n它不依赖于复杂的神经网络或深度学习结构，仅仅基于概率论的基本原理，就能在许多实际应用中展现出令人惊讶的效果。更重要的是，它给了我们一种\"在不确定情况下进行理性决策\"的思维方式。\n第一章：概率论的基石 在进入贝叶斯分类器的核心之前，让我们先回顾一些基础的概率概念。这些概念看似简单，却构成了整个贝叶斯理论的数学大厦。\n1.1 条件概率 条件概率是贝叶斯理论的起点。它的直观含义是：在事件 B 发生的条件下，事件 A 发生的概率是多少？数学记为：\n$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n其中 $P(A \\cap B)$ 表示 A 和 B 同时发生的概率，$P(B)$ 是事件 B 发生的概率。这个公式的直观理解是：如果我们把所有可能的情况看作一个空间，条件概率就是在\"给定 B 发生\"这个子空间内，A 所占的比重。\n1.2 全概率公式 当我们面对一个复杂事件时，常常需要将其分解为若干互不相容的简单事件。这就是全概率公式的思想：\n$$P(A) = \\sum_{i=1}^{n} P(A|B_i) P(B_i)$$\n其中 $B_1, B_2, \\ldots, B_n$ 构成一个完备事件组（即它们互不相容且并集为整个样本空间）。全概率公式的几何直观是：将事件 A 的\"面积\"按照不同条件 $B_i$ 进行\"切片\"，然后将这些切片的面积加起来。\n1.3 贝叶斯公式的诞生 将条件概率公式\"反过来\"使用，就得到了著名的贝叶斯公式：\n$$P(B|A) = \\frac{P(A|B) P(B)}{P(A)}$$\n这个公式看似简单，却蕴含着深刻的哲学意义。它告诉我们：如果我们知道\"在 B 发生的条件下 A 的概率\"（$P(A|B)$），以及\"先验概率\" $P(B)$，就可以推导出\"观察到 A 后，B 的概率\"（$P(B|A)$）。\n这里的关键词是后验概率（Posterior Probability）与先验概率（Prior Probability）的转换。先验概率是在观察到数据之前我们对某个事件可能性的判断，而后验概率是在观察到数据之后更新的判断。\n第二章：从贝叶斯定理到贝叶斯分类器 2.1 分类决策问题的概率视角 现在让我们回到分类问题。假设我们有 $K$ 个类别 $c_1, c_2, \\ldots, c_K$，和一个包含 $d$ 个特征的特征向量 $\\mathbf{x} = (x_1, x_2, \\ldots, x_d)^T$。\n分类的目标是：给定观测到的特征 $\\mathbf{x}$，判断它属于哪个类别。\n从贝叶斯决策的角度来看，我们需要计算后验概率 $P(c_k|\\mathbf{x})$，即\"在观察到特征 $\\mathbf{x}$ 的条件下，样本属于类别 $c_k$ 的概率\"。然后选择后验概率最大的那个类别作为分类结果：\n$$\\hat{y} = \\arg\\max_{c_k} P(c_k|\\mathbf{x})$$\n2.2 贝叶斯公式的应用 根据贝叶斯公式，后验概率可以展开为：\n$$P(c_k|\\mathbf{x}) = \\frac{P(\\mathbf{x}|c_k) P(c_k)}{P(\\mathbf{x})}$$\n这里：\n$P(c_k)$ 是先验概率（Prior），即样本属于类别 $c_k$ 的先验概率 $P(\\mathbf{x}|c_k)$ 是类条件概率（Likelihood），即在类别 $c_k$ 中观察到特征 $\\mathbf{x}$ 的概率 $P(\\mathbf{x})$ 是证据因子（Evidence），即观察到特征 $\\mathbf{x}$ 的总概率 在分类决策中，我们实际上只需要比较不同类别的后验概率，而 $P(\\mathbf{x})$ 对所有类别都是相同的，可以忽略。因此决策规则简化为：\n$$\\hat{y} = \\arg\\max_{c_k} P(\\mathbf{x}|c_k) P(c_k)$$\n这个公式的物理直觉非常清晰：我们将\"先验信息\"（类别的普遍性）与\"观测到的证据\"（特征在各类别中的可能性）结合起来，做出最优决策。\ngraph TD subgraph Training[训练阶段] A[\"训练数据集\"] --\u003e B[\"估计先验概率 P(c_k)\"] A --\u003e C[\"估计条件概率 P(x_i|c_k)\"] B --\u003e D[\"存储模型参数\"] C --\u003e D end subgraph Prediction[预测阶段] E[\"新样本 x\"] --\u003e F[\"加载模型参数\"] D --\u003e F F --\u003e G[\"计算后验概率 P(c_k|x)\"] G --\u003e H{选择最大后验概率} H --\u003e I[\"输出预测类别\"] end classDef primaryNode fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff classDef orangeNode fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff classDef greenNode fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff class A primaryNode class E orangeNode class G greenNode class I primaryNode 这个流程图展示了贝叶斯分类器的完整工作流程。训练阶段负责从数据中学习概率分布参数，预测阶段利用这些参数计算后验概率并做出决策。\n第三章：朴素贝叶斯分类器 3.1 “朴素\"假设的动机 理论上，如果我们能够准确估计 $P(\\mathbf{x}|c_k)$，就可以得到最优的贝叶斯分类器。但这里面临一个致命的困难：当特征维度 $d$ 很大时，$P(\\mathbf{x}|c_k)$ 是一个 $d$ 维的概率分布，其参数数量随维度指数增长。\n例如，假设每个特征有 $m$ 种可能取值，那么需要估计的参数数量是 $(m^d-1) \\times K$。当 $d=20, m=2$ 时，这已经是天文数字了。\n朴素贝叶斯分类器引入了一个\"朴素\"的假设：特征之间相互独立。即：\n$$P(\\mathbf{x}|c_k) = \\prod_{i=1}^{d} P(x_i|c_k)$$\n这个假设在实际中几乎总是不成立的（很少有特征真正完全独立），但却带来了极大的计算简化。更重要的是，实践证明，即使在独立性假设严重违反的情况下，朴素贝叶斯往往仍然能取得很好的分类效果。\n这种现象被称为\"朴素贝叶斯的神奇”，其数学原因之一是：分类只关心后验概率的相对大小，而不是绝对值。即使独立假设导致概率估计不准确，只要各类别的相对顺序保持不变，分类结果依然正确。\n3.2 朴素贝叶斯的核心公式 在独立性假设下，决策准则变为：\n$$\\hat{y} = \\arg\\max_{c_k} P(c_k) \\prod_{i=1}^{d} P(x_i|c_k)$$\n为了避免数值下溢（当 $d$ 很大时，多个小于 1 的数相乘会导致计算机下溢），我们通常取对数：\n$$\\hat{y} = \\arg\\max_{c_k} \\left[ \\log P(c_k) + \\sum_{i=1}^{d} \\log P(x_i|c_k) \\right]$$\n这个公式不仅数值稳定，而且将乘法转化为加法，计算更加高效。\n3.3 三种常见的概率模型 根据特征类型的不同，朴素贝叶斯有三种常见的概率模型。\n3.3.1 多项式朴素贝叶斯（Multinomial Naive Bayes） 适用于计数型特征（Count Features），如文本分类中的词频统计。\n在类别 $c_k$ 中，特征 $x_i$ 的概率分布建模为多项式分布：\n$$P(x_i|c_k) = \\frac{n_{ik} + \\alpha}{N_k + \\alpha d}$$\n其中：\n$n_{ik}$ 是类别 $c_k$ 中特征 $x_i$ 的总计数 $N_k$ 是类别 $c_k$ 中所有特征的总计数 $\\alpha$ 是平滑参数（Laplace 平滑），通常取 $\\alpha = 1$ 平滑参数的作用是避免零概率问题：如果某个特征在训练数据的某个类别中从未出现，不加平滑会导致概率为零，从而整个乘积为零。\n3.3.2 伯努利朴素贝叶斯（Bernoulli Naive Bayes） 适用于二值特征（Binary Features），如\"词是否出现\"。\n每个特征 $x_i$ 只能取 0 或 1，其条件概率为：\n$$P(x_i|c_k) = p_{ik}^{x_i} (1-p_{ik})^{1-x_i}$$\n其中 $p_{ik}$ 是类别 $c_k$ 中特征 $x_i$ 取值为 1 的概率。\n3.3.3 高斯朴素贝叶斯（Gaussian Naive Bayes） 适用于连续型特征（Continuous Features）。\n假设在类别 $c_k$ 中，特征 $x_i$ 服从高斯分布：\n$$P(x_i|c_k) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{ik}} \\exp\\left[ -\\frac{(x_i - \\mu_{ik})^2}{2\\sigma_{ik}^2} \\right]$$\n其中 $\\mu_{ik}$ 和 $\\sigma_{ik}^2$ 分别是类别 $c_k$ 中特征 $x_i$ 的均值和方差。\n第四章：垃圾邮件过滤的实践应用 让我们通过一个具体的例子来理解朴素贝叶斯的工作原理。\n4.1 问题建模 假设我们要构建一个垃圾邮件过滤器。每封邮件表示为一个词袋（Bag of Words），只关注词的出现与否，不关心词序和语法。\n定义：\n类别：$c_0$（正常邮件），$c_1$（垃圾邮件） 特征：$x_i = 1$ 表示词 $w_i$ 在邮件中出现，$x_i = 0$ 表示不出现 词汇表大小：$d$ 4.2 训练阶段 在训练集中，我们需要估计以下参数：\n先验概率： $$P(c_1) = \\frac{\\text{垃圾邮件数量}}{\\text{总邮件数量}}$$ $$P(c_0) = 1 - P(c_1)$$\n条件概率（对每个词 $w_i$）： $$P(x_i=1|c_1) = \\frac{\\text{包含词 } w_i \\text{ 的垃圾邮件数} + \\alpha}{\\text{垃圾邮件总数} + \\alpha d}$$ $$P(x_i=1|c_0) = \\frac{\\text{包含词 } w_i \\text{ 的正常邮件数} + \\alpha}{\\text{正常邮件总数} + \\alpha d}$$\n4.3 预测阶段 给定一封新邮件，计算它属于垃圾邮件的对数后验概率：\n$$\\log P(c_1|\\mathbf{x}) \\propto \\log P(c_1) + \\sum_{i: x_i=1} \\log P(x_i=1|c_1) + \\sum_{i: x_i=0} \\log [1-P(x_i=1|c_1)]$$\n同样计算 $\\log P(c_0|\\mathbf{x})$，比较大小，选择概率较大的类别。\n4.4 一个数值例子 假设词汇表只有三个词：{viagra, lottery, hello}，训练数据如下：\n垃圾邮件：100 封，其中： 60 封包含 “viagra” 50 封包含 “lottery” 10 封包含 “hello” 正常邮件：400 封，其中： 5 封包含 “viagra” 10 封包含 “lottery” 300 封包含 “hello” 先验概率： $$P(c_1) = \\frac{100}{500} = 0.2, \\quad P(c_0) = 0.8$$\n条件概率（使用 Laplace 平滑 $\\alpha=1$）：\n| 词 | $P(x_i=1|c_1)$ | $P(x_i=1|c_0)$ | |—|—|—| | viagra | $\\frac{60+1}{100+3} \\approx 0.592$ | $\\frac{5+1}{400+3} \\approx 0.015$ | | lottery | $\\frac{50+1}{100+3} \\approx 0.495$ | $\\frac{10+1}{400+3} \\approx 0.027$ | | hello | $\\frac{10+1}{100+3} \\approx 0.107$ | $\\frac{300+1}{400+3} \\approx 0.747$ |\n预测一封新邮件：内容为 “viagra lottery hello”\n计算对数后验概率：\n$$\\begin{aligned} \\log P(c_1|\\mathbf{x}) \u0026\\propto \\log 0.2 + \\log 0.592 + \\log 0.495 + \\log 0.107 \\ \u0026= -1.609 - 0.524 - 0.703 - 2.236 \\ \u0026= -5.072 \\end{aligned}$$\n$$\\begin{aligned} \\log P(c_0|\\mathbf{x}) \u0026\\propto \\log 0.8 + \\log 0.015 + \\log 0.027 + \\log 0.747 \\ \u0026= -0.223 - 4.200 - 3.612 - 0.292 \\ \u0026= -8.327 \\end{aligned}$$\n因为 $-5.072 \u003e -8.327$，所以分类为垃圾邮件。\n这个例子展示了朴素贝叶斯的工作机制：即使 “hello” 更常出现在正常邮件中，但 “viagra” 和 “lottery” 这两个词的垃圾邮件指示性太强，足以压倒 “hello” 的正常邮件指示性。\n为了更直观地理解朴素贝叶斯的网络结构，下图展示了邮件分类的贝叶斯网络：\ngraph TD C[\"类别 C\"] X1[\"特征 X1: viagra\"] X2[\"特征 X2: lottery\"] X3[\"特征 X3: hello\"] X4[\"特征 X4: ...\"] Xd[\"特征 Xd\"] C --\u003e X1 C --\u003e X2 C --\u003e X3 C --\u003e X4 C --\u003e Xd classDef blueNode fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff classDef greenNode fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff class C blueNode class X1,X2,X3,X4,Xd greenNode 在这个网络中，类别节点 $C$ 是唯一的父节点，所有特征节点 $X_i$ 只依赖于类别，而特征之间相互独立。这就是朴素贝叶斯的\"朴素\"假设在网络结构上的体现。\n第五章：贝叶斯网络 朴素贝叶斯假设所有特征相互独立，这过于简化。贝叶斯网络（Bayesian Network）是更一般化的概率图模型，它允许特征之间存在有限的依赖关系。\n5.1 贝叶斯网络的定义 贝叶斯网络是一个有向无环图（DAG），其中：\n每个节点表示一个随机变量 边表示变量之间的直接依赖关系 每个节点有一个条件概率表（CPT），给定其父节点的取值 5.2 联合概率的因式分解 贝叶斯网络的核心优势在于：联合概率分布可以因式分解为条件概率的乘积：\n$$P(X_1, X_2, \\ldots, X_n) = \\prod_{i=1}^{n} P(X_i|\\text{Pa}(X_i))$$\n其中 $\\text{Pa}(X_i)$ 表示节点 $X_i$ 的父节点集合。\n这种因式分解大大减少了需要估计的参数数量。例如，一个有 $n$ 个变量的全连接网络需要 $O(2^n)$ 个参数，而一个树状结构的贝叶斯网络只需要 $O(n)$ 个参数。\n5.3 推理与学习 贝叶斯网络的核心任务有两个：\n推理（Inference）：给定部分变量的观测值，推断其他变量的后验概率\n精确推理：变量消元法、信念传播 近似推理：MCMC 采样、变分推断 学习（Learning）：从数据中学习网络结构和参数\n参数学习：最大似然估计、贝叶斯估计 结构学习：基于评分的方法、约束方法 5.4 应用领域 贝叶斯网络的应用非常广泛：\n医疗诊断：症状、疾病、病因之间的关系建模 故障诊断：设备故障的因果推理 推荐系统：用户偏好、商品特征之间的依赖关系 自然语言处理：句法分析、语义理解 结语：贝叶斯思想的不朽价值 贝叶斯分类器不仅仅是一种算法，更是一种思维方式。它教导我们：\n先验知识的重要性：我们不应完全依赖数据，而应该结合先验知识。这与人类的学习方式一致——我们带着已有的经验去理解新事物。\n不确定性是常态：世界充满了不确定性，我们不应该追求绝对的确定性，而应该在不确定性中做出最优决策。\n模型的简洁性：朴素贝叶斯的\"天真\"假设虽然在数学上不成立，但在实践中却屡建奇功。这提醒我们：一个好的模型不一定是最精确的，而是最能在效果和效率之间取得平衡的。\n概率论是智能的数学语言：无论是从哲学层面还是技术层面，概率论都为我们提供了理解和建模不确定性的强大工具。\n在深度学习大行其道的今天，贝叶斯分类器及其衍生方法（变分推断、贝叶斯神经网络等）依然保持着旺盛的生命力。它们不仅在理论上的优雅令人着迷，更在实践中的可靠性让人信赖。\n正如统计学家 George Box 所说：“所有模型都是错的，但有些是有用的。” 贝叶斯分类器就是这样一类\"有用\"的模型——它用简洁的数学形式，捕捉了分类问题的本质，为我们提供了一套在不确定世界中理性决策的工具。\n参考文献：\nBishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer. Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press. Duda, R. O., Hart, P. E., \u0026 Stork, D. G. (2000). Pattern Classification. Wiley. Russell, S., \u0026 Norvig, P. (2020). Artificial Intelligence: A Modern Approach. Pearson. ","wordCount":"736","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/bayesian-classifier.jpg","datePublished":"2026-01-24T17:58:30+08:00","dateModified":"2026-01-24T17:58:30+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-24-bayesian-classifier/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">贝叶斯分类器：从条件概率到智能决策的优雅之旅</h1><div class=post-description>深入解析贝叶斯分类器的数学本质与应用价值，从贝叶斯定理到朴素贝叶斯，从理论推导到垃圾邮件过滤的实践应用</div><div class=post-meta><span title='2026-01-24 17:58:30 +0800 CST'>January 24, 2026</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>736 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/bayesian-classifier.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/bayesian-classifier.jpg alt=贝叶斯网络结构示意图></a><figcaption>贝叶斯网络的条件依赖结构</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e4%b8%8d%e7%a1%ae%e5%ae%9a%e4%b8%96%e7%95%8c%e4%b8%ad%e7%9a%84%e5%86%b3%e7%ad%96%e6%99%ba%e6%85%a7 aria-label=引言：不确定世界中的决策智慧>引言：不确定世界中的决策智慧</a></li><li><a href=#%e7%ac%ac%e4%b8%80%e7%ab%a0%e6%a6%82%e7%8e%87%e8%ae%ba%e7%9a%84%e5%9f%ba%e7%9f%b3 aria-label=第一章：概率论的基石>第一章：概率论的基石</a><ul><li><a href=#11-%e6%9d%a1%e4%bb%b6%e6%a6%82%e7%8e%87 aria-label="1.1 条件概率">1.1 条件概率</a></li><li><a href=#12-%e5%85%a8%e6%a6%82%e7%8e%87%e5%85%ac%e5%bc%8f aria-label="1.2 全概率公式">1.2 全概率公式</a></li><li><a href=#13-%e8%b4%9d%e5%8f%b6%e6%96%af%e5%85%ac%e5%bc%8f%e7%9a%84%e8%af%9e%e7%94%9f aria-label="1.3 贝叶斯公式的诞生">1.3 贝叶斯公式的诞生</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e7%ab%a0%e4%bb%8e%e8%b4%9d%e5%8f%b6%e6%96%af%e5%ae%9a%e7%90%86%e5%88%b0%e8%b4%9d%e5%8f%b6%e6%96%af%e5%88%86%e7%b1%bb%e5%99%a8 aria-label=第二章：从贝叶斯定理到贝叶斯分类器>第二章：从贝叶斯定理到贝叶斯分类器</a><ul><li><a href=#21-%e5%88%86%e7%b1%bb%e5%86%b3%e7%ad%96%e9%97%ae%e9%a2%98%e7%9a%84%e6%a6%82%e7%8e%87%e8%a7%86%e8%a7%92 aria-label="2.1 分类决策问题的概率视角">2.1 分类决策问题的概率视角</a></li><li><a href=#22-%e8%b4%9d%e5%8f%b6%e6%96%af%e5%85%ac%e5%bc%8f%e7%9a%84%e5%ba%94%e7%94%a8 aria-label="2.2 贝叶斯公式的应用">2.2 贝叶斯公式的应用</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e7%ab%a0%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af%e5%88%86%e7%b1%bb%e5%99%a8 aria-label=第三章：朴素贝叶斯分类器>第三章：朴素贝叶斯分类器</a><ul><li><a href=#31-%e6%9c%b4%e7%b4%a0%e5%81%87%e8%ae%be%e7%9a%84%e5%8a%a8%e6%9c%ba aria-label='3.1 &ldquo;朴素"假设的动机'>3.1 &ldquo;朴素"假设的动机</a></li><li><a href=#32-%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af%e7%9a%84%e6%a0%b8%e5%bf%83%e5%85%ac%e5%bc%8f aria-label="3.2 朴素贝叶斯的核心公式">3.2 朴素贝叶斯的核心公式</a></li><li><a href=#33-%e4%b8%89%e7%a7%8d%e5%b8%b8%e8%a7%81%e7%9a%84%e6%a6%82%e7%8e%87%e6%a8%a1%e5%9e%8b aria-label="3.3 三种常见的概率模型">3.3 三种常见的概率模型</a><ul><li><a href=#331-%e5%a4%9a%e9%a1%b9%e5%bc%8f%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%afmultinomial-naive-bayes aria-label="3.3.1 多项式朴素贝叶斯（Multinomial Naive Bayes）">3.3.1 多项式朴素贝叶斯（Multinomial Naive Bayes）</a></li><li><a href=#332-%e4%bc%af%e5%8a%aa%e5%88%a9%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%afbernoulli-naive-bayes aria-label="3.3.2 伯努利朴素贝叶斯（Bernoulli Naive Bayes）">3.3.2 伯努利朴素贝叶斯（Bernoulli Naive Bayes）</a></li><li><a href=#333-%e9%ab%98%e6%96%af%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%afgaussian-naive-bayes aria-label="3.3.3 高斯朴素贝叶斯（Gaussian Naive Bayes）">3.3.3 高斯朴素贝叶斯（Gaussian Naive Bayes）</a></li></ul></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e7%ab%a0%e5%9e%83%e5%9c%be%e9%82%ae%e4%bb%b6%e8%bf%87%e6%bb%a4%e7%9a%84%e5%ae%9e%e8%b7%b5%e5%ba%94%e7%94%a8 aria-label=第四章：垃圾邮件过滤的实践应用>第四章：垃圾邮件过滤的实践应用</a><ul><li><a href=#41-%e9%97%ae%e9%a2%98%e5%bb%ba%e6%a8%a1 aria-label="4.1 问题建模">4.1 问题建模</a></li><li><a href=#42-%e8%ae%ad%e7%bb%83%e9%98%b6%e6%ae%b5 aria-label="4.2 训练阶段">4.2 训练阶段</a></li><li><a href=#43-%e9%a2%84%e6%b5%8b%e9%98%b6%e6%ae%b5 aria-label="4.3 预测阶段">4.3 预测阶段</a></li><li><a href=#44-%e4%b8%80%e4%b8%aa%e6%95%b0%e5%80%bc%e4%be%8b%e5%ad%90 aria-label="4.4 一个数值例子">4.4 一个数值例子</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e7%ab%a0%e8%b4%9d%e5%8f%b6%e6%96%af%e7%bd%91%e7%bb%9c aria-label=第五章：贝叶斯网络>第五章：贝叶斯网络</a><ul><li><a href=#51-%e8%b4%9d%e5%8f%b6%e6%96%af%e7%bd%91%e7%bb%9c%e7%9a%84%e5%ae%9a%e4%b9%89 aria-label="5.1 贝叶斯网络的定义">5.1 贝叶斯网络的定义</a></li><li><a href=#52-%e8%81%94%e5%90%88%e6%a6%82%e7%8e%87%e7%9a%84%e5%9b%a0%e5%bc%8f%e5%88%86%e8%a7%a3 aria-label="5.2 联合概率的因式分解">5.2 联合概率的因式分解</a></li><li><a href=#53-%e6%8e%a8%e7%90%86%e4%b8%8e%e5%ad%a6%e4%b9%a0 aria-label="5.3 推理与学习">5.3 推理与学习</a></li><li><a href=#54-%e5%ba%94%e7%94%a8%e9%a2%86%e5%9f%9f aria-label="5.4 应用领域">5.4 应用领域</a></li></ul></li><li><a href=#%e7%bb%93%e8%af%ad%e8%b4%9d%e5%8f%b6%e6%96%af%e6%80%9d%e6%83%b3%e7%9a%84%e4%b8%8d%e6%9c%bd%e4%bb%b7%e5%80%bc aria-label=结语：贝叶斯思想的不朽价值>结语：贝叶斯思想的不朽价值</a></li></ul></div></details></div><div class=post-content><h2 id=引言不确定世界中的决策智慧>引言：不确定世界中的决策智慧<a hidden class=anchor aria-hidden=true href=#引言不确定世界中的决策智慧>#</a></h2><p>想象你在一家医院工作，面对一位病人。医生告诉你，这位病人有两种可能的疾病：疾病 A 和疾病 B。通过检查，你发现病人出现了某种症状 S。现在的关键问题是：这种症状的出现，是更倾向于指向疾病 A，还是疾病 B？</p><p>这就是分类问题的本质——根据观察到的特征，将样本划分到不同的类别中。而在众多分类算法中，贝叶斯分类器以其优美的数学形式和深刻的思想基础，始终占据着不可替代的位置。</p><p>它不依赖于复杂的神经网络或深度学习结构，仅仅基于概率论的基本原理，就能在许多实际应用中展现出令人惊讶的效果。更重要的是，它给了我们一种"在不确定情况下进行理性决策"的思维方式。</p><h2 id=第一章概率论的基石>第一章：概率论的基石<a hidden class=anchor aria-hidden=true href=#第一章概率论的基石>#</a></h2><p>在进入贝叶斯分类器的核心之前，让我们先回顾一些基础的概率概念。这些概念看似简单，却构成了整个贝叶斯理论的数学大厦。</p><h3 id=11-条件概率>1.1 条件概率<a hidden class=anchor aria-hidden=true href=#11-条件概率>#</a></h3><p>条件概率是贝叶斯理论的起点。它的直观含义是：在事件 B 发生的条件下，事件 A 发生的概率是多少？数学记为：</p><p>$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$</p><p>其中 $P(A \cap B)$ 表示 A 和 B 同时发生的概率，$P(B)$ 是事件 B 发生的概率。这个公式的直观理解是：如果我们把所有可能的情况看作一个空间，条件概率就是在"给定 B 发生"这个子空间内，A 所占的比重。</p><h3 id=12-全概率公式>1.2 全概率公式<a hidden class=anchor aria-hidden=true href=#12-全概率公式>#</a></h3><p>当我们面对一个复杂事件时，常常需要将其分解为若干互不相容的简单事件。这就是全概率公式的思想：</p><p>$$P(A) = \sum_{i=1}^{n} P(A|B_i) P(B_i)$$</p><p>其中 $B_1, B_2, \ldots, B_n$ 构成一个完备事件组（即它们互不相容且并集为整个样本空间）。全概率公式的几何直观是：将事件 A 的"面积"按照不同条件 $B_i$ 进行"切片"，然后将这些切片的面积加起来。</p><h3 id=13-贝叶斯公式的诞生>1.3 贝叶斯公式的诞生<a hidden class=anchor aria-hidden=true href=#13-贝叶斯公式的诞生>#</a></h3><p>将条件概率公式"反过来"使用，就得到了著名的贝叶斯公式：</p><p>$$P(B|A) = \frac{P(A|B) P(B)}{P(A)}$$</p><p>这个公式看似简单，却蕴含着深刻的哲学意义。它告诉我们：如果我们知道"在 B 发生的条件下 A 的概率"（$P(A|B)$），以及"先验概率" $P(B)$，就可以推导出"观察到 A 后，B 的概率"（$P(B|A)$）。</p><p>这里的关键词是<strong>后验概率</strong>（Posterior Probability）与<strong>先验概率</strong>（Prior Probability）的转换。先验概率是在观察到数据之前我们对某个事件可能性的判断，而后验概率是在观察到数据之后更新的判断。</p><h2 id=第二章从贝叶斯定理到贝叶斯分类器>第二章：从贝叶斯定理到贝叶斯分类器<a hidden class=anchor aria-hidden=true href=#第二章从贝叶斯定理到贝叶斯分类器>#</a></h2><h3 id=21-分类决策问题的概率视角>2.1 分类决策问题的概率视角<a hidden class=anchor aria-hidden=true href=#21-分类决策问题的概率视角>#</a></h3><p>现在让我们回到分类问题。假设我们有 $K$ 个类别 $c_1, c_2, \ldots, c_K$，和一个包含 $d$ 个特征的特征向量 $\mathbf{x} = (x_1, x_2, \ldots, x_d)^T$。</p><p>分类的目标是：给定观测到的特征 $\mathbf{x}$，判断它属于哪个类别。</p><p>从贝叶斯决策的角度来看，我们需要计算<strong>后验概率</strong> $P(c_k|\mathbf{x})$，即"在观察到特征 $\mathbf{x}$ 的条件下，样本属于类别 $c_k$ 的概率"。然后选择后验概率最大的那个类别作为分类结果：</p><p>$$\hat{y} = \arg\max_{c_k} P(c_k|\mathbf{x})$$</p><h3 id=22-贝叶斯公式的应用>2.2 贝叶斯公式的应用<a hidden class=anchor aria-hidden=true href=#22-贝叶斯公式的应用>#</a></h3><p>根据贝叶斯公式，后验概率可以展开为：</p><p>$$P(c_k|\mathbf{x}) = \frac{P(\mathbf{x}|c_k) P(c_k)}{P(\mathbf{x})}$$</p><p>这里：</p><ul><li>$P(c_k)$ 是先验概率（Prior），即样本属于类别 $c_k$ 的先验概率</li><li>$P(\mathbf{x}|c_k)$ 是类条件概率（Likelihood），即在类别 $c_k$ 中观察到特征 $\mathbf{x}$ 的概率</li><li>$P(\mathbf{x})$ 是证据因子（Evidence），即观察到特征 $\mathbf{x}$ 的总概率</li></ul><p>在分类决策中，我们实际上只需要比较不同类别的后验概率，而 $P(\mathbf{x})$ 对所有类别都是相同的，可以忽略。因此决策规则简化为：</p><p>$$\hat{y} = \arg\max_{c_k} P(\mathbf{x}|c_k) P(c_k)$$</p><p>这个公式的物理直觉非常清晰：我们将"先验信息"（类别的普遍性）与"观测到的证据"（特征在各类别中的可能性）结合起来，做出最优决策。</p><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>graph TD
subgraph Training[训练阶段]
A["训练数据集"] --> B["估计先验概率 P(c_k)"]
A --> C["估计条件概率 P(x_i|c_k)"]
B --> D["存储模型参数"]
C --> D
end
subgraph Prediction[预测阶段]
E["新样本 x"] --> F["加载模型参数"]
D --> F
F --> G["计算后验概率 P(c_k|x)"]
G --> H{选择最大后验概率}
H --> I["输出预测类别"]
end
classDef primaryNode fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff
classDef orangeNode fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff
classDef greenNode fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
class A primaryNode
class E orangeNode
class G greenNode
class I primaryNode</div></div><p>这个流程图展示了贝叶斯分类器的完整工作流程。训练阶段负责从数据中学习概率分布参数，预测阶段利用这些参数计算后验概率并做出决策。</p><h2 id=第三章朴素贝叶斯分类器>第三章：朴素贝叶斯分类器<a hidden class=anchor aria-hidden=true href=#第三章朴素贝叶斯分类器>#</a></h2><h3 id=31-朴素假设的动机>3.1 &ldquo;朴素"假设的动机<a hidden class=anchor aria-hidden=true href=#31-朴素假设的动机>#</a></h3><p>理论上，如果我们能够准确估计 $P(\mathbf{x}|c_k)$，就可以得到最优的贝叶斯分类器。但这里面临一个致命的困难：当特征维度 $d$ 很大时，$P(\mathbf{x}|c_k)$ 是一个 $d$ 维的概率分布，其参数数量随维度指数增长。</p><p>例如，假设每个特征有 $m$ 种可能取值，那么需要估计的参数数量是 $(m^d-1) \times K$。当 $d=20, m=2$ 时，这已经是天文数字了。</p><p>朴素贝叶斯分类器引入了一个"朴素"的假设：<strong>特征之间相互独立</strong>。即：</p><p>$$P(\mathbf{x}|c_k) = \prod_{i=1}^{d} P(x_i|c_k)$$</p><p>这个假设在实际中几乎总是不成立的（很少有特征真正完全独立），但却带来了极大的计算简化。更重要的是，实践证明，即使在独立性假设严重违反的情况下，朴素贝叶斯往往仍然能取得很好的分类效果。</p><p>这种现象被称为"朴素贝叶斯的神奇&rdquo;，其数学原因之一是：分类只关心后验概率的<strong>相对大小</strong>，而不是绝对值。即使独立假设导致概率估计不准确，只要各类别的相对顺序保持不变，分类结果依然正确。</p><h3 id=32-朴素贝叶斯的核心公式>3.2 朴素贝叶斯的核心公式<a hidden class=anchor aria-hidden=true href=#32-朴素贝叶斯的核心公式>#</a></h3><p>在独立性假设下，决策准则变为：</p><p>$$\hat{y} = \arg\max_{c_k} P(c_k) \prod_{i=1}^{d} P(x_i|c_k)$$</p><p>为了避免数值下溢（当 $d$ 很大时，多个小于 1 的数相乘会导致计算机下溢），我们通常取对数：</p><p>$$\hat{y} = \arg\max_{c_k} \left[ \log P(c_k) + \sum_{i=1}^{d} \log P(x_i|c_k) \right]$$</p><p>这个公式不仅数值稳定，而且将乘法转化为加法，计算更加高效。</p><h3 id=33-三种常见的概率模型>3.3 三种常见的概率模型<a hidden class=anchor aria-hidden=true href=#33-三种常见的概率模型>#</a></h3><p>根据特征类型的不同，朴素贝叶斯有三种常见的概率模型。</p><h4 id=331-多项式朴素贝叶斯multinomial-naive-bayes>3.3.1 多项式朴素贝叶斯（Multinomial Naive Bayes）<a hidden class=anchor aria-hidden=true href=#331-多项式朴素贝叶斯multinomial-naive-bayes>#</a></h4><p>适用于<strong>计数型特征</strong>（Count Features），如文本分类中的词频统计。</p><p>在类别 $c_k$ 中，特征 $x_i$ 的概率分布建模为多项式分布：</p><p>$$P(x_i|c_k) = \frac{n_{ik} + \alpha}{N_k + \alpha d}$$</p><p>其中：</p><ul><li>$n_{ik}$ 是类别 $c_k$ 中特征 $x_i$ 的总计数</li><li>$N_k$ 是类别 $c_k$ 中所有特征的总计数</li><li>$\alpha$ 是平滑参数（Laplace 平滑），通常取 $\alpha = 1$</li></ul><p>平滑参数的作用是避免零概率问题：如果某个特征在训练数据的某个类别中从未出现，不加平滑会导致概率为零，从而整个乘积为零。</p><h4 id=332-伯努利朴素贝叶斯bernoulli-naive-bayes>3.3.2 伯努利朴素贝叶斯（Bernoulli Naive Bayes）<a hidden class=anchor aria-hidden=true href=#332-伯努利朴素贝叶斯bernoulli-naive-bayes>#</a></h4><p>适用于<strong>二值特征</strong>（Binary Features），如"词是否出现"。</p><p>每个特征 $x_i$ 只能取 0 或 1，其条件概率为：</p><p>$$P(x_i|c_k) = p_{ik}^{x_i} (1-p_{ik})^{1-x_i}$$</p><p>其中 $p_{ik}$ 是类别 $c_k$ 中特征 $x_i$ 取值为 1 的概率。</p><h4 id=333-高斯朴素贝叶斯gaussian-naive-bayes>3.3.3 高斯朴素贝叶斯（Gaussian Naive Bayes）<a hidden class=anchor aria-hidden=true href=#333-高斯朴素贝叶斯gaussian-naive-bayes>#</a></h4><p>适用于<strong>连续型特征</strong>（Continuous Features）。</p><p>假设在类别 $c_k$ 中，特征 $x_i$ 服从高斯分布：</p><p>$$P(x_i|c_k) = \frac{1}{\sqrt{2\pi}\sigma_{ik}} \exp\left[ -\frac{(x_i - \mu_{ik})^2}{2\sigma_{ik}^2} \right]$$</p><p>其中 $\mu_{ik}$ 和 $\sigma_{ik}^2$ 分别是类别 $c_k$ 中特征 $x_i$ 的均值和方差。</p><h2 id=第四章垃圾邮件过滤的实践应用>第四章：垃圾邮件过滤的实践应用<a hidden class=anchor aria-hidden=true href=#第四章垃圾邮件过滤的实践应用>#</a></h2><p>让我们通过一个具体的例子来理解朴素贝叶斯的工作原理。</p><h3 id=41-问题建模>4.1 问题建模<a hidden class=anchor aria-hidden=true href=#41-问题建模>#</a></h3><p>假设我们要构建一个垃圾邮件过滤器。每封邮件表示为一个词袋（Bag of Words），只关注词的出现与否，不关心词序和语法。</p><p>定义：</p><ul><li>类别：$c_0$（正常邮件），$c_1$（垃圾邮件）</li><li>特征：$x_i = 1$ 表示词 $w_i$ 在邮件中出现，$x_i = 0$ 表示不出现</li><li>词汇表大小：$d$</li></ul><h3 id=42-训练阶段>4.2 训练阶段<a hidden class=anchor aria-hidden=true href=#42-训练阶段>#</a></h3><p>在训练集中，我们需要估计以下参数：</p><ol><li><p><strong>先验概率</strong>：
$$P(c_1) = \frac{\text{垃圾邮件数量}}{\text{总邮件数量}}$$
$$P(c_0) = 1 - P(c_1)$$</p></li><li><p><strong>条件概率</strong>（对每个词 $w_i$）：
$$P(x_i=1|c_1) = \frac{\text{包含词 } w_i \text{ 的垃圾邮件数} + \alpha}{\text{垃圾邮件总数} + \alpha d}$$
$$P(x_i=1|c_0) = \frac{\text{包含词 } w_i \text{ 的正常邮件数} + \alpha}{\text{正常邮件总数} + \alpha d}$$</p></li></ol><h3 id=43-预测阶段>4.3 预测阶段<a hidden class=anchor aria-hidden=true href=#43-预测阶段>#</a></h3><p>给定一封新邮件，计算它属于垃圾邮件的对数后验概率：</p><p>$$\log P(c_1|\mathbf{x}) \propto \log P(c_1) + \sum_{i: x_i=1} \log P(x_i=1|c_1) + \sum_{i: x_i=0} \log [1-P(x_i=1|c_1)]$$</p><p>同样计算 $\log P(c_0|\mathbf{x})$，比较大小，选择概率较大的类别。</p><h3 id=44-一个数值例子>4.4 一个数值例子<a hidden class=anchor aria-hidden=true href=#44-一个数值例子>#</a></h3><p>假设词汇表只有三个词：{viagra, lottery, hello}，训练数据如下：</p><ul><li>垃圾邮件：100 封，其中：<ul><li>60 封包含 &ldquo;viagra&rdquo;</li><li>50 封包含 &ldquo;lottery&rdquo;</li><li>10 封包含 &ldquo;hello&rdquo;</li></ul></li><li>正常邮件：400 封，其中：<ul><li>5 封包含 &ldquo;viagra&rdquo;</li><li>10 封包含 &ldquo;lottery&rdquo;</li><li>300 封包含 &ldquo;hello&rdquo;</li></ul></li></ul><p><strong>先验概率</strong>：
$$P(c_1) = \frac{100}{500} = 0.2, \quad P(c_0) = 0.8$$</p><p><strong>条件概率</strong>（使用 Laplace 平滑 $\alpha=1$）：</p><p>| 词 | $P(x_i=1|c_1)$ | $P(x_i=1|c_0)$ |
|&mdash;|&mdash;|&mdash;|
| viagra | $\frac{60+1}{100+3} \approx 0.592$ | $\frac{5+1}{400+3} \approx 0.015$ |
| lottery | $\frac{50+1}{100+3} \approx 0.495$ | $\frac{10+1}{400+3} \approx 0.027$ |
| hello | $\frac{10+1}{100+3} \approx 0.107$ | $\frac{300+1}{400+3} \approx 0.747$ |</p><p><strong>预测一封新邮件</strong>：内容为 &ldquo;viagra lottery hello&rdquo;</p><p>计算对数后验概率：</p><p>$$\begin{aligned}
\log P(c_1|\mathbf{x}) &\propto \log 0.2 + \log 0.592 + \log 0.495 + \log 0.107 \
&= -1.609 - 0.524 - 0.703 - 2.236 \
&= -5.072
\end{aligned}$$</p><p>$$\begin{aligned}
\log P(c_0|\mathbf{x}) &\propto \log 0.8 + \log 0.015 + \log 0.027 + \log 0.747 \
&= -0.223 - 4.200 - 3.612 - 0.292 \
&= -8.327
\end{aligned}$$</p><p>因为 $-5.072 > -8.327$，所以分类为<strong>垃圾邮件</strong>。</p><p>这个例子展示了朴素贝叶斯的工作机制：即使 &ldquo;hello&rdquo; 更常出现在正常邮件中，但 &ldquo;viagra&rdquo; 和 &ldquo;lottery&rdquo; 这两个词的垃圾邮件指示性太强，足以压倒 &ldquo;hello&rdquo; 的正常邮件指示性。</p><p>为了更直观地理解朴素贝叶斯的网络结构，下图展示了邮件分类的贝叶斯网络：</p><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>graph TD
C["类别 C"]
X1["特征 X1: viagra"]
X2["特征 X2: lottery"]
X3["特征 X3: hello"]
X4["特征 X4: ..."]
Xd["特征 Xd"]
C --> X1
C --> X2
C --> X3
C --> X4
C --> Xd
classDef blueNode fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff
classDef greenNode fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
class C blueNode
class X1,X2,X3,X4,Xd greenNode</div></div><p>在这个网络中，类别节点 $C$ 是唯一的父节点，所有特征节点 $X_i$ 只依赖于类别，而特征之间相互独立。这就是朴素贝叶斯的"朴素"假设在网络结构上的体现。</p><h2 id=第五章贝叶斯网络>第五章：贝叶斯网络<a hidden class=anchor aria-hidden=true href=#第五章贝叶斯网络>#</a></h2><p>朴素贝叶斯假设所有特征相互独立，这过于简化。贝叶斯网络（Bayesian Network）是更一般化的概率图模型，它允许特征之间存在有限的依赖关系。</p><h3 id=51-贝叶斯网络的定义>5.1 贝叶斯网络的定义<a hidden class=anchor aria-hidden=true href=#51-贝叶斯网络的定义>#</a></h3><p>贝叶斯网络是一个有向无环图（DAG），其中：</p><ul><li>每个节点表示一个随机变量</li><li>边表示变量之间的直接依赖关系</li><li>每个节点有一个条件概率表（CPT），给定其父节点的取值</li></ul><h3 id=52-联合概率的因式分解>5.2 联合概率的因式分解<a hidden class=anchor aria-hidden=true href=#52-联合概率的因式分解>#</a></h3><p>贝叶斯网络的核心优势在于：联合概率分布可以因式分解为条件概率的乘积：</p><p>$$P(X_1, X_2, \ldots, X_n) = \prod_{i=1}^{n} P(X_i|\text{Pa}(X_i))$$</p><p>其中 $\text{Pa}(X_i)$ 表示节点 $X_i$ 的父节点集合。</p><p>这种因式分解大大减少了需要估计的参数数量。例如，一个有 $n$ 个变量的全连接网络需要 $O(2^n)$ 个参数，而一个树状结构的贝叶斯网络只需要 $O(n)$ 个参数。</p><h3 id=53-推理与学习>5.3 推理与学习<a hidden class=anchor aria-hidden=true href=#53-推理与学习>#</a></h3><p>贝叶斯网络的核心任务有两个：</p><ol><li><p><strong>推理（Inference）</strong>：给定部分变量的观测值，推断其他变量的后验概率</p><ul><li>精确推理：变量消元法、信念传播</li><li>近似推理：MCMC 采样、变分推断</li></ul></li><li><p><strong>学习（Learning）</strong>：从数据中学习网络结构和参数</p><ul><li>参数学习：最大似然估计、贝叶斯估计</li><li>结构学习：基于评分的方法、约束方法</li></ul></li></ol><h3 id=54-应用领域>5.4 应用领域<a hidden class=anchor aria-hidden=true href=#54-应用领域>#</a></h3><p>贝叶斯网络的应用非常广泛：</p><ul><li><strong>医疗诊断</strong>：症状、疾病、病因之间的关系建模</li><li><strong>故障诊断</strong>：设备故障的因果推理</li><li><strong>推荐系统</strong>：用户偏好、商品特征之间的依赖关系</li><li><strong>自然语言处理</strong>：句法分析、语义理解</li></ul><h2 id=结语贝叶斯思想的不朽价值>结语：贝叶斯思想的不朽价值<a hidden class=anchor aria-hidden=true href=#结语贝叶斯思想的不朽价值>#</a></h2><p>贝叶斯分类器不仅仅是一种算法，更是一种思维方式。它教导我们：</p><ol><li><p><strong>先验知识的重要性</strong>：我们不应完全依赖数据，而应该结合先验知识。这与人类的学习方式一致——我们带着已有的经验去理解新事物。</p></li><li><p><strong>不确定性是常态</strong>：世界充满了不确定性，我们不应该追求绝对的确定性，而应该在不确定性中做出最优决策。</p></li><li><p><strong>模型的简洁性</strong>：朴素贝叶斯的"天真"假设虽然在数学上不成立，但在实践中却屡建奇功。这提醒我们：一个好的模型不一定是最精确的，而是最能在效果和效率之间取得平衡的。</p></li><li><p><strong>概率论是智能的数学语言</strong>：无论是从哲学层面还是技术层面，概率论都为我们提供了理解和建模不确定性的强大工具。</p></li></ol><p>在深度学习大行其道的今天，贝叶斯分类器及其衍生方法（变分推断、贝叶斯神经网络等）依然保持着旺盛的生命力。它们不仅在理论上的优雅令人着迷，更在实践中的可靠性让人信赖。</p><p>正如统计学家 George Box 所说：&ldquo;所有模型都是错的，但有些是有用的。&rdquo; 贝叶斯分类器就是这样一类"有用"的模型——它用简洁的数学形式，捕捉了分类问题的本质，为我们提供了一套在不确定世界中理性决策的工具。</p><hr><p><strong>参考文献</strong>：</p><ol><li>Bishop, C. M. (2006). <em>Pattern Recognition and Machine Learning</em>. Springer.</li><li>Murphy, K. P. (2012). <em>Machine Learning: A Probabilistic Perspective</em>. MIT Press.</li><li>Duda, R. O., Hart, P. E., & Stork, D. G. (2000). <em>Pattern Classification</em>. Wiley.</li><li>Russell, S., & Norvig, P. (2020). <em>Artificial Intelligence: A Modern Approach</em>. Pearson.</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%AE%97%E6%B3%95/>算法</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%BB%BC%E8%BF%B0/>综述</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-24-gmm-comprehensive-guide/><span class=title>« Prev</span><br><span>高斯混合模型：从数据中解构隐藏结构的艺术</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-24-pca-comprehensive-guide/><span class=title>Next »</span><br><span>PCA 主成分分析：从数据降维的优雅艺术</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 贝叶斯分类器：从条件概率到智能决策的优雅之旅 on x" href="https://x.com/intent/tweet/?text=%e8%b4%9d%e5%8f%b6%e6%96%af%e5%88%86%e7%b1%bb%e5%99%a8%ef%bc%9a%e4%bb%8e%e6%9d%a1%e4%bb%b6%e6%a6%82%e7%8e%87%e5%88%b0%e6%99%ba%e8%83%bd%e5%86%b3%e7%ad%96%e7%9a%84%e4%bc%98%e9%9b%85%e4%b9%8b%e6%97%85&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-24-bayesian-classifier%2f&amp;hashtags=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%2c%e7%ae%97%e6%b3%95%2c%e7%bb%bc%e8%bf%b0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 贝叶斯分类器：从条件概率到智能决策的优雅之旅 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-24-bayesian-classifier%2f&amp;title=%e8%b4%9d%e5%8f%b6%e6%96%af%e5%88%86%e7%b1%bb%e5%99%a8%ef%bc%9a%e4%bb%8e%e6%9d%a1%e4%bb%b6%e6%a6%82%e7%8e%87%e5%88%b0%e6%99%ba%e8%83%bd%e5%86%b3%e7%ad%96%e7%9a%84%e4%bc%98%e9%9b%85%e4%b9%8b%e6%97%85&amp;summary=%e8%b4%9d%e5%8f%b6%e6%96%af%e5%88%86%e7%b1%bb%e5%99%a8%ef%bc%9a%e4%bb%8e%e6%9d%a1%e4%bb%b6%e6%a6%82%e7%8e%87%e5%88%b0%e6%99%ba%e8%83%bd%e5%86%b3%e7%ad%96%e7%9a%84%e4%bc%98%e9%9b%85%e4%b9%8b%e6%97%85&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-24-bayesian-classifier%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 贝叶斯分类器：从条件概率到智能决策的优雅之旅 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-24-bayesian-classifier%2f&title=%e8%b4%9d%e5%8f%b6%e6%96%af%e5%88%86%e7%b1%bb%e5%99%a8%ef%bc%9a%e4%bb%8e%e6%9d%a1%e4%bb%b6%e6%a6%82%e7%8e%87%e5%88%b0%e6%99%ba%e8%83%bd%e5%86%b3%e7%ad%96%e7%9a%84%e4%bc%98%e9%9b%85%e4%b9%8b%e6%97%85"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 贝叶斯分类器：从条件概率到智能决策的优雅之旅 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-24-bayesian-classifier%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var n,t=e.innerHTML,s=!1;for(t.indexOf("<em>")!==-1&&(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("</em>")!==-1&&(n=t.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),n=n.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("<em>$")!==-1&&(n=t.replace(/<em>\$/g,"$"),n!==t&&(t=n,s=!0)),t.indexOf("</em>$")!==-1&&(n=t.replace(/<\/em>\$/g,"$"),n!==t&&(t=n,s=!0));t.indexOf("<em>")!==-1&&t.indexOf("</em>")!==-1;){if(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n===t)break;t=n,s=!0}t.indexOf("_{")!==-1&&(n=t.replace(/_{/g,"_{"),n!==t&&(t=n,s=!0)),t.indexOf("}_")!==-1&&(n=t.replace(/}_/g,"}"),n!==t&&(t=n,s=!0)),t.indexOf("\\\\")!==-1&&(n=t.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),n!==t&&(t=n,s=!0)),s&&(e.innerHTML=t)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var n,t=e.innerHTML,s=!1;for(t.indexOf("<em>")!==-1&&(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("</em>")!==-1&&(n=t.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),n=n.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("<em>$")!==-1&&(n=t.replace(/<em>\$/g,"$"),n!==t&&(t=n,s=!0)),t.indexOf("</em>$")!==-1&&(n=t.replace(/<\/em>\$/g,"$"),n!==t&&(t=n,s=!0));t.indexOf("<em>")!==-1&&t.indexOf("</em>")!==-1;){if(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n===t)break;t=n,s=!0}t.indexOf("_{")!==-1&&(n=t.replace(/_{/g,"_{"),n!==t&&(t=n,s=!0)),t.indexOf("}_")!==-1&&(n=t.replace(/}_/g,"}"),n!==t&&(t=n,s=!0)),t.indexOf("\\\\")!==-1&&(n=t.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),n!==t&&(t=n,s=!0)),s&&(e.innerHTML=t)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>