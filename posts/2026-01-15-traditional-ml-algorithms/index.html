<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>深度学习前夜：十大传统机器学习算法的历史与数学之美 | s-ai-unix's Blog</title><meta name=keywords content="机器学习,算法,数学史"><meta name=description content="回顾机器学习黄金时代，详细推导十大经典算法的数学原理，从线性回归到主成分分析"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-15-traditional-ml-algorithms/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-15-traditional-ml-algorithms/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-15-traditional-ml-algorithms/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="深度学习前夜：十大传统机器学习算法的历史与数学之美"><meta property="og:description" content="回顾机器学习黄金时代，详细推导十大经典算法的数学原理，从线性回归到主成分分析"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-15T22:30:00+08:00"><meta property="article:modified_time" content="2026-01-15T22:30:00+08:00"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="算法"><meta property="article:tag" content="数学史"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/ml-algorithms-legacy.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/ml-algorithms-legacy.jpg"><meta name=twitter:title content="深度学习前夜：十大传统机器学习算法的历史与数学之美"><meta name=twitter:description content="回顾机器学习黄金时代，详细推导十大经典算法的数学原理，从线性回归到主成分分析"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"深度学习前夜：十大传统机器学习算法的历史与数学之美","item":"https://s-ai-unix.github.io/posts/2026-01-15-traditional-ml-algorithms/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"深度学习前夜：十大传统机器学习算法的历史与数学之美","name":"深度学习前夜：十大传统机器学习算法的历史与数学之美","description":"回顾机器学习黄金时代，详细推导十大经典算法的数学原理，从线性回归到主成分分析","keywords":["机器学习","算法","数学史"],"articleBody":"引言：黄金时代 想象一下 2006 年的秋天，深度学习尚未兴起。那时的机器学习领域正经历着一场静悄悄的革命。统计学习方法、核方法、集成学习层出不穷，数学家们用优雅的公式编织着智能的梦想。\n那时，人们相信：只要数据足够、特征工程足够细致，我们就能教机器做任何事。这种信念催生了一批经典算法——它们或许不如今天的深度神经网络那样炫目，但每一款都凝聚着数学家的智慧，每一步推导都闪耀着逻辑的光辉。\n今天，我们回顾这段黄金时代，讲述十个改变了世界的传统机器学习算法的故事。但这次，让我们放慢脚步，亲手推导每一步，感受数学的力量。\n一、线性回归：回归分析的鼻祖 时间：1795 年 - 阿德里安-马里·勒让德 (Adrien-Marie Legendre)\n历史的偶然 1795 年，法国天文学家勒让德正在为一个问题头疼：如何用最简单的方法拟合行星轨道数据？他需要找到一条直线，让所有数据点到这条直线的距离平方和最小。\n这就是最小二乘法的诞生。\n推导过程 让我们从最简单的情况开始。假设我们有 $n$ 个数据点 $(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)$，想要找到一条直线 $y = w_0 + w_1 x$ 来拟合这些数据。\n第一步：定义误差\n对于每个数据点 $(x_i, y_i)$，我们的预测值是 $\\hat{y}_i = w_0 + w_1 x_i$，误差就是观测值和预测值的差：\n$$ e_i = y_i - \\hat{y}_i = y_i - (w_0 + w_1 x_i) $$\n第二步：定义损失函数\n为什么是平方误差？勒让德选择平方误差有几个好处：\n非负：平方后总是非负 可导：处处光滑，便于优化 凸函数：只有一个最小值 损失函数定义为：\n$$ L(w_0, w_1) = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} [y_i - (w_0 + w_1 x_i)]^2 $$\n第三步：求偏导\n为了找到最小值，我们对 $w_0$ 和 $w_1$ 分别求偏导：\n$$ \\frac{\\partial L}{\\partial w_0} = \\sum_{i=1}^{n} 2[y_i - (w_0 + w_1 x_i)] \\cdot (-1) = -2 \\sum_{i=1}^{n} [y_i - (w_0 + w_1 x_i)] $$\n$$ \\frac{\\partial L}{\\partial w_1} = \\sum_{i=1}^{n} 2[y_i - (w_0 + w_1 x_i)] \\cdot (-x_i) = -2 \\sum_{i=1}^{n} x_i [y_i - (w_0 + w_1 x_i)] $$\n第四步：令偏导为零\n$$ \\begin{align} \\frac{\\partial L}{\\partial w_0} \u0026= 0 \\Rightarrow \\sum_{i=1}^{n} [y_i - (w_0 + w_1 x_i)] = 0 \\ \u0026\\Rightarrow \\sum_{i=1}^{n} y_i - n w_0 - w_1 \\sum_{i=1}^{n} x_i = 0 \\ \u0026\\Rightarrow n w_0 + w_1 \\sum_{i=1}^{n} x_i = \\sum_{i=1}^{n} y_i \\end{align} $$\n$$ \\begin{align} \\frac{\\partial L}{\\partial w_1} \u0026= 0 \\Rightarrow \\sum_{i=1}^{n} x_i [y_i - (w_0 + w_1 x_i)] = 0 \\ \u0026\\Rightarrow \\sum_{i=1}^{n} x_i y_i - w_0 \\sum_{i=1}^{n} x_i - w_1 \\sum_{i=1}^{n} x_i^2 = 0 \\ \u0026\\Rightarrow w_0 \\sum_{i=1}^{n} x_i + w_1 \\sum_{i=1}^{n} x_i^2 = \\sum_{i=1}^{n} x_i y_i \\end{align} $$\n第五步：解线性方程组\n记 $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$，$\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$，从第一个方程：\n$$ w_0 = \\bar{y} - w_1 \\bar{x} $$\n代入第二个方程：\n$$ \\begin{align} (\\bar{y} - w_1 \\bar{x}) \\sum_{i=1}^{n} x_i + w_1 \\sum_{i=1}^{n} x_i^2 \u0026= \\sum_{i=1}^{n} x_i y_i \\ \\bar{y} n \\bar{x} - w_1 n \\bar{x}^2 + w_1 \\sum_{i=1}^{n} x_i^2 \u0026= \\sum_{i=1}^{n} x_i y_i \\ w_1 \\left(\\sum_{i=1}^{n} x_i^2 - n \\bar{x}^2\\right) \u0026= \\sum_{i=1}^{n} x_i y_i - n \\bar{x} \\bar{y} \\ w_1 \u0026= \\frac{\\sum_{i=1}^{n} x_i y_i - n \\bar{x} \\bar{y}}{\\sum_{i=1}^{n} x_i^2 - n \\bar{x}^2} \\end{align} $$\n这就是著名的最小二乘估计。\n矩阵形式 对于多元线性回归，我们有 $d$ 个特征。设 $\\mathbf{x}i = (1, x{i,1}, x_{i,2}, \\ldots, x_{i,d})^T$ 是增广特征向量，$\\mathbf{w} = (w_0, w_1, \\ldots, w_d)^T$ 是参数向量。\n损失函数写为：\n$$ L(\\mathbf{w}) = \\sum_{i=1}^{n} (y_i - \\mathbf{w}^T \\mathbf{x}_i)^2 = |\\mathbf{y} - \\mathbf{X}\\mathbf{w}|^2 $$\n其中 $\\mathbf{X} = \\begin{pmatrix} \\mathbf{x}_1^T \\ \\mathbf{x}_2^T \\ \\vdots \\ \\mathbf{x}_n^T \\end{pmatrix}$ 是设计矩阵，$\\mathbf{y} = \\begin{pmatrix} y_1 \\ y_2 \\ \\vdots \\ y_n \\end{pmatrix}$ 是响应向量。\n展开损失函数：\n$$ \\begin{align} L(\\mathbf{w}) \u0026= (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^T (\\mathbf{y} - \\mathbf{X}\\mathbf{w}) \\ \u0026= \\mathbf{y}^T \\mathbf{y} - \\mathbf{y}^T \\mathbf{X}\\mathbf{w} - \\mathbf{w}^T \\mathbf{X}^T \\mathbf{y} + \\mathbf{w}^T \\mathbf{X}^T \\mathbf{X} \\mathbf{w} \\end{align} $$\n注意 $\\mathbf{y}^T \\mathbf{X}\\mathbf{w}$ 是标量，等于其转置 $\\mathbf{w}^T \\mathbf{X}^T \\mathbf{y}$，因此：\n$$ L(\\mathbf{w}) = \\mathbf{y}^T \\mathbf{y} - 2 \\mathbf{w}^T \\mathbf{X}^T \\mathbf{y} + \\mathbf{w}^T \\mathbf{X}^T \\mathbf{X} \\mathbf{w} $$\n求梯度：\n$$ \\begin{align} \\nabla_{\\mathbf{w}} L(\\mathbf{w}) \u0026= \\nabla_{\\mathbf{w}} (\\mathbf{y}^T \\mathbf{y}) - 2 \\nabla_{\\mathbf{w}} (\\mathbf{w}^T \\mathbf{X}^T \\mathbf{y}) + \\nabla_{\\mathbf{w}} (\\mathbf{w}^T \\mathbf{X}^T \\mathbf{X} \\mathbf{w}) \\ \u0026= 0 - 2 \\mathbf{X}^T \\mathbf{y} + 2 \\mathbf{X}^T \\mathbf{X} \\mathbf{w} \\end{align} $$\n令梯度为零：\n$$ \\mathbf{X}^T \\mathbf{X} \\mathbf{w} = \\mathbf{X}^T \\mathbf{y} $$\n这就是著名的正规方程（Normal Equation）。如果 $\\mathbf{X}^T \\mathbf{X}$ 可逆，解为：\n$$ \\mathbf{w}^{\\ast} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y} $$\n几何直观 从几何上看，$\\mathbf{y}$ 在列空间 $\\mathcal{C}(\\mathbf{X})$ 上的投影是：\n$$ \\hat{\\mathbf{y}} = \\mathbf{X} \\mathbf{w}^{\\ast} = \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y} = \\mathbf{H} \\mathbf{y} $$\n其中 $\\mathbf{H} = \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T$ 是帽子矩阵（hat matrix）。它把 $\\mathbf{y}$ “戴上了帽子”。\n残差 $\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}} = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}$ 与列空间正交：\n$$ \\mathbf{X}^T \\mathbf{e} = \\mathbf{X}^T (\\mathbf{I} - \\mathbf{H})\\mathbf{y} = \\mathbf{X}^T \\mathbf{y} - \\mathbf{X}^T \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y} = \\mathbf{X}^T \\mathbf{y} - \\mathbf{X}^T \\mathbf{y} = \\mathbf{0} $$\n这就是正交投影的数学表达！\n二、逻辑回归：从天文学到生物学的跨界 时间：1958 年 - 大卫·考克斯 (David Cox)\n跨界的灵感 1958 年，英国统计学家大卫·考克斯遇到了一个新问题：如何预测二元变量的概率？传统的线性回归给出的是实数值，但概率必须在 $[0, 1]$ 之间。\n考克斯灵机一动，想到了 Sigmoid 函数。\n推导过程 第一步：理解二分类问题\n给定 $n$ 个样本 $(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\ldots, (\\mathbf{x}_n, y_n)$，其中 $\\mathbf{x}_i \\in \\mathbb{R}^d$，$y_i \\in {0, 1}$。我们想要学习一个模型 $f: \\mathbb{R}^d \\to [0, 1]$，使得 $f(\\mathbf{x})$ 表示 $P(y=1|\\mathbf{x})$。\n第二步：为什么不能直接用线性回归？\n如果用线性回归 $y = \\mathbf{w}^T \\mathbf{x}$，输出可以是任意实数，但概率必须满足 $0 \\leq p \\leq 1$。\n第三步：引入 Sigmoid 函数\nSigmoid 函数定义为：\n$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} = \\frac{e^z}{1 + e^z} $$\n它的性质：\n当 $z \\to -\\infty$，$\\sigma(z) \\to 0$ 当 $z \\to +\\infty$，$\\sigma(z) \\to 1$ 当 $z = 0$，$\\sigma(z) = 0.5$ 因此，我们定义：\n$$ p(\\mathbf{x}) = P(y=1|\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x}) = \\frac{1}{1 + e^{-\\mathbf{w}^T \\mathbf{x}}} $$\n第四步：导出似然函数\n对于单个样本 $(\\mathbf{x}_i, y_i)$，其概率可以统一写成：\n$$ P(y_i|\\mathbf{x}_i, \\mathbf{w}) = p(\\mathbf{x}_i)^{y_i} (1 - p(\\mathbf{x}_i))^{1 - y_i} $$\n验证：\n若 $y_i = 1$：$P(y_i=1|\\mathbf{x}_i) = p(\\mathbf{x}_i) \\cdot (1-p(\\mathbf{x}_i))^0 = p(\\mathbf{x}_i)$ ✓ 若 $y_i = 0$：$P(y_i=0|\\mathbf{x}_i) = p(\\mathbf{x}_i)^0 \\cdot (1-p(\\mathbf{x}_i))^1 = 1 - p(\\mathbf{x}_i)$ ✓ 假设样本独立同分布，联合概率（似然）为：\n$$ \\mathcal{L}(\\mathbf{w}) = \\prod_{i=1}^{n} P(y_i|\\mathbf{x}i, \\mathbf{w}) = \\prod{i=1}^{n} p(\\mathbf{x}_i)^{y_i} (1 - p(\\mathbf{x}_i))^{1 - y_i} $$\n第五步：取对数得到对数似然\n取对数简化计算：\n$$ \\begin{align} \\ell(\\mathbf{w}) = \\log \\mathcal{L}(\\mathbf{w}) \u0026= \\sum_{i=1}^{n} \\left[ y_i \\log p(\\mathbf{x}_i) + (1 - y_i) \\log(1 - p(\\mathbf{x}_i)) \\right] \\end{align} $$\n第六步：计算梯度\n我们需要计算 $\\frac{\\partial \\ell}{\\partial w}$。首先计算 $\\frac{\\partial p(x)}{\\partial w}$：\n$$ \\begin{align} p(x) \u0026= \\frac{1}{1 + e^{-w^T x}} \\ \\frac{\\partial p(x)}{\\partial w} \u0026= -\\frac{1}{(1 + e^{-w^T x})^2} \\cdot \\frac{\\partial}{\\partial w} (1 + e^{-w^T x}) \\ \u0026= -\\frac{1}{(1 + e^{-w^T x})^2} \\cdot e^{-w^T x} \\cdot (-x) \\ \u0026= \\frac{e^{-w^T x}}{(1 + e^{-w^T x})^2} x \\end{align} $$\n注意到：\n$$ p(x)(1 - p(x)) = \\frac{1}{1 + e^{-w^T x}} \\cdot \\frac{e^{-w^T x}}{1 + e^{-w^T x}} = \\frac{e^{-w^T x}}{(1 + e^{-w^T x})^2} $$\n因此：\n$$ \\frac{\\partial p(x)}{\\partial w} = p(x)(1 - p(x)) x $$\n这是一个非常优雅的结论！\n第七步：计算对数似然的梯度\n$$ \\frac{\\partial \\ell}{\\partial w} = \\sum_{i=1}^{n} (y_i - p(x_i)) x_i $$\n这就是逻辑回归的梯度公式！\n第八步：梯度上升法\n由于我们要最大化对数似然，使用梯度上升：\n$$ w_{t+1} = w_t + \\eta \\sum_{i=1}^{n} (y_i - p_t(x_i)) x_i $$\n其中 $\\eta$ 是学习率。\nLogit 变换 我们也可以从另一个角度理解逻辑回归。定义 logit 变换：\n$$ \\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right) $$\n对于逻辑回归：\n$$ \\begin{align} \\text{logit}(P(y=1|\\mathbf{x})) \u0026= \\ln\\left(\\frac{P(y=1|\\mathbf{x})}{1 - P(y=1|\\mathbf{x})}\\right) \\ \u0026= \\ln\\left(\\frac{\\sigma(\\mathbf{w}^T \\mathbf{x})}{1 - \\sigma(\\mathbf{w}^T \\mathbf{x})}\\right) \\ \u0026= \\ln\\left(\\frac{\\frac{1}{1 + e^{-\\mathbf{w}^T \\mathbf{x}}}}{\\frac{e^{-\\mathbf{w}^T \\mathbf{x}}}{1 + e^{-\\mathbf{w}^T \\mathbf{x}}}}\\right) \\ \u0026= \\ln(e^{\\mathbf{w}^T \\mathbf{x}}) \\ \u0026= \\mathbf{w}^T \\mathbf{x} \\end{align} $$\n这表明：logit 变换后，概率的对数几率（log-odds）与特征呈线性关系。\n三、朴素贝叶斯：两个世纪前的概率魔法 时间：1763 年 - 托马斯·贝叶斯（Thomas Bayes，定理发表）；1950 年代 - 机器学习应用\n延迟发表的天才 1763 年，英国牧师托马斯·贝叶斯去世两年后，他的朋友理查德·普赖斯整理并发表了他的一篇论文——《关于机会问题的解法》。\n推导过程 第一步：贝叶斯定理\n设 $D$ 为观测数据，$H$ 为假设。贝叶斯定理表述为：\n$$ P(H|D) = \\frac{P(D|H) P(H)}{P(D)} $$\n其中：\n$P(H)$ 是先验概率（prior） $P(D|H)$ 是似然（likelihood） $P(D)$ 是证据（evidence） $P(H|D)$ 是后验概率（posterior） 第二步：应用到分类问题\n对于分类问题，我们有类别 $y \\in {1, 2, \\ldots, C}$，特征 $\\mathbf{x} = (x_1, x_2, \\ldots, x_d)$。根据贝叶斯定理：\n$$ P(y|\\mathbf{x}) = \\frac{P(\\mathbf{x}|y) P(y)}{P(\\mathbf{x})} $$\n决策规则：\n$$ \\hat{y} = \\arg\\max_{y} P(y|\\mathbf{x}) = \\arg\\max_{y} \\frac{P(\\mathbf{x}|y) P(y)}{P(\\mathbf{x})} $$\n由于 $P(\\mathbf{x})$ 对所有类别都相同，可以忽略：\n$$ \\hat{y} = \\arg\\max_{y} P(\\mathbf{x}|y) P(y) $$\n第三步：朴素独立性假设\n$P(\\mathbf{x}|y)$ 的计算困难在于特征之间可能存在依赖关系。朴素贝叶斯做出条件独立性假设：\n$$ P(\\mathbf{x}|y) = P(x_1, x_2, \\ldots, x_d|y) = P(x_1|y) P(x_2|y) \\cdots P(x_d|y) = \\prod_{j=1}^{d} P(x_j|y) $$\n这个假设在现实世界中几乎从不成立，但效果出奇地好。\n第四步：分类决策\n$$ \\hat{y} = \\arg\\max_{y} P(y) \\prod_{j=1}^{d} P(x_j|y) $$\n高斯朴素贝叶斯 对于连续特征，常假设 $P(x_j|y)$ 服从高斯分布：\n$$ P(x_j|y=c) = \\frac{1}{\\sqrt{2\\pi \\sigma_{jc}^2}} \\exp\\left(-\\frac{(x_j - \\mu_{jc})^2}{2\\sigma_{jc}^2}\\right) $$\n参数估计（最大似然）：\n$$ \\hat{P}(y=c) = \\frac{n_c}{n} $$\n$$ \\hat{\\mu}{jc} = \\frac{1}{n_c} \\sum{i: y_i=c} x_{i,j} $$\n$$ \\hat{\\sigma}{jc}^2 = \\frac{1}{n_c} \\sum{i: y_i=c} (x_{i,j} - \\hat{\\mu}_{jc})^2 $$\n多项式朴素贝叶斯（文本分类） 对于文本分类，采用多项式分布。设词汇表大小为 $V$，词 $w$ 在类别 $c$ 中的计数为 $N_{wc}$，类别 $c$ 的总词数为 $N_c = \\sum_{w=1}^{V} N_{wc}$。\n使用拉普拉斯平滑（Laplace smoothing）：\n$$ P(w|c) = \\frac{N_{wc} + 1}{N_c + V} $$\n$$ P(c) = \\frac{\\sum_{w} N_{wc}}{\\sum_{c’, w} N_{w,c’}} $$\n为什么有效？ 虽然独立性假设不成立，但朴素贝叶斯经常表现良好，原因有：\n优化目标不同：我们关心的是分类准确率，而不是概率估计的精确性 去相关：即使特征相关，决策边界可能仍然正确 高维特性：在高维空间中，不同方向的特征对分类的贡献相对独立 四、K 近邻算法：最简单的记忆学习 时间：1951 年 - 伊芙琳·菲克斯 (Evelyn Fix) 和约瑟夫·霍奇斯 (Joseph Hodges)\n未发表的传奇 1951 年，加州大学伯克利分校的统计学家伊芙琳·菲克斯和约瑟夫·霍奇斯写了一篇论文，提出了一个极其简单的想法：要判断一个新样本属于哪一类，就看看训练数据中离它最近的 $k$ 个样本属于哪一类。\n推导过程 KNN 本质上不需要\"推导\"，但我们可以从风险最小化的角度理解它。\n第一步：1-NN 的渐近最优性\n假设数据分布 $P(\\mathbf{x}, y)$，1-NN 的预测是：\n$$ \\hat{y} = \\arg\\max_c P(c|\\text{NN}(\\mathbf{x})) $$\n其中 $\\text{NN}(\\mathbf{x})$ 是 $\\mathbf{x}$ 的最近邻。\n当训练数据量 $n \\to \\infty$ 时，最近邻 $\\text{NN}(\\mathbf{x})$ 会无限接近 $\\mathbf{x}$，因此：\n$$ \\lim_{n \\to \\infty} P(y|\\text{NN}(\\mathbf{x}) = c) = P(y=c|\\mathbf{x}) $$\n于是：\n$$ \\hat{y} = \\arg\\max_c P(y=c|\\mathbf{x}) $$\n这正是贝叶斯最优分类器！\n第二步：1-NN 的风险\n贝叶斯最优分类器的错误率是：\n$$ R^{\\ast} = 1 - \\sum_{x} P(x) \\max_c P(y=c|x) $$\n1-NN 的渐近错误率是：\n$$ R_{\\text{1NN}} = 2 \\sum_{x} P(x) P(y=\\hat{y}{}^{\\ast}|x) (1 - P(y=\\hat{y}{}^{\\ast}|x)) $$\n其中 $\\hat{y}_{}^{\\ast}$ 是贝叶斯最优预测。\n可以证明：$R^{\\ast} \\leq R_{\\text{1NN}} \\leq 2R^{\\ast}$。因此，1-NN 的错误率最多是贝叶斯最优的两倍。\n第三步：K 近邻的决策\n对于 K 近邻，我们投票：\n$$ \\hat{y} = \\arg\\max_{c} \\sum_{i=1}^{K} \\mathbb{I}(y_{(i)} = c) $$\n其中 $y_{(i)}$ 是第 $i$ 个最近邻的标签。\n也可以加权投票：\n$$ \\hat{y} = \\arg\\max_{c} \\sum_{i=1}^{K} w_i \\cdot \\mathbb{I}(y_{(i)} = c) $$\n其中权重 $w_i$ 通常是距离的倒数：$w_i = \\frac{1}{d(\\mathbf{x}, \\mathbf{x}_{(i)})}$。\n距离度量 欧几里得距离：\n$$ d(\\mathbf{x}, \\mathbf{x}’) = |\\mathbf{x} - \\mathbf{x}’|2 = \\sqrt{\\sum{j=1}^{d} (x_j - x’_j)^2} $$\n曼哈顿距离：\n$$ d(\\mathbf{x}, \\mathbf{x}’) = |\\mathbf{x} - \\mathbf{x}’|1 = \\sum{j=1}^{d} |x_j - x’_j| $$\n余弦相似度（常用于文本）：\n$$ s(\\mathbf{x}, \\mathbf{x}’) = \\frac{\\mathbf{x}^T \\mathbf{x}’}{|\\mathbf{x}| \\cdot |\\mathbf{x}’|} $$\n维度的诅咒 KNN 的问题在于高维空间。考虑超立方体 $[0, 1]^d$，边长为 $\\epsilon$ 的立方体体积是 $\\epsilon^d$。随着 $d$ 增加，即使是小的 $\\epsilon$，体积也趋于零。\n这意味着：在高维空间中，任何点之间的距离都趋于相同，最近邻的选择变得随机。解决方法：特征选择、降维（PCA、t-SNE）。\n五、决策树：从 Hunt 算法到 C4.5 时间：1960 年代 - Hunt 的算法；1986 年 - ID3；1993 年 - C4.5\n简单而强大的递归 决策树的思想非常直观：就像医生诊断疾病一样，通过一系列\"是/否\"的问题来逐步缩小可能性。\n推导过程 第一步：理解划分问题\n假设当前数据集为 $D$，我们要选择一个特征 $A$ 和一个划分方式，将 $D$ 划分为子集 ${D_1, D_2, \\ldots, D_k}$。目标是让每个子集尽可能\"纯\"（属于同一类）。\n第二步：纯度的度量——熵\n信息论中，香农熵定义为：\n$$ H(D) = -\\sum_{c=1}^{C} p_c \\log_2 p_c $$\n其中 $p_c = \\frac{|D_c|}{|D|}$ 是类别 $c$ 的比例。\n熵的性质：\n当所有样本属于同一类（$p_c = 1$ 对某个 $c$），$H(D) = 0$（最纯） 当各类均匀分布（$p_c = \\frac{1}{C}$），$H(D) = \\log_2 C$（最不纯） 第三步：条件熵\n如果用特征 $A$ 将数据集划分为 ${D_1, D_2, \\ldots, D_k}$，条件熵为：\n$$ H(D|A) = \\sum_{i=1}^{k} \\frac{|D_i|}{|D|} H(D_i) $$\n这是划分后各子集熵的加权平均。\n第四步：信息增益\n信息增益定义为熵的减少：\n$$ \\text{Gain}(D, A) = H(D) - H(D|A) $$\n信息增益越大，划分越有效。这就是 ID3 算法的标准。\n第五步：ID3 算法步骤\nID3(D, 特征集): 如果 D 中所有样本属于同一类 c: 返回叶子节点，标签为 c 如果 特征集 为空: 返回叶子节点，标签为 D 的多数类 选择信息增益最大的特征 A 根据特征的每个可能值 a，创建子节点 对每个子节点递归调用 ID3 第六步：信息增益率的问题\nID3 倾向于选择取值较多的特征（因为这样能产生更多的划分，条件熵更小）。为了解决这个问题，C4.5 使用信息增益率。\n首先计算特征 $A$ 的固有熵（intrinsic entropy）：\n$$ H_A(D) = -\\sum_{i=1}^{k} \\frac{|D_i|}{|D|} \\log_2 \\frac{|D_i|}{|D|} $$\n然后计算信息增益率：\n$$ \\text{GainRatio}(D, A) = \\frac{\\text{Gain}(D, A)}{H_A(D)} $$\n这样，取值多的特征虽然增益大，但固有熵也大，增益率不会特别高。\n第七步：CART 的基尼系数\nCART（Classification and Regression Trees）算法使用基尼系数（Gini index）：\n$$ \\text{Gini}(D) = 1 - \\sum_{c=1}^{C} p_c^2 = \\sum_{c=1}^{C} p_c (1 - p_c) $$\n基尼系数的含义是：随机抽取两个样本，它们属于不同类的概率。\n基尼系数越小，数据越纯。CART 选择使基尼系数减少最多的划分。\n第八步：剪枝\n决策树容易过拟合，需要剪枝。\n预剪枝（pre-pruning）：\n限制树的深度 限制叶子节点的最小样本数 限制划分所需的最小信息增益 后剪枝（post-pruning）：\n先生成完全生长的树 自底向上评估剪枝后的验证集误差 如果剪枝后误差不增加，则剪掉 决策边界 决策树的决策边界是轴对齐的（axis-aligned），即与坐标轴平行。这意味着决策边界是分段常数函数。这既是优点（可解释性），也是缺点（难以拟合对角线边界）。\n六、支持向量机：最大间隔的艺术 时间：1963 年 - 弗拉基米尔·万普尼克 (Vladimir Vapnik)；1992 年 - 核技巧\n冷战时期的智慧 1963 年，苏联数学家弗拉基米尔·万普尼克提出了一个革命性的想法：不要只关注分类错误，要关注分类边界到最近点的距离。\n推导过程 第一步：线性可分的情况\n假设数据集 ${(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n, y_n)}$ 线性可分，其中 $y_i \\in {-1, +1}$。我们要找一个超平面 $\\mathbf{w}^T \\mathbf{x} + b = 0$ 将两类分开。\n超平面的间隔定义为：\n$$ \\gamma = \\min_i \\frac{|y_i (\\mathbf{w}^T \\mathbf{x}_i + b)|}{|\\mathbf{w}|} $$\n我们要最大化这个间隔。\n第二步：间隔的规范化\n注意到如果 $(\\mathbf{w}, b)$ 是解，那么 $(k\\mathbf{w}, kb)$ 对任意 $k \u003e 0$ 也是解，因为：\n$$ k\\mathbf{w}^T \\mathbf{x} + kb = 0 \\iff \\mathbf{w}^T \\mathbf{x} + b = 0 $$\n且间隔为：\n$$ \\frac{|y_i (k\\mathbf{w}^T \\mathbf{x}_i + kb)|}{|k\\mathbf{w}|} = \\frac{k|y_i (\\mathbf{w}^T \\mathbf{x}_i + b)|}{k|\\mathbf{w}|} = \\frac{|y_i (\\mathbf{w}^T \\mathbf{x}_i + b)|}{|\\mathbf{w}|} $$\n因此，我们可以选择一个特定的尺度。选择让间隔边界的点满足：\n$$ y_i (\\mathbf{w}^T \\mathbf{x}_i + b) = 1 $$\n这些点就是支持向量（support vectors）。于是：\n$$ \\gamma = \\min_i \\frac{|y_i (\\mathbf{w}^T \\mathbf{x}_i + b)|}{|\\mathbf{w}|} = \\frac{1}{|\\mathbf{w}|} $$\n最大化间隔等价于最小化 $|\\mathbf{w}|$。\n第三步：原始问题\n因此，SVM 的优化问题是：\n$$ \\begin{align} \\min_{\\mathbf{w}, b} \\quad \u0026 \\frac{1}{2} |\\mathbf{w}|^2 \\ \\text{s.t.} \\quad \u0026 y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1, \\quad i = 1, \\ldots, n \\end{align} $$\n目标函数加 $\\frac{1}{2}$ 是为了求导方便（$|\\mathbf{w}|^2$ 的导数是 $2\\mathbf{w}$，乘 $\\frac{1}{2}$ 后导数是 $\\mathbf{w}$）。\n第四步：拉格朗日对偶\n引入拉格朗日乘子 $\\alpha_i \\geq 0$：\n$$ \\mathcal{L}(\\mathbf{w}, b, \\alpha) = \\frac{1}{2} |\\mathbf{w}|^2 - \\sum_{i=1}^{n} \\alpha_i [y_i (\\mathbf{w}^T \\mathbf{x}_i + b) - 1] $$\n对偶问题的第一步是对原始变量求极值：\n$$ \\min_{\\mathbf{w}, b} \\max_{\\alpha \\geq 0} \\mathcal{L}(\\mathbf{w}, b, \\alpha) $$\n先对 $\\mathbf{w}$ 求导：\n$$ \\nabla_{\\mathbf{w}} \\mathcal{L} = \\mathbf{w} - \\sum_{i=1}^{n} \\alpha_i y_i \\mathbf{x}i = \\mathbf{0} \\Rightarrow \\mathbf{w} = \\sum{i=1}^{n} \\alpha_i y_i \\mathbf{x}_i $$\n对 $b$ 求导：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b} = -\\sum_{i=1}^{n} \\alpha_i y_i = 0 \\Rightarrow \\sum_{i=1}^{n} \\alpha_i y_i = 0 $$\n将 $w$ 和约束代入：\n$$ \\mathcal{L}(\\alpha) = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j x_i^T x_j - \\sum_{i=1}^{n} \\alpha_i $$\n对偶问题是：\n$$ \\begin{align} \\max_{\\alpha} \\quad \u0026 \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}j \\ \\text{s.t.} \\quad \u0026 \\alpha_i \\geq 0, \\quad i = 1, \\ldots, n \\ \u0026 \\sum{i=1}^{n} \\alpha_i y_i = 0 \\end{align} $$\n第五步：预测\n训练完成后，预测为：\n$$ f(\\mathbf{x}) = \\text{sign}\\left(\\mathbf{w}^T \\mathbf{x} + b\\right) = \\text{sign}\\left(\\sum_{i=1}^{n} \\alpha_i y_i \\mathbf{x}_i^T \\mathbf{x} + b\\right) $$\n注意只有支持向量（$\\alpha_i \u003e 0$）起作用。\n第六步：软间隔\n实际数据可能不是线性可分的。引入松弛变量 $\\xi_i \\geq 0$：\n$$ \\begin{align} \\min_{\\mathbf{w}, b, \\xi} \\quad \u0026 \\frac{1}{2} |\\mathbf{w}|^2 + C \\sum_{i=1}^{n} \\xi_i \\ \\text{s.t.} \\quad \u0026 y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad i = 1, \\ldots, n \\ \u0026 \\xi_i \\geq 0, \\quad i = 1, \\ldots, n \\end{align} $$\n其中 $C$ 是惩罚参数，控制对错分类的容忍度。\n对偶问题形式相同，只是约束变为 $0 \\leq \\alpha_i \\leq C$。\n第七步：核技巧\n非线性可分怎么办？将数据映射到高维空间 $\\phi: \\mathbb{R}^d \\to \\mathbb{R}^D$（$D \\gg d$）。\n对偶问题变为：\n$$ \\max_{\\alpha} \\quad \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j \\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j) $$\n核技巧的魔法：我们不需要显式计算 $\\phi(\\mathbf{x})$，只需要知道内积。定义核函数：\n$$ K(\\mathbf{x}, \\mathbf{x}’) = \\phi(\\mathbf{x})^T \\phi(\\mathbf{x}’) $$\n于是：\n$$ \\max_{\\alpha} \\quad \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j) $$\n预测为：\n$$ f(\\mathbf{x}) = \\text{sign}\\left(\\sum_{i=1}^{n} \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b\\right) $$\n常用的核函数：\n线性核：$K(\\mathbf{x}, \\mathbf{x}’) = \\mathbf{x}^T \\mathbf{x}'$\n多项式核：$K(\\mathbf{x}, \\mathbf{x}’) = (\\mathbf{x}^T \\mathbf{x}’ + c)^d$\n高斯核（RBF）：$K(\\mathbf{x}, \\mathbf{x}’) = \\exp(-\\gamma |\\mathbf{x} - \\mathbf{x}’|^2)$\nSigmoid 核：$K(\\mathbf{x}, \\mathbf{x}’) = \\tanh(\\kappa \\mathbf{x}^T \\mathbf{x}’ + c)$\n为什么高斯核有效？\n考虑高斯核：\n$$ K(\\mathbf{x}, \\mathbf{x}’) = \\exp(-\\gamma |\\mathbf{x} - \\mathbf{x}’|^2) = \\exp\\left(-\\gamma \\sum_{j=1}^{d} (x_j - x’_j)^2\\right) $$\n使用泰勒展开：\n$$ \\exp(-\\gamma |\\mathbf{x} - \\mathbf{x}’|^2) = \\sum_{k=0}^{\\infty} \\frac{(-\\gamma)^k}{k!} |\\mathbf{x} - \\mathbf{x}’|^{2k} $$\n展开 $|\\mathbf{x} - \\mathbf{x}’|^{2k}$ 后，得到无限维的特征映射。因此，高斯核对应无限维的特征空间！\n七、K 均值聚类：迭代收敛的美学 时间：1957 年 - 雨果·斯坦因豪斯 (Hugo Steinhaus)；1965 年 - 劳埃德算法 (Lloyd’s Algorithm)\n聚类的启蒙 1957 年，波兰数学家雨果·斯坦因豪斯在研究\"平面上的点的集合\"问题时，提出了将点集划分为 $k$ 个簇的方法。\n推导过程 第一步：定义目标函数\n给定 $n$ 个数据点 ${\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n}$ 和 $k$ 个簇中心 ${\\mathbf{c}_1, \\mathbf{c}_2, \\ldots, \\mathbf{c}_k}$，我们要最小化簇内平方误差：\n$$ J = \\sum_{i=1}^{n} \\sum_{j=1}^{k} \\mathbb{I}(z_i = j) |\\mathbf{x}_i - \\mathbf{c}_j|^2 $$\n其中 $z_i \\in {1, 2, \\ldots, k}$ 是数据点 $\\mathbf{x}_i$ 的簇标签。\n第二步：交替优化\n这是一个非凸优化问题，很难找到全局最优。但我们可以交替优化 $\\mathbf{c}$ 和 $z$。\nE 步（期望步）：固定簇中心 ${\\mathbf{c}_1, \\ldots, \\mathbf{c}_k}$，更新分配 $z_i$。\n对于每个数据点 $\\mathbf{x}_i$，选择最近的簇中心：\n$$ z_i = \\arg\\min_{j} |\\mathbf{x}_i - \\mathbf{c}_j|^2 $$\nM 步（最大化步）：固定分配 $z$，更新簇中心 $\\mathbf{c}_j$。\n对于每个簇 $j$，最小化 $\\sum_{i: z_i = j} |\\mathbf{x}_i - \\mathbf{c}_j|^2$。\n求导：\n$$ \\frac{\\partial}{\\partial \\mathbf{c}j} \\sum{i: z_i = j} |\\mathbf{x}_i - \\mathbf{c}j|^2 = \\sum{i: z_i = j} -2(\\mathbf{x}_i - \\mathbf{c}_j) = \\mathbf{0} $$\n因此：\n$$ \\mathbf{c}j = \\frac{\\sum{i: z_i = j} \\mathbf{x}_i}{|{i: z_i = j}|} $$\n这是簇 $j$ 中所有点的均值，因此称为\"K 均值\"。\n第三步：收敛性分析\n每次 E 步和 M 步后，目标函数 $J$ 都会减少（或保持不变）：\nE 步：每个点选择最近的簇中心，不会增加距离 M 步：簇中心设为均值，使该簇内误差最小 由于簇的分配方式有限（最多 $k^n$ 种），算法必然在有限步内收敛。\n第四步：K 均值++ 初始化\nK 均值对初始化敏感。K-Means++ 使用概率初始化：\n随机选择第一个中心 $\\mathbf{c}_1$ 对于未选的点 $\\mathbf{x}$，计算 $D(\\mathbf{x}) = \\min_j |\\mathbf{x} - \\mathbf{c}_j|^2$ 以概率 $\\frac{D(\\mathbf{x})}{\\sum_{\\mathbf{x}’} D(\\mathbf{x}’)}$ 选择下一个中心 重复直到选择 $k$ 个中心 这种初始化使得初始中心之间相互远离，提升了聚类质量。\n第五步：选择 K\n肘部法则（Elbow Method）：\n对不同的 $k$ 运行 K 均值 绘制 $k$ vs. 目标函数 $J$ 的曲线 选择肘部（曲线平缓的点）对应的 $k$ 轮廓系数（Silhouette Coefficient）：\n对于数据点 $\\mathbf{x}_i$：\n$a_i = \\frac{1}{|C_{z_i}| - 1} \\sum_{j \\neq i, z_j = z_i} |\\mathbf{x}_i - \\mathbf{x}_j|$（同簇平均距离） $b_i = \\min_{c \\neq z_i} \\frac{1}{|C_c|} \\sum_{j: z_j = c} |\\mathbf{x}_i - \\mathbf{x}_j|$（最近异簇平均距离） 轮廓系数：\n$$ s_i = \\frac{b_i - a_i}{\\max(a_i, b_i)} $$\n平均轮廓系数越大，聚类效果越好。\n八、随机森林：民主投票的胜利 时间：2001 年 - 里奥·布雷曼 (Leo Breiman)\n从决策树到森林 2001 年，统计学家里奥·布雷曼发表了里程碑式的论文，提出了随机森林。\n推导过程 第一步：Bagging 的思想\nBagging（Bootstrap Aggregating）的核心思想：对训练集进行多次 Bootstrap 采样，每次训练一个基学习器，最后聚合。\nBootstrap 采样：从原始训练集有放回地采样 $n$ 个样本（$n$ 是原始样本数）。每次约有 $63.2%$ 的样本被采样到，称为袋内样本（in-bag samples）；未被采样的 $36.8%$ 称为袋外样本（out-of-bag samples, OOB）。\n对于回归问题，随机森林的预测是 $T$ 棵决策树预测的平均：\n$$ \\hat{f}(\\mathbf{x}) = \\frac{1}{T} \\sum_{t=1}^{T} f_t(\\mathbf{x}) $$\n对于分类问题，采用多数投票：\n$$ \\hat{y} = \\arg\\max_c \\sum_{t=1}^{T} \\mathbb{I}(f_t(\\mathbf{x}) = c) $$\n第二步：偏差-方差分解\n对于回归问题，定义均方误差：\n$$ \\text{MSE} = \\mathbb{E}[(\\hat{f}(\\mathbf{x}) - y)^2] $$\n分解为偏差和方差：\n$$ \\text{MSE} = \\text{Bias}^2 + \\text{Variance} + \\text{Noise} $$\n其中：\n$\\text{Bias}^2 = (\\mathbb{E}[\\hat{f}(\\mathbf{x})] - f^{\\ast}(\\mathbf{x}))^2$（模型平均与真实值的差距） $\\text{Variance} = \\mathbb{E}[(\\hat{f}(\\mathbf{x}) - \\mathbb{E}[\\hat{f}(\\mathbf{x})])^2]$（模型预测的不稳定性） $\\text{Noise} = \\mathbb{E}[(y - f^{\\ast}(\\mathbf{x}))^2]$（不可约误差） Bagging 减少方差的推导：\n设 $T$ 个基学习器 $f_1, f_2, \\ldots, f_T$，满足：\n$\\mathbb{E}[f_t] = \\bar{f}$（无偏） $\\text{Var}(f_t) = \\sigma^2$（同方差） $\\text{Cov}(f_i, f_j) = \\rho \\sigma^2$（同协方差） Bagging 预测为 $\\hat{f} = \\frac{1}{T} \\sum_{t=1}^{T} f_t$，其方差为：\n$$ \\begin{align} \\text{Var}(\\hat{f}) \u0026= \\text{Var}\\left(\\frac{1}{T} \\sum_{t=1}^{T} f_t\\right) \\ \u0026= \\frac{1}{T^2} \\text{Var}\\left(\\sum_{t=1}^{T} f_t\\right) \\ \u0026= \\frac{1}{T^2} \\left( \\sum_{t=1}^{T} \\text{Var}(f_t) + 2 \\sum_{i \u003c j} \\text{Cov}(f_i, f_j) \\right) \\ \u0026= \\frac{1}{T^2} \\left( T \\sigma^2 + 2 \\cdot \\frac{T(T-1)}{2} \\rho \\sigma^2 \\right) \\ \u0026= \\frac{1}{T^2} \\left( T \\sigma^2 + T(T-1) \\rho \\sigma^2 \\right) \\ \u0026= \\frac{\\sigma^2}{T} + \\frac{T-1}{T} \\rho \\sigma^2 \\ \u0026= \\rho \\sigma^2 + \\frac{1 - \\rho}{T} \\sigma^2 \\end{align} $$\n当 $T \\to \\infty$，$\\text{Var}(\\hat{f}) \\to \\rho \\sigma^2$。\n如果基学习器完全相关（$\\rho = 1$），$\\text{Var}(\\hat{f}) = \\sigma^2$，Bagging 没有作用。 如果基学习器独立（$\\rho = 0$），$\\text{Var}(\\hat{f}) = \\frac{\\sigma^2}{T} \\to 0$，方差趋于零！\n因此，Bagging 的关键是降低基学习器之间的相关性。\n第三步：随机森林的去相关机制\n随机森林引入两个随机化：\nBootstrap 采样：每棵树看到不同的训练数据 随机特征选择：在每个分裂点，随机选择 $m \\leq d$ 个特征，从中选择最优分裂 对于分类问题，常用 $m = \\lfloor \\sqrt{d} \\rfloor$；对于回归问题，常用 $m = \\lfloor d/3 \\rfloor$。\n第四步：OOB 误差估计\n对于每棵树 $t$，使用袋外样本预测。对于数据点 $\\mathbf{x}_i$，只有未用于训练第 $t$ 棵树的树（即 $\\mathbf{x}_i$ 是第 $t$ 棵树的 OOB 样本）才能预测 $\\mathbf{x}_i$。\n设 $\\mathcal{T}_i = {t : \\mathbf{x}_i \\text{ is OOB for tree } t}$，则 OOB 预测为：\n$$ \\hat{y}i^{\\text{OOB}} = \\begin{cases} \\arg\\max_c \\sum{t \\in \\mathcal{T}_i} \\mathbb{I}(f_t(\\mathbf{x}_i) = c) \u0026 \\text{classification} \\ \\frac{1}{|\\mathcal{T}i|} \\sum{t \\in \\mathcal{T}_i} f_t(\\mathbf{x}_i) \u0026 \\text{regression} \\end{cases} $$\nOOB 误差是无偏的交叉验证估计，无需额外的验证集。\n第五步：特征重要性\n置换重要性（Permutation Importance）：\n计算原始 OOB 误差 $e_{\\text{original}}$ 对特征 $j$，在 OOB 样本中随机置换该特征的值 重新计算 OOB 误差 $e_{\\text{permuted}}$ 特征重要性：$\\text{Importance}j = e{\\text{permuted}} - e_{\\text{original}}$ 置换后的误差增加越多，说明该特征越重要。\n九、梯度提升机：贪婪优化之美 时间：2001 年 - 杰罗姆·弗里德曼 (Jerome Friedman)\n函数空间中的梯度下降 2001 年，杰罗姆·弗里德曼提出了梯度提升机（Gradient Boosting Machine, GBM）。\n推导过程 第一步：理解前向分步算法\n我们想学习函数 $F: \\mathcal{X} \\to \\mathbb{R}$，使得期望损失最小：\n$$ F^{\\ast} = \\arg\\min_{F} \\mathbb{E}_{\\mathbf{x}, y}[L(y, F(\\mathbf{x}))] $$\nGBM 采用前向分步算法（Forward Stagewise Algorithm）。假设我们已经构建了 $m-1$ 轮模型 $F_{m-1}(\\mathbf{x})$，第 $m$ 轮的目标是找到一个新模型 $h(\\mathbf{x})$ 和步长 $\\rho$：\n$$ (F_m, \\rho) = \\arg\\min_{h, \\rho} \\sum_{i=1}^{n} L(y_i, F_{m-1}(\\mathbf{x}_i) + \\rho h(\\mathbf{x}_i)) $$\n然后更新：\n$$ F_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\rho h(\\mathbf{x}) $$\n第二步：函数优化的梯度下降\n在欧几里得空间中，目标函数 $f: \\mathbb{R}^d \\to \\mathbb{R}$ 的梯度下降为：\n$$ \\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta \\nabla f(\\mathbf{x}_t) $$\n其中 $\\eta$ 是学习率。\n在函数空间中，我们对函数 $F$ 进行优化。定义损失函数：\n$$ \\mathcal{L}(F) = \\sum_{i=1}^{n} L(y_i, F(\\mathbf{x}_i)) $$\n$\\mathcal{L}(F)$ 在函数空间中的\"梯度\"是：\n$$ \\nabla_F \\mathcal{L}(F) = \\left( \\frac{\\partial \\mathcal{L}(F)}{\\partial F(\\mathbf{x}_1)}, \\frac{\\partial \\mathcal{L}(F)}{\\partial F(\\mathbf{x}_2)}, \\ldots, \\frac{\\partial \\mathcal{L}(F)}{\\partial F(\\mathbf{x}_n)} \\right) $$\n计算：\n$$ \\frac{\\partial \\mathcal{L}(F)}{\\partial F(\\mathbf{x}_i)} = \\frac{\\partial L(y_i, F(\\mathbf{x}_i))}{\\partial F(\\mathbf{x}i)} = \\left. \\frac{\\partial L(y, F)}{\\partial F} \\right|{y=y_i, F=F(\\mathbf{x}_i)} $$\n因此，“梯度下降\"更新为：\n$$ F_{m}(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) - \\eta \\left. \\frac{\\partial L(y, F)}{\\partial F} \\right|{y=y, F=F{m-1}(\\mathbf{x})} $$\n第三步：拟合负梯度\n对于每个样本 $\\mathbf{x}_i$，负梯度为：\n$$ r_i = -\\left. \\frac{\\partial L(y, F)}{\\partial F} \\right|{y=y_i, F=F{m-1}(\\mathbf{x}_i)} $$\n我们要用一个弱学习器 $h(\\mathbf{x})$ 拟合这些负梯度：\n$$ h = \\arg\\min_{h} \\sum_{i=1}^{n} (r_i - h(\\mathbf{x}_i))^2 $$\n然后更新：\n$$ F_{m}(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\eta h(\\mathbf{x}) $$\n第四步：平方损失的情况\n对于平方损失 $L(y, F) = \\frac{1}{2}(y - F)^2$：\n$$ \\frac{\\partial L}{\\partial F} = -(y - F) $$\n负梯度为：\n$$ r_i = -(y_i - F_{m-1}(\\mathbf{x}i)) = F{m-1}(\\mathbf{x}_i) - y_i $$\n这正是残差！因此，GBM 的每一步都是在拟合残差。\n第五步：逻辑损失的情况\n对于逻辑损失（二元分类）：\n$$ L(y, F) = \\log(1 + e^{-y F}), \\quad y \\in {-1, +1} $$\n计算梯度：\n$$ \\frac{\\partial L}{\\partial F} = \\frac{-y e^{-y F}}{1 + e^{-y F}} = -y \\cdot \\sigma(-y F) $$\n其中 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ 是 Sigmoid 函数。\n负梯度为：\n$$ r_i = y_i \\cdot \\sigma(-y_i F_{m-1}(\\mathbf{x}_i)) $$\n拟合这个负梯度后，更新：\n$$ F_{m}(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\eta h(\\mathbf{x}) $$\n预测概率为 $\\sigma(F(\\mathbf{x}))$。\n第六步：行搜索优化步长\n拟合 $h(\\mathbf{x})$ 后，我们可以用行搜索优化步长 $\\rho$：\n$$ \\rho = \\arg\\min_{\\rho} \\sum_{i=1}^{n} L(y_i, F_{m-1}(\\mathbf{x}_i) + \\rho h(\\mathbf{x}_i)) $$\n对于平方损失：\n$$ \\frac{\\partial}{\\partial \\rho} \\sum_{i=1}^{n} \\frac{1}{2}(y_i - F_{m-1}(\\mathbf{x}_i) - \\rho h(\\mathbf{x}_i))^2 = 0 $$\n$$ \\sum_{i=1}^{n} (y_i - F_{m-1}(\\mathbf{x}_i) - \\rho h(\\mathbf{x}_i)) \\cdot (-h(\\mathbf{x}_i)) = 0 $$\n$$ \\sum_{i=1}^{n} (F_{m-1}(\\mathbf{x}_i) - y_i) h(\\mathbf{x}i) = \\rho \\sum{i=1}^{n} h(\\mathbf{x}_i)^2 $$\n$$ \\rho = \\frac{\\sum_{i=1}^{n} (F_{m-1}(\\mathbf{x}_i) - y_i) h(\\mathbf{x}i)}{\\sum{i=1}^{n} h(\\mathbf{x}_i)^2} $$\n第七步：正则化\nGBM 容易过拟合，常用的正则化方法：\n学习率：使用较小的学习率 $\\eta$（如 0.01 或 0.1），配合更多的迭代次数。\n树深限制：限制决策树的深度（如 max_depth = 3）。\n叶子节点最小样本数：限制叶子节点的最小样本数（如 min_samples_leaf = 10）。\n子采样：每次迭代只使用部分样本（如 subsample = 0.8），类似随机森林。\n第八步：XGBoost 的二阶近似\nXGBoost 使用泰勒展开到二阶：\n$$ L(y, F + \\Delta F) \\approx L(y, F) + \\frac{\\partial L}{\\partial F} \\Delta F + \\frac{1}{2} \\frac{\\partial^2 L}{\\partial F^2} (\\Delta F)^2 $$\n定义：\n一阶导数：$g = \\frac{\\partial L}{\\partial F}$ 二阶导数：$h = \\frac{\\partial^2 L}{\\partial F^2}$ 目标函数近似为：\n$$ \\mathcal{L}(F + \\Delta F) \\approx \\sum_{i=1}^{n} [L(y_i, F(\\mathbf{x}_i)) + g_i \\Delta F(\\mathbf{x}_i) + \\frac{1}{2} h_i (\\Delta F(\\mathbf{x}_i))^2] + \\Omega(f) $$\n其中 $\\Omega(f)$ 是正则项：\n$$ \\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 $$\n这里 $T$ 是叶子节点数，$w_j$ 是叶子节点的输出值，$\\gamma$ 和 $\\lambda$ 是正则化系数。\n通过推导，叶子节点 $j$ 的最优值为：\n$$ w_j^{\\ast} = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda} $$\n其中 $I_j$ 是叶子节点 $j$ 中的样本集合。\n十、主成分分析：降维的艺术 时间：1901 年 - 卡尔·皮尔逊 (Karl Pearson)\n最早的降维方法 1901 年，英国数学家卡尔·皮尔逊提出了主成分分析（PCA）。这是数学史上最早的降维方法之一。\n推导过程 第一步：数据标准化\n给定 $n$ 个 $d$ 维数据点 $\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n$，首先中心化：\n$$ \\tilde{\\mathbf{x}}_i = \\mathbf{x}_i - \\bar{\\mathbf{x}} $$\n其中 $\\bar{\\mathbf{x}} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{x}_i$ 是均值。\n如果特征量纲不同，还需要标准化：\n$$ z_{i,j} = \\frac{x_{i,j} - \\bar{x}_j}{\\sigma_j} $$\n其中 $\\sigma_j$ 是特征 $j$ 的标准差。\n第二步：最大化投影方差\n我们要找到一个单位向量 $\\mathbf{v}$（$|\\mathbf{v}| = 1$），使得数据在 $\\mathbf{v}$ 方向上的投影方差最大。\n投影为：\n$$ y_i = \\mathbf{v}^T \\tilde{\\mathbf{x}}_i $$\n投影方差为：\n$$ \\text{Var}(y) = \\frac{1}{n} \\sum_{i=1}^{n} y_i^2 = \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{v}^T \\tilde{\\mathbf{x}}i)^2 = \\frac{1}{n} \\mathbf{v}^T \\left( \\sum{i=1}^{n} \\tilde{\\mathbf{x}}_i \\tilde{\\mathbf{x}}_i^T \\right) \\mathbf{v} $$\n定义协方差矩阵：\n$$ \\mathbf{C} = \\frac{1}{n} \\sum_{i=1}^{n} \\tilde{\\mathbf{x}}_i \\tilde{\\mathbf{x}}_i^T = \\frac{1}{n} \\tilde{\\mathbf{X}}^T \\tilde{\\mathbf{X}} $$\n其中 $\\tilde{\\mathbf{X}}$ 是中心化后的数据矩阵。\n因此，优化问题为：\n$$ \\max_{\\mathbf{v}} \\mathbf{v}^T \\mathbf{C} \\mathbf{v} \\quad \\text{s.t.} \\quad \\mathbf{v}^T \\mathbf{v} = 1 $$\n第三步：使用拉格朗日乘子法\n引入拉格朗日乘子 $\\lambda$：\n$$ \\mathcal{L}(\\mathbf{v}, \\lambda) = \\mathbf{v}^T \\mathbf{C} \\mathbf{v} - \\lambda (\\mathbf{v}^T \\mathbf{v} - 1) $$\n对 $\\mathbf{v}$ 求导：\n$$ \\nabla_{\\mathbf{v}} \\mathcal{L} = 2\\mathbf{C} \\mathbf{v} - 2\\lambda \\mathbf{v} = \\mathbf{0} $$\n因此：\n$$ \\mathbf{C} \\mathbf{v} = \\lambda \\mathbf{v} $$\n这正是特征值问题！$\\mathbf{v}$ 是 $\\mathbf{C}$ 的特征向量，$\\lambda$ 是对应的特征值。\n投影方差为：\n$$ \\mathbf{v}^T \\mathbf{C} \\mathbf{v} = \\mathbf{v}^T (\\lambda \\mathbf{v}) = \\lambda \\mathbf{v}^T \\mathbf{v} = \\lambda $$\n因此，投影方差等于对应的特征值。最大化方差等价于选择最大特征值对应的特征向量。\n第四步：多个主成分\n第一个主成分 $\\mathbf{v}_1$ 是 $\\mathbf{C}$ 的最大特征值对应的特征向量。\n第二个主成分 $\\mathbf{v}_2$ 在与 $\\mathbf{v}_1$ 正交的单位向量中最大化投影方差：\n$$ \\max_{\\mathbf{v}} \\mathbf{v}^T \\mathbf{C} \\mathbf{v} \\quad \\text{s.t.} \\quad \\mathbf{v}^T \\mathbf{v} = 1, \\mathbf{v}^T \\mathbf{v}_1 = 0 $$\n使用拉格朗日乘子法：\n$$ \\mathcal{L}(\\mathbf{v}, \\lambda, \\mu) = \\mathbf{v}^T \\mathbf{C} \\mathbf{v} - \\lambda (\\mathbf{v}^T \\mathbf{v} - 1) - \\mu \\mathbf{v}^T \\mathbf{v}_1 $$\n对 $\\mathbf{v}$ 求导：\n$$ 2\\mathbf{C} \\mathbf{v} - 2\\lambda \\mathbf{v} - \\mu \\mathbf{v}_1 = \\mathbf{0} $$\n左乘 $\\mathbf{v}_1^T$：\n$$ 2\\mathbf{v}_1^T \\mathbf{C} \\mathbf{v} - 2\\lambda \\mathbf{v}_1^T \\mathbf{v} - \\mu \\mathbf{v}_1^T \\mathbf{v}_1 = 0 $$\n由于 $\\mathbf{v}_1^T \\mathbf{v} = 0$ 且 $\\mathbf{v}_1^T \\mathbf{v}_1 = 1$：\n$$ \\mathbf{v}_1^T \\mathbf{C} \\mathbf{v} = \\frac{\\mu}{2} $$\n又因为 $\\mathbf{C} \\mathbf{v}_1 = \\lambda_1 \\mathbf{v}_1$：\n$$ \\mathbf{v}_1^T \\mathbf{C} \\mathbf{v} = (\\mathbf{C} \\mathbf{v}_1)^T \\mathbf{v} = \\lambda_1 \\mathbf{v}_1^T \\mathbf{v} = 0 $$\n因此 $\\mu = 0$，回到特征值问题 $\\mathbf{C} \\mathbf{v} = \\lambda \\mathbf{v}$。\n$\\mathbf{v}_2$ 是 $\\mathbf{C}$ 的第二大特征值对应的特征向量。\n一般地，第 $k$ 个主成分是 $\\mathbf{C}$ 的第 $k$ 大特征值对应的特征向量。\n第五步：奇异值分解（SVD）\n协方差矩阵 $\\mathbf{C} = \\frac{1}{n} \\tilde{\\mathbf{X}}^T \\tilde{\\mathbf{X}}$ 的特征值分解等价于 $\\tilde{\\mathbf{X}}$ 的奇异值分解。\n设 $\\tilde{\\mathbf{X}}$ 的 SVD 为：\n$$ \\tilde{\\mathbf{X}} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T $$\n其中：\n$\\mathbf{U} \\in \\mathbb{R}^{n \\times d}$ 是左奇异矩阵（$\\mathbf{U}^T \\mathbf{U} = \\mathbf{I}$） $\\mathbf{\\Sigma} \\in \\mathbb{R}^{d \\times d}$ 是对角矩阵，对角元素是奇异值 $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_d \\geq 0$ $\\mathbf{V} \\in \\mathbb{R}^{d \\times d}$ 是右奇异矩阵（$\\mathbf{V}^T \\mathbf{V} = \\mathbf{I}$） 协方差矩阵为：\n$$ \\mathbf{C} = \\frac{1}{n} \\tilde{\\mathbf{X}}^T \\tilde{\\mathbf{X}} = \\frac{1}{n} \\mathbf{V} \\mathbf{\\Sigma}^T \\mathbf{U}^T \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T = \\frac{1}{n} \\mathbf{V} \\mathbf{\\Sigma}^2 \\mathbf{V}^T $$\n因此，$\\mathbf{C}$ 的特征向量是 $\\mathbf{V}$ 的列，特征值是 $\\frac{\\sigma_i^2}{n}$。\n第六步：降维与重构\n保留前 $k$ 个主成分：\n$$ \\tilde{\\mathbf{X}}’ = \\tilde{\\mathbf{X}} \\mathbf{V}_{[:, 1:k]} $$\n重构为：\n$$ \\tilde{\\mathbf{X}}’’ = \\tilde{\\mathbf{X}}’ \\mathbf{V}{[:, 1:k]}^T = \\tilde{\\mathbf{X}} \\mathbf{V}{[:, 1:k]} \\mathbf{V}_{[:, 1:k]}^T $$\n重构误差为：\n$$ |\\tilde{\\mathbf{X}} - \\tilde{\\mathbf{X}}’’|F^2 = \\sum{j=k+1}^{d} \\sigma_j^2 $$\n其中 $|\\cdot|_F$ 是 Frobenius 范数。\n第七步：选择主成分数量\n保留前 $k$ 个主成分的解释方差比为：\n$$ \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{d} \\lambda_i} = \\frac{\\sum_{i=1}^{k} \\sigma_i^2}{\\sum_{i=1}^{d} \\sigma_i^2} $$\n通常选择 $k$ 使得解释方差比达到 80%-95%。\n肘部法则：绘制 $k$ vs. 累积解释方差比，选择曲线趋于平缓的点。\n结语：黄金时代的遗产 2006 年，深度学习在 Hinton 等人的推动下开始兴起。2012 年，AlexNet 在 ImageNet 上的横空出世标志着新纪元的开始。\n但请不要忘记，那些传统的机器学习算法并没有消失。它们在各自的领域依然发挥着重要作用：\n线性回归和逻辑回归仍然是解释性模型的首选 朴素贝叶斯在文本分类中不可替代 随机森林和梯度提升机在结构化数据上屡战屡胜 PCA 是数据可视化和特征工程的基础工具 更重要的是，这些算法背后蕴含的数学思想——最小二乘、最大似然、贝叶斯推断、拉格朗日对偶、梯度优化——构成了现代机器学习的基础。即便是今天的深度神经网络，也离不开这些古老的智慧。\n黄金时代或许已经过去，但它留给我们的遗产将永存。让我们怀着敬意，回顾那些用公式编织智能梦想的年代。\n参考文献：\nLegendre, A. M. (1805). Nouvelles méthodes pour la détermination des orbites des comètes. Cox, D. R. (1958). “The regression analysis of binary sequences”. Journal of the Royal Statistical Society. Fix, E., \u0026 Hodges, J. L. (1951). Discriminatory analysis, nonparametric discrimination: Consistency properties. Quinlan, J. R. (1986). “Induction of decision trees”. Machine Learning. Vapnik, V. N. (1995). The Nature of Statistical Learning Theory. Breiman, L. (2001). “Random Forests”. Machine Learning. Friedman, J. H. (2001). “Greedy Function Approximation: A Gradient Boosting Machine”. Annals of Statistics. Pearson, K. (1901). “On lines and planes of closest fit to systems of points in space”. Philosophical Magazine. ","wordCount":"3481","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/ml-algorithms-legacy.jpg","datePublished":"2026-01-15T22:30:00+08:00","dateModified":"2026-01-15T22:30:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-15-traditional-ml-algorithms/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">深度学习前夜：十大传统机器学习算法的历史与数学之美</h1><div class=post-description>回顾机器学习黄金时代，详细推导十大经典算法的数学原理，从线性回归到主成分分析</div><div class=post-meta><span title='2026-01-15 22:30:00 +0800 CST'>January 15, 2026</span>&nbsp;·&nbsp;<span>17 min</span>&nbsp;·&nbsp;<span>3481 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/ml-algorithms-legacy.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/ml-algorithms-legacy.jpg alt=抽象几何图案></a><figcaption>数学的优雅与智慧</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e9%bb%84%e9%87%91%e6%97%b6%e4%bb%a3 aria-label=引言：黄金时代>引言：黄金时代</a></li><li><a href=#%e4%b8%80%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e5%9b%9e%e5%bd%92%e5%88%86%e6%9e%90%e7%9a%84%e9%bc%bb%e7%a5%96 aria-label=一、线性回归：回归分析的鼻祖>一、线性回归：回归分析的鼻祖</a><ul><li><a href=#%e5%8e%86%e5%8f%b2%e7%9a%84%e5%81%b6%e7%84%b6 aria-label=历史的偶然>历史的偶然</a></li><li><a href=#%e6%8e%a8%e5%af%bc%e8%bf%87%e7%a8%8b aria-label=推导过程>推导过程</a></li><li><a href=#%e7%9f%a9%e9%98%b5%e5%bd%a2%e5%bc%8f aria-label=矩阵形式>矩阵形式</a></li><li><a href=#%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%82 aria-label=几何直观>几何直观</a></li></ul></li><li><a href=#%e4%ba%8c%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%e4%bb%8e%e5%a4%a9%e6%96%87%e5%ad%a6%e5%88%b0%e7%94%9f%e7%89%a9%e5%ad%a6%e7%9a%84%e8%b7%a8%e7%95%8c aria-label=二、逻辑回归：从天文学到生物学的跨界>二、逻辑回归：从天文学到生物学的跨界</a><ul><li><a href=#%e8%b7%a8%e7%95%8c%e7%9a%84%e7%81%b5%e6%84%9f aria-label=跨界的灵感>跨界的灵感</a></li><li><a href=#%e6%8e%a8%e5%af%bc%e8%bf%87%e7%a8%8b-1 aria-label=推导过程>推导过程</a></li><li><a href=#logit-%e5%8f%98%e6%8d%a2 aria-label="Logit 变换">Logit 变换</a></li></ul></li><li><a href=#%e4%b8%89%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af%e4%b8%a4%e4%b8%aa%e4%b8%96%e7%ba%aa%e5%89%8d%e7%9a%84%e6%a6%82%e7%8e%87%e9%ad%94%e6%b3%95 aria-label=三、朴素贝叶斯：两个世纪前的概率魔法>三、朴素贝叶斯：两个世纪前的概率魔法</a><ul><li><a href=#%e5%bb%b6%e8%bf%9f%e5%8f%91%e8%a1%a8%e7%9a%84%e5%a4%a9%e6%89%8d aria-label=延迟发表的天才>延迟发表的天才</a></li><li><a href=#%e6%8e%a8%e5%af%bc%e8%bf%87%e7%a8%8b-2 aria-label=推导过程>推导过程</a></li><li><a href=#%e9%ab%98%e6%96%af%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af aria-label=高斯朴素贝叶斯>高斯朴素贝叶斯</a></li><li><a href=#%e5%a4%9a%e9%a1%b9%e5%bc%8f%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af%e6%96%87%e6%9c%ac%e5%88%86%e7%b1%bb aria-label=多项式朴素贝叶斯（文本分类）>多项式朴素贝叶斯（文本分类）</a></li><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e6%9c%89%e6%95%88 aria-label=为什么有效？>为什么有效？</a></li></ul></li><li><a href=#%e5%9b%9bk-%e8%bf%91%e9%82%bb%e7%ae%97%e6%b3%95%e6%9c%80%e7%ae%80%e5%8d%95%e7%9a%84%e8%ae%b0%e5%bf%86%e5%ad%a6%e4%b9%a0 aria-label="四、K 近邻算法：最简单的记忆学习">四、K 近邻算法：最简单的记忆学习</a><ul><li><a href=#%e6%9c%aa%e5%8f%91%e8%a1%a8%e7%9a%84%e4%bc%a0%e5%a5%87 aria-label=未发表的传奇>未发表的传奇</a></li><li><a href=#%e6%8e%a8%e5%af%bc%e8%bf%87%e7%a8%8b-3 aria-label=推导过程>推导过程</a></li><li><a href=#%e8%b7%9d%e7%a6%bb%e5%ba%a6%e9%87%8f aria-label=距离度量>距离度量</a></li><li><a href=#%e7%bb%b4%e5%ba%a6%e7%9a%84%e8%af%85%e5%92%92 aria-label=维度的诅咒>维度的诅咒</a></li></ul></li><li><a href=#%e4%ba%94%e5%86%b3%e7%ad%96%e6%a0%91%e4%bb%8e-hunt-%e7%ae%97%e6%b3%95%e5%88%b0-c45 aria-label="五、决策树：从 Hunt 算法到 C4.5">五、决策树：从 Hunt 算法到 C4.5</a><ul><li><a href=#%e7%ae%80%e5%8d%95%e8%80%8c%e5%bc%ba%e5%a4%a7%e7%9a%84%e9%80%92%e5%bd%92 aria-label=简单而强大的递归>简单而强大的递归</a></li><li><a href=#%e6%8e%a8%e5%af%bc%e8%bf%87%e7%a8%8b-4 aria-label=推导过程>推导过程</a></li><li><a href=#%e5%86%b3%e7%ad%96%e8%be%b9%e7%95%8c aria-label=决策边界>决策边界</a></li></ul></li><li><a href=#%e5%85%ad%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f%e6%9c%ba%e6%9c%80%e5%a4%a7%e9%97%b4%e9%9a%94%e7%9a%84%e8%89%ba%e6%9c%af aria-label=六、支持向量机：最大间隔的艺术>六、支持向量机：最大间隔的艺术</a><ul><li><a href=#%e5%86%b7%e6%88%98%e6%97%b6%e6%9c%9f%e7%9a%84%e6%99%ba%e6%85%a7 aria-label=冷战时期的智慧>冷战时期的智慧</a></li><li><a href=#%e6%8e%a8%e5%af%bc%e8%bf%87%e7%a8%8b-5 aria-label=推导过程>推导过程</a></li></ul></li><li><a href=#%e4%b8%83k-%e5%9d%87%e5%80%bc%e8%81%9a%e7%b1%bb%e8%bf%ad%e4%bb%a3%e6%94%b6%e6%95%9b%e7%9a%84%e7%be%8e%e5%ad%a6 aria-label="七、K 均值聚类：迭代收敛的美学">七、K 均值聚类：迭代收敛的美学</a><ul><li><a href=#%e8%81%9a%e7%b1%bb%e7%9a%84%e5%90%af%e8%92%99 aria-label=聚类的启蒙>聚类的启蒙</a></li><li><a href=#%e6%8e%a8%e5%af%bc%e8%bf%87%e7%a8%8b-6 aria-label=推导过程>推导过程</a></li></ul></li><li><a href=#%e5%85%ab%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97%e6%b0%91%e4%b8%bb%e6%8a%95%e7%a5%a8%e7%9a%84%e8%83%9c%e5%88%a9 aria-label=八、随机森林：民主投票的胜利>八、随机森林：民主投票的胜利</a><ul><li><a href=#%e4%bb%8e%e5%86%b3%e7%ad%96%e6%a0%91%e5%88%b0%e6%a3%ae%e6%9e%97 aria-label=从决策树到森林>从决策树到森林</a></li><li><a href=#%e6%8e%a8%e5%af%bc%e8%bf%87%e7%a8%8b-7 aria-label=推导过程>推导过程</a></li></ul></li><li><a href=#%e4%b9%9d%e6%a2%af%e5%ba%a6%e6%8f%90%e5%8d%87%e6%9c%ba%e8%b4%aa%e5%a9%aa%e4%bc%98%e5%8c%96%e4%b9%8b%e7%be%8e aria-label=九、梯度提升机：贪婪优化之美>九、梯度提升机：贪婪优化之美</a><ul><li><a href=#%e5%87%bd%e6%95%b0%e7%a9%ba%e9%97%b4%e4%b8%ad%e7%9a%84%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d aria-label=函数空间中的梯度下降>函数空间中的梯度下降</a></li><li><a href=#%e6%8e%a8%e5%af%bc%e8%bf%87%e7%a8%8b-8 aria-label=推导过程>推导过程</a></li></ul></li><li><a href=#%e5%8d%81%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90%e9%99%8d%e7%bb%b4%e7%9a%84%e8%89%ba%e6%9c%af aria-label=十、主成分分析：降维的艺术>十、主成分分析：降维的艺术</a><ul><li><a href=#%e6%9c%80%e6%97%a9%e7%9a%84%e9%99%8d%e7%bb%b4%e6%96%b9%e6%b3%95 aria-label=最早的降维方法>最早的降维方法</a></li><li><a href=#%e6%8e%a8%e5%af%bc%e8%bf%87%e7%a8%8b-9 aria-label=推导过程>推导过程</a></li></ul></li><li><a href=#%e7%bb%93%e8%af%ad%e9%bb%84%e9%87%91%e6%97%b6%e4%bb%a3%e7%9a%84%e9%81%97%e4%ba%a7 aria-label=结语：黄金时代的遗产>结语：黄金时代的遗产</a></li></ul></div></details></div><div class=post-content><h2 id=引言黄金时代>引言：黄金时代<a hidden class=anchor aria-hidden=true href=#引言黄金时代>#</a></h2><p>想象一下 2006 年的秋天，深度学习尚未兴起。那时的机器学习领域正经历着一场静悄悄的革命。统计学习方法、核方法、集成学习层出不穷，数学家们用优雅的公式编织着智能的梦想。</p><p>那时，人们相信：只要数据足够、特征工程足够细致，我们就能教机器做任何事。这种信念催生了一批经典算法——它们或许不如今天的深度神经网络那样炫目，但每一款都凝聚着数学家的智慧，每一步推导都闪耀着逻辑的光辉。</p><p>今天，我们回顾这段黄金时代，讲述十个改变了世界的传统机器学习算法的故事。但这次，让我们放慢脚步，亲手推导每一步，感受数学的力量。</p><hr><h2 id=一线性回归回归分析的鼻祖>一、线性回归：回归分析的鼻祖<a hidden class=anchor aria-hidden=true href=#一线性回归回归分析的鼻祖>#</a></h2><p><strong>时间：1795 年 - 阿德里安-马里·勒让德 (Adrien-Marie Legendre)</strong></p><h3 id=历史的偶然>历史的偶然<a hidden class=anchor aria-hidden=true href=#历史的偶然>#</a></h3><p>1795 年，法国天文学家勒让德正在为一个问题头疼：如何用最简单的方法拟合行星轨道数据？他需要找到一条直线，让所有数据点到这条直线的距离平方和最小。</p><p>这就是<strong>最小二乘法</strong>的诞生。</p><h3 id=推导过程>推导过程<a hidden class=anchor aria-hidden=true href=#推导过程>#</a></h3><p>让我们从最简单的情况开始。假设我们有 $n$ 个数据点 $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$，想要找到一条直线 $y = w_0 + w_1 x$ 来拟合这些数据。</p><p><strong>第一步：定义误差</strong></p><p>对于每个数据点 $(x_i, y_i)$，我们的预测值是 $\hat{y}_i = w_0 + w_1 x_i$，误差就是观测值和预测值的差：</p><p>$$
e_i = y_i - \hat{y}_i = y_i - (w_0 + w_1 x_i)
$$</p><p><strong>第二步：定义损失函数</strong></p><p>为什么是平方误差？勒让德选择平方误差有几个好处：</p><ol><li>非负：平方后总是非负</li><li>可导：处处光滑，便于优化</li><li>凸函数：只有一个最小值</li></ol><p>损失函数定义为：</p><p>$$
L(w_0, w_1) = \sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} [y_i - (w_0 + w_1 x_i)]^2
$$</p><p><strong>第三步：求偏导</strong></p><p>为了找到最小值，我们对 $w_0$ 和 $w_1$ 分别求偏导：</p><p>$$
\frac{\partial L}{\partial w_0} = \sum_{i=1}^{n} 2[y_i - (w_0 + w_1 x_i)] \cdot (-1) = -2 \sum_{i=1}^{n} [y_i - (w_0 + w_1 x_i)]
$$</p><p>$$
\frac{\partial L}{\partial w_1} = \sum_{i=1}^{n} 2[y_i - (w_0 + w_1 x_i)] \cdot (-x_i) = -2 \sum_{i=1}^{n} x_i [y_i - (w_0 + w_1 x_i)]
$$</p><p><strong>第四步：令偏导为零</strong></p><p>$$
\begin{align}
\frac{\partial L}{\partial w_0} &= 0 \Rightarrow \sum_{i=1}^{n} [y_i - (w_0 + w_1 x_i)] = 0 \
&\Rightarrow \sum_{i=1}^{n} y_i - n w_0 - w_1 \sum_{i=1}^{n} x_i = 0 \
&\Rightarrow n w_0 + w_1 \sum_{i=1}^{n} x_i = \sum_{i=1}^{n} y_i
\end{align}
$$</p><p>$$
\begin{align}
\frac{\partial L}{\partial w_1} &= 0 \Rightarrow \sum_{i=1}^{n} x_i [y_i - (w_0 + w_1 x_i)] = 0 \
&\Rightarrow \sum_{i=1}^{n} x_i y_i - w_0 \sum_{i=1}^{n} x_i - w_1 \sum_{i=1}^{n} x_i^2 = 0 \
&\Rightarrow w_0 \sum_{i=1}^{n} x_i + w_1 \sum_{i=1}^{n} x_i^2 = \sum_{i=1}^{n} x_i y_i
\end{align}
$$</p><p><strong>第五步：解线性方程组</strong></p><p>记 $\bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i$，$\bar{y} = \frac{1}{n}\sum_{i=1}^{n} y_i$，从第一个方程：</p><p>$$
w_0 = \bar{y} - w_1 \bar{x}
$$</p><p>代入第二个方程：</p><p>$$
\begin{align}
(\bar{y} - w_1 \bar{x}) \sum_{i=1}^{n} x_i + w_1 \sum_{i=1}^{n} x_i^2 &= \sum_{i=1}^{n} x_i y_i \
\bar{y} n \bar{x} - w_1 n \bar{x}^2 + w_1 \sum_{i=1}^{n} x_i^2 &= \sum_{i=1}^{n} x_i y_i \
w_1 \left(\sum_{i=1}^{n} x_i^2 - n \bar{x}^2\right) &= \sum_{i=1}^{n} x_i y_i - n \bar{x} \bar{y} \
w_1 &= \frac{\sum_{i=1}^{n} x_i y_i - n \bar{x} \bar{y}}{\sum_{i=1}^{n} x_i^2 - n \bar{x}^2}
\end{align}
$$</p><p>这就是著名的<strong>最小二乘估计</strong>。</p><h3 id=矩阵形式>矩阵形式<a hidden class=anchor aria-hidden=true href=#矩阵形式>#</a></h3><p>对于多元线性回归，我们有 $d$ 个特征。设 $\mathbf{x}<em>i = (1, x</em>{i,1}, x_{i,2}, \ldots, x_{i,d})^T$ 是增广特征向量，$\mathbf{w} = (w_0, w_1, \ldots, w_d)^T$ 是参数向量。</p><p>损失函数写为：</p><p>$$
L(\mathbf{w}) = \sum_{i=1}^{n} (y_i - \mathbf{w}^T \mathbf{x}_i)^2 = |\mathbf{y} - \mathbf{X}\mathbf{w}|^2
$$</p><p>其中 $\mathbf{X} = \begin{pmatrix} \mathbf{x}_1^T \ \mathbf{x}_2^T \ \vdots \ \mathbf{x}_n^T \end{pmatrix}$ 是设计矩阵，$\mathbf{y} = \begin{pmatrix} y_1 \ y_2 \ \vdots \ y_n \end{pmatrix}$ 是响应向量。</p><p>展开损失函数：</p><p>$$
\begin{align}
L(\mathbf{w}) &= (\mathbf{y} - \mathbf{X}\mathbf{w})^T (\mathbf{y} - \mathbf{X}\mathbf{w}) \
&= \mathbf{y}^T \mathbf{y} - \mathbf{y}^T \mathbf{X}\mathbf{w} - \mathbf{w}^T \mathbf{X}^T \mathbf{y} + \mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w}
\end{align}
$$</p><p>注意 $\mathbf{y}^T \mathbf{X}\mathbf{w}$ 是标量，等于其转置 $\mathbf{w}^T \mathbf{X}^T \mathbf{y}$，因此：</p><p>$$
L(\mathbf{w}) = \mathbf{y}^T \mathbf{y} - 2 \mathbf{w}^T \mathbf{X}^T \mathbf{y} + \mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w}
$$</p><p>求梯度：</p><p>$$
\begin{align}
\nabla_{\mathbf{w}} L(\mathbf{w}) &= \nabla_{\mathbf{w}} (\mathbf{y}^T \mathbf{y}) - 2 \nabla_{\mathbf{w}} (\mathbf{w}^T \mathbf{X}^T \mathbf{y}) + \nabla_{\mathbf{w}} (\mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w}) \
&= 0 - 2 \mathbf{X}^T \mathbf{y} + 2 \mathbf{X}^T \mathbf{X} \mathbf{w}
\end{align}
$$</p><p>令梯度为零：</p><p>$$
\mathbf{X}^T \mathbf{X} \mathbf{w} = \mathbf{X}^T \mathbf{y}
$$</p><p>这就是著名的<strong>正规方程</strong>（Normal Equation）。如果 $\mathbf{X}^T \mathbf{X}$ 可逆，解为：</p><p>$$
\mathbf{w}^{\ast} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
$$</p><h3 id=几何直观>几何直观<a hidden class=anchor aria-hidden=true href=#几何直观>#</a></h3><p>从几何上看，$\mathbf{y}$ 在列空间 $\mathcal{C}(\mathbf{X})$ 上的投影是：</p><p>$$
\hat{\mathbf{y}} = \mathbf{X} \mathbf{w}^{\ast} = \mathbf{X} (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} = \mathbf{H} \mathbf{y}
$$</p><p>其中 $\mathbf{H} = \mathbf{X} (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T$ 是<strong>帽子矩阵</strong>（hat matrix）。它把 $\mathbf{y}$ &ldquo;戴上了帽子&rdquo;。</p><p>残差 $\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}} = (\mathbf{I} - \mathbf{H})\mathbf{y}$ 与列空间正交：</p><p>$$
\mathbf{X}^T \mathbf{e} = \mathbf{X}^T (\mathbf{I} - \mathbf{H})\mathbf{y} = \mathbf{X}^T \mathbf{y} - \mathbf{X}^T \mathbf{X} (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} = \mathbf{X}^T \mathbf{y} - \mathbf{X}^T \mathbf{y} = \mathbf{0}
$$</p><p>这就是正交投影的数学表达！</p><hr><h2 id=二逻辑回归从天文学到生物学的跨界>二、逻辑回归：从天文学到生物学的跨界<a hidden class=anchor aria-hidden=true href=#二逻辑回归从天文学到生物学的跨界>#</a></h2><p><strong>时间：1958 年 - 大卫·考克斯 (David Cox)</strong></p><h3 id=跨界的灵感>跨界的灵感<a hidden class=anchor aria-hidden=true href=#跨界的灵感>#</a></h3><p>1958 年，英国统计学家大卫·考克斯遇到了一个新问题：如何预测二元变量的概率？传统的线性回归给出的是实数值，但概率必须在 $[0, 1]$ 之间。</p><p>考克斯灵机一动，想到了 Sigmoid 函数。</p><h3 id=推导过程-1>推导过程<a hidden class=anchor aria-hidden=true href=#推导过程-1>#</a></h3><p><strong>第一步：理解二分类问题</strong></p><p>给定 $n$ 个样本 $(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \ldots, (\mathbf{x}_n, y_n)$，其中 $\mathbf{x}_i \in \mathbb{R}^d$，$y_i \in {0, 1}$。我们想要学习一个模型 $f: \mathbb{R}^d \to [0, 1]$，使得 $f(\mathbf{x})$ 表示 $P(y=1|\mathbf{x})$。</p><p><strong>第二步：为什么不能直接用线性回归？</strong></p><p>如果用线性回归 $y = \mathbf{w}^T \mathbf{x}$，输出可以是任意实数，但概率必须满足 $0 \leq p \leq 1$。</p><p><strong>第三步：引入 Sigmoid 函数</strong></p><p>Sigmoid 函数定义为：</p><p>$$
\sigma(z) = \frac{1}{1 + e^{-z}} = \frac{e^z}{1 + e^z}
$$</p><p>它的性质：</p><ul><li>当 $z \to -\infty$，$\sigma(z) \to 0$</li><li>当 $z \to +\infty$，$\sigma(z) \to 1$</li><li>当 $z = 0$，$\sigma(z) = 0.5$</li></ul><p>因此，我们定义：</p><p>$$
p(\mathbf{x}) = P(y=1|\mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x}) = \frac{1}{1 + e^{-\mathbf{w}^T \mathbf{x}}}
$$</p><p><strong>第四步：导出似然函数</strong></p><p>对于单个样本 $(\mathbf{x}_i, y_i)$，其概率可以统一写成：</p><p>$$
P(y_i|\mathbf{x}_i, \mathbf{w}) = p(\mathbf{x}_i)^{y_i} (1 - p(\mathbf{x}_i))^{1 - y_i}
$$</p><p>验证：</p><ul><li>若 $y_i = 1$：$P(y_i=1|\mathbf{x}_i) = p(\mathbf{x}_i) \cdot (1-p(\mathbf{x}_i))^0 = p(\mathbf{x}_i)$ ✓</li><li>若 $y_i = 0$：$P(y_i=0|\mathbf{x}_i) = p(\mathbf{x}_i)^0 \cdot (1-p(\mathbf{x}_i))^1 = 1 - p(\mathbf{x}_i)$ ✓</li></ul><p>假设样本独立同分布，联合概率（似然）为：</p><p>$$
\mathcal{L}(\mathbf{w}) = \prod_{i=1}^{n} P(y_i|\mathbf{x}<em>i, \mathbf{w}) = \prod</em>{i=1}^{n} p(\mathbf{x}_i)^{y_i} (1 - p(\mathbf{x}_i))^{1 - y_i}
$$</p><p><strong>第五步：取对数得到对数似然</strong></p><p>取对数简化计算：</p><p>$$
\begin{align}
\ell(\mathbf{w}) = \log \mathcal{L}(\mathbf{w}) &= \sum_{i=1}^{n} \left[ y_i \log p(\mathbf{x}_i) + (1 - y_i) \log(1 - p(\mathbf{x}_i)) \right]
\end{align}
$$</p><p><strong>第六步：计算梯度</strong></p><p>我们需要计算 $\frac{\partial \ell}{\partial w}$。首先计算 $\frac{\partial p(x)}{\partial w}$：</p><p>$$
\begin{align}
p(x) &= \frac{1}{1 + e^{-w^T x}} \
\frac{\partial p(x)}{\partial w} &= -\frac{1}{(1 + e^{-w^T x})^2} \cdot \frac{\partial}{\partial w} (1 + e^{-w^T x}) \
&= -\frac{1}{(1 + e^{-w^T x})^2} \cdot e^{-w^T x} \cdot (-x) \
&= \frac{e^{-w^T x}}{(1 + e^{-w^T x})^2} x
\end{align}
$$</p><p>注意到：</p><p>$$
p(x)(1 - p(x)) = \frac{1}{1 + e^{-w^T x}} \cdot \frac{e^{-w^T x}}{1 + e^{-w^T x}} = \frac{e^{-w^T x}}{(1 + e^{-w^T x})^2}
$$</p><p>因此：</p><p>$$
\frac{\partial p(x)}{\partial w} = p(x)(1 - p(x)) x
$$</p><p>这是一个非常优雅的结论！</p><p><strong>第七步：计算对数似然的梯度</strong></p><p>$$
\frac{\partial \ell}{\partial w} = \sum_{i=1}^{n} (y_i - p(x_i)) x_i
$$</p><p>这就是逻辑回归的梯度公式！</p><p><strong>第八步：梯度上升法</strong></p><p>由于我们要最大化对数似然，使用梯度上升：</p><p>$$
w_{t+1} = w_t + \eta \sum_{i=1}^{n} (y_i - p_t(x_i)) x_i
$$</p><p>其中 $\eta$ 是学习率。</p><h3 id=logit-变换>Logit 变换<a hidden class=anchor aria-hidden=true href=#logit-变换>#</a></h3><p>我们也可以从另一个角度理解逻辑回归。定义 <strong>logit 变换</strong>：</p><p>$$
\text{logit}(p) = \ln\left(\frac{p}{1-p}\right)
$$</p><p>对于逻辑回归：</p><p>$$
\begin{align}
\text{logit}(P(y=1|\mathbf{x})) &= \ln\left(\frac{P(y=1|\mathbf{x})}{1 - P(y=1|\mathbf{x})}\right) \
&= \ln\left(\frac{\sigma(\mathbf{w}^T \mathbf{x})}{1 - \sigma(\mathbf{w}^T \mathbf{x})}\right) \
&= \ln\left(\frac{\frac{1}{1 + e^{-\mathbf{w}^T \mathbf{x}}}}{\frac{e^{-\mathbf{w}^T \mathbf{x}}}{1 + e^{-\mathbf{w}^T \mathbf{x}}}}\right) \
&= \ln(e^{\mathbf{w}^T \mathbf{x}}) \
&= \mathbf{w}^T \mathbf{x}
\end{align}
$$</p><p>这表明：logit 变换后，概率的对数几率（log-odds）与特征呈线性关系。</p><hr><h2 id=三朴素贝叶斯两个世纪前的概率魔法>三、朴素贝叶斯：两个世纪前的概率魔法<a hidden class=anchor aria-hidden=true href=#三朴素贝叶斯两个世纪前的概率魔法>#</a></h2><p><strong>时间：1763 年 - 托马斯·贝叶斯（Thomas Bayes，定理发表）；1950 年代 - 机器学习应用</strong></p><h3 id=延迟发表的天才>延迟发表的天才<a hidden class=anchor aria-hidden=true href=#延迟发表的天才>#</a></h3><p>1763 年，英国牧师托马斯·贝叶斯去世两年后，他的朋友理查德·普赖斯整理并发表了他的一篇论文——《关于机会问题的解法》。</p><h3 id=推导过程-2>推导过程<a hidden class=anchor aria-hidden=true href=#推导过程-2>#</a></h3><p><strong>第一步：贝叶斯定理</strong></p><p>设 $D$ 为观测数据，$H$ 为假设。贝叶斯定理表述为：</p><p>$$
P(H|D) = \frac{P(D|H) P(H)}{P(D)}
$$</p><p>其中：</p><ul><li>$P(H)$ 是先验概率（prior）</li><li>$P(D|H)$ 是似然（likelihood）</li><li>$P(D)$ 是证据（evidence）</li><li>$P(H|D)$ 是后验概率（posterior）</li></ul><p><strong>第二步：应用到分类问题</strong></p><p>对于分类问题，我们有类别 $y \in {1, 2, \ldots, C}$，特征 $\mathbf{x} = (x_1, x_2, \ldots, x_d)$。根据贝叶斯定理：</p><p>$$
P(y|\mathbf{x}) = \frac{P(\mathbf{x}|y) P(y)}{P(\mathbf{x})}
$$</p><p>决策规则：</p><p>$$
\hat{y} = \arg\max_{y} P(y|\mathbf{x}) = \arg\max_{y} \frac{P(\mathbf{x}|y) P(y)}{P(\mathbf{x})}
$$</p><p>由于 $P(\mathbf{x})$ 对所有类别都相同，可以忽略：</p><p>$$
\hat{y} = \arg\max_{y} P(\mathbf{x}|y) P(y)
$$</p><p><strong>第三步：朴素独立性假设</strong></p><p>$P(\mathbf{x}|y)$ 的计算困难在于特征之间可能存在依赖关系。朴素贝叶斯做出<strong>条件独立性假设</strong>：</p><p>$$
P(\mathbf{x}|y) = P(x_1, x_2, \ldots, x_d|y) = P(x_1|y) P(x_2|y) \cdots P(x_d|y) = \prod_{j=1}^{d} P(x_j|y)
$$</p><p>这个假设在现实世界中几乎从不成立，但效果出奇地好。</p><p><strong>第四步：分类决策</strong></p><p>$$
\hat{y} = \arg\max_{y} P(y) \prod_{j=1}^{d} P(x_j|y)
$$</p><h3 id=高斯朴素贝叶斯>高斯朴素贝叶斯<a hidden class=anchor aria-hidden=true href=#高斯朴素贝叶斯>#</a></h3><p>对于连续特征，常假设 $P(x_j|y)$ 服从高斯分布：</p><p>$$
P(x_j|y=c) = \frac{1}{\sqrt{2\pi \sigma_{jc}^2}} \exp\left(-\frac{(x_j - \mu_{jc})^2}{2\sigma_{jc}^2}\right)
$$</p><p>参数估计（最大似然）：</p><p>$$
\hat{P}(y=c) = \frac{n_c}{n}
$$</p><p>$$
\hat{\mu}<em>{jc} = \frac{1}{n_c} \sum</em>{i: y_i=c} x_{i,j}
$$</p><p>$$
\hat{\sigma}<em>{jc}^2 = \frac{1}{n_c} \sum</em>{i: y_i=c} (x_{i,j} - \hat{\mu}_{jc})^2
$$</p><h3 id=多项式朴素贝叶斯文本分类>多项式朴素贝叶斯（文本分类）<a hidden class=anchor aria-hidden=true href=#多项式朴素贝叶斯文本分类>#</a></h3><p>对于文本分类，采用多项式分布。设词汇表大小为 $V$，词 $w$ 在类别 $c$ 中的计数为 $N_{wc}$，类别 $c$ 的总词数为 $N_c = \sum_{w=1}^{V} N_{wc}$。</p><p>使用<strong>拉普拉斯平滑</strong>（Laplace smoothing）：</p><p>$$
P(w|c) = \frac{N_{wc} + 1}{N_c + V}
$$</p><p>$$
P(c) = \frac{\sum_{w} N_{wc}}{\sum_{c&rsquo;, w} N_{w,c&rsquo;}}
$$</p><h3 id=为什么有效>为什么有效？<a hidden class=anchor aria-hidden=true href=#为什么有效>#</a></h3><p>虽然独立性假设不成立，但朴素贝叶斯经常表现良好，原因有：</p><ol><li><strong>优化目标不同</strong>：我们关心的是分类准确率，而不是概率估计的精确性</li><li><strong>去相关</strong>：即使特征相关，决策边界可能仍然正确</li><li><strong>高维特性</strong>：在高维空间中，不同方向的特征对分类的贡献相对独立</li></ol><hr><h2 id=四k-近邻算法最简单的记忆学习>四、K 近邻算法：最简单的记忆学习<a hidden class=anchor aria-hidden=true href=#四k-近邻算法最简单的记忆学习>#</a></h2><p><strong>时间：1951 年 - 伊芙琳·菲克斯 (Evelyn Fix) 和约瑟夫·霍奇斯 (Joseph Hodges)</strong></p><h3 id=未发表的传奇>未发表的传奇<a hidden class=anchor aria-hidden=true href=#未发表的传奇>#</a></h3><p>1951 年，加州大学伯克利分校的统计学家伊芙琳·菲克斯和约瑟夫·霍奇斯写了一篇论文，提出了一个极其简单的想法：要判断一个新样本属于哪一类，就看看训练数据中离它最近的 $k$ 个样本属于哪一类。</p><h3 id=推导过程-3>推导过程<a hidden class=anchor aria-hidden=true href=#推导过程-3>#</a></h3><p>KNN 本质上不需要"推导"，但我们可以从<strong>风险最小化</strong>的角度理解它。</p><p><strong>第一步：1-NN 的渐近最优性</strong></p><p>假设数据分布 $P(\mathbf{x}, y)$，1-NN 的预测是：</p><p>$$
\hat{y} = \arg\max_c P(c|\text{NN}(\mathbf{x}))
$$</p><p>其中 $\text{NN}(\mathbf{x})$ 是 $\mathbf{x}$ 的最近邻。</p><p>当训练数据量 $n \to \infty$ 时，最近邻 $\text{NN}(\mathbf{x})$ 会无限接近 $\mathbf{x}$，因此：</p><p>$$
\lim_{n \to \infty} P(y|\text{NN}(\mathbf{x}) = c) = P(y=c|\mathbf{x})
$$</p><p>于是：</p><p>$$
\hat{y} = \arg\max_c P(y=c|\mathbf{x})
$$</p><p>这正是贝叶斯最优分类器！</p><p><strong>第二步：1-NN 的风险</strong></p><p>贝叶斯最优分类器的错误率是：</p><p>$$
R^{\ast} = 1 - \sum_{x} P(x) \max_c P(y=c|x)
$$</p><p>1-NN 的渐近错误率是：</p><p>$$
R_{\text{1NN}} = 2 \sum_{x} P(x) P(y=\hat{y}<em>{}^{\ast}|x) (1 - P(y=\hat{y}</em>{}^{\ast}|x))
$$</p><p>其中 $\hat{y}_{}^{\ast}$ 是贝叶斯最优预测。</p><p>可以证明：$R^{\ast} \leq R_{\text{1NN}} \leq 2R^{\ast}$。因此，1-NN 的错误率最多是贝叶斯最优的两倍。</p><p><strong>第三步：K 近邻的决策</strong></p><p>对于 K 近邻，我们投票：</p><p>$$
\hat{y} = \arg\max_{c} \sum_{i=1}^{K} \mathbb{I}(y_{(i)} = c)
$$</p><p>其中 $y_{(i)}$ 是第 $i$ 个最近邻的标签。</p><p>也可以加权投票：</p><p>$$
\hat{y} = \arg\max_{c} \sum_{i=1}^{K} w_i \cdot \mathbb{I}(y_{(i)} = c)
$$</p><p>其中权重 $w_i$ 通常是距离的倒数：$w_i = \frac{1}{d(\mathbf{x}, \mathbf{x}_{(i)})}$。</p><h3 id=距离度量>距离度量<a hidden class=anchor aria-hidden=true href=#距离度量>#</a></h3><p><strong>欧几里得距离</strong>：</p><p>$$
d(\mathbf{x}, \mathbf{x}&rsquo;) = |\mathbf{x} - \mathbf{x}&rsquo;|<em>2 = \sqrt{\sum</em>{j=1}^{d} (x_j - x&rsquo;_j)^2}
$$</p><p><strong>曼哈顿距离</strong>：</p><p>$$
d(\mathbf{x}, \mathbf{x}&rsquo;) = |\mathbf{x} - \mathbf{x}&rsquo;|<em>1 = \sum</em>{j=1}^{d} |x_j - x&rsquo;_j|
$$</p><p><strong>余弦相似度</strong>（常用于文本）：</p><p>$$
s(\mathbf{x}, \mathbf{x}&rsquo;) = \frac{\mathbf{x}^T \mathbf{x}&rsquo;}{|\mathbf{x}| \cdot |\mathbf{x}&rsquo;|}
$$</p><h3 id=维度的诅咒>维度的诅咒<a hidden class=anchor aria-hidden=true href=#维度的诅咒>#</a></h3><p>KNN 的问题在于高维空间。考虑超立方体 $[0, 1]^d$，边长为 $\epsilon$ 的立方体体积是 $\epsilon^d$。随着 $d$ 增加，即使是小的 $\epsilon$，体积也趋于零。</p><p>这意味着：在高维空间中，任何点之间的距离都趋于相同，最近邻的选择变得随机。解决方法：特征选择、降维（PCA、t-SNE）。</p><hr><h2 id=五决策树从-hunt-算法到-c45>五、决策树：从 Hunt 算法到 C4.5<a hidden class=anchor aria-hidden=true href=#五决策树从-hunt-算法到-c45>#</a></h2><p><strong>时间：1960 年代 - Hunt 的算法；1986 年 - ID3；1993 年 - C4.5</strong></p><h3 id=简单而强大的递归>简单而强大的递归<a hidden class=anchor aria-hidden=true href=#简单而强大的递归>#</a></h3><p>决策树的思想非常直观：就像医生诊断疾病一样，通过一系列"是/否"的问题来逐步缩小可能性。</p><h3 id=推导过程-4>推导过程<a hidden class=anchor aria-hidden=true href=#推导过程-4>#</a></h3><p><strong>第一步：理解划分问题</strong></p><p>假设当前数据集为 $D$，我们要选择一个特征 $A$ 和一个划分方式，将 $D$ 划分为子集 ${D_1, D_2, \ldots, D_k}$。目标是让每个子集尽可能"纯"（属于同一类）。</p><p><strong>第二步：纯度的度量——熵</strong></p><p>信息论中，香农熵定义为：</p><p>$$
H(D) = -\sum_{c=1}^{C} p_c \log_2 p_c
$$</p><p>其中 $p_c = \frac{|D_c|}{|D|}$ 是类别 $c$ 的比例。</p><p>熵的性质：</p><ul><li>当所有样本属于同一类（$p_c = 1$ 对某个 $c$），$H(D) = 0$（最纯）</li><li>当各类均匀分布（$p_c = \frac{1}{C}$），$H(D) = \log_2 C$（最不纯）</li></ul><p><strong>第三步：条件熵</strong></p><p>如果用特征 $A$ 将数据集划分为 ${D_1, D_2, \ldots, D_k}$，条件熵为：</p><p>$$
H(D|A) = \sum_{i=1}^{k} \frac{|D_i|}{|D|} H(D_i)
$$</p><p>这是划分后各子集熵的加权平均。</p><p><strong>第四步：信息增益</strong></p><p>信息增益定义为熵的减少：</p><p>$$
\text{Gain}(D, A) = H(D) - H(D|A)
$$</p><p>信息增益越大，划分越有效。这就是 ID3 算法的标准。</p><p><strong>第五步：ID3 算法步骤</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>ID3(D, 特征集):
</span></span><span class=line><span class=cl>    如果 D 中所有样本属于同一类 c:
</span></span><span class=line><span class=cl>        返回叶子节点，标签为 c
</span></span><span class=line><span class=cl>    如果 特征集 为空:
</span></span><span class=line><span class=cl>        返回叶子节点，标签为 D 的多数类
</span></span><span class=line><span class=cl>    选择信息增益最大的特征 A
</span></span><span class=line><span class=cl>    根据特征的每个可能值 a，创建子节点
</span></span><span class=line><span class=cl>    对每个子节点递归调用 ID3
</span></span></code></pre></div><p><strong>第六步：信息增益率的问题</strong></p><p>ID3 倾向于选择取值较多的特征（因为这样能产生更多的划分，条件熵更小）。为了解决这个问题，C4.5 使用<strong>信息增益率</strong>。</p><p>首先计算特征 $A$ 的<strong>固有熵</strong>（intrinsic entropy）：</p><p>$$
H_A(D) = -\sum_{i=1}^{k} \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}
$$</p><p>然后计算信息增益率：</p><p>$$
\text{GainRatio}(D, A) = \frac{\text{Gain}(D, A)}{H_A(D)}
$$</p><p>这样，取值多的特征虽然增益大，但固有熵也大，增益率不会特别高。</p><p><strong>第七步：CART 的基尼系数</strong></p><p>CART（Classification and Regression Trees）算法使用<strong>基尼系数</strong>（Gini index）：</p><p>$$
\text{Gini}(D) = 1 - \sum_{c=1}^{C} p_c^2 = \sum_{c=1}^{C} p_c (1 - p_c)
$$</p><p>基尼系数的含义是：随机抽取两个样本，它们属于不同类的概率。</p><p>基尼系数越小，数据越纯。CART 选择使基尼系数减少最多的划分。</p><p><strong>第八步：剪枝</strong></p><p>决策树容易过拟合，需要剪枝。</p><p><strong>预剪枝</strong>（pre-pruning）：</p><ul><li>限制树的深度</li><li>限制叶子节点的最小样本数</li><li>限制划分所需的最小信息增益</li></ul><p><strong>后剪枝</strong>（post-pruning）：</p><ul><li>先生成完全生长的树</li><li>自底向上评估剪枝后的验证集误差</li><li>如果剪枝后误差不增加，则剪掉</li></ul><h3 id=决策边界>决策边界<a hidden class=anchor aria-hidden=true href=#决策边界>#</a></h3><p>决策树的决策边界是<strong>轴对齐</strong>的（axis-aligned），即与坐标轴平行。这意味着决策边界是分段常数函数。这既是优点（可解释性），也是缺点（难以拟合对角线边界）。</p><hr><h2 id=六支持向量机最大间隔的艺术>六、支持向量机：最大间隔的艺术<a hidden class=anchor aria-hidden=true href=#六支持向量机最大间隔的艺术>#</a></h2><p><strong>时间：1963 年 - 弗拉基米尔·万普尼克 (Vladimir Vapnik)；1992 年 - 核技巧</strong></p><h3 id=冷战时期的智慧>冷战时期的智慧<a hidden class=anchor aria-hidden=true href=#冷战时期的智慧>#</a></h3><p>1963 年，苏联数学家弗拉基米尔·万普尼克提出了一个革命性的想法：不要只关注分类错误，要关注分类边界到最近点的距离。</p><h3 id=推导过程-5>推导过程<a hidden class=anchor aria-hidden=true href=#推导过程-5>#</a></h3><p><strong>第一步：线性可分的情况</strong></p><p>假设数据集 ${(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)}$ 线性可分，其中 $y_i \in {-1, +1}$。我们要找一个超平面 $\mathbf{w}^T \mathbf{x} + b = 0$ 将两类分开。</p><p>超平面的间隔定义为：</p><p>$$
\gamma = \min_i \frac{|y_i (\mathbf{w}^T \mathbf{x}_i + b)|}{|\mathbf{w}|}
$$</p><p>我们要最大化这个间隔。</p><p><strong>第二步：间隔的规范化</strong></p><p>注意到如果 $(\mathbf{w}, b)$ 是解，那么 $(k\mathbf{w}, kb)$ 对任意 $k > 0$ 也是解，因为：</p><p>$$
k\mathbf{w}^T \mathbf{x} + kb = 0 \iff \mathbf{w}^T \mathbf{x} + b = 0
$$</p><p>且间隔为：</p><p>$$
\frac{|y_i (k\mathbf{w}^T \mathbf{x}_i + kb)|}{|k\mathbf{w}|} = \frac{k|y_i (\mathbf{w}^T \mathbf{x}_i + b)|}{k|\mathbf{w}|} = \frac{|y_i (\mathbf{w}^T \mathbf{x}_i + b)|}{|\mathbf{w}|}
$$</p><p>因此，我们可以选择一个特定的尺度。选择让间隔边界的点满足：</p><p>$$
y_i (\mathbf{w}^T \mathbf{x}_i + b) = 1
$$</p><p>这些点就是<strong>支持向量</strong>（support vectors）。于是：</p><p>$$
\gamma = \min_i \frac{|y_i (\mathbf{w}^T \mathbf{x}_i + b)|}{|\mathbf{w}|} = \frac{1}{|\mathbf{w}|}
$$</p><p>最大化间隔等价于最小化 $|\mathbf{w}|$。</p><p><strong>第三步：原始问题</strong></p><p>因此，SVM 的优化问题是：</p><p>$$
\begin{align}
\min_{\mathbf{w}, b} \quad & \frac{1}{2} |\mathbf{w}|^2 \
\text{s.t.} \quad & y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1, \quad i = 1, \ldots, n
\end{align}
$$</p><p>目标函数加 $\frac{1}{2}$ 是为了求导方便（$|\mathbf{w}|^2$ 的导数是 $2\mathbf{w}$，乘 $\frac{1}{2}$ 后导数是 $\mathbf{w}$）。</p><p><strong>第四步：拉格朗日对偶</strong></p><p>引入拉格朗日乘子 $\alpha_i \geq 0$：</p><p>$$
\mathcal{L}(\mathbf{w}, b, \alpha) = \frac{1}{2} |\mathbf{w}|^2 - \sum_{i=1}^{n} \alpha_i [y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1]
$$</p><p>对偶问题的第一步是对原始变量求极值：</p><p>$$
\min_{\mathbf{w}, b} \max_{\alpha \geq 0} \mathcal{L}(\mathbf{w}, b, \alpha)
$$</p><p>先对 $\mathbf{w}$ 求导：</p><p>$$
\nabla_{\mathbf{w}} \mathcal{L} = \mathbf{w} - \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}<em>i = \mathbf{0} \Rightarrow \mathbf{w} = \sum</em>{i=1}^{n} \alpha_i y_i \mathbf{x}_i
$$</p><p>对 $b$ 求导：</p><p>$$
\frac{\partial \mathcal{L}}{\partial b} = -\sum_{i=1}^{n} \alpha_i y_i = 0 \Rightarrow \sum_{i=1}^{n} \alpha_i y_i = 0
$$</p><p>将 $w$ 和约束代入：</p><p>$$
\mathcal{L}(\alpha) = \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j x_i^T x_j - \sum_{i=1}^{n} \alpha_i
$$</p><p>对偶问题是：</p><p>$$
\begin{align}
\max_{\alpha} \quad & \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}<em>j \
\text{s.t.} \quad & \alpha_i \geq 0, \quad i = 1, \ldots, n \
& \sum</em>{i=1}^{n} \alpha_i y_i = 0
\end{align}
$$</p><p><strong>第五步：预测</strong></p><p>训练完成后，预测为：</p><p>$$
f(\mathbf{x}) = \text{sign}\left(\mathbf{w}^T \mathbf{x} + b\right) = \text{sign}\left(\sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i^T \mathbf{x} + b\right)
$$</p><p>注意只有支持向量（$\alpha_i > 0$）起作用。</p><p><strong>第六步：软间隔</strong></p><p>实际数据可能不是线性可分的。引入松弛变量 $\xi_i \geq 0$：</p><p>$$
\begin{align}
\min_{\mathbf{w}, b, \xi} \quad & \frac{1}{2} |\mathbf{w}|^2 + C \sum_{i=1}^{n} \xi_i \
\text{s.t.} \quad & y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 - \xi_i, \quad i = 1, \ldots, n \
& \xi_i \geq 0, \quad i = 1, \ldots, n
\end{align}
$$</p><p>其中 $C$ 是惩罚参数，控制对错分类的容忍度。</p><p>对偶问题形式相同，只是约束变为 $0 \leq \alpha_i \leq C$。</p><p><strong>第七步：核技巧</strong></p><p>非线性可分怎么办？将数据映射到高维空间 $\phi: \mathbb{R}^d \to \mathbb{R}^D$（$D \gg d$）。</p><p>对偶问题变为：</p><p>$$
\max_{\alpha} \quad \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j \phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j)
$$</p><p><strong>核技巧的魔法</strong>：我们不需要显式计算 $\phi(\mathbf{x})$，只需要知道内积。定义核函数：</p><p>$$
K(\mathbf{x}, \mathbf{x}&rsquo;) = \phi(\mathbf{x})^T \phi(\mathbf{x}&rsquo;)
$$</p><p>于是：</p><p>$$
\max_{\alpha} \quad \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j K(\mathbf{x}_i, \mathbf{x}_j)
$$</p><p>预测为：</p><p>$$
f(\mathbf{x}) = \text{sign}\left(\sum_{i=1}^{n} \alpha_i y_i K(\mathbf{x}_i, \mathbf{x}) + b\right)
$$</p><p><strong>常用的核函数</strong>：</p><ol><li><p><strong>线性核</strong>：$K(\mathbf{x}, \mathbf{x}&rsquo;) = \mathbf{x}^T \mathbf{x}'$</p></li><li><p><strong>多项式核</strong>：$K(\mathbf{x}, \mathbf{x}&rsquo;) = (\mathbf{x}^T \mathbf{x}&rsquo; + c)^d$</p></li><li><p><strong>高斯核（RBF）</strong>：$K(\mathbf{x}, \mathbf{x}&rsquo;) = \exp(-\gamma |\mathbf{x} - \mathbf{x}&rsquo;|^2)$</p></li><li><p><strong>Sigmoid 核</strong>：$K(\mathbf{x}, \mathbf{x}&rsquo;) = \tanh(\kappa \mathbf{x}^T \mathbf{x}&rsquo; + c)$</p></li></ol><p><strong>为什么高斯核有效？</strong></p><p>考虑高斯核：</p><p>$$
K(\mathbf{x}, \mathbf{x}&rsquo;) = \exp(-\gamma |\mathbf{x} - \mathbf{x}&rsquo;|^2) = \exp\left(-\gamma \sum_{j=1}^{d} (x_j - x&rsquo;_j)^2\right)
$$</p><p>使用泰勒展开：</p><p>$$
\exp(-\gamma |\mathbf{x} - \mathbf{x}&rsquo;|^2) = \sum_{k=0}^{\infty} \frac{(-\gamma)^k}{k!} |\mathbf{x} - \mathbf{x}&rsquo;|^{2k}
$$</p><p>展开 $|\mathbf{x} - \mathbf{x}&rsquo;|^{2k}$ 后，得到无限维的特征映射。因此，高斯核对应无限维的特征空间！</p><hr><h2 id=七k-均值聚类迭代收敛的美学>七、K 均值聚类：迭代收敛的美学<a hidden class=anchor aria-hidden=true href=#七k-均值聚类迭代收敛的美学>#</a></h2><p><strong>时间：1957 年 - 雨果·斯坦因豪斯 (Hugo Steinhaus)；1965 年 - 劳埃德算法 (Lloyd&rsquo;s Algorithm)</strong></p><h3 id=聚类的启蒙>聚类的启蒙<a hidden class=anchor aria-hidden=true href=#聚类的启蒙>#</a></h3><p>1957 年，波兰数学家雨果·斯坦因豪斯在研究"平面上的点的集合"问题时，提出了将点集划分为 $k$ 个簇的方法。</p><h3 id=推导过程-6>推导过程<a hidden class=anchor aria-hidden=true href=#推导过程-6>#</a></h3><p><strong>第一步：定义目标函数</strong></p><p>给定 $n$ 个数据点 ${\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n}$ 和 $k$ 个簇中心 ${\mathbf{c}_1, \mathbf{c}_2, \ldots, \mathbf{c}_k}$，我们要最小化簇内平方误差：</p><p>$$
J = \sum_{i=1}^{n} \sum_{j=1}^{k} \mathbb{I}(z_i = j) |\mathbf{x}_i - \mathbf{c}_j|^2
$$</p><p>其中 $z_i \in {1, 2, \ldots, k}$ 是数据点 $\mathbf{x}_i$ 的簇标签。</p><p><strong>第二步：交替优化</strong></p><p>这是一个非凸优化问题，很难找到全局最优。但我们可以交替优化 $\mathbf{c}$ 和 $z$。</p><p><strong>E 步（期望步）</strong>：固定簇中心 ${\mathbf{c}_1, \ldots, \mathbf{c}_k}$，更新分配 $z_i$。</p><p>对于每个数据点 $\mathbf{x}_i$，选择最近的簇中心：</p><p>$$
z_i = \arg\min_{j} |\mathbf{x}_i - \mathbf{c}_j|^2
$$</p><p><strong>M 步（最大化步）</strong>：固定分配 $z$，更新簇中心 $\mathbf{c}_j$。</p><p>对于每个簇 $j$，最小化 $\sum_{i: z_i = j} |\mathbf{x}_i - \mathbf{c}_j|^2$。</p><p>求导：</p><p>$$
\frac{\partial}{\partial \mathbf{c}<em>j} \sum</em>{i: z_i = j} |\mathbf{x}_i - \mathbf{c}<em>j|^2 = \sum</em>{i: z_i = j} -2(\mathbf{x}_i - \mathbf{c}_j) = \mathbf{0}
$$</p><p>因此：</p><p>$$
\mathbf{c}<em>j = \frac{\sum</em>{i: z_i = j} \mathbf{x}_i}{|{i: z_i = j}|}
$$</p><p>这是簇 $j$ 中所有点的均值，因此称为"K 均值"。</p><p><strong>第三步：收敛性分析</strong></p><p>每次 E 步和 M 步后，目标函数 $J$ 都会减少（或保持不变）：</p><ul><li>E 步：每个点选择最近的簇中心，不会增加距离</li><li>M 步：簇中心设为均值，使该簇内误差最小</li></ul><p>由于簇的分配方式有限（最多 $k^n$ 种），算法必然在有限步内收敛。</p><p><strong>第四步：K 均值++ 初始化</strong></p><p>K 均值对初始化敏感。K-Means++ 使用概率初始化：</p><ol><li>随机选择第一个中心 $\mathbf{c}_1$</li><li>对于未选的点 $\mathbf{x}$，计算 $D(\mathbf{x}) = \min_j |\mathbf{x} - \mathbf{c}_j|^2$</li><li>以概率 $\frac{D(\mathbf{x})}{\sum_{\mathbf{x}&rsquo;} D(\mathbf{x}&rsquo;)}$ 选择下一个中心</li><li>重复直到选择 $k$ 个中心</li></ol><p>这种初始化使得初始中心之间相互远离，提升了聚类质量。</p><p><strong>第五步：选择 K</strong></p><p>肘部法则（Elbow Method）：</p><ol><li>对不同的 $k$ 运行 K 均值</li><li>绘制 $k$ vs. 目标函数 $J$ 的曲线</li><li>选择肘部（曲线平缓的点）对应的 $k$</li></ol><p>轮廓系数（Silhouette Coefficient）：</p><p>对于数据点 $\mathbf{x}_i$：</p><ul><li>$a_i = \frac{1}{|C_{z_i}| - 1} \sum_{j \neq i, z_j = z_i} |\mathbf{x}_i - \mathbf{x}_j|$（同簇平均距离）</li><li>$b_i = \min_{c \neq z_i} \frac{1}{|C_c|} \sum_{j: z_j = c} |\mathbf{x}_i - \mathbf{x}_j|$（最近异簇平均距离）</li></ul><p>轮廓系数：</p><p>$$
s_i = \frac{b_i - a_i}{\max(a_i, b_i)}
$$</p><p>平均轮廓系数越大，聚类效果越好。</p><hr><h2 id=八随机森林民主投票的胜利>八、随机森林：民主投票的胜利<a hidden class=anchor aria-hidden=true href=#八随机森林民主投票的胜利>#</a></h2><p><strong>时间：2001 年 - 里奥·布雷曼 (Leo Breiman)</strong></p><h3 id=从决策树到森林>从决策树到森林<a hidden class=anchor aria-hidden=true href=#从决策树到森林>#</a></h3><p>2001 年，统计学家里奥·布雷曼发表了里程碑式的论文，提出了随机森林。</p><h3 id=推导过程-7>推导过程<a hidden class=anchor aria-hidden=true href=#推导过程-7>#</a></h3><p><strong>第一步：Bagging 的思想</strong></p><p>Bagging（Bootstrap Aggregating）的核心思想：对训练集进行多次 Bootstrap 采样，每次训练一个基学习器，最后聚合。</p><p>Bootstrap 采样：从原始训练集有放回地采样 $n$ 个样本（$n$ 是原始样本数）。每次约有 $63.2%$ 的样本被采样到，称为<strong>袋内样本</strong>（in-bag samples）；未被采样的 $36.8%$ 称为<strong>袋外样本</strong>（out-of-bag samples, OOB）。</p><p>对于回归问题，随机森林的预测是 $T$ 棵决策树预测的平均：</p><p>$$
\hat{f}(\mathbf{x}) = \frac{1}{T} \sum_{t=1}^{T} f_t(\mathbf{x})
$$</p><p>对于分类问题，采用多数投票：</p><p>$$
\hat{y} = \arg\max_c \sum_{t=1}^{T} \mathbb{I}(f_t(\mathbf{x}) = c)
$$</p><p><strong>第二步：偏差-方差分解</strong></p><p>对于回归问题，定义均方误差：</p><p>$$
\text{MSE} = \mathbb{E}[(\hat{f}(\mathbf{x}) - y)^2]
$$</p><p>分解为偏差和方差：</p><p>$$
\text{MSE} = \text{Bias}^2 + \text{Variance} + \text{Noise}
$$</p><p>其中：</p><ul><li>$\text{Bias}^2 = (\mathbb{E}[\hat{f}(\mathbf{x})] - f^{\ast}(\mathbf{x}))^2$（模型平均与真实值的差距）</li><li>$\text{Variance} = \mathbb{E}[(\hat{f}(\mathbf{x}) - \mathbb{E}[\hat{f}(\mathbf{x})])^2]$（模型预测的不稳定性）</li><li>$\text{Noise} = \mathbb{E}[(y - f^{\ast}(\mathbf{x}))^2]$（不可约误差）</li></ul><p>Bagging 减少方差的推导：</p><p>设 $T$ 个基学习器 $f_1, f_2, \ldots, f_T$，满足：</p><ul><li>$\mathbb{E}[f_t] = \bar{f}$（无偏）</li><li>$\text{Var}(f_t) = \sigma^2$（同方差）</li><li>$\text{Cov}(f_i, f_j) = \rho \sigma^2$（同协方差）</li></ul><p>Bagging 预测为 $\hat{f} = \frac{1}{T} \sum_{t=1}^{T} f_t$，其方差为：</p><p>$$
\begin{align}
\text{Var}(\hat{f}) &= \text{Var}\left(\frac{1}{T} \sum_{t=1}^{T} f_t\right) \
&= \frac{1}{T^2} \text{Var}\left(\sum_{t=1}^{T} f_t\right) \
&= \frac{1}{T^2} \left( \sum_{t=1}^{T} \text{Var}(f_t) + 2 \sum_{i &lt; j} \text{Cov}(f_i, f_j) \right) \
&= \frac{1}{T^2} \left( T \sigma^2 + 2 \cdot \frac{T(T-1)}{2} \rho \sigma^2 \right) \
&= \frac{1}{T^2} \left( T \sigma^2 + T(T-1) \rho \sigma^2 \right) \
&= \frac{\sigma^2}{T} + \frac{T-1}{T} \rho \sigma^2 \
&= \rho \sigma^2 + \frac{1 - \rho}{T} \sigma^2
\end{align}
$$</p><p>当 $T \to \infty$，$\text{Var}(\hat{f}) \to \rho \sigma^2$。</p><p>如果基学习器完全相关（$\rho = 1$），$\text{Var}(\hat{f}) = \sigma^2$，Bagging 没有作用。
如果基学习器独立（$\rho = 0$），$\text{Var}(\hat{f}) = \frac{\sigma^2}{T} \to 0$，方差趋于零！</p><p>因此，Bagging 的关键是<strong>降低基学习器之间的相关性</strong>。</p><p><strong>第三步：随机森林的去相关机制</strong></p><p>随机森林引入两个随机化：</p><ol><li><strong>Bootstrap 采样</strong>：每棵树看到不同的训练数据</li><li><strong>随机特征选择</strong>：在每个分裂点，随机选择 $m \leq d$ 个特征，从中选择最优分裂</li></ol><p>对于分类问题，常用 $m = \lfloor \sqrt{d} \rfloor$；对于回归问题，常用 $m = \lfloor d/3 \rfloor$。</p><p><strong>第四步：OOB 误差估计</strong></p><p>对于每棵树 $t$，使用袋外样本预测。对于数据点 $\mathbf{x}_i$，只有未用于训练第 $t$ 棵树的树（即 $\mathbf{x}_i$ 是第 $t$ 棵树的 OOB 样本）才能预测 $\mathbf{x}_i$。</p><p>设 $\mathcal{T}_i = {t : \mathbf{x}_i \text{ is OOB for tree } t}$，则 OOB 预测为：</p><p>$$
\hat{y}<em>i^{\text{OOB}} = \begin{cases}
\arg\max_c \sum</em>{t \in \mathcal{T}_i} \mathbb{I}(f_t(\mathbf{x}_i) = c) & \text{classification} \
\frac{1}{|\mathcal{T}<em>i|} \sum</em>{t \in \mathcal{T}_i} f_t(\mathbf{x}_i) & \text{regression}
\end{cases}
$$</p><p>OOB 误差是无偏的交叉验证估计，无需额外的验证集。</p><p><strong>第五步：特征重要性</strong></p><p><strong>置换重要性</strong>（Permutation Importance）：</p><ol><li>计算原始 OOB 误差 $e_{\text{original}}$</li><li>对特征 $j$，在 OOB 样本中随机置换该特征的值</li><li>重新计算 OOB 误差 $e_{\text{permuted}}$</li><li>特征重要性：$\text{Importance}<em>j = e</em>{\text{permuted}} - e_{\text{original}}$</li></ol><p>置换后的误差增加越多，说明该特征越重要。</p><hr><h2 id=九梯度提升机贪婪优化之美>九、梯度提升机：贪婪优化之美<a hidden class=anchor aria-hidden=true href=#九梯度提升机贪婪优化之美>#</a></h2><p><strong>时间：2001 年 - 杰罗姆·弗里德曼 (Jerome Friedman)</strong></p><h3 id=函数空间中的梯度下降>函数空间中的梯度下降<a hidden class=anchor aria-hidden=true href=#函数空间中的梯度下降>#</a></h3><p>2001 年，杰罗姆·弗里德曼提出了梯度提升机（Gradient Boosting Machine, GBM）。</p><h3 id=推导过程-8>推导过程<a hidden class=anchor aria-hidden=true href=#推导过程-8>#</a></h3><p><strong>第一步：理解前向分步算法</strong></p><p>我们想学习函数 $F: \mathcal{X} \to \mathbb{R}$，使得期望损失最小：</p><p>$$
F^{\ast} = \arg\min_{F} \mathbb{E}_{\mathbf{x}, y}[L(y, F(\mathbf{x}))]
$$</p><p>GBM 采用<strong>前向分步算法</strong>（Forward Stagewise Algorithm）。假设我们已经构建了 $m-1$ 轮模型 $F_{m-1}(\mathbf{x})$，第 $m$ 轮的目标是找到一个新模型 $h(\mathbf{x})$ 和步长 $\rho$：</p><p>$$
(F_m, \rho) = \arg\min_{h, \rho} \sum_{i=1}^{n} L(y_i, F_{m-1}(\mathbf{x}_i) + \rho h(\mathbf{x}_i))
$$</p><p>然后更新：</p><p>$$
F_m(\mathbf{x}) = F_{m-1}(\mathbf{x}) + \rho h(\mathbf{x})
$$</p><p><strong>第二步：函数优化的梯度下降</strong></p><p>在欧几里得空间中，目标函数 $f: \mathbb{R}^d \to \mathbb{R}$ 的梯度下降为：</p><p>$$
\mathbf{x}_{t+1} = \mathbf{x}_t - \eta \nabla f(\mathbf{x}_t)
$$</p><p>其中 $\eta$ 是学习率。</p><p>在<strong>函数空间</strong>中，我们对函数 $F$ 进行优化。定义损失函数：</p><p>$$
\mathcal{L}(F) = \sum_{i=1}^{n} L(y_i, F(\mathbf{x}_i))
$$</p><p>$\mathcal{L}(F)$ 在函数空间中的"梯度"是：</p><p>$$
\nabla_F \mathcal{L}(F) = \left( \frac{\partial \mathcal{L}(F)}{\partial F(\mathbf{x}_1)}, \frac{\partial \mathcal{L}(F)}{\partial F(\mathbf{x}_2)}, \ldots, \frac{\partial \mathcal{L}(F)}{\partial F(\mathbf{x}_n)} \right)
$$</p><p>计算：</p><p>$$
\frac{\partial \mathcal{L}(F)}{\partial F(\mathbf{x}_i)} = \frac{\partial L(y_i, F(\mathbf{x}_i))}{\partial F(\mathbf{x}<em>i)} = \left. \frac{\partial L(y, F)}{\partial F} \right|</em>{y=y_i, F=F(\mathbf{x}_i)}
$$</p><p>因此，&ldquo;梯度下降"更新为：</p><p>$$
F_{m}(\mathbf{x}) = F_{m-1}(\mathbf{x}) - \eta \left. \frac{\partial L(y, F)}{\partial F} \right|<em>{y=y, F=F</em>{m-1}(\mathbf{x})}
$$</p><p><strong>第三步：拟合负梯度</strong></p><p>对于每个样本 $\mathbf{x}_i$，负梯度为：</p><p>$$
r_i = -\left. \frac{\partial L(y, F)}{\partial F} \right|<em>{y=y_i, F=F</em>{m-1}(\mathbf{x}_i)}
$$</p><p>我们要用一个弱学习器 $h(\mathbf{x})$ 拟合这些负梯度：</p><p>$$
h = \arg\min_{h} \sum_{i=1}^{n} (r_i - h(\mathbf{x}_i))^2
$$</p><p>然后更新：</p><p>$$
F_{m}(\mathbf{x}) = F_{m-1}(\mathbf{x}) + \eta h(\mathbf{x})
$$</p><p><strong>第四步：平方损失的情况</strong></p><p>对于平方损失 $L(y, F) = \frac{1}{2}(y - F)^2$：</p><p>$$
\frac{\partial L}{\partial F} = -(y - F)
$$</p><p>负梯度为：</p><p>$$
r_i = -(y_i - F_{m-1}(\mathbf{x}<em>i)) = F</em>{m-1}(\mathbf{x}_i) - y_i
$$</p><p>这正是残差！因此，GBM 的每一步都是在拟合残差。</p><p><strong>第五步：逻辑损失的情况</strong></p><p>对于逻辑损失（二元分类）：</p><p>$$
L(y, F) = \log(1 + e^{-y F}), \quad y \in {-1, +1}
$$</p><p>计算梯度：</p><p>$$
\frac{\partial L}{\partial F} = \frac{-y e^{-y F}}{1 + e^{-y F}} = -y \cdot \sigma(-y F)
$$</p><p>其中 $\sigma(z) = \frac{1}{1 + e^{-z}}$ 是 Sigmoid 函数。</p><p>负梯度为：</p><p>$$
r_i = y_i \cdot \sigma(-y_i F_{m-1}(\mathbf{x}_i))
$$</p><p>拟合这个负梯度后，更新：</p><p>$$
F_{m}(\mathbf{x}) = F_{m-1}(\mathbf{x}) + \eta h(\mathbf{x})
$$</p><p>预测概率为 $\sigma(F(\mathbf{x}))$。</p><p><strong>第六步：行搜索优化步长</strong></p><p>拟合 $h(\mathbf{x})$ 后，我们可以用行搜索优化步长 $\rho$：</p><p>$$
\rho = \arg\min_{\rho} \sum_{i=1}^{n} L(y_i, F_{m-1}(\mathbf{x}_i) + \rho h(\mathbf{x}_i))
$$</p><p>对于平方损失：</p><p>$$
\frac{\partial}{\partial \rho} \sum_{i=1}^{n} \frac{1}{2}(y_i - F_{m-1}(\mathbf{x}_i) - \rho h(\mathbf{x}_i))^2 = 0
$$</p><p>$$
\sum_{i=1}^{n} (y_i - F_{m-1}(\mathbf{x}_i) - \rho h(\mathbf{x}_i)) \cdot (-h(\mathbf{x}_i)) = 0
$$</p><p>$$
\sum_{i=1}^{n} (F_{m-1}(\mathbf{x}_i) - y_i) h(\mathbf{x}<em>i) = \rho \sum</em>{i=1}^{n} h(\mathbf{x}_i)^2
$$</p><p>$$
\rho = \frac{\sum_{i=1}^{n} (F_{m-1}(\mathbf{x}_i) - y_i) h(\mathbf{x}<em>i)}{\sum</em>{i=1}^{n} h(\mathbf{x}_i)^2}
$$</p><p><strong>第七步：正则化</strong></p><p>GBM 容易过拟合，常用的正则化方法：</p><ol><li><p><strong>学习率</strong>：使用较小的学习率 $\eta$（如 0.01 或 0.1），配合更多的迭代次数。</p></li><li><p><strong>树深限制</strong>：限制决策树的深度（如 max_depth = 3）。</p></li><li><p><strong>叶子节点最小样本数</strong>：限制叶子节点的最小样本数（如 min_samples_leaf = 10）。</p></li><li><p><strong>子采样</strong>：每次迭代只使用部分样本（如 subsample = 0.8），类似随机森林。</p></li></ol><p><strong>第八步：XGBoost 的二阶近似</strong></p><p>XGBoost 使用泰勒展开到二阶：</p><p>$$
L(y, F + \Delta F) \approx L(y, F) + \frac{\partial L}{\partial F} \Delta F + \frac{1}{2} \frac{\partial^2 L}{\partial F^2} (\Delta F)^2
$$</p><p>定义：</p><ul><li>一阶导数：$g = \frac{\partial L}{\partial F}$</li><li>二阶导数：$h = \frac{\partial^2 L}{\partial F^2}$</li></ul><p>目标函数近似为：</p><p>$$
\mathcal{L}(F + \Delta F) \approx \sum_{i=1}^{n} [L(y_i, F(\mathbf{x}_i)) + g_i \Delta F(\mathbf{x}_i) + \frac{1}{2} h_i (\Delta F(\mathbf{x}_i))^2] + \Omega(f)
$$</p><p>其中 $\Omega(f)$ 是正则项：</p><p>$$
\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T} w_j^2
$$</p><p>这里 $T$ 是叶子节点数，$w_j$ 是叶子节点的输出值，$\gamma$ 和 $\lambda$ 是正则化系数。</p><p>通过推导，叶子节点 $j$ 的最优值为：</p><p>$$
w_j^{\ast} = -\frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda}
$$</p><p>其中 $I_j$ 是叶子节点 $j$ 中的样本集合。</p><hr><h2 id=十主成分分析降维的艺术>十、主成分分析：降维的艺术<a hidden class=anchor aria-hidden=true href=#十主成分分析降维的艺术>#</a></h2><p><strong>时间：1901 年 - 卡尔·皮尔逊 (Karl Pearson)</strong></p><h3 id=最早的降维方法>最早的降维方法<a hidden class=anchor aria-hidden=true href=#最早的降维方法>#</a></h3><p>1901 年，英国数学家卡尔·皮尔逊提出了主成分分析（PCA）。这是数学史上最早的降维方法之一。</p><h3 id=推导过程-9>推导过程<a hidden class=anchor aria-hidden=true href=#推导过程-9>#</a></h3><p><strong>第一步：数据标准化</strong></p><p>给定 $n$ 个 $d$ 维数据点 $\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n$，首先中心化：</p><p>$$
\tilde{\mathbf{x}}_i = \mathbf{x}_i - \bar{\mathbf{x}}
$$</p><p>其中 $\bar{\mathbf{x}} = \frac{1}{n} \sum_{i=1}^{n} \mathbf{x}_i$ 是均值。</p><p>如果特征量纲不同，还需要标准化：</p><p>$$
z_{i,j} = \frac{x_{i,j} - \bar{x}_j}{\sigma_j}
$$</p><p>其中 $\sigma_j$ 是特征 $j$ 的标准差。</p><p><strong>第二步：最大化投影方差</strong></p><p>我们要找到一个单位向量 $\mathbf{v}$（$|\mathbf{v}| = 1$），使得数据在 $\mathbf{v}$ 方向上的投影方差最大。</p><p>投影为：</p><p>$$
y_i = \mathbf{v}^T \tilde{\mathbf{x}}_i
$$</p><p>投影方差为：</p><p>$$
\text{Var}(y) = \frac{1}{n} \sum_{i=1}^{n} y_i^2 = \frac{1}{n} \sum_{i=1}^{n} (\mathbf{v}^T \tilde{\mathbf{x}}<em>i)^2 = \frac{1}{n} \mathbf{v}^T \left( \sum</em>{i=1}^{n} \tilde{\mathbf{x}}_i \tilde{\mathbf{x}}_i^T \right) \mathbf{v}
$$</p><p>定义协方差矩阵：</p><p>$$
\mathbf{C} = \frac{1}{n} \sum_{i=1}^{n} \tilde{\mathbf{x}}_i \tilde{\mathbf{x}}_i^T = \frac{1}{n} \tilde{\mathbf{X}}^T \tilde{\mathbf{X}}
$$</p><p>其中 $\tilde{\mathbf{X}}$ 是中心化后的数据矩阵。</p><p>因此，优化问题为：</p><p>$$
\max_{\mathbf{v}} \mathbf{v}^T \mathbf{C} \mathbf{v} \quad \text{s.t.} \quad \mathbf{v}^T \mathbf{v} = 1
$$</p><p><strong>第三步：使用拉格朗日乘子法</strong></p><p>引入拉格朗日乘子 $\lambda$：</p><p>$$
\mathcal{L}(\mathbf{v}, \lambda) = \mathbf{v}^T \mathbf{C} \mathbf{v} - \lambda (\mathbf{v}^T \mathbf{v} - 1)
$$</p><p>对 $\mathbf{v}$ 求导：</p><p>$$
\nabla_{\mathbf{v}} \mathcal{L} = 2\mathbf{C} \mathbf{v} - 2\lambda \mathbf{v} = \mathbf{0}
$$</p><p>因此：</p><p>$$
\mathbf{C} \mathbf{v} = \lambda \mathbf{v}
$$</p><p>这正是<strong>特征值问题</strong>！$\mathbf{v}$ 是 $\mathbf{C}$ 的特征向量，$\lambda$ 是对应的特征值。</p><p>投影方差为：</p><p>$$
\mathbf{v}^T \mathbf{C} \mathbf{v} = \mathbf{v}^T (\lambda \mathbf{v}) = \lambda \mathbf{v}^T \mathbf{v} = \lambda
$$</p><p>因此，<strong>投影方差等于对应的特征值</strong>。最大化方差等价于选择最大特征值对应的特征向量。</p><p><strong>第四步：多个主成分</strong></p><p>第一个主成分 $\mathbf{v}_1$ 是 $\mathbf{C}$ 的最大特征值对应的特征向量。</p><p>第二个主成分 $\mathbf{v}_2$ 在与 $\mathbf{v}_1$ 正交的单位向量中最大化投影方差：</p><p>$$
\max_{\mathbf{v}} \mathbf{v}^T \mathbf{C} \mathbf{v} \quad \text{s.t.} \quad \mathbf{v}^T \mathbf{v} = 1, \mathbf{v}^T \mathbf{v}_1 = 0
$$</p><p>使用拉格朗日乘子法：</p><p>$$
\mathcal{L}(\mathbf{v}, \lambda, \mu) = \mathbf{v}^T \mathbf{C} \mathbf{v} - \lambda (\mathbf{v}^T \mathbf{v} - 1) - \mu \mathbf{v}^T \mathbf{v}_1
$$</p><p>对 $\mathbf{v}$ 求导：</p><p>$$
2\mathbf{C} \mathbf{v} - 2\lambda \mathbf{v} - \mu \mathbf{v}_1 = \mathbf{0}
$$</p><p>左乘 $\mathbf{v}_1^T$：</p><p>$$
2\mathbf{v}_1^T \mathbf{C} \mathbf{v} - 2\lambda \mathbf{v}_1^T \mathbf{v} - \mu \mathbf{v}_1^T \mathbf{v}_1 = 0
$$</p><p>由于 $\mathbf{v}_1^T \mathbf{v} = 0$ 且 $\mathbf{v}_1^T \mathbf{v}_1 = 1$：</p><p>$$
\mathbf{v}_1^T \mathbf{C} \mathbf{v} = \frac{\mu}{2}
$$</p><p>又因为 $\mathbf{C} \mathbf{v}_1 = \lambda_1 \mathbf{v}_1$：</p><p>$$
\mathbf{v}_1^T \mathbf{C} \mathbf{v} = (\mathbf{C} \mathbf{v}_1)^T \mathbf{v} = \lambda_1 \mathbf{v}_1^T \mathbf{v} = 0
$$</p><p>因此 $\mu = 0$，回到特征值问题 $\mathbf{C} \mathbf{v} = \lambda \mathbf{v}$。</p><p>$\mathbf{v}_2$ 是 $\mathbf{C}$ 的第二大特征值对应的特征向量。</p><p>一般地，第 $k$ 个主成分是 $\mathbf{C}$ 的第 $k$ 大特征值对应的特征向量。</p><p><strong>第五步：奇异值分解（SVD）</strong></p><p>协方差矩阵 $\mathbf{C} = \frac{1}{n} \tilde{\mathbf{X}}^T \tilde{\mathbf{X}}$ 的特征值分解等价于 $\tilde{\mathbf{X}}$ 的奇异值分解。</p><p>设 $\tilde{\mathbf{X}}$ 的 SVD 为：</p><p>$$
\tilde{\mathbf{X}} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T
$$</p><p>其中：</p><ul><li>$\mathbf{U} \in \mathbb{R}^{n \times d}$ 是左奇异矩阵（$\mathbf{U}^T \mathbf{U} = \mathbf{I}$）</li><li>$\mathbf{\Sigma} \in \mathbb{R}^{d \times d}$ 是对角矩阵，对角元素是奇异值 $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_d \geq 0$</li><li>$\mathbf{V} \in \mathbb{R}^{d \times d}$ 是右奇异矩阵（$\mathbf{V}^T \mathbf{V} = \mathbf{I}$）</li></ul><p>协方差矩阵为：</p><p>$$
\mathbf{C} = \frac{1}{n} \tilde{\mathbf{X}}^T \tilde{\mathbf{X}} = \frac{1}{n} \mathbf{V} \mathbf{\Sigma}^T \mathbf{U}^T \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T = \frac{1}{n} \mathbf{V} \mathbf{\Sigma}^2 \mathbf{V}^T
$$</p><p>因此，$\mathbf{C}$ 的特征向量是 $\mathbf{V}$ 的列，特征值是 $\frac{\sigma_i^2}{n}$。</p><p><strong>第六步：降维与重构</strong></p><p>保留前 $k$ 个主成分：</p><p>$$
\tilde{\mathbf{X}}&rsquo; = \tilde{\mathbf{X}} \mathbf{V}_{[:, 1:k]}
$$</p><p>重构为：</p><p>$$
\tilde{\mathbf{X}}&rsquo;&rsquo; = \tilde{\mathbf{X}}&rsquo; \mathbf{V}<em>{[:, 1:k]}^T = \tilde{\mathbf{X}} \mathbf{V}</em>{[:, 1:k]} \mathbf{V}_{[:, 1:k]}^T
$$</p><p>重构误差为：</p><p>$$
|\tilde{\mathbf{X}} - \tilde{\mathbf{X}}&rsquo;&rsquo;|<em>F^2 = \sum</em>{j=k+1}^{d} \sigma_j^2
$$</p><p>其中 $|\cdot|_F$ 是 Frobenius 范数。</p><p><strong>第七步：选择主成分数量</strong></p><p>保留前 $k$ 个主成分的解释方差比为：</p><p>$$
\frac{\sum_{i=1}^{k} \lambda_i}{\sum_{i=1}^{d} \lambda_i} = \frac{\sum_{i=1}^{k} \sigma_i^2}{\sum_{i=1}^{d} \sigma_i^2}
$$</p><p>通常选择 $k$ 使得解释方差比达到 80%-95%。</p><p>肘部法则：绘制 $k$ vs. 累积解释方差比，选择曲线趋于平缓的点。</p><hr><h2 id=结语黄金时代的遗产>结语：黄金时代的遗产<a hidden class=anchor aria-hidden=true href=#结语黄金时代的遗产>#</a></h2><p>2006 年，深度学习在 Hinton 等人的推动下开始兴起。2012 年，AlexNet 在 ImageNet 上的横空出世标志着新纪元的开始。</p><p>但请不要忘记，那些传统的机器学习算法并没有消失。它们在各自的领域依然发挥着重要作用：</p><ul><li>线性回归和逻辑回归仍然是解释性模型的首选</li><li>朴素贝叶斯在文本分类中不可替代</li><li>随机森林和梯度提升机在结构化数据上屡战屡胜</li><li>PCA 是数据可视化和特征工程的基础工具</li></ul><p>更重要的是，这些算法背后蕴含的数学思想——最小二乘、最大似然、贝叶斯推断、拉格朗日对偶、梯度优化——构成了现代机器学习的基础。即便是今天的深度神经网络，也离不开这些古老的智慧。</p><p>黄金时代或许已经过去，但它留给我们的遗产将永存。让我们怀着敬意，回顾那些用公式编织智能梦想的年代。</p><hr><p><strong>参考文献</strong>：</p><ol><li>Legendre, A. M. (1805). <em>Nouvelles méthodes pour la détermination des orbites des comètes</em>.</li><li>Cox, D. R. (1958). &ldquo;The regression analysis of binary sequences&rdquo;. <em>Journal of the Royal Statistical Society</em>.</li><li>Fix, E., & Hodges, J. L. (1951). <em>Discriminatory analysis, nonparametric discrimination: Consistency properties</em>.</li><li>Quinlan, J. R. (1986). &ldquo;Induction of decision trees&rdquo;. <em>Machine Learning</em>.</li><li>Vapnik, V. N. (1995). <em>The Nature of Statistical Learning Theory</em>.</li><li>Breiman, L. (2001). &ldquo;Random Forests&rdquo;. <em>Machine Learning</em>.</li><li>Friedman, J. H. (2001). &ldquo;Greedy Function Approximation: A Gradient Boosting Machine&rdquo;. <em>Annals of Statistics</em>.</li><li>Pearson, K. (1901). &ldquo;On lines and planes of closest fit to systems of points in space&rdquo;. <em>Philosophical Magazine</em>.</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%AE%97%E6%B3%95/>算法</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%95%B0%E5%AD%A6%E5%8F%B2/>数学史</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-15-neural-network-evolution/><span class=title>« Prev</span><br><span>神经网络算法演进：从感知机到 Transformer 的七十年征程</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-14-greens-gauss-stokes-formulas-guide/><span class=title>Next »</span><br><span>微积分的三大公式：格林、高斯与斯托克斯定理的统一视角</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 深度学习前夜：十大传统机器学习算法的历史与数学之美 on x" href="https://x.com/intent/tweet/?text=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%89%8d%e5%a4%9c%ef%bc%9a%e5%8d%81%e5%a4%a7%e4%bc%a0%e7%bb%9f%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e7%9a%84%e5%8e%86%e5%8f%b2%e4%b8%8e%e6%95%b0%e5%ad%a6%e4%b9%8b%e7%be%8e&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-15-traditional-ml-algorithms%2f&amp;hashtags=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%2c%e7%ae%97%e6%b3%95%2c%e6%95%b0%e5%ad%a6%e5%8f%b2"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 深度学习前夜：十大传统机器学习算法的历史与数学之美 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-15-traditional-ml-algorithms%2f&amp;title=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%89%8d%e5%a4%9c%ef%bc%9a%e5%8d%81%e5%a4%a7%e4%bc%a0%e7%bb%9f%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e7%9a%84%e5%8e%86%e5%8f%b2%e4%b8%8e%e6%95%b0%e5%ad%a6%e4%b9%8b%e7%be%8e&amp;summary=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%89%8d%e5%a4%9c%ef%bc%9a%e5%8d%81%e5%a4%a7%e4%bc%a0%e7%bb%9f%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e7%9a%84%e5%8e%86%e5%8f%b2%e4%b8%8e%e6%95%b0%e5%ad%a6%e4%b9%8b%e7%be%8e&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-15-traditional-ml-algorithms%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 深度学习前夜：十大传统机器学习算法的历史与数学之美 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-15-traditional-ml-algorithms%2f&title=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%89%8d%e5%a4%9c%ef%bc%9a%e5%8d%81%e5%a4%a7%e4%bc%a0%e7%bb%9f%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e7%9a%84%e5%8e%86%e5%8f%b2%e4%b8%8e%e6%95%b0%e5%ad%a6%e4%b9%8b%e7%be%8e"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 深度学习前夜：十大传统机器学习算法的历史与数学之美 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-15-traditional-ml-algorithms%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>