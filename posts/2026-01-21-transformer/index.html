<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Transformer：重塑AI世界的架构革命 | s-ai-unix's Blog</title><meta name=keywords content="深度学习,神经网络,算法"><meta name=description content="深入解读 Transformer 架构的核心原理，从自注意力机制到多头注意力，探索这个重塑 AI 世界的重要架构"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-21-transformer/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-21-transformer/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-21-transformer/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="Transformer：重塑AI世界的架构革命"><meta property="og:description" content="深入解读 Transformer 架构的核心原理，从自注意力机制到多头注意力，探索这个重塑 AI 世界的重要架构"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-21T10:00:00+08:00"><meta property="article:modified_time" content="2026-01-21T10:00:00+08:00"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="神经网络"><meta property="article:tag" content="算法"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/transformer-architecture.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/transformer-architecture.jpg"><meta name=twitter:title content="Transformer：重塑AI世界的架构革命"><meta name=twitter:description content="深入解读 Transformer 架构的核心原理，从自注意力机制到多头注意力，探索这个重塑 AI 世界的重要架构"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Transformer：重塑AI世界的架构革命","item":"https://s-ai-unix.github.io/posts/2026-01-21-transformer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Transformer：重塑AI世界的架构革命","name":"Transformer：重塑AI世界的架构革命","description":"深入解读 Transformer 架构的核心原理，从自注意力机制到多头注意力，探索这个重塑 AI 世界的重要架构","keywords":["深度学习","神经网络","算法"],"articleBody":"引言 在人工智能的发展历程中，有几个时刻标志着技术范式的根本性转变。2017年10月就是这样一个时刻——Google Research 和多伦多大学的研究者们发表了一篇名为《Attention Is All You Need》的论文，提出了 Transformer 架构。\n这篇论文的标题本身就是一种宣言：在这篇论文中，作者们向世界宣告，在处理序列数据时，注意力机制就是你所需要的一切。这篇论文不仅解决了长期困扰自然语言处理领域的难题，更开创了一个全新的 AI 时代。从 BERT 到 GPT 系列，从 PaLM 到 Claude，支撑现代大语言模型的核心架构都是 Transformer。\n但 Transformer 到底是什么？它为什么如此重要？它是如何工作的？作为一个 AI 领域的深度从业者，我希望通过这篇文章，用最通俗易懂的方式，为你彻底解读这个重塑 AI 世界的重要架构。\n第一章 背景：为什么我们需要 Transformer？ 1.1 序列数据处理的困境 在深入 Transformer 之前，让我们先理解它试图解决的问题。在自然语言处理、语音识别、机器翻译等任务中，我们面对的都是序列数据——句子是一系列词语的序列，语音是一系列声波的序列，DNA 是一系列碱基的序列。\n对于序列数据的处理，传统的做法是使用循环神经网络（RNN）及其变体，如长短期记忆网络（LSTM）和门控循环单元（GRU）。这些网络的设计理念是：按顺序处理序列中的每个元素，将信息一步一步地传递下去。\nRNN 的工作原理：想象你在读一本书。你的眼睛一次看一个字（或者一个词），然后大脑会记住这个字的意思，并结合之前记住的内容来理解整个句子。RNN 就是这样工作的——它按顺序处理输入序列，将之前的信息\"记住\"在隐藏状态中，然后用于处理下一个输入。\n1.2 RNN 的致命缺陷 然而，RNN 存在几个根本性的问题：\n第一个问题是长距离依赖问题。在处理长序列时，RNN 很难捕获序列前端和序列后端之间的关联。想象一个很长的句子：“那个在巴黎出生的，后来搬到纽约生活的，最后在北京去世的老人，年轻时是个著名的科学家。“要让 RNN 理解\"老人\"和\"年轻时\"之间的关系，信息需要从句子的一端传递到另一端。在这个过程中，信息会逐渐衰减，最终可能完全丢失。\n第二个问题是计算效率问题。RNN 必须按顺序处理序列，这意味着第一步计算完成后才能开始第二步。这种串行计算的方式无法充分利用现代 GPU 的并行计算能力。在处理长序列时，计算变得非常耗时。\n第三个问题是梯度消失和梯度爆炸问题。在反向传播过程中，梯度需要通过多个时间步传播。当序列很长时，梯度可能会变得非常小（消失）或非常大（爆炸），导致训练困难。\n1.3 注意力机制的兴起 为了解决 RNN 的问题，研究者们提出了注意力机制（Attention Mechanism）。注意力机制的核心思想是：在处理序列中的每个元素时，我们不应该只依赖之前的信息，而应该能够\"回顾\"序列中的任意位置。\n注意力的直观理解：想象你在嘈杂的咖啡馆里听朋友说话。即使周围很吵，你的大脑也能够聚焦于朋友的声音，而忽略背景噪音。注意力机制就是模拟这个过程——它让模型学会在处理每个词时，应该\"关注\"输入序列的哪些部分。\nBahdanau 等人在 2014 年提出了第一个注意力机制，用于机器翻译。这个注意力机制允许解码器在生成每个目标词时，关注源句子中的相关部分。这大大改善了机器翻译的性能。\n但早期的注意力机制仍然是与 RNN 结合使用的。真正的革命性突破来自于 2017 年的那篇论文——作者们意识到，如果只使用注意力机制，我们就可以完全摆脱 RNN 的束缚。\n第二章 核心概念：什么是注意力机制？ 2.1 注意力机制的本质 在 Transformer 中，核心是\"自注意力”（Self-Attention）机制，也称为\"缩放点积注意力”（Scaled Dot-Product Attention）。理解自注意力是理解 Transformer 的关键。\n自注意力的核心问题：对于序列中的每个元素，我们想了解它与序列中其他所有元素的关系。换句话说，当我们处理序列中的一个词时，我们想知道它应该\"关注\"序列中的哪些其他词。\n2.2 Q、K、V：注意力机制的三个角色 在实现上，自注意力机制引入了三个重要的向量：查询（Query）、键（Key）和值（Value）。这三个向量的命名来自于信息检索的类比。\n类比说明：想象你在图书馆找书。你有一个查询（Query）——你想找的主题。图书馆有一个索引系统，由一系列键（Key）组成——每本书的标题、作者等。你的查询会与每个键进行比较，找到最匹配的书，然后获取相应的值（Value）——书的内容。\n在自注意力中：\n查询（Q）：代表当前正在处理的元素\"想要问的问题\" 键（K）：代表序列中每个元素\"能回答的问题\" 值（V）：代表序列中每个元素\"包含的信息\" flowchart LR subgraph Input[\"输入序列\"] X1[\"X₁\"] X2[\"X₂\"] X3[\"X₃\"] end subgraph QKV[\"Q、K、V 生成\"] direction TB WQ[\"W_Q\"] WK[\"W_K\"] WV[\"W_V\"] end subgraph Attention[\"注意力计算\"] Score[\"注意力分数QK^T\"] Scale[\"缩放÷√d_k\"] Softmax[\"Softmax\"] Weight[\"加权求和αV\"] end subgraph Output[\"输出\"] Y1[\"Y₁\"] Y2[\"Y₂\"] Y3[\"Y₃\"] end X1 \u0026 X2 \u0026 X3 --\u003e QKV QKV --\u003e Score Score --\u003e Scale Scale --\u003e Softmax Softmax --\u003e Weight Weight --\u003e Y1 \u0026 Y2 \u0026 Y3 style Input fill:#007AFF,color:#ffffff,stroke:#007AFF,stroke-width:3px style QKV fill:#34C759,color:#ffffff,stroke:#34C759,stroke-width:2px style Attention fill:#FF9500,color:#ffffff,stroke:#FF9500,stroke-width:2px style Output fill:#007AFF,color:#ffffff,stroke:#007AFF,stroke-width:3px 2.3 详细的数学过程 让我们一步步解析自注意力的计算过程。\n第一步：线性变换\n对于输入序列中的每个元素，我们首先通过线性变换将其投影为三个向量：\n$$ Q = XW_Q $$ $$ K = XW_K $$ $$ V = XW_V $$\n其中 $X$ 是输入矩阵，每一行代表序列中的一个元素（通常是一个词向量）。$W_Q$、$W_K$、$W_V$ 是可学习的权重矩阵。\n第二步：计算注意力分数\n对于每个查询，我们计算它与所有键的相似度。在 Transformer 中，使用点积作为相似度度量：\n$$ \\text{Attention Score} = QK^T $$\n第三步：缩放\n为了防止点积结果过大导致梯度不稳定，我们对结果进行缩放：\n$$ \\text{Scaled Score} = \\frac{QK^T}{\\sqrt{d_k}} $$\n其中 $d_k$ 是键向量的维度。\n第四步：Softmax\n将缩放后的分数通过 Softmax 函数转换为概率分布：\n$$ \\alpha = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) $$\nSoftmax 确保所有注意力权重的和为 1，每个权重代表当前元素应该\"关注\"序列中其他元素的程度。\n第五步：加权求和\n最后，用注意力权重对值向量进行加权求和：\n$$ \\text{Output} = \\alpha V $$\n2.4 完整的自注意力公式 将上述步骤合并，完整的自注意力公式是：\n$$ \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n这个公式看起来很简单，但它蕴含着强大的能力。通过这个运算，每个位置的输出都包含了整个序列的信息，而且每个位置关注的程度是由数据自动学习得到的。\n2.5 多头注意力：并行多个\"视角\" Transformer 不仅仅使用一个自注意力，而是使用\"多头注意力\"（Multi-Head Attention）。这意味着我们并行地运行多组自注意力计算，然后将结果拼接起来。\n为什么需要多头？\n想象你要理解一段文字。你可能会从多个角度来理解：语法结构、语义含义、情感色彩、上下文关联等。多头注意力就是模拟这个过程——每组注意力头可以专注于不同类型的关系。\n数学表达：\n$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, head_2, …, head_h)W^O $$\n其中每个 $head_i$ 是：\n$$ head_i = \\text{Attention}(QW_{Q_i}, KW_{K_i}, VW_{V_i}) $$\n通常，Transformer 使用 8 个或 16 个注意力头。每个头的维度是原始维度的 $1/h$，所以总的计算量与单头注意力相当。\n第三章 Transformer 架构：逐层解析 3.1 整体架构概览 Transformer 的架构可以用一句话概括：编码器-解码器结构，完全基于注意力机制。\n整个模型由两部分组成：\n编码器（Encoder）：处理输入序列，生成一个表示序列信息的向量 解码器（Decoder）：基于编码器的输出和之前已经生成的内容，生成目标序列 3.2 编码器结构 每个编码器层（Encoder Layer）包含两个主要的子层：\n多头自注意力机制 前馈神经网络 每个子层后面都跟着：\n残差连接（Residual Connection）：将子层的输入直接加到输出上 层归一化（Layer Normalization）：对输出进行归一化 用公式表示：\n$$ \\text{LayerNorm}(x + \\text{Sublayer}(x)) $$\n3.3 解码器结构 解码器层（Decoder Layer）稍微复杂一些，包含三个子层：\n掩码多头自注意力（Masked Multi-Head Self-Attention） 编码器-解码器注意力（Encoder-Decoder Attention） 前馈神经网络 掩码自注意力的作用：在解码器中，我们只能\"看到\"之前已经生成的内容，不能\"看到\"未来的内容。掩码机制通过将未来的注意力权重设置为负无穷（Softmax 后变为 0）来实现这一点。\n编码器-解码器注意力：这是解码器\"关注\"编码器输出的方式。在机器翻译任务中，这允许解码器在生成每个目标词时，关注源句子中相关的部分。\n3.4 位置编码：让模型感知序列顺序 Transformer 完全基于注意力机制，没有循环结构，因此无法感知序列中元素的顺序。为了解决这个问题，Transformer 引入了位置编码（Positional Encoding）。\n位置编码的直观理解：给序列中的每个位置分配一个独特的\"位置标签\"，将这个标签加入到输入嵌入中，这样模型就能知道每个元素的相对位置。\nTransformer 使用正弦和余弦函数生成位置编码：\n$$ PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$ $$ PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\n其中 $pos$ 是位置，$i$ 是维度索引，$d_{model}$ 是模型的维度。\n为什么选择正弦和余弦函数？ 因为它们具有一个有用的性质：对于任何固定的偏移量 $k$，$PE_{pos+k}$ 可以表示为 $PE_{pos}$ 的线性组合。这意味着模型可以通过相对位置来学习关注模式。\n3.5 前馈神经网络 除了注意力层，Transformer 的每个编码器和解码器层都包含一个前馈神经网络：\n$$ \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2 $$\n这个前馈网络对每个位置独立地进行相同的变换。它的作用是为模型提供非线性变换能力和额外的表达能力。\n3.6 完整的 Transformer 架构 flowchart TB subgraph Encoder[\"编码器 Encoder\"] direction TB EEmb[\"输入嵌入Input Embedding\"] EPE[\"位置编码Positional Encoding\"] subgraph ELayers[\"编码器层 × N\"] direction TB ESA[\"多头自注意力Multi-Head Attention\"] EFN[\"前馈网络Feed Forward\"] end end subgraph Decoder[\"解码器 Decoder\"] direction TB DEmb[\"输出嵌入Output Embedding\"] DPE[\"位置编码Positional Encoding\"] subgraph DLayers[\"解码器层 × N\"] direction TB DMSA[\"掩码自注意力Masked Attention\"] EDA[\"编码器-解码器注意力Encoder-Decoder Attention\"] end Linear[\"线性层 + Softmax\"] end EEmb --\u003e EPE --\u003e ELayers ELayers --\u003e DED[\"编码器输出\"] DED --\u003e EDA DEmb --\u003e DPE --\u003e DLayers DLayers --\u003e Linear style Encoder fill:#007AFF,color:#ffffff,stroke:#007AFF,stroke-width:3px style Decoder fill:#34C759,color:#ffffff,stroke:#34C759,stroke-width:3px style ELayers fill:#FF9500,color:#ffffff,stroke:#FF9500,stroke-width:2px style DLayers fill:#FF9500,color:#ffffff,stroke:#FF9500,stroke-width:2px 现在，让我们把所有的组件放在一起，看看完整的 Transformer 架构：\n编码器部分：\n输入嵌入（Input Embedding） 位置编码（Positional Encoding） N 个编码器层（每层包含多头自注意力和前馈网络） 解码器部分：\n输出嵌入（Output Embedding） 位置编码 N 个解码器层（每层包含掩码自注意力、编码器-解码器注意力和前馈网络） 线性层和 Softmax（将输出转换为概率分布） 在原始论文中，$N=6$，意味着有 6 个编码器层和 6 个解码器层。\n第四章 为什么 Transformer 如此重要？ 4.1 并行计算：效率的革命 Transformer 相比 RNN 的最大优势之一是计算并行化。\nRNN 的局限性：在 RNN 中，第 t 步的计算依赖于第 t-1 步的隐藏状态。这意味着必须按顺序进行计算，无法并行处理序列中的不同位置。\nTransformer 的突破：在自注意力机制中，序列中所有位置之间的注意力分数可以同时计算。查询、键、值的计算也是并行进行的。这使得 Transformer 能够充分利用 GPU 的并行计算能力。\n实际影响：在处理长序列时，Transformer 的速度可以比 RNN 快几个数量级。这意味着我们可以训练更大规模的模型，处理更长的序列。\n4.2 长距离依赖：直接建立关联 RNN 的信息传递：在 RNN 中，从序列开头到序列结尾的信息需要通过多个步骤逐步传递。在这个过程中，信息可能会衰减或丢失。\nTransformer 的直接连接：在自注意力机制中，序列中的任何两个位置都可以直接相互作用。通过一次注意力计算，序列开头的元素就可以\"关注\"序列结尾的元素。\n数学上的优势：从数学上看，自注意力计算的是一个全连接图上的信息传递。所有位置之间的关系都是直接建模的，不存在信息传递的\"路径长度\"问题。\n4.3 可解释性：看见模型的\"思考过程\" 相比 RNN 和其他深度学习模型，Transformer 的注意力权重提供了一定程度的可解释性。\n可视化注意力：我们可以将注意力权重可视化，观察模型在处理某个输入时\"关注\"了哪些部分。这对于理解模型行为、调试模型和建立信任都非常有帮助。\n例如：在翻译任务中，我们可以观察解码器在生成某个目标词时，注意力权重集中在源句子的哪些词上。这让我们能够直观地看到模型的\"翻译策略\"。\n4.4 通用性：一个架构，多种任务 Transformer 架构的通用性是其另一个重要优势。\n最初设计用于机器翻译：Transformer 是为机器翻译任务设计的，但它很快被应用于各种其他任务。\nNLP 任务：文本分类、命名实体识别、问答、摘要生成、文本生成等。\n其他领域：图像处理（Vision Transformer）、语音识别、音乐生成、蛋白质结构预测（AlphaFold）、代码生成等。\n为什么 Transformer 如此通用？ 因为序列是表示各种数据的自然方式。无论是文本、图像（可以分割成 patches）、还是蛋白质序列（氨基酸序列），都可以表示为序列。Transformer 的自注意力机制能够捕获序列内部的长距离依赖关系，这使得它成为一个通用的序列建模框架。\n4.5 规模化的成功：越大越好 Transformer 架构的一个重要特性是它能够从规模化中获益。\n神经网络规模化的规律：在深度学习中，模型的性能通常随着参数数量、训练数据和计算量的增加而提升。这就是著名的\"规模定律\"（Scaling Laws）。\nTransformer 适合规模化：Transformer 的并行计算特性使得它可以高效地利用大量计算资源。随着模型规模的增大，Transformer 的性能持续提升，没有明显的饱和迹象。\nGPT 系列的例子：\nGPT-1：1.17 亿参数 GPT-2：15 亿参数 GPT-3：1750 亿参数 GPT-4：参数数量未知，但可能超过万亿 随着规模的增大，这些模型展现出了越来越强大的能力，包括涌现能力（Emergent Abilities）——在小模型上不存在的能力在大模型上突然出现。\n第五章 深入理解：关键概念的更多细节 5.1 注意力机制的变体 除了原始的缩放点积注意力，Transformer 论文还提到了另一种注意力：加性注意力（Additive Attention）。\n加性注意力的公式：\n$$ \\text{Attention}(Q, K, V) = \\text{Softmax}(w^T \\tanh(W_q Q + W_k K))V $$\n这种注意力使用一个前馈网络来计算相似度，而不是点积。它在早期的一些模型中使用，但计算效率不如点积注意力。\n现代的变体：\nRotary Position Embedding（RoPE）：在注意力计算中引入相对位置信息 ALiBi（Attention with Linear Biases）：通过简单的偏置项来编码位置信息 FlashAttention：优化的注意力计算，利用 GPU 内存层次结构提高效率 5.2 层归一化的位置 在原始的 Transformer 论文中，层归一化位于残差连接之后：\n$$ y = \\text{LayerNorm}(x + \\text{Sublayer}(x)) $$\n这被称为\"Post-LN\" Transformer。\n后来的研究（如 GPT-2）发现，将层归一化放在注意力层和前馈网络之前（Pre-LN）可能更有利于训练：\n$$ y = x + \\text{Sublayer}(\\text{LayerNorm}(x)) $$\n这种变化使得梯度流动更加稳定，便于训练更深的模型。\n5.3 Dropout 的使用 Transformer 在多个地方使用 Dropout 来防止过拟合：\n注意力权重计算后 前馈网络的激活函数后 嵌入层 原始论文使用 0.1 的 Dropout 率。\n5.4 训练技巧 标签平滑（Label Smoothing）：在计算交叉熵损失时，不使用硬标签（0 或 1），而是使用平滑后的标签（如 0.1 和 0.9）。这可以防止模型对预测结果过于自信。\nwarmup 学习率调度：学习率不是一开始就使用最大值，而是从一个很小的值逐渐增加到最大值，然后再按某种策略衰减。公式是：\n$$ lr = d_{model}^{-0.5} \\cdot \\min(step^{-0.5}, step \\cdot warmup^{-1.5}) $$\n这种策略有助于模型在训练初期找到好的优化方向。\n第六章 Transformer 的应用：从 NLP 到多模态 6.1 自然语言处理 BERT（Bidirectional Encoder Representations from Transformers）：仅使用 Transformer 编码器的模型，通过掩码语言建模和下一句预测进行预训练。BERT 在各种 NLP 基准测试中取得了突破性的成果。\nGPT（Generative Pre-trained Transformer）：仅使用 Transformer 解码器的模型，通过预测下一个词进行训练。GPT 系列模型展示了大语言模型的强大能力。\nT5（Text-to-Text Transfer Transformer）：将所有 NLP 任务统一为文本到文本的格式，使用完整的编码器-解码器结构。\n6.2 计算机视觉 Vision Transformer（ViT）：将图像分割成固定大小的 patches，然后将这些 patches 视为一个序列，用 Transformer 进行处理。ViT 证明了 Transformer 也可以很好地处理图像任务。\nDETR（Detection Transformer）：使用 Transformer 进行目标检测，通过注意力机制直接预测物体的边界框。\n6.3 多模态模型 CLIP（Contrastive Language-Image Pre-training）：使用 Transformer 编码器处理图像和文本，学习它们之间的对应关系。\nGPT-4V 和 Gemini：支持图像输入的多模态大语言模型，可以理解和生成关于图像的内容。\n6.4 其他领域 语音识别：Transformer 已被应用于端到端语音识别模型。\n音乐生成：如 MusicLM 等模型使用 Transformer 生成音乐。\n生物学：AlphaFold 使用 Transformer 架构预测蛋白质的三维结构。\n代码生成：如 CodeBERT、Codex 等模型使用 Transformer 理解和生成代码。\n第七章 Transformer 的局限性与未来 7.1 计算复杂度 Transformer 的一个主要局限是计算复杂度。对于长度为 $n$ 的序列，自注意力的计算复杂度是 $O(n^2)$。当序列很长时，这成为一个严重的瓶颈。\n解决方案：\n稀疏注意力：只计算部分位置之间的注意力 线性注意力：使用核函数将复杂度降低到 $O(n)$ 局部-全局混合：将长序列分成多个短片段 7.2 位置编码的局限 虽然位置编码允许 Transformer 感知序列顺序，但它们有一些局限性：\n外推能力有限：对训练时未见过的序列长度表现不佳 相对位置信息编码不够自然 改进方案：\nRoPE（Rotary Position Embedding） ALiBi（Attention with Linear Biases） 绝对位置编码的替代方案 7.3 Transformer 的变体 为了解决上述问题，研究者们提出了许多 Transformer 的变体：\n高效 Transformer：\nLongformer：结合局部注意力和全局注意力 BigBird：使用随机注意力和局部注意力 Performer：使用核函数实现线性复杂度 状态空间模型：\nMamba：最近引起广泛关注的选择性状态空间模型，在保持序列建模能力的同时实现了线性复杂度 7.4 未来的方向 更高效的架构：研究者们继续探索比 Transformer 更高效、更强大的架构。\n多模态融合：将 Transformer 扩展到更多模态，实现真正的多模态理解。\n持续学习：让模型能够持续学习新知识，而不会遗忘旧知识。\n可解释性和安全性：深入理解 Transformer 的工作原理，提高模型的可解释性和安全性。\n第八章 实践：如何开始使用 Transformer 8.1 Hugging Face Transformers 库 Hugging Face 提供了最流行的 Transformer 库，使得使用和微调预训练模型变得非常简单。\n基本使用示例：\nfrom transformers import pipeline # 情感分析 classifier = pipeline(\"sentiment-analysis\") result = classifier(\"I love learning about Transformers!\") # 问答 question_answerer = pipeline(\"question-answering\") result = question_answerer(question=\"What is Transformer?\", context=\"Transformer is a deep learning model.\") # 使用预训练模型 from transformers import AutoModelForSequenceClassification, AutoTokenizer model_name = \"bert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForSequenceClassification.from_pretrained(model_name) 8.2 微调 Transformer 模型 微调是让预训练模型适应特定任务的关键技术：\nfrom transformers import Trainer, TrainingArguments training_args = TrainingArguments( output_dir=\"./results\", num_train_epochs=3, per_device_train_batch_size=16, evaluation_strategy=\"epoch\", ) trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset, ) trainer.train() 8.3 从头实现 Transformer 对于想要深入理解 Transformer 的读者，从头实现是一个很好的学习方式：\n关键组件的实现：\n多头注意力层 前馈神经网络 编码器层 解码器层 完整模型 参考资源：\n原始论文《Attention Is All You Need》 Harvard NLP 团队的注释版论文（http://nlp.seas.harvard.edu/2018/04/03/attention.html） 各种在线教程和课程 结语 从 2017 年论文发表至今，Transformer 已经彻底改变了人工智能领域。它不仅仅是一个技术突破，更代表了一种新的思维方式——通过注意力机制直接建模序列元素之间的关系。\nTransformer 的成功教会我们一个重要的 lesson：有时候，简单的想法如果足够强大，也可以改变世界。自注意力的概念并不复杂，但它所释放的能量是巨大的。\n今天，我们正处于大语言模型时代的浪尖。从 ChatGPT 到 Claude，从 Gemini 到 Copilot，这些令人惊叹的 AI 应用都建立在 Transformer 架构之上。理解 Transformer，不仅是理解现代 AI 的基础，更是参与 AI 革命的入场券。\n但这只是开始。Transformer 之后，研究者们提出了许多改进和新架构。人工智能领域仍在快速演进，新的突破随时可能出现。作为这个时代的见证者和参与者，我们有幸能够亲历这一切，也有责任去理解和贡献。\n最后，用一句话总结 Transformer 的意义：它证明了，当我们给予模型足够的能力和对数据的恰当归纳偏置，它就能学会我们无法预先设计的复杂模式。这或许就是深度学习最深刻的哲学启示。\n参考文献 Vaswani, A., et al. (2017). “Attention Is All You Need”. Advances in Neural Information Processing Systems. Devlin, J., et al. (2018). “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. NAACL-HLT. Brown, T., et al. (2020). “Language Models are Few-Shot Learners”. NeurIPS. Dosovitskiy, A., et al. (2020). “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”. ICLR. Bahdanau, D., et al. (2014). “Neural Machine Translation by Jointly Learning to Align and Translate”. ICLR. ","wordCount":"985","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/transformer-architecture.jpg","datePublished":"2026-01-21T10:00:00+08:00","dateModified":"2026-01-21T10:00:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-21-transformer/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Transformer：重塑AI世界的架构革命</h1><div class=post-description>深入解读 Transformer 架构的核心原理，从自注意力机制到多头注意力，探索这个重塑 AI 世界的重要架构</div><div class=post-meta><span title='2026-01-21 10:00:00 +0800 CST'>January 21, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>985 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/transformer-architecture.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/transformer-architecture.jpg alt="Transformer 架构的艺术化呈现"></a><figcaption>注意力机制：AI 架构的革命性突破</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80 aria-label=引言>引言</a><ul><li><a href=#%e7%ac%ac%e4%b8%80%e7%ab%a0-%e8%83%8c%e6%99%af%e4%b8%ba%e4%bb%80%e4%b9%88%e6%88%91%e4%bb%ac%e9%9c%80%e8%a6%81-transformer aria-label="第一章 背景：为什么我们需要 Transformer？">第一章 背景：为什么我们需要 Transformer？</a><ul><li><a href=#11-%e5%ba%8f%e5%88%97%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86%e7%9a%84%e5%9b%b0%e5%a2%83 aria-label="1.1 序列数据处理的困境">1.1 序列数据处理的困境</a></li><li><a href=#12-rnn-%e7%9a%84%e8%87%b4%e5%91%bd%e7%bc%ba%e9%99%b7 aria-label="1.2 RNN 的致命缺陷">1.2 RNN 的致命缺陷</a></li><li><a href=#13-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e5%85%b4%e8%b5%b7 aria-label="1.3 注意力机制的兴起">1.3 注意力机制的兴起</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e7%ab%a0-%e6%a0%b8%e5%bf%83%e6%a6%82%e5%bf%b5%e4%bb%80%e4%b9%88%e6%98%af%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6 aria-label="第二章 核心概念：什么是注意力机制？">第二章 核心概念：什么是注意力机制？</a><ul><li><a href=#21-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e6%9c%ac%e8%b4%a8 aria-label="2.1 注意力机制的本质">2.1 注意力机制的本质</a></li><li><a href=#22-qkv%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e4%b8%89%e4%b8%aa%e8%a7%92%e8%89%b2 aria-label="2.2 Q、K、V：注意力机制的三个角色">2.2 Q、K、V：注意力机制的三个角色</a></li><li><a href=#23-%e8%af%a6%e7%bb%86%e7%9a%84%e6%95%b0%e5%ad%a6%e8%bf%87%e7%a8%8b aria-label="2.3 详细的数学过程">2.3 详细的数学过程</a></li><li><a href=#24-%e5%ae%8c%e6%95%b4%e7%9a%84%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%85%ac%e5%bc%8f aria-label="2.4 完整的自注意力公式">2.4 完整的自注意力公式</a></li><li><a href=#25-%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%b9%b6%e8%a1%8c%e5%a4%9a%e4%b8%aa%e8%a7%86%e8%a7%92 aria-label='2.5 多头注意力：并行多个"视角"'>2.5 多头注意力：并行多个"视角"</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e7%ab%a0-transformer-%e6%9e%b6%e6%9e%84%e9%80%90%e5%b1%82%e8%a7%a3%e6%9e%90 aria-label="第三章 Transformer 架构：逐层解析">第三章 Transformer 架构：逐层解析</a><ul><li><a href=#31-%e6%95%b4%e4%bd%93%e6%9e%b6%e6%9e%84%e6%a6%82%e8%a7%88 aria-label="3.1 整体架构概览">3.1 整体架构概览</a></li><li><a href=#32-%e7%bc%96%e7%a0%81%e5%99%a8%e7%bb%93%e6%9e%84 aria-label="3.2 编码器结构">3.2 编码器结构</a></li><li><a href=#33-%e8%a7%a3%e7%a0%81%e5%99%a8%e7%bb%93%e6%9e%84 aria-label="3.3 解码器结构">3.3 解码器结构</a></li><li><a href=#34-%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81%e8%ae%a9%e6%a8%a1%e5%9e%8b%e6%84%9f%e7%9f%a5%e5%ba%8f%e5%88%97%e9%a1%ba%e5%ba%8f aria-label="3.4 位置编码：让模型感知序列顺序">3.4 位置编码：让模型感知序列顺序</a></li><li><a href=#35-%e5%89%8d%e9%a6%88%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="3.5 前馈神经网络">3.5 前馈神经网络</a></li><li><a href=#36-%e5%ae%8c%e6%95%b4%e7%9a%84-transformer-%e6%9e%b6%e6%9e%84 aria-label="3.6 完整的 Transformer 架构">3.6 完整的 Transformer 架构</a></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e7%ab%a0-%e4%b8%ba%e4%bb%80%e4%b9%88-transformer-%e5%a6%82%e6%ad%a4%e9%87%8d%e8%a6%81 aria-label="第四章 为什么 Transformer 如此重要？">第四章 为什么 Transformer 如此重要？</a><ul><li><a href=#41-%e5%b9%b6%e8%a1%8c%e8%ae%a1%e7%ae%97%e6%95%88%e7%8e%87%e7%9a%84%e9%9d%a9%e5%91%bd aria-label="4.1 并行计算：效率的革命">4.1 并行计算：效率的革命</a></li><li><a href=#42-%e9%95%bf%e8%b7%9d%e7%a6%bb%e4%be%9d%e8%b5%96%e7%9b%b4%e6%8e%a5%e5%bb%ba%e7%ab%8b%e5%85%b3%e8%81%94 aria-label="4.2 长距离依赖：直接建立关联">4.2 长距离依赖：直接建立关联</a></li><li><a href=#43-%e5%8f%af%e8%a7%a3%e9%87%8a%e6%80%a7%e7%9c%8b%e8%a7%81%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%80%9d%e8%80%83%e8%bf%87%e7%a8%8b aria-label='4.3 可解释性：看见模型的"思考过程"'>4.3 可解释性：看见模型的"思考过程"</a></li><li><a href=#44-%e9%80%9a%e7%94%a8%e6%80%a7%e4%b8%80%e4%b8%aa%e6%9e%b6%e6%9e%84%e5%a4%9a%e7%a7%8d%e4%bb%bb%e5%8a%a1 aria-label="4.4 通用性：一个架构，多种任务">4.4 通用性：一个架构，多种任务</a></li><li><a href=#45-%e8%a7%84%e6%a8%a1%e5%8c%96%e7%9a%84%e6%88%90%e5%8a%9f%e8%b6%8a%e5%a4%a7%e8%b6%8a%e5%a5%bd aria-label="4.5 规模化的成功：越大越好">4.5 规模化的成功：越大越好</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e7%ab%a0-%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3%e5%85%b3%e9%94%ae%e6%a6%82%e5%bf%b5%e7%9a%84%e6%9b%b4%e5%a4%9a%e7%bb%86%e8%8a%82 aria-label="第五章 深入理解：关键概念的更多细节">第五章 深入理解：关键概念的更多细节</a><ul><li><a href=#51-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e5%8f%98%e4%bd%93 aria-label="5.1 注意力机制的变体">5.1 注意力机制的变体</a></li><li><a href=#52-%e5%b1%82%e5%bd%92%e4%b8%80%e5%8c%96%e7%9a%84%e4%bd%8d%e7%bd%ae aria-label="5.2 层归一化的位置">5.2 层归一化的位置</a></li><li><a href=#53-dropout-%e7%9a%84%e4%bd%bf%e7%94%a8 aria-label="5.3 Dropout 的使用">5.3 Dropout 的使用</a></li><li><a href=#54-%e8%ae%ad%e7%bb%83%e6%8a%80%e5%b7%a7 aria-label="5.4 训练技巧">5.4 训练技巧</a></li></ul></li><li><a href=#%e7%ac%ac%e5%85%ad%e7%ab%a0-transformer-%e7%9a%84%e5%ba%94%e7%94%a8%e4%bb%8e-nlp-%e5%88%b0%e5%a4%9a%e6%a8%a1%e6%80%81 aria-label="第六章 Transformer 的应用：从 NLP 到多模态">第六章 Transformer 的应用：从 NLP 到多模态</a><ul><li><a href=#61-%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%e5%a4%84%e7%90%86 aria-label="6.1 自然语言处理">6.1 自然语言处理</a></li><li><a href=#62-%e8%ae%a1%e7%ae%97%e6%9c%ba%e8%a7%86%e8%a7%89 aria-label="6.2 计算机视觉">6.2 计算机视觉</a></li><li><a href=#63-%e5%a4%9a%e6%a8%a1%e6%80%81%e6%a8%a1%e5%9e%8b aria-label="6.3 多模态模型">6.3 多模态模型</a></li><li><a href=#64-%e5%85%b6%e4%bb%96%e9%a2%86%e5%9f%9f aria-label="6.4 其他领域">6.4 其他领域</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%83%e7%ab%a0-transformer-%e7%9a%84%e5%b1%80%e9%99%90%e6%80%a7%e4%b8%8e%e6%9c%aa%e6%9d%a5 aria-label="第七章 Transformer 的局限性与未来">第七章 Transformer 的局限性与未来</a><ul><li><a href=#71-%e8%ae%a1%e7%ae%97%e5%a4%8d%e6%9d%82%e5%ba%a6 aria-label="7.1 计算复杂度">7.1 计算复杂度</a></li><li><a href=#72-%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81%e7%9a%84%e5%b1%80%e9%99%90 aria-label="7.2 位置编码的局限">7.2 位置编码的局限</a></li><li><a href=#73-transformer-%e7%9a%84%e5%8f%98%e4%bd%93 aria-label="7.3 Transformer 的变体">7.3 Transformer 的变体</a></li><li><a href=#74-%e6%9c%aa%e6%9d%a5%e7%9a%84%e6%96%b9%e5%90%91 aria-label="7.4 未来的方向">7.4 未来的方向</a></li></ul></li><li><a href=#%e7%ac%ac%e5%85%ab%e7%ab%a0-%e5%ae%9e%e8%b7%b5%e5%a6%82%e4%bd%95%e5%bc%80%e5%a7%8b%e4%bd%bf%e7%94%a8-transformer aria-label="第八章 实践：如何开始使用 Transformer">第八章 实践：如何开始使用 Transformer</a><ul><li><a href=#81-hugging-face-transformers-%e5%ba%93 aria-label="8.1 Hugging Face Transformers 库">8.1 Hugging Face Transformers 库</a></li><li><a href=#82-%e5%be%ae%e8%b0%83-transformer-%e6%a8%a1%e5%9e%8b aria-label="8.2 微调 Transformer 模型">8.2 微调 Transformer 模型</a></li><li><a href=#83-%e4%bb%8e%e5%a4%b4%e5%ae%9e%e7%8e%b0-transformer aria-label="8.3 从头实现 Transformer">8.3 从头实现 Transformer</a></li></ul></li><li><a href=#%e7%bb%93%e8%af%ad aria-label=结语>结语</a></li><li><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae aria-label=参考文献>参考文献</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=引言>引言<a hidden class=anchor aria-hidden=true href=#引言>#</a></h1><p>在人工智能的发展历程中，有几个时刻标志着技术范式的根本性转变。2017年10月就是这样一个时刻——Google Research 和多伦多大学的研究者们发表了一篇名为《Attention Is All You Need》的论文，提出了 Transformer 架构。</p><p>这篇论文的标题本身就是一种宣言：在这篇论文中，作者们向世界宣告，在处理序列数据时，注意力机制就是你所需要的一切。这篇论文不仅解决了长期困扰自然语言处理领域的难题，更开创了一个全新的 AI 时代。从 BERT 到 GPT 系列，从 PaLM 到 Claude，支撑现代大语言模型的核心架构都是 Transformer。</p><p>但 Transformer 到底是什么？它为什么如此重要？它是如何工作的？作为一个 AI 领域的深度从业者，我希望通过这篇文章，用最通俗易懂的方式，为你彻底解读这个重塑 AI 世界的重要架构。</p><h2 id=第一章-背景为什么我们需要-transformer>第一章 背景：为什么我们需要 Transformer？<a hidden class=anchor aria-hidden=true href=#第一章-背景为什么我们需要-transformer>#</a></h2><h3 id=11-序列数据处理的困境>1.1 序列数据处理的困境<a hidden class=anchor aria-hidden=true href=#11-序列数据处理的困境>#</a></h3><p>在深入 Transformer 之前，让我们先理解它试图解决的问题。在自然语言处理、语音识别、机器翻译等任务中，我们面对的都是序列数据——句子是一系列词语的序列，语音是一系列声波的序列，DNA 是一系列碱基的序列。</p><p>对于序列数据的处理，传统的做法是使用循环神经网络（RNN）及其变体，如长短期记忆网络（LSTM）和门控循环单元（GRU）。这些网络的设计理念是：按顺序处理序列中的每个元素，将信息一步一步地传递下去。</p><p><strong>RNN 的工作原理</strong>：想象你在读一本书。你的眼睛一次看一个字（或者一个词），然后大脑会记住这个字的意思，并结合之前记住的内容来理解整个句子。RNN 就是这样工作的——它按顺序处理输入序列，将之前的信息"记住"在隐藏状态中，然后用于处理下一个输入。</p><h3 id=12-rnn-的致命缺陷>1.2 RNN 的致命缺陷<a hidden class=anchor aria-hidden=true href=#12-rnn-的致命缺陷>#</a></h3><p>然而，RNN 存在几个根本性的问题：</p><p><strong>第一个问题是长距离依赖问题</strong>。在处理长序列时，RNN 很难捕获序列前端和序列后端之间的关联。想象一个很长的句子：&ldquo;那个在巴黎出生的，后来搬到纽约生活的，最后在北京去世的老人，年轻时是个著名的科学家。&ldquo;要让 RNN 理解"老人"和"年轻时"之间的关系，信息需要从句子的一端传递到另一端。在这个过程中，信息会逐渐衰减，最终可能完全丢失。</p><p><strong>第二个问题是计算效率问题</strong>。RNN 必须按顺序处理序列，这意味着第一步计算完成后才能开始第二步。这种串行计算的方式无法充分利用现代 GPU 的并行计算能力。在处理长序列时，计算变得非常耗时。</p><p><strong>第三个问题是梯度消失和梯度爆炸问题</strong>。在反向传播过程中，梯度需要通过多个时间步传播。当序列很长时，梯度可能会变得非常小（消失）或非常大（爆炸），导致训练困难。</p><h3 id=13-注意力机制的兴起>1.3 注意力机制的兴起<a hidden class=anchor aria-hidden=true href=#13-注意力机制的兴起>#</a></h3><p>为了解决 RNN 的问题，研究者们提出了注意力机制（Attention Mechanism）。注意力机制的核心思想是：在处理序列中的每个元素时，我们不应该只依赖之前的信息，而应该能够"回顾"序列中的任意位置。</p><p><strong>注意力的直观理解</strong>：想象你在嘈杂的咖啡馆里听朋友说话。即使周围很吵，你的大脑也能够聚焦于朋友的声音，而忽略背景噪音。注意力机制就是模拟这个过程——它让模型学会在处理每个词时，应该"关注"输入序列的哪些部分。</p><p>Bahdanau 等人在 2014 年提出了第一个注意力机制，用于机器翻译。这个注意力机制允许解码器在生成每个目标词时，关注源句子中的相关部分。这大大改善了机器翻译的性能。</p><p>但早期的注意力机制仍然是与 RNN 结合使用的。真正的革命性突破来自于 2017 年的那篇论文——作者们意识到，如果只使用注意力机制，我们就可以完全摆脱 RNN 的束缚。</p><h2 id=第二章-核心概念什么是注意力机制>第二章 核心概念：什么是注意力机制？<a hidden class=anchor aria-hidden=true href=#第二章-核心概念什么是注意力机制>#</a></h2><h3 id=21-注意力机制的本质>2.1 注意力机制的本质<a hidden class=anchor aria-hidden=true href=#21-注意力机制的本质>#</a></h3><p>在 Transformer 中，核心是"自注意力&rdquo;（Self-Attention）机制，也称为"缩放点积注意力&rdquo;（Scaled Dot-Product Attention）。理解自注意力是理解 Transformer 的关键。</p><p><strong>自注意力的核心问题</strong>：对于序列中的每个元素，我们想了解它与序列中其他所有元素的关系。换句话说，当我们处理序列中的一个词时，我们想知道它应该"关注"序列中的哪些其他词。</p><h3 id=22-qkv注意力机制的三个角色>2.2 Q、K、V：注意力机制的三个角色<a hidden class=anchor aria-hidden=true href=#22-qkv注意力机制的三个角色>#</a></h3><p>在实现上，自注意力机制引入了三个重要的向量：查询（Query）、键（Key）和值（Value）。这三个向量的命名来自于信息检索的类比。</p><p><strong>类比说明</strong>：想象你在图书馆找书。你有一个查询（Query）——你想找的主题。图书馆有一个索引系统，由一系列键（Key）组成——每本书的标题、作者等。你的查询会与每个键进行比较，找到最匹配的书，然后获取相应的值（Value）——书的内容。</p><p>在自注意力中：</p><ul><li><strong>查询（Q）</strong>：代表当前正在处理的元素"想要问的问题"</li><li><strong>键（K）</strong>：代表序列中每个元素"能回答的问题"</li><li><strong>值（V）</strong>：代表序列中每个元素"包含的信息"</li></ul><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>flowchart LR
subgraph Input["输入序列"]
X1["X₁"]
X2["X₂"]
X3["X₃"]
end
subgraph QKV["Q、K、V 生成"]
direction TB
WQ["W_Q"]
WK["W_K"]
WV["W_V"]
end
subgraph Attention["注意力计算"]
Score["注意力分数<br>QK^T"]
Scale["缩放<br>÷√d_k"]
Softmax["Softmax"]
Weight["加权求和<br>αV"]
end
subgraph Output["输出"]
Y1["Y₁"]
Y2["Y₂"]
Y3["Y₃"]
end
X1 & X2 & X3 --> QKV
QKV --> Score
Score --> Scale
Scale --> Softmax
Softmax --> Weight
Weight --> Y1 & Y2 & Y3
style Input fill:#007AFF,color:#ffffff,stroke:#007AFF,stroke-width:3px
style QKV fill:#34C759,color:#ffffff,stroke:#34C759,stroke-width:2px
style Attention fill:#FF9500,color:#ffffff,stroke:#FF9500,stroke-width:2px
style Output fill:#007AFF,color:#ffffff,stroke:#007AFF,stroke-width:3px</div></div><h3 id=23-详细的数学过程>2.3 详细的数学过程<a hidden class=anchor aria-hidden=true href=#23-详细的数学过程>#</a></h3><p>让我们一步步解析自注意力的计算过程。</p><p><strong>第一步：线性变换</strong></p><p>对于输入序列中的每个元素，我们首先通过线性变换将其投影为三个向量：</p><p>$$ Q = XW_Q $$
$$ K = XW_K $$
$$ V = XW_V $$</p><p>其中 $X$ 是输入矩阵，每一行代表序列中的一个元素（通常是一个词向量）。$W_Q$、$W_K$、$W_V$ 是可学习的权重矩阵。</p><p><strong>第二步：计算注意力分数</strong></p><p>对于每个查询，我们计算它与所有键的相似度。在 Transformer 中，使用点积作为相似度度量：</p><p>$$ \text{Attention Score} = QK^T $$</p><p><strong>第三步：缩放</strong></p><p>为了防止点积结果过大导致梯度不稳定，我们对结果进行缩放：</p><p>$$ \text{Scaled Score} = \frac{QK^T}{\sqrt{d_k}} $$</p><p>其中 $d_k$ 是键向量的维度。</p><p><strong>第四步：Softmax</strong></p><p>将缩放后的分数通过 Softmax 函数转换为概率分布：</p><p>$$ \alpha = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) $$</p><p>Softmax 确保所有注意力权重的和为 1，每个权重代表当前元素应该"关注"序列中其他元素的程度。</p><p><strong>第五步：加权求和</strong></p><p>最后，用注意力权重对值向量进行加权求和：</p><p>$$ \text{Output} = \alpha V $$</p><h3 id=24-完整的自注意力公式>2.4 完整的自注意力公式<a hidden class=anchor aria-hidden=true href=#24-完整的自注意力公式>#</a></h3><p>将上述步骤合并，完整的自注意力公式是：</p><p>$$ \text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$</p><p>这个公式看起来很简单，但它蕴含着强大的能力。通过这个运算，每个位置的输出都包含了整个序列的信息，而且每个位置关注的程度是由数据自动学习得到的。</p><h3 id=25-多头注意力并行多个视角>2.5 多头注意力：并行多个"视角"<a hidden class=anchor aria-hidden=true href=#25-多头注意力并行多个视角>#</a></h3><p>Transformer 不仅仅使用一个自注意力，而是使用"多头注意力"（Multi-Head Attention）。这意味着我们并行地运行多组自注意力计算，然后将结果拼接起来。</p><p><strong>为什么需要多头？</strong></p><p>想象你要理解一段文字。你可能会从多个角度来理解：语法结构、语义含义、情感色彩、上下文关联等。多头注意力就是模拟这个过程——每组注意力头可以专注于不同类型的关系。</p><p><strong>数学表达</strong>：</p><p>$$ \text{MultiHead}(Q, K, V) = \text{Concat}(head_1, head_2, &mldr;, head_h)W^O $$</p><p>其中每个 $head_i$ 是：</p><p>$$ head_i = \text{Attention}(QW_{Q_i}, KW_{K_i}, VW_{V_i}) $$</p><p>通常，Transformer 使用 8 个或 16 个注意力头。每个头的维度是原始维度的 $1/h$，所以总的计算量与单头注意力相当。</p><h2 id=第三章-transformer-架构逐层解析>第三章 Transformer 架构：逐层解析<a hidden class=anchor aria-hidden=true href=#第三章-transformer-架构逐层解析>#</a></h2><h3 id=31-整体架构概览>3.1 整体架构概览<a hidden class=anchor aria-hidden=true href=#31-整体架构概览>#</a></h3><p>Transformer 的架构可以用一句话概括：<strong>编码器-解码器结构，完全基于注意力机制</strong>。</p><p>整个模型由两部分组成：</p><ul><li><strong>编码器（Encoder）</strong>：处理输入序列，生成一个表示序列信息的向量</li><li><strong>解码器（Decoder）</strong>：基于编码器的输出和之前已经生成的内容，生成目标序列</li></ul><h3 id=32-编码器结构>3.2 编码器结构<a hidden class=anchor aria-hidden=true href=#32-编码器结构>#</a></h3><p>每个编码器层（Encoder Layer）包含两个主要的子层：</p><ol><li><strong>多头自注意力机制</strong></li><li><strong>前馈神经网络</strong></li></ol><p>每个子层后面都跟着：</p><ul><li><strong>残差连接（Residual Connection）</strong>：将子层的输入直接加到输出上</li><li><strong>层归一化（Layer Normalization）</strong>：对输出进行归一化</li></ul><p>用公式表示：</p><p>$$ \text{LayerNorm}(x + \text{Sublayer}(x)) $$</p><h3 id=33-解码器结构>3.3 解码器结构<a hidden class=anchor aria-hidden=true href=#33-解码器结构>#</a></h3><p>解码器层（Decoder Layer）稍微复杂一些，包含三个子层：</p><ol><li><strong>掩码多头自注意力（Masked Multi-Head Self-Attention）</strong></li><li><strong>编码器-解码器注意力（Encoder-Decoder Attention）</strong></li><li><strong>前馈神经网络</strong></li></ol><p><strong>掩码自注意力的作用</strong>：在解码器中，我们只能"看到"之前已经生成的内容，不能"看到"未来的内容。掩码机制通过将未来的注意力权重设置为负无穷（Softmax 后变为 0）来实现这一点。</p><p><strong>编码器-解码器注意力</strong>：这是解码器"关注"编码器输出的方式。在机器翻译任务中，这允许解码器在生成每个目标词时，关注源句子中相关的部分。</p><h3 id=34-位置编码让模型感知序列顺序>3.4 位置编码：让模型感知序列顺序<a hidden class=anchor aria-hidden=true href=#34-位置编码让模型感知序列顺序>#</a></h3><p>Transformer 完全基于注意力机制，没有循环结构，因此无法感知序列中元素的顺序。为了解决这个问题，Transformer 引入了位置编码（Positional Encoding）。</p><p><strong>位置编码的直观理解</strong>：给序列中的每个位置分配一个独特的"位置标签"，将这个标签加入到输入嵌入中，这样模型就能知道每个元素的相对位置。</p><p><strong>Transformer 使用正弦和余弦函数生成位置编码</strong>：</p><p>$$ PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right) $$
$$ PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right) $$</p><p>其中 $pos$ 是位置，$i$ 是维度索引，$d_{model}$ 是模型的维度。</p><p><strong>为什么选择正弦和余弦函数？</strong> 因为它们具有一个有用的性质：对于任何固定的偏移量 $k$，$PE_{pos+k}$ 可以表示为 $PE_{pos}$ 的线性组合。这意味着模型可以通过相对位置来学习关注模式。</p><h3 id=35-前馈神经网络>3.5 前馈神经网络<a hidden class=anchor aria-hidden=true href=#35-前馈神经网络>#</a></h3><p>除了注意力层，Transformer 的每个编码器和解码器层都包含一个前馈神经网络：</p><p>$$ \text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2 $$</p><p>这个前馈网络对每个位置独立地进行相同的变换。它的作用是为模型提供非线性变换能力和额外的表达能力。</p><h3 id=36-完整的-transformer-架构>3.6 完整的 Transformer 架构<a hidden class=anchor aria-hidden=true href=#36-完整的-transformer-架构>#</a></h3><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>flowchart TB
subgraph Encoder["编码器 Encoder"]
direction TB
EEmb["输入嵌入<br>Input Embedding"]
EPE["位置编码<br>Positional Encoding"]
subgraph ELayers["编码器层 × N"]
direction TB
ESA["多头自注意力<br>Multi-Head Attention"]
EFN["前馈网络<br>Feed Forward"]
end
end
subgraph Decoder["解码器 Decoder"]
direction TB
DEmb["输出嵌入<br>Output Embedding"]
DPE["位置编码<br>Positional Encoding"]
subgraph DLayers["解码器层 × N"]
direction TB
DMSA["掩码自注意力<br>Masked Attention"]
EDA["编码器-解码器注意力<br>Encoder-Decoder Attention"]
end
Linear["线性层 + Softmax"]
end
EEmb --> EPE --> ELayers
ELayers --> DED["编码器输出"]
DED --> EDA
DEmb --> DPE --> DLayers
DLayers --> Linear
style Encoder fill:#007AFF,color:#ffffff,stroke:#007AFF,stroke-width:3px
style Decoder fill:#34C759,color:#ffffff,stroke:#34C759,stroke-width:3px
style ELayers fill:#FF9500,color:#ffffff,stroke:#FF9500,stroke-width:2px
style DLayers fill:#FF9500,color:#ffffff,stroke:#FF9500,stroke-width:2px</div></div><p>现在，让我们把所有的组件放在一起，看看完整的 Transformer 架构：</p><p><strong>编码器部分</strong>：</p><ol><li>输入嵌入（Input Embedding）</li><li>位置编码（Positional Encoding）</li><li>N 个编码器层（每层包含多头自注意力和前馈网络）</li></ol><p><strong>解码器部分</strong>：</p><ol><li>输出嵌入（Output Embedding）</li><li>位置编码</li><li>N 个解码器层（每层包含掩码自注意力、编码器-解码器注意力和前馈网络）</li><li>线性层和 Softmax（将输出转换为概率分布）</li></ol><p>在原始论文中，$N=6$，意味着有 6 个编码器层和 6 个解码器层。</p><h2 id=第四章-为什么-transformer-如此重要>第四章 为什么 Transformer 如此重要？<a hidden class=anchor aria-hidden=true href=#第四章-为什么-transformer-如此重要>#</a></h2><h3 id=41-并行计算效率的革命>4.1 并行计算：效率的革命<a hidden class=anchor aria-hidden=true href=#41-并行计算效率的革命>#</a></h3><p>Transformer 相比 RNN 的最大优势之一是计算并行化。</p><p><strong>RNN 的局限性</strong>：在 RNN 中，第 t 步的计算依赖于第 t-1 步的隐藏状态。这意味着必须按顺序进行计算，无法并行处理序列中的不同位置。</p><p><strong>Transformer 的突破</strong>：在自注意力机制中，序列中所有位置之间的注意力分数可以同时计算。查询、键、值的计算也是并行进行的。这使得 Transformer 能够充分利用 GPU 的并行计算能力。</p><p><strong>实际影响</strong>：在处理长序列时，Transformer 的速度可以比 RNN 快几个数量级。这意味着我们可以训练更大规模的模型，处理更长的序列。</p><h3 id=42-长距离依赖直接建立关联>4.2 长距离依赖：直接建立关联<a hidden class=anchor aria-hidden=true href=#42-长距离依赖直接建立关联>#</a></h3><p><strong>RNN 的信息传递</strong>：在 RNN 中，从序列开头到序列结尾的信息需要通过多个步骤逐步传递。在这个过程中，信息可能会衰减或丢失。</p><p><strong>Transformer 的直接连接</strong>：在自注意力机制中，序列中的任何两个位置都可以直接相互作用。通过一次注意力计算，序列开头的元素就可以"关注"序列结尾的元素。</p><p><strong>数学上的优势</strong>：从数学上看，自注意力计算的是一个全连接图上的信息传递。所有位置之间的关系都是直接建模的，不存在信息传递的"路径长度"问题。</p><h3 id=43-可解释性看见模型的思考过程>4.3 可解释性：看见模型的"思考过程"<a hidden class=anchor aria-hidden=true href=#43-可解释性看见模型的思考过程>#</a></h3><p>相比 RNN 和其他深度学习模型，Transformer 的注意力权重提供了一定程度的可解释性。</p><p><strong>可视化注意力</strong>：我们可以将注意力权重可视化，观察模型在处理某个输入时"关注"了哪些部分。这对于理解模型行为、调试模型和建立信任都非常有帮助。</p><p><strong>例如</strong>：在翻译任务中，我们可以观察解码器在生成某个目标词时，注意力权重集中在源句子的哪些词上。这让我们能够直观地看到模型的"翻译策略"。</p><h3 id=44-通用性一个架构多种任务>4.4 通用性：一个架构，多种任务<a hidden class=anchor aria-hidden=true href=#44-通用性一个架构多种任务>#</a></h3><p>Transformer 架构的通用性是其另一个重要优势。</p><p><strong>最初设计用于机器翻译</strong>：Transformer 是为机器翻译任务设计的，但它很快被应用于各种其他任务。</p><p><strong>NLP 任务</strong>：文本分类、命名实体识别、问答、摘要生成、文本生成等。</p><p><strong>其他领域</strong>：图像处理（Vision Transformer）、语音识别、音乐生成、蛋白质结构预测（AlphaFold）、代码生成等。</p><p><strong>为什么 Transformer 如此通用？</strong> 因为序列是表示各种数据的自然方式。无论是文本、图像（可以分割成 patches）、还是蛋白质序列（氨基酸序列），都可以表示为序列。Transformer 的自注意力机制能够捕获序列内部的长距离依赖关系，这使得它成为一个通用的序列建模框架。</p><h3 id=45-规模化的成功越大越好>4.5 规模化的成功：越大越好<a hidden class=anchor aria-hidden=true href=#45-规模化的成功越大越好>#</a></h3><p>Transformer 架构的一个重要特性是它能够从规模化中获益。</p><p><strong>神经网络规模化的规律</strong>：在深度学习中，模型的性能通常随着参数数量、训练数据和计算量的增加而提升。这就是著名的"规模定律"（Scaling Laws）。</p><p><strong>Transformer 适合规模化</strong>：Transformer 的并行计算特性使得它可以高效地利用大量计算资源。随着模型规模的增大，Transformer 的性能持续提升，没有明显的饱和迹象。</p><p><strong>GPT 系列的例子</strong>：</p><ul><li>GPT-1：1.17 亿参数</li><li>GPT-2：15 亿参数</li><li>GPT-3：1750 亿参数</li><li>GPT-4：参数数量未知，但可能超过万亿</li></ul><p>随着规模的增大，这些模型展现出了越来越强大的能力，包括涌现能力（Emergent Abilities）——在小模型上不存在的能力在大模型上突然出现。</p><h2 id=第五章-深入理解关键概念的更多细节>第五章 深入理解：关键概念的更多细节<a hidden class=anchor aria-hidden=true href=#第五章-深入理解关键概念的更多细节>#</a></h2><h3 id=51-注意力机制的变体>5.1 注意力机制的变体<a hidden class=anchor aria-hidden=true href=#51-注意力机制的变体>#</a></h3><p>除了原始的缩放点积注意力，Transformer 论文还提到了另一种注意力：加性注意力（Additive Attention）。</p><p><strong>加性注意力的公式</strong>：</p><p>$$ \text{Attention}(Q, K, V) = \text{Softmax}(w^T \tanh(W_q Q + W_k K))V $$</p><p>这种注意力使用一个前馈网络来计算相似度，而不是点积。它在早期的一些模型中使用，但计算效率不如点积注意力。</p><p><strong>现代的变体</strong>：</p><ul><li><strong>Rotary Position Embedding（RoPE）</strong>：在注意力计算中引入相对位置信息</li><li><strong>ALiBi（Attention with Linear Biases）</strong>：通过简单的偏置项来编码位置信息</li><li><strong>FlashAttention</strong>：优化的注意力计算，利用 GPU 内存层次结构提高效率</li></ul><h3 id=52-层归一化的位置>5.2 层归一化的位置<a hidden class=anchor aria-hidden=true href=#52-层归一化的位置>#</a></h3><p>在原始的 Transformer 论文中，层归一化位于残差连接之后：</p><p>$$ y = \text{LayerNorm}(x + \text{Sublayer}(x)) $$</p><p>这被称为"Post-LN" Transformer。</p><p>后来的研究（如 GPT-2）发现，将层归一化放在注意力层和前馈网络之前（Pre-LN）可能更有利于训练：</p><p>$$ y = x + \text{Sublayer}(\text{LayerNorm}(x)) $$</p><p>这种变化使得梯度流动更加稳定，便于训练更深的模型。</p><h3 id=53-dropout-的使用>5.3 Dropout 的使用<a hidden class=anchor aria-hidden=true href=#53-dropout-的使用>#</a></h3><p>Transformer 在多个地方使用 Dropout 来防止过拟合：</p><ul><li>注意力权重计算后</li><li>前馈网络的激活函数后</li><li>嵌入层</li></ul><p>原始论文使用 0.1 的 Dropout 率。</p><h3 id=54-训练技巧>5.4 训练技巧<a hidden class=anchor aria-hidden=true href=#54-训练技巧>#</a></h3><p><strong>标签平滑（Label Smoothing）</strong>：在计算交叉熵损失时，不使用硬标签（0 或 1），而是使用平滑后的标签（如 0.1 和 0.9）。这可以防止模型对预测结果过于自信。</p><p><strong>warmup 学习率调度</strong>：学习率不是一开始就使用最大值，而是从一个很小的值逐渐增加到最大值，然后再按某种策略衰减。公式是：</p><p>$$ lr = d_{model}^{-0.5} \cdot \min(step^{-0.5}, step \cdot warmup^{-1.5}) $$</p><p>这种策略有助于模型在训练初期找到好的优化方向。</p><h2 id=第六章-transformer-的应用从-nlp-到多模态>第六章 Transformer 的应用：从 NLP 到多模态<a hidden class=anchor aria-hidden=true href=#第六章-transformer-的应用从-nlp-到多模态>#</a></h2><h3 id=61-自然语言处理>6.1 自然语言处理<a hidden class=anchor aria-hidden=true href=#61-自然语言处理>#</a></h3><p><strong>BERT（Bidirectional Encoder Representations from Transformers）</strong>：仅使用 Transformer 编码器的模型，通过掩码语言建模和下一句预测进行预训练。BERT 在各种 NLP 基准测试中取得了突破性的成果。</p><p><strong>GPT（Generative Pre-trained Transformer）</strong>：仅使用 Transformer 解码器的模型，通过预测下一个词进行训练。GPT 系列模型展示了大语言模型的强大能力。</p><p><strong>T5（Text-to-Text Transfer Transformer）</strong>：将所有 NLP 任务统一为文本到文本的格式，使用完整的编码器-解码器结构。</p><h3 id=62-计算机视觉>6.2 计算机视觉<a hidden class=anchor aria-hidden=true href=#62-计算机视觉>#</a></h3><p><strong>Vision Transformer（ViT）</strong>：将图像分割成固定大小的 patches，然后将这些 patches 视为一个序列，用 Transformer 进行处理。ViT 证明了 Transformer 也可以很好地处理图像任务。</p><p><strong>DETR（Detection Transformer）</strong>：使用 Transformer 进行目标检测，通过注意力机制直接预测物体的边界框。</p><h3 id=63-多模态模型>6.3 多模态模型<a hidden class=anchor aria-hidden=true href=#63-多模态模型>#</a></h3><p><strong>CLIP（Contrastive Language-Image Pre-training）</strong>：使用 Transformer 编码器处理图像和文本，学习它们之间的对应关系。</p><p><strong>GPT-4V 和 Gemini</strong>：支持图像输入的多模态大语言模型，可以理解和生成关于图像的内容。</p><h3 id=64-其他领域>6.4 其他领域<a hidden class=anchor aria-hidden=true href=#64-其他领域>#</a></h3><p><strong>语音识别</strong>：Transformer 已被应用于端到端语音识别模型。</p><p><strong>音乐生成</strong>：如 MusicLM 等模型使用 Transformer 生成音乐。</p><p><strong>生物学</strong>：AlphaFold 使用 Transformer 架构预测蛋白质的三维结构。</p><p><strong>代码生成</strong>：如 CodeBERT、Codex 等模型使用 Transformer 理解和生成代码。</p><h2 id=第七章-transformer-的局限性与未来>第七章 Transformer 的局限性与未来<a hidden class=anchor aria-hidden=true href=#第七章-transformer-的局限性与未来>#</a></h2><h3 id=71-计算复杂度>7.1 计算复杂度<a hidden class=anchor aria-hidden=true href=#71-计算复杂度>#</a></h3><p>Transformer 的一个主要局限是计算复杂度。对于长度为 $n$ 的序列，自注意力的计算复杂度是 $O(n^2)$。当序列很长时，这成为一个严重的瓶颈。</p><p><strong>解决方案</strong>：</p><ul><li><strong>稀疏注意力</strong>：只计算部分位置之间的注意力</li><li><strong>线性注意力</strong>：使用核函数将复杂度降低到 $O(n)$</li><li><strong>局部-全局混合</strong>：将长序列分成多个短片段</li></ul><h3 id=72-位置编码的局限>7.2 位置编码的局限<a hidden class=anchor aria-hidden=true href=#72-位置编码的局限>#</a></h3><p>虽然位置编码允许 Transformer 感知序列顺序，但它们有一些局限性：</p><ul><li>外推能力有限：对训练时未见过的序列长度表现不佳</li><li>相对位置信息编码不够自然</li></ul><p><strong>改进方案</strong>：</p><ul><li>RoPE（Rotary Position Embedding）</li><li>ALiBi（Attention with Linear Biases）</li><li>绝对位置编码的替代方案</li></ul><h3 id=73-transformer-的变体>7.3 Transformer 的变体<a hidden class=anchor aria-hidden=true href=#73-transformer-的变体>#</a></h3><p>为了解决上述问题，研究者们提出了许多 Transformer 的变体：</p><p><strong>高效 Transformer</strong>：</p><ul><li><strong>Longformer</strong>：结合局部注意力和全局注意力</li><li><strong>BigBird</strong>：使用随机注意力和局部注意力</li><li><strong>Performer</strong>：使用核函数实现线性复杂度</li></ul><p><strong>状态空间模型</strong>：</p><ul><li><strong>Mamba</strong>：最近引起广泛关注的选择性状态空间模型，在保持序列建模能力的同时实现了线性复杂度</li></ul><h3 id=74-未来的方向>7.4 未来的方向<a hidden class=anchor aria-hidden=true href=#74-未来的方向>#</a></h3><p><strong>更高效的架构</strong>：研究者们继续探索比 Transformer 更高效、更强大的架构。</p><p><strong>多模态融合</strong>：将 Transformer 扩展到更多模态，实现真正的多模态理解。</p><p><strong>持续学习</strong>：让模型能够持续学习新知识，而不会遗忘旧知识。</p><p><strong>可解释性和安全性</strong>：深入理解 Transformer 的工作原理，提高模型的可解释性和安全性。</p><h2 id=第八章-实践如何开始使用-transformer>第八章 实践：如何开始使用 Transformer<a hidden class=anchor aria-hidden=true href=#第八章-实践如何开始使用-transformer>#</a></h2><h3 id=81-hugging-face-transformers-库>8.1 Hugging Face Transformers 库<a hidden class=anchor aria-hidden=true href=#81-hugging-face-transformers-库>#</a></h3><p>Hugging Face 提供了最流行的 Transformer 库，使得使用和微调预训练模型变得非常简单。</p><p><strong>基本使用示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>pipeline</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 情感分析</span>
</span></span><span class=line><span class=cl><span class=n>classifier</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span><span class=s2>&#34;sentiment-analysis&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>classifier</span><span class=p>(</span><span class=s2>&#34;I love learning about Transformers!&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 问答</span>
</span></span><span class=line><span class=cl><span class=n>question_answerer</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span><span class=s2>&#34;question-answering&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>question_answerer</span><span class=p>(</span><span class=n>question</span><span class=o>=</span><span class=s2>&#34;What is Transformer?&#34;</span><span class=p>,</span> <span class=n>context</span><span class=o>=</span><span class=s2>&#34;Transformer is a deep learning model.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用预训练模型</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForSequenceClassification</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_name</span> <span class=o>=</span> <span class=s2>&#34;bert-base-uncased&#34;</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=82-微调-transformer-模型>8.2 微调 Transformer 模型<a hidden class=anchor aria-hidden=true href=#82-微调-transformer-模型>#</a></h3><p>微调是让预训练模型适应特定任务的关键技术：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>Trainer</span><span class=p>,</span> <span class=n>TrainingArguments</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>training_args</span> <span class=o>=</span> <span class=n>TrainingArguments</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>output_dir</span><span class=o>=</span><span class=s2>&#34;./results&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_train_epochs</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>per_device_train_batch_size</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>evaluation_strategy</span><span class=o>=</span><span class=s2>&#34;epoch&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>trainer</span> <span class=o>=</span> <span class=n>Trainer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>args</span><span class=o>=</span><span class=n>training_args</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>train_dataset</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>eval_dataset</span><span class=o>=</span><span class=n>eval_dataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span></code></pre></div><h3 id=83-从头实现-transformer>8.3 从头实现 Transformer<a hidden class=anchor aria-hidden=true href=#83-从头实现-transformer>#</a></h3><p>对于想要深入理解 Transformer 的读者，从头实现是一个很好的学习方式：</p><p><strong>关键组件的实现</strong>：</p><ol><li>多头注意力层</li><li>前馈神经网络</li><li>编码器层</li><li>解码器层</li><li>完整模型</li></ol><p><strong>参考资源</strong>：</p><ul><li>原始论文《Attention Is All You Need》</li><li>Harvard NLP 团队的注释版论文（http://nlp.seas.harvard.edu/2018/04/03/attention.html）</li><li>各种在线教程和课程</li></ul><h2 id=结语>结语<a hidden class=anchor aria-hidden=true href=#结语>#</a></h2><p>从 2017 年论文发表至今，Transformer 已经彻底改变了人工智能领域。它不仅仅是一个技术突破，更代表了一种新的思维方式——通过注意力机制直接建模序列元素之间的关系。</p><p>Transformer 的成功教会我们一个重要的 lesson：有时候，简单的想法如果足够强大，也可以改变世界。自注意力的概念并不复杂，但它所释放的能量是巨大的。</p><p>今天，我们正处于大语言模型时代的浪尖。从 ChatGPT 到 Claude，从 Gemini 到 Copilot，这些令人惊叹的 AI 应用都建立在 Transformer 架构之上。理解 Transformer，不仅是理解现代 AI 的基础，更是参与 AI 革命的入场券。</p><p>但这只是开始。Transformer 之后，研究者们提出了许多改进和新架构。人工智能领域仍在快速演进，新的突破随时可能出现。作为这个时代的见证者和参与者，我们有幸能够亲历这一切，也有责任去理解和贡献。</p><p>最后，用一句话总结 Transformer 的意义：<strong>它证明了，当我们给予模型足够的能力和对数据的恰当归纳偏置，它就能学会我们无法预先设计的复杂模式</strong>。这或许就是深度学习最深刻的哲学启示。</p><hr><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><ol><li>Vaswani, A., et al. (2017). &ldquo;Attention Is All You Need&rdquo;. Advances in Neural Information Processing Systems.</li><li>Devlin, J., et al. (2018). &ldquo;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&rdquo;. NAACL-HLT.</li><li>Brown, T., et al. (2020). &ldquo;Language Models are Few-Shot Learners&rdquo;. NeurIPS.</li><li>Dosovitskiy, A., et al. (2020). &ldquo;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&rdquo;. ICLR.</li><li>Bahdanau, D., et al. (2014). &ldquo;Neural Machine Translation by Jointly Learning to Align and Translate&rdquo;. ICLR.</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%AE%97%E6%B3%95/>算法</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-21-monte-carlo-method/><span class=title>« Prev</span><br><span>蒙特卡罗算法：从原子弹到人工智能的随机之旅</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-21-laplace-transform-history/><span class=title>Next »</span><br><span>拉普拉斯变换：从概率论到工程数学的百年旅程</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformer：重塑AI世界的架构革命 on x" href="https://x.com/intent/tweet/?text=Transformer%ef%bc%9a%e9%87%8d%e5%a1%91AI%e4%b8%96%e7%95%8c%e7%9a%84%e6%9e%b6%e6%9e%84%e9%9d%a9%e5%91%bd&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-transformer%2f&amp;hashtags=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%2c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%2c%e7%ae%97%e6%b3%95"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformer：重塑AI世界的架构革命 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-transformer%2f&amp;title=Transformer%ef%bc%9a%e9%87%8d%e5%a1%91AI%e4%b8%96%e7%95%8c%e7%9a%84%e6%9e%b6%e6%9e%84%e9%9d%a9%e5%91%bd&amp;summary=Transformer%ef%bc%9a%e9%87%8d%e5%a1%91AI%e4%b8%96%e7%95%8c%e7%9a%84%e6%9e%b6%e6%9e%84%e9%9d%a9%e5%91%bd&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-transformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformer：重塑AI世界的架构革命 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-transformer%2f&title=Transformer%ef%bc%9a%e9%87%8d%e5%a1%91AI%e4%b8%96%e7%95%8c%e7%9a%84%e6%9e%b6%e6%9e%84%e9%9d%a9%e5%91%bd"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformer：重塑AI世界的架构革命 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-transformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>