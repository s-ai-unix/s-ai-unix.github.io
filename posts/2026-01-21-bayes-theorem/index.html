<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>贝叶斯公式：从牧师遗作到人工智能基石 | s-ai-unix's Blog</title><meta name=keywords content="贝叶斯,概率论,数学史,机器学习,人工智能"><meta name=description content="本文系统梳理贝叶斯公式从18世纪诞生到21世纪成为人工智能核心方法的完整发展历程，包含严谨的数学推导、丰富的历史背景和现代应用案例。"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-21-bayes-theorem/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-21-bayes-theorem/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-21-bayes-theorem/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="贝叶斯公式：从牧师遗作到人工智能基石"><meta property="og:description" content="本文系统梳理贝叶斯公式从18世纪诞生到21世纪成为人工智能核心方法的完整发展历程，包含严谨的数学推导、丰富的历史背景和现代应用案例。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-21T10:00:00+08:00"><meta property="article:modified_time" content="2026-01-21T10:00:00+08:00"><meta property="article:tag" content="贝叶斯"><meta property="article:tag" content="概率论"><meta property="article:tag" content="数学史"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="人工智能"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/bayes-theorem.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/bayes-theorem.jpg"><meta name=twitter:title content="贝叶斯公式：从牧师遗作到人工智能基石"><meta name=twitter:description content="本文系统梳理贝叶斯公式从18世纪诞生到21世纪成为人工智能核心方法的完整发展历程，包含严谨的数学推导、丰富的历史背景和现代应用案例。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"贝叶斯公式：从牧师遗作到人工智能基石","item":"https://s-ai-unix.github.io/posts/2026-01-21-bayes-theorem/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"贝叶斯公式：从牧师遗作到人工智能基石","name":"贝叶斯公式：从牧师遗作到人工智能基石","description":"本文系统梳理贝叶斯公式从18世纪诞生到21世纪成为人工智能核心方法的完整发展历程，包含严谨的数学推导、丰富的历史背景和现代应用案例。","keywords":["贝叶斯","概率论","数学史","机器学习","人工智能"],"articleBody":"引言：从\"上帝的视角\"到\"凡人的推断\" 想象你是一名医生，患者刚刚做完某种疾病的筛查测试。测试结果是阳性。那么，这位患者真正患病的概率是多少？\n如果你回答\"既然测试准确率是95%，那么患病的概率就是95%\"，那你和大多数人的直觉一样——但也一样错了。\n正确答案可能让你吃惊：哪怕测试准确率达到95%，如果这种疾病在人群中发病率只有1%，那么一个阳性结果意味着患者真正患病的概率可能只有16%左右。\n这个反直觉的结果，正是贝叶斯公式的核心思想：我们的信念应该随着新证据的到来而更新，但更新的方式不是简单的替换，而是要结合我们已有的知识（先验信息）。\n贝叶斯公式不仅是一个数学定理，更是一种思维方式。它告诉我们：在信息不完整的世界里，我们如何从有限的数据中学习，如何科学地调整我们的信念。从18世纪的一位英国牧师兼数学家，到21世纪的人工智能，贝叶斯的思想经历了一段跌宕起伏的旅程。\ntimeline title 贝叶斯公式发展历程 section 18世纪 1763年 : 贝叶斯遗作发表 1812年 : 拉普拉斯系统阐述 section 19-20世纪 20世纪初 : 频率学派占据主导 20世纪中叶 : 萨瓦奇、杰弗里斯复兴贝叶斯思想 1980年代 : MCMC方法实用化 section 21世纪 21世纪 : 贝叶斯方法成为AI核心 第一章：贝叶斯牧师的那篇遗作 1.1 托马斯·贝叶斯其人 托马斯·贝叶斯（Thomas Bayes，1701-1761）是18世纪英国的一位长老会牧师，同时也是一位业余数学家。他出生于英格兰的一个显赫家庭，父亲是非国教牧师乔舒亚·贝叶斯。托马斯在爱丁堡大学学习神学和逻辑学，后来接任父亲的教职，在坦布里奇韦尔斯（Tunbridge Wells）担任牧师。\n尽管贝叶斯在世时并未在数学领域发表太多作品，但他对概率论有着深刻的思考。他最著名的著作《机会问题的求解方法》（An Essay towards solving a Problem in the Doctrine of Chances）在他去世后于1763年发表在《皇家学会哲学汇刊》上。这篇论文由他的朋友理查德·普莱斯（Richard Price）整理并提交。\n1.2 问题的提出：从\"已知原因求结果\"到\"已知结果求原因\" 贝叶斯关注的是一个根本性的哲学和数学问题：如果我们观察到某个结果，如何推断导致这个结果的原因？\n在贝叶斯之前，概率论主要处理\"正向概率\"问题：如果我们知道某种原因，可以计算它产生特定结果的概率。例如，如果一枚硬币是均匀的，那么抛掷得到正面的概率是50%。\n但现实中我们经常面临\"逆向概率\"问题：我们观察到了结果，想要推断原因。例如，我们观察到病人有某种症状，想推断他患某种疾病的概率；或者我们观察到数据，想推断产生这些数据的参数。\n贝叶斯的天才之处在于，他用条件概率建立了因果推断的数学框架。\n1.3 贝叶斯的台球模型 贝叶斯在原文中使用了一个巧妙的台球模型来说明他的思想：\n想象一张台球桌，一个球被随机抛到桌面上，停在一个未知位置（这个位置决定了某种\"未知的概率\"）。然后另一个球被反复抛出，我们观察它落在第一个球左侧还是右侧。\n通过观察第二个球落在左侧的频率，贝叶斯想要推断：第一个球停在哪里（即\"未知的概率\"是多少）？\n这个模型的关键洞察是：即使我们永远无法直接\"看见\"第一个球的位置（真实的概率值），我们也可以通过第二个球的表现（观察数据）不断更新对第一个球位置的信念。\n第二章：从条件概率到贝叶斯公式 2.1 条件概率的基础 在介绍贝叶斯公式之前，我们需要先理解条件概率。条件概率 $P(A|B)$ 表示在事件 $B$ 已经发生的条件下，事件 $A$ 发生的概率。\n定义：若 $P(B) \u003e 0$，则 $$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} $$\n其中 $P(A \\cap B)$ 表示 $A$ 和 $B$ 同时发生的概率。\n这个定义很直观：在 $B$ 发生的所有可能情况中，$A$ 也发生的比例是多少。\n2.2 乘法公式 从条件概率的定义，我们可以直接得到乘法公式： $$ P(A \\cap B) = P(A|B) \\cdot P(B) = P(B|A) \\cdot P(A) $$\n这个公式告诉我们：两个事件同时发生的概率，等于一个事件发生的概率乘以在此条件下另一个事件的条件概率。\n2.3 全概率公式 如果我们有一个完备的事件组 ${B_1, B_2, \\ldots, B_n}$，满足：\n互斥性：$i \\neq j$ 时，$B_i \\cap B_j = \\emptyset$ 完备性：$\\bigcup_{i=1}^n B_i = \\Omega$（整个样本空间） $P(B_i) \u003e 0$ 对所有 $i$ 那么对于任意事件 $A$，有： $$ P(A) = \\sum_{i=1}^n P(A|B_i) \\cdot P(B_i) $$\n这就是全概率公式。它的直观含义是：事件 $A$ 可以通过各种\"原因\" $B_i$ 发生，把所有可能路径的概率加起来，就得到 $A$ 的总概率。\n2.4 贝叶斯公式的推导 现在我们可以推导贝叶斯公式了。根据条件概率的定义： $$ P(B_i|A) = \\frac{P(B_i \\cap A)}{P(A)} $$\n利用乘法公式 $P(B_i \\cap A) = P(A|B_i) \\cdot P(B_i)$ 和全概率公式，我们得到： $$ P(B_i|A) = \\frac{P(A|B_i) \\cdot P(B_i)}{\\sum_{j=1}^n P(A|B_j) \\cdot P(B_j)} $$\n这就是贝叶斯公式！\n让我们解释每一项的含义：\n$P(B_i)$：先验概率（Prior），在观察证据之前对假设 $B_i$ 的信念 $P(A|B_i)$：似然度（Likelihood），如果假设 $B_i$ 为真，观察到证据 $A$ 的概率 $P(B_i|A)$：后验概率（Posterior），在观察到证据 $A$ 后对假设 $B_i$ 的更新信念 $\\sum_{j=1}^n P(A|B_j) \\cdot P(B_j)$：证据因子（Evidence），证据 $A$ 在所有可能假设下的总概率 2.5 贝叶斯公式的连续形式 如果参数 $\\theta$ 是连续变量（而不是离散假设），贝叶斯公式变为： $$ p(\\theta|D) = \\frac{p(D|\\theta) \\cdot p(\\theta)}{\\int p(D|\\theta’) \\cdot p(\\theta’) , d\\theta’} $$\n其中：\n$D$ 表示观测数据 $p(\\theta)$ 是参数的先验分布 $p(D|\\theta)$ 是似然函数 $p(\\theta|D)$ 是后验分布 分母是边缘似然（Marginal Likelihood），也称证据 连续形式的直观理解：数据 $D$ 更新了我们对参数 $\\theta$ 的整个概率分布，而不仅仅是得到一个点估计。\n2.6 一个具体例子：疾病诊断 让我们回到开头提到的疾病诊断问题。定义：\n$D$：患病事件 $\\neg D$：不患病 $T^+$：测试阳性 $T^-$：测试阴性 已知：\n先验概率（发病率）：$P(D) = 0.01$ 测试准确率：$P(T^+|D) = 0.95$（真阳性率，灵敏度） 假阳性率：$P(T^+|\\neg D) = 0.05$ 我们要求：在测试阳性的条件下，真正患病的概率 $P(D|T^+)$\n根据贝叶斯公式： $$ \\begin{align} P(D|T^+) \u0026= \\frac{P(T^+|D) \\cdot P(D)}{P(T^+|D) \\cdot P(D) + P(T^+|\\neg D) \\cdot P(\\neg D)} \\ \u0026= \\frac{0.95 \\times 0.01}{0.95 \\times 0.01 + 0.05 \\times 0.99} \\ \u0026= \\frac{0.0095}{0.0095 + 0.0495} \\ \u0026= \\frac{0.0095}{0.059} \\ \u0026\\approx 0.161 \\end{align} $$\n所以即使测试阳性，真正患病的概率只有约16.1%！\n为什么直觉会错？ 因为假阳性的\"基数\"太大：虽然健康人假阳性的概率只有5%，但健康人占总人群的99%，所以假阳性的绝对数量（$0.05 \\times 0.99 = 0.0495$）远超真阳性（$0.95 \\times 0.01 = 0.0095$）。\n下图直观展示了贝叶斯更新的三个步骤：\n图1：从先验概率到后验概率的更新过程。尽管测试准确率很高（95%），但由于疾病发病率很低（1%），假阳性的绝对数量（4.95%）远超真阳性（0.95%），导致测试阳性后的真实患病概率只有16.1%。\n下面用一个流程图展示贝叶斯推断的完整过程：\nflowchart LR subgraph \"第一步：先验知识\" PRIOR[\"先验分布 p(θ)\"] end subgraph \"第二步：收集证据\" DATA[\"观测数据 D\"] LIKELIHOOD[\"似然函数 p(D|θ)\"] end subgraph \"第三步：贝叶斯更新\" MULTIPLY[\"乘法运算\"] NORMALIZE[\"归一化\"] end subgraph \"第四步：后验信念\" POSTERIOR[\"后验分布 p(θ|D)\"] end PRIOR --\u003e|结合| MULTI DATA --\u003e LIKELIHOOD LIKELIHOOD --\u003e|参与| MULTI MULTI --\u003e|未归一化| NORMALIZE NORMALIZE --\u003e|得到| POSTERIOR POSTERIOR -.-\u003e|成为下一次的先验| PRIOR style PRIOR fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff style DATA fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff style POSTERIOR fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style LIKELIHOOD fill:#5856D6,stroke:#5856D6,stroke-width:2px,color:#ffffff 这个流程图展示了贝叶斯学习的核心：后验分布成为下一次更新的先验，形成了一个不断学习的循环。\n第三章：从被遗忘到被重新发现 在深入了解贝叶斯理论的发展之前，让我们先通过一个图表来看关键人物之间的关系：\nflowchart TD subgraph \"18世纪先驱\" BAYES[\"Thomas Bayes (1701-1761)\"] LAPLACE[\"Laplace (1749-1827)\"] end subgraph \"20世纪频率学派\" PEARSON[\"Pearson\"] FISHER[\"Fisher\"] NEYMAN[\"Neyman\"] end subgraph \"20世纪贝叶斯复兴\" DE_FINETTI[\"de Finetti\"] JEFFREYS[\"Jeffreys\"] SAVAGE[\"Savage\"] end subgraph \"现代计算突破\" METROPOLIS[\"Metropolis\"] GEMAN[\"Geman Brothers\"] end BAYES --\u003e|1763年论文| LAPLACE LAPLACE --\u003e|1812年系统化| PEARSON PEARSON --\u003e FISHER FISHER --\u003e|批评贝叶斯| JEFFREYS JEFFREYS --\u003e|1939年著作| SAVAGE SAVAGE --\u003e|1954年理论| METROPOLIS METROPOLIS --\u003e|1984年应用| GEMAN style BAYES fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff style LAPLACE fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff style FISHER fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff style JEFFREYS fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style SAVAGE fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style METROPOLIS fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style GEMAN fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff 3.1 拉普拉斯的独立贡献 在贝叶斯论文发表后不久，法国数学家皮埃尔-西蒙·拉普拉斯（Pierre-Simon Laplace，1749-1827）独立推导出了类似的公式。1812年，拉普拉斯在《概率的分析理论》中系统阐述了贝叶斯方法，并将其应用于天文学和统计学问题。\n拉普拉斯给出了一个经典例子：如果太阳连续 $n$ 天升起，那么明天太阳继续升起的概率是多少？\n使用贝叶斯方法，拉普拉斯推导出著名的拉普拉斯 succession law： $$ P(\\text{明天升起} | \\text{过去 } n \\text{ 天都升起}) = \\frac{n+1}{n+2} $$\n如果你观察到太阳连续100天升起，那么明天它升起的概率是 $\\frac{101}{102} \\approx 99.02%$。\n3.2 频率学派的崛起与贝叶斯的边缘化 19世纪末到20世纪初，统计学领域出现了频率学派（Frequentist School）的崛起，代表人物包括卡尔·皮尔逊（Karl Pearson）、罗纳德·费希尔（Ronald Fisher）、耶日·内曼（Jerzy Neyman）等。\n频率学派对贝叶斯方法提出了严厉批评：\n主观性：贝叶斯方法中的\"先验概率\"被视为主观的、不科学的 不可知论：频率学派认为概率是长期频率，不能谈论\"参数的概率\"（参数是客观存在的固定值，不是随机变量） 实用主义：频率学派发展出了置信区间、假设检验等不需要先验的方法 费希尔提出了极大似然估计（Maximum Likelihood Estimation, MLE），这种方法只需要似然函数，不需要先验分布，成为20世纪统计学的主流方法。\n在很长一段时间里，贝叶斯方法被视为异端，被主流统计学边缘化。\n3.3 贝叶斯复兴的先驱 尽管受到压制，贝叶斯思想仍有一些坚定的支持者：\n布鲁诺·德·芬内蒂（Bruno de Finetti，1906-1985）：意大利概率学家，提出了\"可换性\"（Exchangeability）概念，证明了一切概率本质上都是主观的。他的名言：“概率不存在”（Probability does not exist）——意思是客观概率不存在，只有主观信念。\n哈罗德·杰弗里斯（Harold Jeffreys，1891-1989）：英国地球物理学家和统计学家，1939年发表《概率理论》，系统阐述了贝叶斯方法在科学推断中的应用。他提出了无信息先验（Non-informative Prior）的概念，试图解决先验选择的主观性问题。\n伦纳德·萨瓦奇（Leonard Savage，1917-1971）：美国统计学家，1954年发表《统计学基础》，将概率解释为个人信念，并用效用理论论证了贝叶斯决策的合理性。\n丹尼斯·林德利（Dennis Lindley，1923-2013）：英国统计学家，著名的贝叶斯倡导者，他的名言包括：“贝叶斯方法是统计学中唯一连贯、一致、可辩护的方法”。\n3.4 计算机的突破：MCMC方法 贝叶斯方法在实际应用中的一个重大障碍是计算困难。对于复杂模型，后验分布的归一化常数（即贝叶斯公式分母中的积分）往往是高维积分，难以解析计算。\n20世纪80-90年代，马尔可夫链蒙特卡洛方法（Markov Chain Monte Carlo, MCMC）的实用化改变了这一切。MCMC方法的核心思想是：不需要精确计算后验分布，只需从后验分布中采样，然后用样本均值近似期望。\nMetropolis-Hastings算法（1953年提出，但直到90年代才广泛应用）和Gibbs采样（Geman \u0026 Geman, 1984）使得贝叶斯方法可以处理以前无法想象的高维复杂模型。\n1990年代，BUGS软件（Bayesian inference Using Gibbs Sampling）的开发，让非统计学家也能方便地使用贝叶斯方法。这标志着贝叶斯方法从理论走向大规模应用。\n第四章：现代应用：机器学习与人工智能 4.1 贝叶斯机器学习 在机器学习领域，贝叶斯方法提供了一种优雅的框架来处理不确定性：\n贝叶斯线性回归： 传统线性回归给出参数的点估计 $\\hat{\\beta}$，而贝叶斯线性回归给出参数的后验分布 $p(\\beta|D)$，从而可以对预测进行不确定性量化。\n预测分布为： $$ p(y^{\\ast}|x^{\\ast}, D) = \\int p(y^{\\ast}|x^{\\ast}, \\beta) \\cdot p(\\beta|D) , d\\beta $$\n这个积分考虑了参数的所有可能取值，给出了更全面的预测。\n高斯过程（Gaussian Process, GP）： 高斯过程是贝叶斯非参数方法的重要代表。它不是对参数建模，而是直接对函数建模：假设函数 $f(x)$ 是高斯过程，则任何有限个函数值的联合分布都是多元高斯分布。\n高斯过程不仅给出预测值，还给出预测方差（不确定性），在超参数优化、贝叶斯优化等领域有重要应用。\n4.2 贝叶斯神经网络 传统神经网络使用最大似然估计训练权重，容易过拟合。贝叶斯神经网络将权重视为随机变量，计算权重的后验分布：\n$$ p(W|D) = \\frac{p(D|W) \\cdot p(W)}{p(D)} $$\n这使得神经网络可以：\n量化不确定性：知道模型什么时候\"不确定\" 正则化效果：先验分布相当于L2正则化 主动学习：选择信息量大的样本标注 虽然精确贝叶斯神经网络计算困难，但近年来发展出了变分推断（Variational Inference）、Dropout近似等方法，使得贝叶斯深度学习成为可能。\n4.3 贝叶斯优化 贝叶斯优化是黑盒函数优化的强大工具，特别适用于：\n超参数调优 实验设计 机器人控制 核心思想：\n用高斯过程建模目标函数 用采集函数（Acquisition Function，如Expected Improvement）决定下一个评估点 观察新数据，更新高斯过程 重复直到收敛 期望改进（Expected Improvement）： $$ EI(x) = \\mathbb{E}[\\max(f(x) - f(x^+), 0)] $$\n其中 $f(x^+)$ 是当前最优值。贝叶斯优化平衡了\"开发\"（exploitation，在已知好区域搜索）和\"探索\"（exploration，在不确定区域搜索）。\n4.4 朴素贝叶斯分类器 朴素贝叶斯是最简单但最有效的贝叶斯方法之一。给定特征 $x = (x_1, x_2, \\ldots, x_n)$，预测类别 $y$：\n$$ P(y|x_1, \\ldots, x_n) \\propto P(y) \\prod_{i=1}^n P(x_i|y) $$\n“朴素\"假设：特征之间条件独立\n尽管这个假设在实际中很少成立，朴素贝叶斯在文本分类、垃圾邮件过滤等任务上表现惊人地好。\n文本分类应用：\n特征：单词是否出现 类别：文档主题 先验：$P(y)$ = 类 $y$ 的文档比例 似然：$P(x_i|y)$ = 类 $y$ 中单词 $x_i$ 的频率 4.5 AlphaGo与蒙特卡洛树搜索 2016年，DeepMind的AlphaGo击败人类围棋冠军，其核心算法之一是蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）结合深度神经网络。\nMCTS本质上是一个贝叶斯决策过程：\n先验：神经网络给出的策略网络 $p(a|s)$ 更新：通过模拟对局更新动作价值 $Q(s, a)$ 后验：结合先验和模拟结果的PUCT算法 PUCT选择公式： $$ a = \\arg\\max_a \\left( Q(s, a) + c \\cdot P(s, a) \\cdot \\frac{\\sqrt{\\sum_b N(s, b)}}{1 + N(s, a)} \\right) $$\n其中第一项是\"开发”（exploitation），第二项是\"探索\"（exploration），完美体现了贝叶斯思想。\n4.6 现代贝叶斯工具箱 今天，贝叶斯方法已经有一整套成熟的工具：\n概率编程语言（Probabilistic Programming Languages）：\nStan：基于Hamiltonian Monte Carlo（HMC），性能强大 PyMC：Python生态，易用性强 TensorFlow Probability：与TensorFlow深度集成 Pyro：基于PyTorch，支持变分推断 这些工具使得复杂的贝叶斯模型可以像写普通代码一样实现，后端自动进行推断。\n第五章：深入理解——先验选择与哲学思考 5.1 先验分布的选择 贝叶斯方法中最具争议（也最有趣）的问题是：如何选择先验？\n无信息先验（Non-informative Priors）：\n均匀先验：$p(\\theta) \\propto 1$ Jeffreys先验：$p(\\theta) \\propto \\sqrt{I(\\theta)}$，其中 $I(\\theta)$ 是Fisher信息 共轭先验（Conjugate Priors）： 如果先验和后验属于同一分布族，则称该先验为共轭先验。这简化了计算。\n例子：二项分布的Beta共轭先验\n似然：$p(k|\\theta) = \\binom{n}{k} \\theta^k (1-\\theta)^{n-k}$ 先验：$p(\\theta) = \\text{Beta}(\\alpha, \\beta) \\propto \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}$ 后验：$p(\\theta|k) \\propto \\theta^{k+\\alpha-1}(1-\\theta)^{n-k+\\beta-1} = \\text{Beta}(k+\\alpha, n-k+\\beta)$ 下图展示了Beta-二项共轭先验的更新过程：从均匀先验开始，随着观测数据增加，后验分布逐渐收敛到真实参数值。\n图2：Beta-二项共轭先验的更新过程。从均匀先验Beta(1,1)开始，随着观测到\"3正7负\"、“10正5负”、“30正20负”，后验分布逐渐从宽泛变得尖锐，均值从0.50逐渐收敛到0.60。\n层次先验（Hierarchical Priors）： 先验本身也有超参数，超参数也有超先验……形成层次结构： $$ p(\\theta, \\phi, \\psi) = p(\\theta|\\phi) \\cdot p(\\phi|\\psi) \\cdot p(\\psi) $$\n5.2 贝叶斯vs频率学派的哲学之争 核心分歧：概率的解释\n频率学派：概率是长期频率，参数是固定值 贝叶斯学派：概率是主观信念，参数是随机变量 实际影响：\n问题 频率学派 贝叶斯学派 参数估计 点估计（MLE） 后验分布 区间估计 置信区间（包含真值的概率是0或1） 可信区间（参数在区间内的概率为95%） 假设检验 p值（$P(\\text{数据} H_0)$） 预测 点预测+标准误 预测分布 下图直观对比了频率学派和贝叶斯学派的根本差异：\n图3：频率学派认为参数是固定值，通过MLE得到点估计；贝叶斯学派认为参数是随机变量，通过后验分布描述不确定性。随着数据量增加，贝叶斯后验分布逐渐收敛到真实参数值。\n现实态度： 现代统计学家大多采用实用主义态度：\n简单问题用频率方法（计算快） 复杂问题用贝叶斯方法（更自然地处理不确定性） 两者都是工具箱中的工具 5.3 贝叶斯公式的认知启示 贝叶斯公式不仅是数学工具，更是一种认知方式：\n1. 信念应该随着证据更新 我们不应对新证据教条，也不应轻率放弃旧有知识。贝叶斯公式告诉我们要平衡两者。\n2. 先验知识很重要 数据有限时，先验知识可以避免过拟合；数据充足时，先验的影响会被\"洗掉\"。\n3. 不确定性是内在的 我们永远无法获得\"上帝视角\"，只能在有限信息下做出最优推断。贝叶斯方法诚实地量化了这种不确定性。\n4. 理性学习的过程 贝叶斯更新可以看作是\"理性学习\"的数学模型：观察-更新-预测，循环往复。\n下图展示了贝叶斯信念更新的动态过程：\n图4：硬币偏差估计的贝叶斯更新过程。从先验均值0.5开始，随着观测次数增加，后验均值（蓝线）逐渐收敛到真实偏差0.7（红线），95%可信区间（浅蓝色区域）也逐渐收窄。这展示了贝叶斯学习的核心特性：不确定性随数据增加而减少。\n结语：信念更新的数学之美 从贝叶斯牧师18世纪的台球桌，到21世纪人工智能的神经网络，贝叶斯公式走过了一条漫长而曲折的道路。它曾经被主流统计学边缘化，被认为是主观和不科学的；但今天，它已成为机器学习、人工智能、数据科学的核心方法。\n贝叶斯公式的魅力在于，它用简洁的数学表达了一个深刻的哲学思想：我们的所有知识都是临时的，应该在证据面前随时准备更新。这不是摇摆不定，而是理性的最高形式——既不固执己见，也不轻信盲从。\n在信息爆炸、假新闻泛滥的时代，贝叶斯思维比以往任何时候都更有价值。它提醒我们：不要让单一证据颠覆判断，也不要因先入为主拒绝新知。先验、似然、后验——这三者的舞蹈，就是理性思考的本质。\n正如统计学家George Box所说：\"所有模型都是错的，但有些是有用的\"。贝叶斯方法不承诺给我们\"真理\"，但给了我们在不确定世界中做出最优决策的数学框架。这或许就是它从18世纪穿越到21世纪，依然焕发生机的原因。\n参考文献：\nBayes, T. (1763). An Essay towards solving a Problem in the Doctrine of Chances. Philosophical Transactions of the Royal Society, 53, 370-418.\nLaplace, P. S. (1812). Théorie Analytique des Probabilités. Paris: Courcier.\nMcGrayne, S. B. (2011). The Theory That Would Not Die: How Bayes’ Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy. Yale University Press.\nGelman, A., et al. (2013). Bayesian Data Analysis (3rd ed.). CRC Press.\nBishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.\nMurphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.\n延伸阅读：\n书籍：Sharon Bertsch McGrayne的《The Theory That Would Not Die》生动讲述了贝叶斯理论的历史 在线课程：Statistical Rethinking（Richard McElreath）是贝叶斯统计的经典课程 实践工具：推荐从PyMC或Stan开始，亲手实现贝叶斯模型 ","wordCount":"886","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/bayes-theorem.jpg","datePublished":"2026-01-21T10:00:00+08:00","dateModified":"2026-01-21T10:00:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-21-bayes-theorem/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">贝叶斯公式：从牧师遗作到人工智能基石</h1><div class=post-description>本文系统梳理贝叶斯公式从18世纪诞生到21世纪成为人工智能核心方法的完整发展历程，包含严谨的数学推导、丰富的历史背景和现代应用案例。</div><div class=post-meta><span title='2026-01-21 10:00:00 +0800 CST'>January 21, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>886 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/bayes-theorem.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/bayes-theorem.jpg alt=贝叶斯公式可视化></a><figcaption>从条件概率到信念更新</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e4%bb%8e%e4%b8%8a%e5%b8%9d%e7%9a%84%e8%a7%86%e8%a7%92%e5%88%b0%e5%87%a1%e4%ba%ba%e7%9a%84%e6%8e%a8%e6%96%ad aria-label='引言：从"上帝的视角"到"凡人的推断"'>引言：从"上帝的视角"到"凡人的推断"</a></li><li><a href=#%e7%ac%ac%e4%b8%80%e7%ab%a0%e8%b4%9d%e5%8f%b6%e6%96%af%e7%89%a7%e5%b8%88%e7%9a%84%e9%82%a3%e7%af%87%e9%81%97%e4%bd%9c aria-label=第一章：贝叶斯牧师的那篇遗作>第一章：贝叶斯牧师的那篇遗作</a><ul><li><a href=#11-%e6%89%98%e9%a9%ac%e6%96%af%e8%b4%9d%e5%8f%b6%e6%96%af%e5%85%b6%e4%ba%ba aria-label="1.1 托马斯·贝叶斯其人">1.1 托马斯·贝叶斯其人</a></li><li><a href=#12-%e9%97%ae%e9%a2%98%e7%9a%84%e6%8f%90%e5%87%ba%e4%bb%8e%e5%b7%b2%e7%9f%a5%e5%8e%9f%e5%9b%a0%e6%b1%82%e7%bb%93%e6%9e%9c%e5%88%b0%e5%b7%b2%e7%9f%a5%e7%bb%93%e6%9e%9c%e6%b1%82%e5%8e%9f%e5%9b%a0 aria-label='1.2 问题的提出：从"已知原因求结果"到"已知结果求原因"'>1.2 问题的提出：从"已知原因求结果"到"已知结果求原因"</a></li><li><a href=#13-%e8%b4%9d%e5%8f%b6%e6%96%af%e7%9a%84%e5%8f%b0%e7%90%83%e6%a8%a1%e5%9e%8b aria-label="1.3 贝叶斯的台球模型">1.3 贝叶斯的台球模型</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e7%ab%a0%e4%bb%8e%e6%9d%a1%e4%bb%b6%e6%a6%82%e7%8e%87%e5%88%b0%e8%b4%9d%e5%8f%b6%e6%96%af%e5%85%ac%e5%bc%8f aria-label=第二章：从条件概率到贝叶斯公式>第二章：从条件概率到贝叶斯公式</a><ul><li><a href=#21-%e6%9d%a1%e4%bb%b6%e6%a6%82%e7%8e%87%e7%9a%84%e5%9f%ba%e7%a1%80 aria-label="2.1 条件概率的基础">2.1 条件概率的基础</a></li><li><a href=#22-%e4%b9%98%e6%b3%95%e5%85%ac%e5%bc%8f aria-label="2.2 乘法公式">2.2 乘法公式</a></li><li><a href=#23-%e5%85%a8%e6%a6%82%e7%8e%87%e5%85%ac%e5%bc%8f aria-label="2.3 全概率公式">2.3 全概率公式</a></li><li><a href=#24-%e8%b4%9d%e5%8f%b6%e6%96%af%e5%85%ac%e5%bc%8f%e7%9a%84%e6%8e%a8%e5%af%bc aria-label="2.4 贝叶斯公式的推导">2.4 贝叶斯公式的推导</a></li><li><a href=#25-%e8%b4%9d%e5%8f%b6%e6%96%af%e5%85%ac%e5%bc%8f%e7%9a%84%e8%bf%9e%e7%bb%ad%e5%bd%a2%e5%bc%8f aria-label="2.5 贝叶斯公式的连续形式">2.5 贝叶斯公式的连续形式</a></li><li><a href=#26-%e4%b8%80%e4%b8%aa%e5%85%b7%e4%bd%93%e4%be%8b%e5%ad%90%e7%96%be%e7%97%85%e8%af%8a%e6%96%ad aria-label="2.6 一个具体例子：疾病诊断">2.6 一个具体例子：疾病诊断</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e7%ab%a0%e4%bb%8e%e8%a2%ab%e9%81%97%e5%bf%98%e5%88%b0%e8%a2%ab%e9%87%8d%e6%96%b0%e5%8f%91%e7%8e%b0 aria-label=第三章：从被遗忘到被重新发现>第三章：从被遗忘到被重新发现</a><ul><li><a href=#31-%e6%8b%89%e6%99%ae%e6%8b%89%e6%96%af%e7%9a%84%e7%8b%ac%e7%ab%8b%e8%b4%a1%e7%8c%ae aria-label="3.1 拉普拉斯的独立贡献">3.1 拉普拉斯的独立贡献</a></li><li><a href=#32-%e9%a2%91%e7%8e%87%e5%ad%a6%e6%b4%be%e7%9a%84%e5%b4%9b%e8%b5%b7%e4%b8%8e%e8%b4%9d%e5%8f%b6%e6%96%af%e7%9a%84%e8%be%b9%e7%bc%98%e5%8c%96 aria-label="3.2 频率学派的崛起与贝叶斯的边缘化">3.2 频率学派的崛起与贝叶斯的边缘化</a></li><li><a href=#33-%e8%b4%9d%e5%8f%b6%e6%96%af%e5%a4%8d%e5%85%b4%e7%9a%84%e5%85%88%e9%a9%b1 aria-label="3.3 贝叶斯复兴的先驱">3.3 贝叶斯复兴的先驱</a></li><li><a href=#34-%e8%ae%a1%e7%ae%97%e6%9c%ba%e7%9a%84%e7%aa%81%e7%a0%b4mcmc%e6%96%b9%e6%b3%95 aria-label="3.4 计算机的突破：MCMC方法">3.4 计算机的突破：MCMC方法</a></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e7%ab%a0%e7%8e%b0%e4%bb%a3%e5%ba%94%e7%94%a8%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%8e%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd aria-label=第四章：现代应用：机器学习与人工智能>第四章：现代应用：机器学习与人工智能</a><ul><li><a href=#41-%e8%b4%9d%e5%8f%b6%e6%96%af%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0 aria-label="4.1 贝叶斯机器学习">4.1 贝叶斯机器学习</a></li><li><a href=#42-%e8%b4%9d%e5%8f%b6%e6%96%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="4.2 贝叶斯神经网络">4.2 贝叶斯神经网络</a></li><li><a href=#43-%e8%b4%9d%e5%8f%b6%e6%96%af%e4%bc%98%e5%8c%96 aria-label="4.3 贝叶斯优化">4.3 贝叶斯优化</a></li><li><a href=#44-%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af%e5%88%86%e7%b1%bb%e5%99%a8 aria-label="4.4 朴素贝叶斯分类器">4.4 朴素贝叶斯分类器</a></li><li><a href=#45-alphago%e4%b8%8e%e8%92%99%e7%89%b9%e5%8d%a1%e6%b4%9b%e6%a0%91%e6%90%9c%e7%b4%a2 aria-label="4.5 AlphaGo与蒙特卡洛树搜索">4.5 AlphaGo与蒙特卡洛树搜索</a></li><li><a href=#46-%e7%8e%b0%e4%bb%a3%e8%b4%9d%e5%8f%b6%e6%96%af%e5%b7%a5%e5%85%b7%e7%ae%b1 aria-label="4.6 现代贝叶斯工具箱">4.6 现代贝叶斯工具箱</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e7%ab%a0%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3%e5%85%88%e9%aa%8c%e9%80%89%e6%8b%a9%e4%b8%8e%e5%93%b2%e5%ad%a6%e6%80%9d%e8%80%83 aria-label=第五章：深入理解——先验选择与哲学思考>第五章：深入理解——先验选择与哲学思考</a><ul><li><a href=#51-%e5%85%88%e9%aa%8c%e5%88%86%e5%b8%83%e7%9a%84%e9%80%89%e6%8b%a9 aria-label="5.1 先验分布的选择">5.1 先验分布的选择</a></li><li><a href=#52-%e8%b4%9d%e5%8f%b6%e6%96%afvs%e9%a2%91%e7%8e%87%e5%ad%a6%e6%b4%be%e7%9a%84%e5%93%b2%e5%ad%a6%e4%b9%8b%e4%ba%89 aria-label="5.2 贝叶斯vs频率学派的哲学之争">5.2 贝叶斯vs频率学派的哲学之争</a></li><li><a href=#53-%e8%b4%9d%e5%8f%b6%e6%96%af%e5%85%ac%e5%bc%8f%e7%9a%84%e8%ae%a4%e7%9f%a5%e5%90%af%e7%a4%ba aria-label="5.3 贝叶斯公式的认知启示">5.3 贝叶斯公式的认知启示</a></li></ul></li><li><a href=#%e7%bb%93%e8%af%ad%e4%bf%a1%e5%bf%b5%e6%9b%b4%e6%96%b0%e7%9a%84%e6%95%b0%e5%ad%a6%e4%b9%8b%e7%be%8e aria-label=结语：信念更新的数学之美>结语：信念更新的数学之美</a></li></ul></div></details></div><div class=post-content><h2 id=引言从上帝的视角到凡人的推断>引言：从"上帝的视角"到"凡人的推断"<a hidden class=anchor aria-hidden=true href=#引言从上帝的视角到凡人的推断>#</a></h2><p>想象你是一名医生，患者刚刚做完某种疾病的筛查测试。测试结果是阳性。那么，这位患者真正患病的概率是多少？</p><p>如果你回答"既然测试准确率是95%，那么患病的概率就是95%"，那你和大多数人的直觉一样——但也一样错了。</p><p>正确答案可能让你吃惊：哪怕测试准确率达到95%，如果这种疾病在人群中发病率只有1%，那么一个阳性结果意味着患者真正患病的概率可能只有16%左右。</p><p>这个反直觉的结果，正是贝叶斯公式的核心思想：<strong>我们的信念应该随着新证据的到来而更新</strong>，但更新的方式不是简单的替换，而是要结合我们已有的知识（先验信息）。</p><p>贝叶斯公式不仅是一个数学定理，更是一种思维方式。它告诉我们：在信息不完整的世界里，我们如何从有限的数据中学习，如何科学地调整我们的信念。从18世纪的一位英国牧师兼数学家，到21世纪的人工智能，贝叶斯的思想经历了一段跌宕起伏的旅程。</p><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>timeline
title 贝叶斯公式发展历程
section 18世纪
1763年 : 贝叶斯遗作发表
1812年 : 拉普拉斯系统阐述
section 19-20世纪
20世纪初 : 频率学派占据主导
20世纪中叶 : 萨瓦奇、杰弗里斯复兴贝叶斯思想
1980年代 : MCMC方法实用化
section 21世纪
21世纪 : 贝叶斯方法成为AI核心</div></div><h2 id=第一章贝叶斯牧师的那篇遗作>第一章：贝叶斯牧师的那篇遗作<a hidden class=anchor aria-hidden=true href=#第一章贝叶斯牧师的那篇遗作>#</a></h2><h3 id=11-托马斯贝叶斯其人>1.1 托马斯·贝叶斯其人<a hidden class=anchor aria-hidden=true href=#11-托马斯贝叶斯其人>#</a></h3><p>托马斯·贝叶斯（Thomas Bayes，1701-1761）是18世纪英国的一位长老会牧师，同时也是一位业余数学家。他出生于英格兰的一个显赫家庭，父亲是非国教牧师乔舒亚·贝叶斯。托马斯在爱丁堡大学学习神学和逻辑学，后来接任父亲的教职，在坦布里奇韦尔斯（Tunbridge Wells）担任牧师。</p><p>尽管贝叶斯在世时并未在数学领域发表太多作品，但他对概率论有着深刻的思考。他最著名的著作《机会问题的求解方法》（<em>An Essay towards solving a Problem in the Doctrine of Chances</em>）在他去世后于1763年发表在《皇家学会哲学汇刊》上。这篇论文由他的朋友理查德·普莱斯（Richard Price）整理并提交。</p><h3 id=12-问题的提出从已知原因求结果到已知结果求原因>1.2 问题的提出：从"已知原因求结果"到"已知结果求原因"<a hidden class=anchor aria-hidden=true href=#12-问题的提出从已知原因求结果到已知结果求原因>#</a></h3><p>贝叶斯关注的是一个根本性的哲学和数学问题：<strong>如果我们观察到某个结果，如何推断导致这个结果的原因？</strong></p><p>在贝叶斯之前，概率论主要处理"正向概率"问题：如果我们知道某种原因，可以计算它产生特定结果的概率。例如，如果一枚硬币是均匀的，那么抛掷得到正面的概率是50%。</p><p>但现实中我们经常面临"逆向概率"问题：我们观察到了结果，想要推断原因。例如，我们观察到病人有某种症状，想推断他患某种疾病的概率；或者我们观察到数据，想推断产生这些数据的参数。</p><p>贝叶斯的天才之处在于，他用条件概率建立了因果推断的数学框架。</p><h3 id=13-贝叶斯的台球模型>1.3 贝叶斯的台球模型<a hidden class=anchor aria-hidden=true href=#13-贝叶斯的台球模型>#</a></h3><p>贝叶斯在原文中使用了一个巧妙的台球模型来说明他的思想：</p><p>想象一张台球桌，一个球被随机抛到桌面上，停在一个未知位置（这个位置决定了某种"未知的概率"）。然后另一个球被反复抛出，我们观察它落在第一个球左侧还是右侧。</p><p>通过观察第二个球落在左侧的频率，贝叶斯想要推断：第一个球停在哪里（即"未知的概率"是多少）？</p><p>这个模型的关键洞察是：即使我们永远无法直接"看见"第一个球的位置（真实的概率值），我们也可以通过第二个球的表现（观察数据）不断更新对第一个球位置的信念。</p><h2 id=第二章从条件概率到贝叶斯公式>第二章：从条件概率到贝叶斯公式<a hidden class=anchor aria-hidden=true href=#第二章从条件概率到贝叶斯公式>#</a></h2><h3 id=21-条件概率的基础>2.1 条件概率的基础<a hidden class=anchor aria-hidden=true href=#21-条件概率的基础>#</a></h3><p>在介绍贝叶斯公式之前，我们需要先理解条件概率。条件概率 $P(A|B)$ 表示在事件 $B$ 已经发生的条件下，事件 $A$ 发生的概率。</p><p><strong>定义</strong>：若 $P(B) > 0$，则
$$
P(A|B) = \frac{P(A \cap B)}{P(B)}
$$</p><p>其中 $P(A \cap B)$ 表示 $A$ 和 $B$ 同时发生的概率。</p><p>这个定义很直观：在 $B$ 发生的所有可能情况中，$A$ 也发生的比例是多少。</p><h3 id=22-乘法公式>2.2 乘法公式<a hidden class=anchor aria-hidden=true href=#22-乘法公式>#</a></h3><p>从条件概率的定义，我们可以直接得到乘法公式：
$$
P(A \cap B) = P(A|B) \cdot P(B) = P(B|A) \cdot P(A)
$$</p><p>这个公式告诉我们：两个事件同时发生的概率，等于一个事件发生的概率乘以在此条件下另一个事件的条件概率。</p><h3 id=23-全概率公式>2.3 全概率公式<a hidden class=anchor aria-hidden=true href=#23-全概率公式>#</a></h3><p>如果我们有一个<strong>完备的事件组</strong> ${B_1, B_2, \ldots, B_n}$，满足：</p><ol><li>互斥性：$i \neq j$ 时，$B_i \cap B_j = \emptyset$</li><li>完备性：$\bigcup_{i=1}^n B_i = \Omega$（整个样本空间）</li><li>$P(B_i) > 0$ 对所有 $i$</li></ol><p>那么对于任意事件 $A$，有：
$$
P(A) = \sum_{i=1}^n P(A|B_i) \cdot P(B_i)
$$</p><p>这就是<strong>全概率公式</strong>。它的直观含义是：事件 $A$ 可以通过各种"原因" $B_i$ 发生，把所有可能路径的概率加起来，就得到 $A$ 的总概率。</p><h3 id=24-贝叶斯公式的推导>2.4 贝叶斯公式的推导<a hidden class=anchor aria-hidden=true href=#24-贝叶斯公式的推导>#</a></h3><p>现在我们可以推导贝叶斯公式了。根据条件概率的定义：
$$
P(B_i|A) = \frac{P(B_i \cap A)}{P(A)}
$$</p><p>利用乘法公式 $P(B_i \cap A) = P(A|B_i) \cdot P(B_i)$ 和全概率公式，我们得到：
$$
P(B_i|A) = \frac{P(A|B_i) \cdot P(B_i)}{\sum_{j=1}^n P(A|B_j) \cdot P(B_j)}
$$</p><p>这就是<strong>贝叶斯公式</strong>！</p><p>让我们解释每一项的含义：</p><ul><li><strong>$P(B_i)$</strong>：<strong>先验概率</strong>（Prior），在观察证据之前对假设 $B_i$ 的信念</li><li><strong>$P(A|B_i)$</strong>：<strong>似然度</strong>（Likelihood），如果假设 $B_i$ 为真，观察到证据 $A$ 的概率</li><li><strong>$P(B_i|A)$</strong>：<strong>后验概率</strong>（Posterior），在观察到证据 $A$ 后对假设 $B_i$ 的更新信念</li><li><strong>$\sum_{j=1}^n P(A|B_j) \cdot P(B_j)$</strong>：<strong>证据因子</strong>（Evidence），证据 $A$ 在所有可能假设下的总概率</li></ul><h3 id=25-贝叶斯公式的连续形式>2.5 贝叶斯公式的连续形式<a hidden class=anchor aria-hidden=true href=#25-贝叶斯公式的连续形式>#</a></h3><p>如果参数 $\theta$ 是连续变量（而不是离散假设），贝叶斯公式变为：
$$
p(\theta|D) = \frac{p(D|\theta) \cdot p(\theta)}{\int p(D|\theta&rsquo;) \cdot p(\theta&rsquo;) , d\theta&rsquo;}
$$</p><p>其中：</p><ul><li>$D$ 表示观测数据</li><li>$p(\theta)$ 是参数的<strong>先验分布</strong></li><li>$p(D|\theta)$ 是<strong>似然函数</strong></li><li>$p(\theta|D)$ 是<strong>后验分布</strong></li><li>分母是<strong>边缘似然</strong>（Marginal Likelihood），也称证据</li></ul><p><strong>连续形式的直观理解</strong>：数据 $D$ 更新了我们对参数 $\theta$ 的整个概率分布，而不仅仅是得到一个点估计。</p><h3 id=26-一个具体例子疾病诊断>2.6 一个具体例子：疾病诊断<a hidden class=anchor aria-hidden=true href=#26-一个具体例子疾病诊断>#</a></h3><p>让我们回到开头提到的疾病诊断问题。定义：</p><ul><li>$D$：患病事件</li><li>$\neg D$：不患病</li><li>$T^+$：测试阳性</li><li>$T^-$：测试阴性</li></ul><p>已知：</p><ul><li>先验概率（发病率）：$P(D) = 0.01$</li><li>测试准确率：$P(T^+|D) = 0.95$（真阳性率，灵敏度）</li><li>假阳性率：$P(T^+|\neg D) = 0.05$</li></ul><p>我们要求：在测试阳性的条件下，真正患病的概率 $P(D|T^+)$</p><p>根据贝叶斯公式：
$$
\begin{align}
P(D|T^+) &= \frac{P(T^+|D) \cdot P(D)}{P(T^+|D) \cdot P(D) + P(T^+|\neg D) \cdot P(\neg D)} \
&= \frac{0.95 \times 0.01}{0.95 \times 0.01 + 0.05 \times 0.99} \
&= \frac{0.0095}{0.0095 + 0.0495} \
&= \frac{0.0095}{0.059} \
&\approx 0.161
\end{align}
$$</p><p>所以即使测试阳性，真正患病的概率只有约16.1%！</p><p><strong>为什么直觉会错？</strong> 因为假阳性的"基数"太大：虽然健康人假阳性的概率只有5%，但健康人占总人群的99%，所以假阳性的绝对数量（$0.05 \times 0.99 = 0.0495$）远超真阳性（$0.95 \times 0.01 = 0.0095$）。</p><p>下图直观展示了贝叶斯更新的三个步骤：</p><p><img alt=疾病诊断的贝叶斯更新 loading=lazy src=/images/plots/bayes-disease-diagnosis.png></p><p><em>图1：从先验概率到后验概率的更新过程。尽管测试准确率很高（95%），但由于疾病发病率很低（1%），假阳性的绝对数量（4.95%）远超真阳性（0.95%），导致测试阳性后的真实患病概率只有16.1%。</em></p><p>下面用一个流程图展示贝叶斯推断的完整过程：</p><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>flowchart LR
subgraph "第一步：先验知识"
PRIOR["先验分布 p(θ)"]
end
subgraph "第二步：收集证据"
DATA["观测数据 D"]
LIKELIHOOD["似然函数 p(D|θ)"]
end
subgraph "第三步：贝叶斯更新"
MULTIPLY["乘法运算"]
NORMALIZE["归一化"]
end
subgraph "第四步：后验信念"
POSTERIOR["后验分布 p(θ|D)"]
end
PRIOR -->|结合| MULTI
DATA --> LIKELIHOOD
LIKELIHOOD -->|参与| MULTI
MULTI -->|未归一化| NORMALIZE
NORMALIZE -->|得到| POSTERIOR
POSTERIOR -.->|成为下一次的先验| PRIOR
style PRIOR fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff
style DATA fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff
style POSTERIOR fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style LIKELIHOOD fill:#5856D6,stroke:#5856D6,stroke-width:2px,color:#ffffff</div></div><p>这个流程图展示了贝叶斯学习的核心：<strong>后验分布成为下一次更新的先验</strong>，形成了一个不断学习的循环。</p><h2 id=第三章从被遗忘到被重新发现>第三章：从被遗忘到被重新发现<a hidden class=anchor aria-hidden=true href=#第三章从被遗忘到被重新发现>#</a></h2><p>在深入了解贝叶斯理论的发展之前，让我们先通过一个图表来看关键人物之间的关系：</p><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>flowchart TD
subgraph "18世纪先驱"
BAYES["Thomas Bayes (1701-1761)"]
LAPLACE["Laplace (1749-1827)"]
end
subgraph "20世纪频率学派"
PEARSON["Pearson"]
FISHER["Fisher"]
NEYMAN["Neyman"]
end
subgraph "20世纪贝叶斯复兴"
DE_FINETTI["de Finetti"]
JEFFREYS["Jeffreys"]
SAVAGE["Savage"]
end
subgraph "现代计算突破"
METROPOLIS["Metropolis"]
GEMAN["Geman Brothers"]
end
BAYES -->|1763年论文| LAPLACE
LAPLACE -->|1812年系统化| PEARSON
PEARSON --> FISHER
FISHER -->|批评贝叶斯| JEFFREYS
JEFFREYS -->|1939年著作| SAVAGE
SAVAGE -->|1954年理论| METROPOLIS
METROPOLIS -->|1984年应用| GEMAN
style BAYES fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff
style LAPLACE fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff
style FISHER fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff
style JEFFREYS fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style SAVAGE fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style METROPOLIS fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style GEMAN fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff</div></div><h3 id=31-拉普拉斯的独立贡献>3.1 拉普拉斯的独立贡献<a hidden class=anchor aria-hidden=true href=#31-拉普拉斯的独立贡献>#</a></h3><p>在贝叶斯论文发表后不久，法国数学家皮埃尔-西蒙·拉普拉斯（Pierre-Simon Laplace，1749-1827）独立推导出了类似的公式。1812年，拉普拉斯在《概率的分析理论》中系统阐述了贝叶斯方法，并将其应用于天文学和统计学问题。</p><p>拉普拉斯给出了一个经典例子：如果太阳连续 $n$ 天升起，那么明天太阳继续升起的概率是多少？</p><p>使用贝叶斯方法，拉普拉斯推导出著名的<strong>拉普拉斯 succession law</strong>：
$$
P(\text{明天升起} | \text{过去 } n \text{ 天都升起}) = \frac{n+1}{n+2}
$$</p><p>如果你观察到太阳连续100天升起，那么明天它升起的概率是 $\frac{101}{102} \approx 99.02%$。</p><h3 id=32-频率学派的崛起与贝叶斯的边缘化>3.2 频率学派的崛起与贝叶斯的边缘化<a hidden class=anchor aria-hidden=true href=#32-频率学派的崛起与贝叶斯的边缘化>#</a></h3><p>19世纪末到20世纪初，统计学领域出现了<strong>频率学派</strong>（Frequentist School）的崛起，代表人物包括卡尔·皮尔逊（Karl Pearson）、罗纳德·费希尔（Ronald Fisher）、耶日·内曼（Jerzy Neyman）等。</p><p>频率学派对贝叶斯方法提出了严厉批评：</p><ol><li><strong>主观性</strong>：贝叶斯方法中的"先验概率"被视为主观的、不科学的</li><li><strong>不可知论</strong>：频率学派认为概率是长期频率，不能谈论"参数的概率"（参数是客观存在的固定值，不是随机变量）</li><li><strong>实用主义</strong>：频率学派发展出了置信区间、假设检验等不需要先验的方法</li></ol><p>费希尔提出了<strong>极大似然估计</strong>（Maximum Likelihood Estimation, MLE），这种方法只需要似然函数，不需要先验分布，成为20世纪统计学的主流方法。</p><p>在很长一段时间里，贝叶斯方法被视为异端，被主流统计学边缘化。</p><h3 id=33-贝叶斯复兴的先驱>3.3 贝叶斯复兴的先驱<a hidden class=anchor aria-hidden=true href=#33-贝叶斯复兴的先驱>#</a></h3><p>尽管受到压制，贝叶斯思想仍有一些坚定的支持者：</p><p><strong>布鲁诺·德·芬内蒂（Bruno de Finetti，1906-1985）</strong>：意大利概率学家，提出了"可换性"（Exchangeability）概念，证明了一切概率本质上都是主观的。他的名言：&ldquo;概率不存在&rdquo;（Probability does not exist）——意思是客观概率不存在，只有主观信念。</p><p><strong>哈罗德·杰弗里斯（Harold Jeffreys，1891-1989）</strong>：英国地球物理学家和统计学家，1939年发表《概率理论》，系统阐述了贝叶斯方法在科学推断中的应用。他提出了<strong>无信息先验</strong>（Non-informative Prior）的概念，试图解决先验选择的主观性问题。</p><p><strong>伦纳德·萨瓦奇（Leonard Savage，1917-1971）</strong>：美国统计学家，1954年发表《统计学基础》，将概率解释为个人信念，并用效用理论论证了贝叶斯决策的合理性。</p><p><strong>丹尼斯·林德利（Dennis Lindley，1923-2013）</strong>：英国统计学家，著名的贝叶斯倡导者，他的名言包括：&ldquo;贝叶斯方法是统计学中唯一连贯、一致、可辩护的方法&rdquo;。</p><h3 id=34-计算机的突破mcmc方法>3.4 计算机的突破：MCMC方法<a hidden class=anchor aria-hidden=true href=#34-计算机的突破mcmc方法>#</a></h3><p>贝叶斯方法在实际应用中的一个重大障碍是<strong>计算困难</strong>。对于复杂模型，后验分布的归一化常数（即贝叶斯公式分母中的积分）往往是高维积分，难以解析计算。</p><p>20世纪80-90年代，<strong>马尔可夫链蒙特卡洛方法</strong>（Markov Chain Monte Carlo, MCMC）的实用化改变了这一切。MCMC方法的核心思想是：不需要精确计算后验分布，只需从后验分布中采样，然后用样本均值近似期望。</p><p><strong>Metropolis-Hastings算法</strong>（1953年提出，但直到90年代才广泛应用）和<strong>Gibbs采样</strong>（Geman & Geman, 1984）使得贝叶斯方法可以处理以前无法想象的高维复杂模型。</p><p>1990年代，<strong>BUGS软件</strong>（Bayesian inference Using Gibbs Sampling）的开发，让非统计学家也能方便地使用贝叶斯方法。这标志着贝叶斯方法从理论走向大规模应用。</p><h2 id=第四章现代应用机器学习与人工智能>第四章：现代应用：机器学习与人工智能<a hidden class=anchor aria-hidden=true href=#第四章现代应用机器学习与人工智能>#</a></h2><h3 id=41-贝叶斯机器学习>4.1 贝叶斯机器学习<a hidden class=anchor aria-hidden=true href=#41-贝叶斯机器学习>#</a></h3><p>在机器学习领域，贝叶斯方法提供了一种优雅的框架来处理不确定性：</p><p><strong>贝叶斯线性回归</strong>：
传统线性回归给出参数的点估计 $\hat{\beta}$，而贝叶斯线性回归给出参数的后验分布 $p(\beta|D)$，从而可以对预测进行不确定性量化。</p><p>预测分布为：
$$
p(y^{\ast}|x^{\ast}, D) = \int p(y^{\ast}|x^{\ast}, \beta) \cdot p(\beta|D) , d\beta
$$</p><p>这个积分考虑了参数的所有可能取值，给出了更全面的预测。</p><p><strong>高斯过程</strong>（Gaussian Process, GP）：
高斯过程是贝叶斯非参数方法的重要代表。它不是对参数建模，而是直接对函数建模：假设函数 $f(x)$ 是高斯过程，则任何有限个函数值的联合分布都是多元高斯分布。</p><p>高斯过程不仅给出预测值，还给出预测方差（不确定性），在超参数优化、贝叶斯优化等领域有重要应用。</p><h3 id=42-贝叶斯神经网络>4.2 贝叶斯神经网络<a hidden class=anchor aria-hidden=true href=#42-贝叶斯神经网络>#</a></h3><p>传统神经网络使用最大似然估计训练权重，容易过拟合。贝叶斯神经网络将权重视为随机变量，计算权重的后验分布：</p><p>$$
p(W|D) = \frac{p(D|W) \cdot p(W)}{p(D)}
$$</p><p>这使得神经网络可以：</p><ol><li><strong>量化不确定性</strong>：知道模型什么时候"不确定"</li><li><strong>正则化效果</strong>：先验分布相当于L2正则化</li><li><strong>主动学习</strong>：选择信息量大的样本标注</li></ol><p>虽然精确贝叶斯神经网络计算困难，但近年来发展出了<strong>变分推断</strong>（Variational Inference）、<strong>Dropout近似</strong>等方法，使得贝叶斯深度学习成为可能。</p><h3 id=43-贝叶斯优化>4.3 贝叶斯优化<a hidden class=anchor aria-hidden=true href=#43-贝叶斯优化>#</a></h3><p>贝叶斯优化是黑盒函数优化的强大工具，特别适用于：</p><ul><li>超参数调优</li><li>实验设计</li><li>机器人控制</li></ul><p><strong>核心思想</strong>：</p><ol><li>用高斯过程建模目标函数</li><li>用采集函数（Acquisition Function，如Expected Improvement）决定下一个评估点</li><li>观察新数据，更新高斯过程</li><li>重复直到收敛</li></ol><p><strong>期望改进</strong>（Expected Improvement）：
$$
EI(x) = \mathbb{E}[\max(f(x) - f(x^+), 0)]
$$</p><p>其中 $f(x^+)$ 是当前最优值。贝叶斯优化平衡了"开发"（exploitation，在已知好区域搜索）和"探索"（exploration，在不确定区域搜索）。</p><h3 id=44-朴素贝叶斯分类器>4.4 朴素贝叶斯分类器<a hidden class=anchor aria-hidden=true href=#44-朴素贝叶斯分类器>#</a></h3><p>朴素贝叶斯是最简单但最有效的贝叶斯方法之一。给定特征 $x = (x_1, x_2, \ldots, x_n)$，预测类别 $y$：</p><p>$$
P(y|x_1, \ldots, x_n) \propto P(y) \prod_{i=1}^n P(x_i|y)
$$</p><p><strong>&ldquo;朴素"假设</strong>：特征之间条件独立</p><p>尽管这个假设在实际中很少成立，朴素贝叶斯在文本分类、垃圾邮件过滤等任务上表现惊人地好。</p><p><strong>文本分类应用</strong>：</p><ul><li>特征：单词是否出现</li><li>类别：文档主题</li><li>先验：$P(y)$ = 类 $y$ 的文档比例</li><li>似然：$P(x_i|y)$ = 类 $y$ 中单词 $x_i$ 的频率</li></ul><h3 id=45-alphago与蒙特卡洛树搜索>4.5 AlphaGo与蒙特卡洛树搜索<a hidden class=anchor aria-hidden=true href=#45-alphago与蒙特卡洛树搜索>#</a></h3><p>2016年，DeepMind的AlphaGo击败人类围棋冠军，其核心算法之一是<strong>蒙特卡洛树搜索</strong>（Monte Carlo Tree Search, MCTS）结合深度神经网络。</p><p>MCTS本质上是一个贝叶斯决策过程：</p><ul><li><strong>先验</strong>：神经网络给出的策略网络 $p(a|s)$</li><li><strong>更新</strong>：通过模拟对局更新动作价值 $Q(s, a)$</li><li><strong>后验</strong>：结合先验和模拟结果的PUCT算法</li></ul><p>PUCT选择公式：
$$
a = \arg\max_a \left( Q(s, a) + c \cdot P(s, a) \cdot \frac{\sqrt{\sum_b N(s, b)}}{1 + N(s, a)} \right)
$$</p><p>其中第一项是"开发&rdquo;（exploitation），第二项是"探索"（exploration），完美体现了贝叶斯思想。</p><h3 id=46-现代贝叶斯工具箱>4.6 现代贝叶斯工具箱<a hidden class=anchor aria-hidden=true href=#46-现代贝叶斯工具箱>#</a></h3><p>今天，贝叶斯方法已经有一整套成熟的工具：</p><p><strong>概率编程语言</strong>（Probabilistic Programming Languages）：</p><ul><li><strong>Stan</strong>：基于Hamiltonian Monte Carlo（HMC），性能强大</li><li><strong>PyMC</strong>：Python生态，易用性强</li><li><strong>TensorFlow Probability</strong>：与TensorFlow深度集成</li><li><strong>Pyro</strong>：基于PyTorch，支持变分推断</li></ul><p>这些工具使得复杂的贝叶斯模型可以像写普通代码一样实现，后端自动进行推断。</p><h2 id=第五章深入理解先验选择与哲学思考>第五章：深入理解——先验选择与哲学思考<a hidden class=anchor aria-hidden=true href=#第五章深入理解先验选择与哲学思考>#</a></h2><h3 id=51-先验分布的选择>5.1 先验分布的选择<a hidden class=anchor aria-hidden=true href=#51-先验分布的选择>#</a></h3><p>贝叶斯方法中最具争议（也最有趣）的问题是：<strong>如何选择先验？</strong></p><p><strong>无信息先验</strong>（Non-informative Priors）：</p><ul><li><strong>均匀先验</strong>：$p(\theta) \propto 1$</li><li><strong>Jeffreys先验</strong>：$p(\theta) \propto \sqrt{I(\theta)}$，其中 $I(\theta)$ 是Fisher信息</li></ul><p><strong>共轭先验</strong>（Conjugate Priors）：
如果先验和后验属于同一分布族，则称该先验为共轭先验。这简化了计算。</p><p>例子：二项分布的Beta共轭先验</p><ul><li>似然：$p(k|\theta) = \binom{n}{k} \theta^k (1-\theta)^{n-k}$</li><li>先验：$p(\theta) = \text{Beta}(\alpha, \beta) \propto \theta^{\alpha-1}(1-\theta)^{\beta-1}$</li><li>后验：$p(\theta|k) \propto \theta^{k+\alpha-1}(1-\theta)^{n-k+\beta-1} = \text{Beta}(k+\alpha, n-k+\beta)$</li></ul><p>下图展示了Beta-二项共轭先验的更新过程：从均匀先验开始，随着观测数据增加，后验分布逐渐收敛到真实参数值。</p><p><img alt=Beta-二项共轭先验更新 loading=lazy src=/images/plots/bayes-beta-conjugate.png></p><p><em>图2：Beta-二项共轭先验的更新过程。从均匀先验Beta(1,1)开始，随着观测到"3正7负"、&ldquo;10正5负&rdquo;、&ldquo;30正20负&rdquo;，后验分布逐渐从宽泛变得尖锐，均值从0.50逐渐收敛到0.60。</em></p><p><strong>层次先验</strong>（Hierarchical Priors）：
先验本身也有超参数，超参数也有超先验……形成层次结构：
$$
p(\theta, \phi, \psi) = p(\theta|\phi) \cdot p(\phi|\psi) \cdot p(\psi)
$$</p><h3 id=52-贝叶斯vs频率学派的哲学之争>5.2 贝叶斯vs频率学派的哲学之争<a hidden class=anchor aria-hidden=true href=#52-贝叶斯vs频率学派的哲学之争>#</a></h3><p><strong>核心分歧</strong>：概率的解释</p><ul><li><strong>频率学派</strong>：概率是长期频率，参数是固定值</li><li><strong>贝叶斯学派</strong>：概率是主观信念，参数是随机变量</li></ul><p><strong>实际影响</strong>：</p><table><thead><tr><th>问题</th><th>频率学派</th><th>贝叶斯学派</th></tr></thead><tbody><tr><td>参数估计</td><td>点估计（MLE）</td><td>后验分布</td></tr><tr><td>区间估计</td><td>置信区间（包含真值的概率是0或1）</td><td>可信区间（参数在区间内的概率为95%）</td></tr><tr><td>假设检验</td><td>p值（$P(\text{数据}</td><td>H_0)$）</td></tr><tr><td>预测</td><td>点预测+标准误</td><td>预测分布</td></tr></tbody></table><p>下图直观对比了频率学派和贝叶斯学派的根本差异：</p><p><img alt=频率学派vs贝叶斯学派 loading=lazy src=/images/plots/bayes-frequentist-comparison.png></p><p><em>图3：频率学派认为参数是固定值，通过MLE得到点估计；贝叶斯学派认为参数是随机变量，通过后验分布描述不确定性。随着数据量增加，贝叶斯后验分布逐渐收敛到真实参数值。</em></p><p><strong>现实态度</strong>：
现代统计学家大多采用实用主义态度：</p><ul><li>简单问题用频率方法（计算快）</li><li>复杂问题用贝叶斯方法（更自然地处理不确定性）</li><li>两者都是工具箱中的工具</li></ul><h3 id=53-贝叶斯公式的认知启示>5.3 贝叶斯公式的认知启示<a hidden class=anchor aria-hidden=true href=#53-贝叶斯公式的认知启示>#</a></h3><p>贝叶斯公式不仅是数学工具，更是一种认知方式：</p><p><strong>1. 信念应该随着证据更新</strong>
我们不应对新证据教条，也不应轻率放弃旧有知识。贝叶斯公式告诉我们要平衡两者。</p><p><strong>2. 先验知识很重要</strong>
数据有限时，先验知识可以避免过拟合；数据充足时，先验的影响会被"洗掉"。</p><p><strong>3. 不确定性是内在的</strong>
我们永远无法获得"上帝视角"，只能在有限信息下做出最优推断。贝叶斯方法诚实地量化了这种不确定性。</p><p><strong>4. 理性学习的过程</strong>
贝叶斯更新可以看作是"理性学习"的数学模型：观察-更新-预测，循环往复。</p><p>下图展示了贝叶斯信念更新的动态过程：</p><p><img alt=贝叶斯信念更新过程 loading=lazy src=/images/plots/bayes-belief-update.png></p><p><em>图4：硬币偏差估计的贝叶斯更新过程。从先验均值0.5开始，随着观测次数增加，后验均值（蓝线）逐渐收敛到真实偏差0.7（红线），95%可信区间（浅蓝色区域）也逐渐收窄。这展示了贝叶斯学习的核心特性：不确定性随数据增加而减少。</em></p><h2 id=结语信念更新的数学之美>结语：信念更新的数学之美<a hidden class=anchor aria-hidden=true href=#结语信念更新的数学之美>#</a></h2><p>从贝叶斯牧师18世纪的台球桌，到21世纪人工智能的神经网络，贝叶斯公式走过了一条漫长而曲折的道路。它曾经被主流统计学边缘化，被认为是主观和不科学的；但今天，它已成为机器学习、人工智能、数据科学的核心方法。</p><p>贝叶斯公式的魅力在于，它用简洁的数学表达了一个深刻的哲学思想：<strong>我们的所有知识都是临时的，应该在证据面前随时准备更新</strong>。这不是摇摆不定，而是理性的最高形式——既不固执己见，也不轻信盲从。</p><p>在信息爆炸、假新闻泛滥的时代，贝叶斯思维比以往任何时候都更有价值。它提醒我们：不要让单一证据颠覆判断，也不要因先入为主拒绝新知。先验、似然、后验——这三者的舞蹈，就是理性思考的本质。</p><p>正如统计学家George Box所说："<strong>所有模型都是错的，但有些是有用的</strong>"。贝叶斯方法不承诺给我们"真理"，但给了我们在不确定世界中做出最优决策的数学框架。这或许就是它从18世纪穿越到21世纪，依然焕发生机的原因。</p><hr><p><strong>参考文献</strong>：</p><ol><li><p>Bayes, T. (1763). An Essay towards solving a Problem in the Doctrine of Chances. <em>Philosophical Transactions of the Royal Society</em>, 53, 370-418.</p></li><li><p>Laplace, P. S. (1812). <em>Théorie Analytique des Probabilités</em>. Paris: Courcier.</p></li><li><p>McGrayne, S. B. (2011). <em>The Theory That Would Not Die: How Bayes&rsquo; Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy</em>. Yale University Press.</p></li><li><p>Gelman, A., et al. (2013). <em>Bayesian Data Analysis</em> (3rd ed.). CRC Press.</p></li><li><p>Bishop, C. M. (2006). <em>Pattern Recognition and Machine Learning</em>. Springer.</p></li><li><p>Murphy, K. P. (2012). <em>Machine Learning: A Probabilistic Perspective</em>. MIT Press.</p></li></ol><hr><p><strong>延伸阅读</strong>：</p><ul><li><strong>书籍</strong>：Sharon Bertsch McGrayne的《The Theory That Would Not Die》生动讲述了贝叶斯理论的历史</li><li><strong>在线课程</strong>：Statistical Rethinking（Richard McElreath）是贝叶斯统计的经典课程</li><li><strong>实践工具</strong>：推荐从PyMC或Stan开始，亲手实现贝叶斯模型</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/>贝叶斯</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/>概率论</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%95%B0%E5%AD%A6%E5%8F%B2/>数学史</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/>人工智能</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-21-gaussian-distribution-history/><span class=title>« Prev</span><br><span>正态分布：从赌桌到宇宙的完美曲线</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-21-shannon-entropy-comprehensive-guide/><span class=title>Next »</span><br><span>香农信息熵：不确定性的数学刻度</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 贝叶斯公式：从牧师遗作到人工智能基石 on x" href="https://x.com/intent/tweet/?text=%e8%b4%9d%e5%8f%b6%e6%96%af%e5%85%ac%e5%bc%8f%ef%bc%9a%e4%bb%8e%e7%89%a7%e5%b8%88%e9%81%97%e4%bd%9c%e5%88%b0%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e5%9f%ba%e7%9f%b3&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-bayes-theorem%2f&amp;hashtags=%e8%b4%9d%e5%8f%b6%e6%96%af%2c%e6%a6%82%e7%8e%87%e8%ae%ba%2c%e6%95%b0%e5%ad%a6%e5%8f%b2%2c%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%2c%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 贝叶斯公式：从牧师遗作到人工智能基石 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-bayes-theorem%2f&amp;title=%e8%b4%9d%e5%8f%b6%e6%96%af%e5%85%ac%e5%bc%8f%ef%bc%9a%e4%bb%8e%e7%89%a7%e5%b8%88%e9%81%97%e4%bd%9c%e5%88%b0%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e5%9f%ba%e7%9f%b3&amp;summary=%e8%b4%9d%e5%8f%b6%e6%96%af%e5%85%ac%e5%bc%8f%ef%bc%9a%e4%bb%8e%e7%89%a7%e5%b8%88%e9%81%97%e4%bd%9c%e5%88%b0%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e5%9f%ba%e7%9f%b3&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-bayes-theorem%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 贝叶斯公式：从牧师遗作到人工智能基石 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-bayes-theorem%2f&title=%e8%b4%9d%e5%8f%b6%e6%96%af%e5%85%ac%e5%bc%8f%ef%bc%9a%e4%bb%8e%e7%89%a7%e5%b8%88%e9%81%97%e4%bd%9c%e5%88%b0%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e5%9f%ba%e7%9f%b3"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 贝叶斯公式：从牧师遗作到人工智能基石 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-bayes-theorem%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>