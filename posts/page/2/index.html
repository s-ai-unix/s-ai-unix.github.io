<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | s-ai-unix's Blog</title><meta name=keywords content><meta name=description content="Posts - s-ai-unix's Blog"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://s-ai-unix.github.io/posts/index.xml title=rss><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="Posts"><meta property="og:description" content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Posts"><meta name=twitter:description content="s-ai-unix 的个人技术博客，分享数学、AI、产品设计、数据科学、智能汽车(设计、研发、质量、合规)、历史等领域的知识和思考"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"}]}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body class=list id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span class=active>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a></div><h1>Posts</h1></header><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/word2vec-cover.jpg alt="Word2Vec 词向量可视化"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：Word2Vec - 词向量的革命</h2></header><div class=entry-content><p>“You shall know a word by the company it keeps.” — John Rupert Firth
引言：从符号到语义 想象一下，你正在阅读一篇关于"苹果"的文章。在"乔布斯推出了划时代的苹果产品"这句话中，“苹果"显然指的是一家公司；而在"我喜欢吃新鲜的苹果"中，它则是一种水果。人类能够毫不费力地根据上下文理解这种歧义，但对于计算机而言，这曾是一个巨大的挑战。
在 Word2Vec 出现之前，自然语言处理主要依赖独热编码（One-Hot Encoding）：每个词都被表示为一个高维稀疏向量，向量中只有对应位置为 $1$，其余全为 $0$。“苹果"可能是 $[0, 0, 1, 0, \ldots, 0]$，“香蕉"是 $[0, 0, 0, 1, \ldots, 0]$。这种方法的问题显而易见：任意两个词之间的余弦相似度都是 $0$，模型完全无法捕捉"苹果"和"香蕉"都是水果这一语义关系。
2013 年，Tomas Mikolov 等人在 Google 提出了 Word2Vec，这是一种能够从大规模语料库中学习词向量表示的浅层神经网络。其核心思想简单却深刻：语义相近的词，其上下文也相似。这一方法不仅在多项语义和语法任务上取得了当时最先进的性能，更开启了深度学习在自然语言处理领域的广泛应用。
本文将带你深入理解 Word2Vec 的数学原理，从神经概率语言模型出发，完整推导 CBOW 和 Skip-gram 两种架构，并探讨其在现代 NLP 中的深远影响。
第一章：从词袋到神经语言模型 1.1 统计语言模型的演进 语言模型的核心任务是计算一个句子出现的概率。对于包含 $n$ 个词的句子
$$w_1, w_2, \ldots, w_n$$ 其联合概率可以分解为：
$$P(w_1, w_2, \ldots, w_n) = \prod_{i=1}^{n} P(w_i \mid w_1, \ldots, w_{i-1})$$ 这个分解基于链式法则，但直接估计这些条件概率面临维度灾难——历史词的组合数是指数级的。
...</p></div><footer class=entry-footer><span title='2026-01-30 09:00:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>7 min</span>&nbsp;·&nbsp;<span>1442 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：Word2Vec - 词向量的革命" href=https://s-ai-unix.github.io/posts/2026-01-30-word2vec-paper-explained/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/gpt3-paper-cover.jpg alt="GPT-3 论文解读封面"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：GPT-3——当语言模型学会举一反三</h2></header><div class=entry-content><p>引言：从海量数据中学习 2020 年 6 月，OpenAI 发表了一篇注定载入人工智能史册的论文：《Language Models are Few-Shot Learners》。这篇论文介绍了 GPT-3——一个拥有 1750 亿参数的巨型语言模型。这个数字意味着什么？如果将 GPT-3 的参数全部打印出来，使用标准字体，这些纸张可以从地球堆到月球——再返回地球好几个来回。
但 GPT-3 的真正革命性之处不在于它的规模，而在于它展现出的少样本学习能力（Few-Shot Learning）。在此之前，如果我们想让一个 AI 模型完成翻译任务，需要用成千上万对双语句子"教"它；而 GPT-3 只需要看几个例子，就能理解任务并给出合理的输出。
这篇文章将带你走进 GPT-3 的世界，理解它背后的数学原理、技术架构，以及它如何改变了我们对人工智能的认知。
第一章：从 GPT-1 到 GPT-3 的演进之路 1.1 语言的统计本质 在深入 GPT-3 之前，让我们先思考一个基本问题：什么是语言模型？
从数学角度看，语言模型试图回答这样一个问题：给定一段已出现的词序列
$$\mathbf{x}_{...</p></div><footer class=entry-footer><span title='2026-01-30 08:50:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>1 min</span>&nbsp;·&nbsp;<span>38 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：GPT-3——当语言模型学会举一反三" href=https://s-ai-unix.github.io/posts/2026-01-30-gpt3-few-shot-learners-paper/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/vit-cover.jpg alt="AI 论文解读系列 Vision Transformer cover image"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：Vision Transformer 视觉Transformer</h2></header><div class=entry-content><p>AI 论文解读系列：Vision Transformer 视觉 Transformer 引言 2020 年，Google Research 发表了一篇极具颠覆性的论文《An Image is Worth 16$\times$16 Words: Transformers for Image Recognition at Scale》。这篇论文提出了 Vision Transformer（ViT），一个纯粹基于 Transformer 架构的视觉模型，在 ImageNet 分类任务上取得了与最先进的卷积神经网络（CNN）相媲美甚至超越的成绩。
这个成果的震撼之处在于：在计算机视觉领域统治了整整十年的卷积神经网络，终于遇到了真正的挑战者。CNN 凭借其归纳偏置（局部性、平移等变性）在视觉任务中表现出色，而 Transformer 原本是为自然语言处理设计的序列模型。ViT 的成功证明，只要有足够的数据和计算资源，纯粹的注意力机制同样可以在视觉任务中大放异彩。
本文将从注意力机制的基础出发，循序渐进地剖析 ViT 的架构设计、数学原理和训练策略，揭示为何"一张图片相当于 16$\times$16 个单词"这一简单想法能够改变计算机视觉的格局。
第一章：从 CNN 到 Transformer 的范式转移 1.1 卷积神经网络的统治时代 自 2012 年 AlexNet 在 ImageNet 竞赛中取得突破性成果以来，卷积神经网络（CNN）一直是计算机视觉领域的主流架构。CNN 的成功建立在几个关键设计之上：
局部感受野（Local Receptive Fields）：每个神经元只与输入的局部区域连接，捕捉局部特征如边缘、纹理。
权重共享（Weight Sharing）：同一个卷积核在整个输入上滑动，检测相同特征的不同位置。
平移等变性（Translation Equivariance）：输入图像平移，特征图也相应平移，保持空间关系。
这些归纳偏置（Inductive Bias）使 CNN 非常适合处理图像数据，但也带来了一些限制：
感受野有限，需要堆叠多层才能获取全局信息 对长距离依赖的建模能力较弱 难以直接捕捉空间上相距较远的像素之间的关系 1.2 Transformer 在自然语言处理中的成功 2017 年，Google 在论文《Attention Is All You Need》中提出了 Transformer 架构，彻底改变了自然语言处理（NLP）领域。Transformer 完全基于自注意力机制（Self-Attention），摒弃了循环和卷积结构。
...</p></div><footer class=entry-footer><span title='2026-01-30 08:46:42 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>986 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：Vision Transformer 视觉Transformer" href=https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97vision-transformer-%E8%A7%86%E8%A7%89transformer/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/resnet-cover.jpg alt="AI 论文解读系列 ResNet 深度残差学习 cover image"></figure><header class=entry-header><h2 class=entry-hint-parent>AI 论文解读系列：ResNet 深度残差学习</h2></header><div class=entry-content><p>AI 论文解读系列：ResNet 深度残差学习 引言 2015 年，微软研究院的何恺明等人在 ImageNet 竞赛中提出了一个看似简单却极具革命性的想法：如果神经网络学习的是残差而非直接的映射，会发生什么？这个想法催生了 ResNet（Residual Network），一个拥有 152 层甚至 1000 多层的深度网络，不仅赢得了 ImageNet 2015 的冠军，更重要的是，它解决了困扰深度学习领域多年的一个核心问题——深层网络的退化。
在 ResNet 出现之前，人们普遍认为更深的网络应该具有更强的表达能力。然而实践却给出了反直觉的结果：当网络层数增加到一定程度后，训练准确率反而下降。这不是过拟合，因为在训练集上的表现同样变差了。ResNet 的巧妙之处在于，它通过一个极其简单的跳跃连接（skip connection），让网络可以选择学习残差映射 $\mathcal{F}(\mathbf{x}) = \mathcal{H}(\mathbf{x}) - \mathbf{x}$，而非直接学习 $\mathcal{H}(\mathbf{x})$。
本文将系统性地解读这篇经典论文，从问题背景、核心思想、数学推导、架构设计到实验验证，循序渐进地揭示 ResNet 为何如此有效。
第一章：深层网络的困境 1.1 从浅层到深层：一个自然的假设 深度学习的成功在很大程度上归功于深层神经网络强大的表示能力。从 LeNet-5 的 5 层，到 AlexNet 的 8 层，再到 VGGNet 的 16-19 层，网络深度的增加似乎与性能提升正相关。这种趋势背后的直觉很简单：更深的网络可以学习更复杂的特征层次结构。
让我们形式化地思考这个问题。假设我们有一个浅层网络，它能够学习某个映射 $\mathcal{H}(\mathbf{x})$。如果我们在其后面添加更多层，直觉上，这些额外的层可以学习恒等映射（identity mapping），即直接输出输入：$\mathbf{y} = \mathbf{x}$。这样，深层网络至少应该和浅层网络表现一样好。
然而，实践观察到的却是另一番景象。
1.2 退化问题：理论与现实的鸿沟 2015 年之前的研究者发现，当网络层数超过 20 层后，出现了一个令人困惑的现象：随着网络加深，训练误差不降反升。
上图展示了在 CIFAR-10 数据集上的典型实验结果。20 层网络的训练误差约为 8%，而 56 层网络的训练误差却上升到了 20%。请注意，这是在训练集上的表现，因此这不是过拟合问题，而是优化问题。
这个现象被称为退化问题（Degradation Problem）。它的存在表明：
...</p></div><footer class=entry-footer><span title='2026-01-30 08:38:11 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>1008 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to AI 论文解读系列：ResNet 深度残差学习" href=https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97%E4%B9%8Bresnet-%E6%B7%B1%E5%BA%A6%E6%AE%8B%E5%B7%AE%E5%AD%A6%E4%B9%A0/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>从弯曲到一致性：微分几何中的芬切尔定理与舒尔定理</h2></header><div class=entry-content><p>引言：从古希腊到现代几何 在古希腊的亚历山大图书馆里，数学家们就已经开始思考曲线和曲面的性质。欧几里得的《几何原本》建立了几何学的公理体系，但那是关于直线的几何——平坦的、规则的、完美的。然而，自然界中却充满了弯曲：行星轨道、海岸线、山脉的轮廓，甚至光线在引力场中的路径。
当我们从平坦的欧几里得空间迈向弯曲的几何世界时，一个自然的问题浮现：如何量化"弯曲"？一条曲线究竟有多"弯曲"？一个曲面在哪个方向最"弯曲"？
这个问题催生了微分几何的诞生。从高斯的曲面理论到黎曼的一般流形，数学家们发展了一套精妙的语言来描述弯曲。在这个过程中，两个深刻定理脱颖而出：芬切尔定理（Fenchel’s Theorem）和舒尔定理（Schur’s Theorem）。
芬切尔定理，由 Werner Fenchel 在 1929 年证明，给出了闭曲线弯曲程度的一个基本下界：任何简单闭曲线的全曲率至少为 $2\pi$。这个数字 $2\pi$ 不仅仅是圆周的长度，更蕴含着拓扑学的深刻含义——它与曲线的"旋转"次数有关。
而舒尔定理，由 Friedrich Schur 在 1917 年发现，则在更高维度的黎曼几何中提供了一个关于"一致性"的深刻洞察。它告诉我们：如果一个空间在某个方向的弯曲程度最大，那么其他方向的弯曲程度如何。这是比较几何学（Comparison Geometry）的开端。
本文将从曲线的基本概念开始，娓娓道来地介绍这两条定理。我们将看到，数学的优美不仅在于其严谨性，更在于它揭示了自然界中深刻的统一性。
第一章：曲线的几何——曲率与 Frenet 标架 在进入芬切尔定理之前，我们需要掌握描述曲线弯曲的基本工具。让我们从最直观的概念开始。
1.1 参数化曲线 设 $\gamma: [a, b] \to \mathbb{R}^3$ 是一条可微曲线。为了简化讨论，我们通常假设曲线是正则的（regular），即 $\gamma’(t) \neq 0$ 对所有 $t$ 成立。这里 $\gamma’(t)$ 是曲线的切向量，它告诉我们曲线在参数 $t$ 处的方向。
为了消除参数化对曲线描述的影响，我们常常使用弧长参数化。设 $s(t)$ 是从起点 $\gamma(a)$ 到 $\gamma(t)$ 的弧长：
$$ s(t) = \int_a^t \lVert \gamma’(\tau) \rVert , d\tau $$
反函数 $t(s)$ 允许我们将曲线表示为弧长的函数 $\gamma(s)$。弧长参数化的优点是切向量具有单位长度：
$$ \lVert \frac{d\gamma}{ds} \rVert = \frac{ds}{dt} \cdot \left\lVert \frac{d\gamma}{dt} \right\rVert^{-1} = 1 $$
...</p></div><footer class=entry-footer><span title='2026-01-29 20:50:00 +0800 CST'>January 29, 2026</span>&nbsp;·&nbsp;<span>6 min</span>&nbsp;·&nbsp;<span>1246 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 从弯曲到一致性：微分几何中的芬切尔定理与舒尔定理" href=https://s-ai-unix.github.io/posts/2026-01-29-fenchel-schur-theorems/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/1555255707-c07966088b7b.jpg alt=Monge-Ampere方程与最优传输></figure><header class=entry-header><h2 class=entry-hint-parent>蒙日-安培方程：从经典几何到现代分析的系统综述</h2></header><div class=entry-content><p>引言：一个跨越两个半世纪的数学传奇 1771年，法国数学家加斯帕尔·蒙日（Gaspard Monge）在研究曲面和曲线理论时，写下了一个看似简单的方程。他或许不会想到，这个方程将在接下来的两个半世纪里，成为连接微分几何、偏微分方程、变分法和概率论的深刻纽带，并最终在2018年帮助阿莱西奥·菲加利（Alessio Figalli）获得菲尔兹奖。
这个方程就是蒙日-安培方程（Monge-Ampère Equation）。
图1：蒙日-安培方程从18世纪到现代的发展历程，涵盖了几何、分析和应用数学的多个里程碑。
蒙日-安培方程的特殊之处在于它的完全非线性特性。与拉普拉斯方程或热方程这类线性方程不同，蒙日-安培方程涉及未知函数二阶导数的行列式——这是所有二阶导数的非线性组合。这种结构既带来了深刻的数学挑战，也赋予了它独特的几何意义。
在本文中，我们将从三个维度深入探索这一优美的数学对象：
历史维度：从蒙日的几何洞察到现代正则性理论 理论维度：方程的结构、椭圆性理论和解的适定性 应用维度：从凸几何到最优传输，从气象学到机器学习 第一章：历史渊源——从蒙日到现代 1.1 蒙日的几何洞察（1771-1807） 加斯帕尔·蒙日（1746-1818）是法国大革命时期的杰出数学家，被誉为画法几何的奠基人。他对曲面的研究源于工程学的实际问题：如何在二维平面上精确表示三维物体？
1771年，蒙日在论文《Memoire sur les developpées, les rayons de courbure et les différens genres d’inflexions des courbes à double courbure》中首次研究了一类涉及曲面曲率的偏微分方程。他考虑的核心问题是：给定曲面的曲率信息，能否重建曲面本身？
蒙日的洞察在于认识到曲面的高斯曲率与函数二阶导数之间的深刻联系。对于一个由 $z = u(x, y)$ 给出的曲面，其高斯曲率 $K$ 可以表示为：
$$ K = \frac{u_{xx}u_{yy} - u_{xy}^2}{(1 + u_x^2 + u_y^2)^2} $$ 分子中的 $u_{xx}u_{yy} - u_{xy}^2$ 正是函数 $u$ 的Hessian行列式——蒙日-安培方程的核心结构。
1.2 安培的分析贡献（1820s） 安德烈-玛丽·安培（André-Marie Ampère，1775-1836）更为人熟知的是他在电磁学方面的贡献（电流单位"安培"即以他命名）。但在1820年代，安培对蒙日的方程进行了系统的分析研究，将其推广到更一般的形式。
安培考虑了方程的一般二阶形式：
$$ A(u_{xx}u_{yy} - u_{xy}^2) + Bu_{xx} + Cu_{xy} + Du_{yy} + E = 0 $$ 其中系数 $A, B, C, D, E$ 可以依赖于 $(x, y, u, u_x, u_y)$。当 $A \neq 0$ 时，方程具有典型的蒙日-安培结构。
...</p></div><footer class=entry-footer><span title='2026-01-29 19:30:00 +0800 CST'>January 29, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>1027 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 蒙日-安培方程：从经典几何到现代分析的系统综述" href=https://s-ai-unix.github.io/posts/2026-01-29-monge-ampere-equation-detailed-introduction/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/singularity-theorem-cover.jpg alt=黑洞奇点与时空结构></figure><header class=entry-header><h2 class=entry-hint-parent>彭罗斯-霍金奇点定理：广义相对论的终极边界</h2></header><div class=entry-content><p>引言：时空的终极命运 1965年的一个春日，年轻的数学家罗杰·彭罗斯（Roger Penrose）正坐在剑桥大学的一个咖啡馆里，凝视着手中咖啡杯里旋转的泡沫。那一刻，一个改变物理学史的洞见在他脑海中闪现：如果一个恒星坍缩得足够致密，奇点的形成将是不可避免的——这不是由于某种特殊的对称性假设，而是源于引力的普遍性质。
这个洞见最终发展成了著名的彭罗斯奇点定理（Penrose Singularity Theorem），它与斯蒂芬·霍金（Stephen Hawking）在1970年证明的霍金奇点定理一起，构成了广义相对论中最深刻的成果之一。彭罗斯因此在2020年获得了诺贝尔物理学奖，表彰他"发现黑洞形成是广义相对论的稳健预言"。
但是，这些定理究竟说了什么？它们如何证明？又对我们的宇宙理解意味着什么？
让我们从一个简单的观察开始：在牛顿引力理论中，如果向太空中抛掷一个球，它可能会落回地面，也可能逃逸到无穷远，这取决于初速度。但在广义相对论中，情况变得更为微妙——一旦物质足够集中，时空本身就会"撕裂"，产生奇点。
图1：时空中的光锥结构。光锥将时空划分为未来、过去和类空区域，是理解因果结构的基石。
在本文中，我们将踏上一段深入的数学物理之旅，从微分几何的基础概念出发，逐步构建理解奇点定理所需的理论框架，最终揭示这些定理的深刻内涵。
第一章：预备知识——时空的数学结构 1.1 什么是时空？ 在广义相对论中，时空是一个四维的洛伦兹流形 $(M, g)$，其中：
$M$ 是一个四维光滑流形 $g$ 是一个洛伦兹度规，其符号差为 $(-, +, +, +)$ 或 $(+, -, -, -)$ 这意味着在每一点 $p \in M$，度规 $g_p$ 在切空间 $T_p M$ 上定义了一个内积，允许我们计算向量的"长度"和"夹角"。但与黎曼几何不同，洛伦兹度规可以取负值，这导致了类时（timelike）、类光（null）和类空（spacelike）向量的区分。
$$ g(v, w) = g_{\mu\nu} v^{\mu} w^{\nu} $$ 对于任意向量 $v \in T_p M$：
若 $g(v, v) &lt; 0$：$v$ 是类时向量（对应实物体的世界线） 若 $g(v, v) = 0$：$v$ 是类光向量（对应光线的世界线） 若 $g(v, v) > 0$：$v$ 是类空向量（连接同时事件的线） 1.2 测地线与自由落体 在广义相对论中，不受外力的粒子沿测地线运动。测地线是"最直"的曲线，满足测地线方程：
...</p></div><footer class=entry-footer><span title='2026-01-29 19:00:00 +0800 CST'>January 29, 2026</span>&nbsp;·&nbsp;<span>3 min</span>&nbsp;·&nbsp;<span>624 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 彭罗斯-霍金奇点定理：广义相对论的终极边界" href=https://s-ai-unix.github.io/posts/2026-01-29-penrose-hawking-singularity-theorems/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/minimal-surface-cover.jpg alt=极小曲面></figure><header class=entry-header><h2 class=entry-hint-parent>极小曲面：从肥皂泡到数学之美</h2></header><div class=entry-content><p>引言：肥皂泡的数学秘密 小时候，我们都玩过吹肥皂泡。当肥皂泡漂浮在空中时，它那薄膜表面在阳光下闪烁着彩虹般的光芒。但你有没有想过：*为什么肥皂泡总是球形？
答案藏在数学中。肥皂泡的表面张力使得薄膜尽可能地"收缩"，以达到能量最小的稳定状态。对于封闭的肥皂泡，球形是表面积最小的形状——这就是为什么肥皂泡总是圆的。
但如果我们用金属丝弯成不同的形状，再蘸上肥皂液，会得到什么样的曲面呢？
1776年，意大利数学家拉格朗日（Joseph-Louis Lagrange）首次提出了这个问题：给定空间中的一条闭合曲线，寻找张在这条曲线上且面积最小的曲面。这就是极小曲面问题的起源。
从那个简单的肥皂泡开始，极小曲面理论已经发展成为微分几何中最美丽、最深刻的分支之一。它不仅有着优雅的数学结构，还在建筑学、材料科学、生物学等领域有着广泛的应用。
让我们一起走进这个弯曲而优雅的数学世界。
第一章：什么是极小曲面？ 1.1 直观理解 极小曲面（Minimal Surface）是局部上面积最小的曲面。更准确地说：
一个曲面 $S$ 称为极小曲面，如果它在每一点的平均曲率（mean curvature）都为零。
平均曲率是什么？ 让我们先建立直观理解。
想象你在一个曲面的某一点上。如果你沿着不同的方向切开这个曲面，会得到不同的曲线，每条曲线在该点都有一个曲率。所有这些曲率的平均值（实际上是主曲率的算术平均）就是平均曲率 $H$。
图 1：平均曲率描述了曲面在某一点向各个方向弯曲的程度。椭圆抛物面处处向同一方向弯曲（$H > 0$），而双曲抛物面在不同方向上弯曲方向相反。
对于极小曲面，$H = 0$ 意味着在每个点，曲面向相反方向弯曲的程度恰好抵消。这种"鞍形"结构使得曲面在所有方向上的拉伸达到平衡。
1.2 物理意义：面积最小化 极小曲面的名称来源于其变分性质：
极小曲面是面积泛函的临界点（critical point）。
这是什么意思？想象你在曲面 $S$ 上做一个微小的变形，就像轻轻按压肥皂膜。如果 $S$ 是极小曲面，那么在变形的一阶近似下，面积不变。
图 2：变分原理示意。极小曲面在微小扰动下，面积的一阶变分为零，对应于稳定平衡状态。
1.3 高斯曲率与平均曲率 对于曲面上的每一点，存在两个互相垂直的主方向，沿这两个方向的曲率分别达到最大值 $k_1$ 和最小值 $k_2$。这两个曲率称为主曲率。
高斯曲率：$K = k_1 \cdot k_2$
平均曲率：$H = \frac{k_1 + k_2}{2}$
极小曲面的定义：$H = 0$
这一观察给出了极小曲面的一个重要特征：极小曲面的高斯曲率处处非正（$K \leq 0$）。
第二章：从变分法到极小曲面方程 2.1 Plateau 问题 比利时物理学家约瑟夫·普拉托（Joseph Plateau）在19世纪进行了一系列关于肥皂膜的实验。他发现，将金属丝框架浸入肥皂液后形成的薄膜，总是对应于张在框架上的面积最小的曲面。
...</p></div><footer class=entry-footer><span title='2026-01-29 08:31:25 +0800 CST'>January 29, 2026</span>&nbsp;·&nbsp;<span>1 min</span>&nbsp;·&nbsp;<span>168 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 极小曲面：从肥皂泡到数学之美" href=https://s-ai-unix.github.io/posts/2026-01-29-minimal-surfaces/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/decision-tree-cover.jpg alt=决策树></figure><header class=entry-header><h2 class=entry-hint-parent>决策树及其衍生算法：从ID3到现代梯度提升</h2></header><div class=entry-content><p>引言：从二十个问题到机器学习 想象你在玩一个经典游戏——“二十个问题”。你需要通过最多二十个 yes/no 问题，猜出对手心中想的一个物体。聪明的玩家会问这样的问题：
“它是活的吗？” “如果活着，它是动物吗？” “如果是动物，它会飞吗？” 每一个问题都将可能的答案空间一分为二，逐步缩小范围，直到锁定目标。这种分而治之的策略，正是决策树算法的核心思想。
决策树是机器学习中最直观、最易于解释的算法之一。从医学诊断到信用评估，从游戏 AI 到推荐系统，决策树及其衍生算法无处不在。它的魅力在于：
可解释性强：决策路径清晰，非技术人员也能理解 非参数化：不需要假设数据的分布形式 处理混合数据：能同时处理数值和类别特征 捕捉非线性关系：通过分层划分，自动学习复杂的决策边界 从1986年 Ross Quinlan 提出 ID3 算法，到今天 XGBoost、LightGBM 在 Kaggle 竞赛中称霸，决策树算法已经走过了近四十年的演进历程。本文将带你从最基本的树结构出发，逐步深入到现代梯度提升框架的数学原理，揭示这一算法的优雅与力量。
第一章：决策树基础 1.1 什么是决策树？ 决策树（Decision Tree）是一种树形结构的预测模型，其中：
内部节点表示对某个特征的测试或判断 分支表示测试的结果 叶节点表示最终的预测结果（类别或数值） 图 1：决策树的基本结构。从根节点开始，根据特征值进行判断，沿着分支走到叶节点得到预测结果。
决策树既可以用于分类（预测离散类别），也可以用于回归（预测连续数值）。前者的代表是 ID3、C4.5、CART（分类树），后者的代表是 CART（回归树）。
1.2 决策树的学习过程 构建决策树的核心问题是：*如何选择每个节点的分裂特征和分裂点？
这涉及三个关键决策：
*1. 特征选择准则
我们需要一个指标来度量分裂的"好坏"。常用的准则包括：
信息增益（Information Gain）：基于信息熵的减少 基尼指数（Gini Index）：基于概率分布的纯度 均方误差（MSE）：用于回归问题 *2. 分裂点选择
对于数值特征，需要确定最优的分裂阈值。通常采用贪婪搜索：遍历所有可能的分裂点，选择使准则最优化的那个。
*3. 停止条件
递归分裂何时停止？常见的停止条件：
节点中样本数少于阈值 节点纯度达到阈值 树深度达到上限 分裂带来的增益小于阈值 1.3 决策树的预测过程 预测一个新样本时，从根节点开始：
检查当前节点的分裂特征 根据样本在该特征上的取值，选择对应的分支 移动到子节点 重复直到到达叶节点 叶节点的标签（分类）或平均值（回归）即为预测结果 时间复杂度为 $O(\log n)$，其中 $n$ 是树的高度。这意味着即使对于大规模数据集，预测速度也非常快。
...</p></div><footer class=entry-footer><span title='2026-01-29 08:11:01 +0800 CST'>January 29, 2026</span>&nbsp;·&nbsp;<span>6 min</span>&nbsp;·&nbsp;<span>1197 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 决策树及其衍生算法：从ID3到现代梯度提升" href=https://s-ai-unix.github.io/posts/2026-01-29-decision-trees-and-beyond-from-id3-to-modern-gradient-boosting/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://s-ai-unix.github.io/images/covers/tensor-cover.jpg alt=张量与多维数据></figure><header class=entry-header><h2 class=entry-hint-parent>张量：从数学抽象到深度学习核心的系统综述</h2></header><div class=entry-content><p>引言：多维世界的数学语言 想象你正在观察一个正在旋转的陀螺。描述它需要多少参数？
位置：$3$ 个坐标 $(x, y, z)$ 方向：$3$ 个欧拉角 角速度：$3$ 个分量 转动惯量：$9$ 个数（$3 \times 3$ 矩阵） 这些量不仅仅是数字的集合，它们有特定的变换规则。当坐标系旋转时，位置和角速度按向量规则变换，而转动惯量则按更复杂的规则变换——这就是张量。
在物理学中，张量是描述场的通用语言。爱因斯坦的广义相对论用张量写下：
$$G_{\mu\nu} + \Lambda g_{\mu\nu} = \frac{8\pi G}{c^4} T_{\mu\nu}$$
在深度学习中，一张 $224 \times 224$ 的彩色图像是 $224 \times 224 \times 3$ 的三阶张量。一批 $32$ 张这样的图像是 $32 \times 224 \times 224 \times 3$ 的四阶张量。
本文将带你走进张量的世界，从数学定义到物理直觉，从代数运算到现代应用，理解为什么张量成为描述复杂系统的核心工具。
第一章：张量的本质——超越矩阵的多维数组 1.1 从标量到张量 在数学中，我们熟悉不同维度的对象：
图 1：张量的维度层级。从0阶标量（单个数字）到1阶向量、2阶矩阵，再到3阶及更高阶张量，维度不断增加。
*0阶张量：标量
标量只有一个数值，没有方向：
$$a = 5, \quad T = 300\text{K}, \quad E = mc^2$$
标量在坐标变换下不变——无论你从哪个角度看，温度始终是 $300$K。
...</p></div><footer class=entry-footer><span title='2026-01-29 08:00:00 +0800 CST'>January 29, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>1019 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></footer><a class=entry-link aria-label="post link to 张量：从数学抽象到深度学习核心的系统综述" href=https://s-ai-unix.github.io/posts/2026-01-29-tensor-comprehensive-guide/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://s-ai-unix.github.io/posts/>«&nbsp;Prev&nbsp;
</a><a class=next href=https://s-ai-unix.github.io/posts/page/3/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>