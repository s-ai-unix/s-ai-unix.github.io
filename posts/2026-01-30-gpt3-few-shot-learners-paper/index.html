<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AI 论文解读系列：GPT-3——当语言模型学会举一反三 | s-ai-unix's Blog</title><meta name=keywords content="机器学习,神经网络,自然语言处理,综述"><meta name=description content="深入解读 OpenAI 里程碑式论文 GPT-3: Language Models are Few-Shot Learners，从 Transformer 架构到少样本学习的范式转变，探讨大规模语言模型的涌现能力与未来前景。"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-30-gpt3-few-shot-learners-paper/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-30-gpt3-few-shot-learners-paper/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-30-gpt3-few-shot-learners-paper/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="AI 论文解读系列：GPT-3——当语言模型学会举一反三"><meta property="og:description" content="深入解读 OpenAI 里程碑式论文 GPT-3: Language Models are Few-Shot Learners，从 Transformer 架构到少样本学习的范式转变，探讨大规模语言模型的涌现能力与未来前景。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-30T08:50:00+08:00"><meta property="article:modified_time" content="2026-01-30T08:50:00+08:00"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="神经网络"><meta property="article:tag" content="自然语言处理"><meta property="article:tag" content="综述"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/gpt3-paper-cover.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/gpt3-paper-cover.jpg"><meta name=twitter:title content="AI 论文解读系列：GPT-3——当语言模型学会举一反三"><meta name=twitter:description content="深入解读 OpenAI 里程碑式论文 GPT-3: Language Models are Few-Shot Learners，从 Transformer 架构到少样本学习的范式转变，探讨大规模语言模型的涌现能力与未来前景。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"AI 论文解读系列：GPT-3——当语言模型学会举一反三","item":"https://s-ai-unix.github.io/posts/2026-01-30-gpt3-few-shot-learners-paper/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"AI 论文解读系列：GPT-3——当语言模型学会举一反三","name":"AI 论文解读系列：GPT-3——当语言模型学会举一反三","description":"深入解读 OpenAI 里程碑式论文 GPT-3: Language Models are Few-Shot Learners，从 Transformer 架构到少样本学习的范式转变，探讨大规模语言模型的涌现能力与未来前景。","keywords":["机器学习","神经网络","自然语言处理","综述"],"articleBody":"引言：从海量数据中学习 2020 年 6 月，OpenAI 发表了一篇注定载入人工智能史册的论文：《Language Models are Few-Shot Learners》。这篇论文介绍了 GPT-3——一个拥有 1750 亿参数的巨型语言模型。这个数字意味着什么？如果将 GPT-3 的参数全部打印出来，使用标准字体，这些纸张可以从地球堆到月球——再返回地球好几个来回。\n但 GPT-3 的真正革命性之处不在于它的规模，而在于它展现出的少样本学习能力（Few-Shot Learning）。在此之前，如果我们想让一个 AI 模型完成翻译任务，需要用成千上万对双语句子\"教\"它；而 GPT-3 只需要看几个例子，就能理解任务并给出合理的输出。\n这篇文章将带你走进 GPT-3 的世界，理解它背后的数学原理、技术架构，以及它如何改变了我们对人工智能的认知。\n第一章：从 GPT-1 到 GPT-3 的演进之路 1.1 语言的统计本质 在深入 GPT-3 之前，让我们先思考一个基本问题：什么是语言模型？\n从数学角度看，语言模型试图回答这样一个问题：给定一段已出现的词序列\n$$\\mathbf{x}_{","wordCount":"38","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/gpt3-paper-cover.jpg","datePublished":"2026-01-30T08:50:00+08:00","dateModified":"2026-01-30T08:50:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-30-gpt3-few-shot-learners-paper/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">AI 论文解读系列：GPT-3——当语言模型学会举一反三</h1><div class=post-description>深入解读 OpenAI 里程碑式论文 GPT-3: Language Models are Few-Shot Learners，从 Transformer 架构到少样本学习的范式转变，探讨大规模语言模型的涌现能力与未来前景。</div><div class=post-meta><span title='2026-01-30 08:50:00 +0800 CST'>January 30, 2026</span>&nbsp;·&nbsp;<span>1 min</span>&nbsp;·&nbsp;<span>38 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/gpt3-paper-cover.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/gpt3-paper-cover.jpg alt="GPT-3 论文解读封面"></a><figcaption>Photo by AI generated</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e4%bb%8e%e6%b5%b7%e9%87%8f%e6%95%b0%e6%8d%ae%e4%b8%ad%e5%ad%a6%e4%b9%a0 aria-label=引言：从海量数据中学习>引言：从海量数据中学习</a></li><li><a href=#%e7%ac%ac%e4%b8%80%e7%ab%a0%e4%bb%8e-gpt-1-%e5%88%b0-gpt-3-%e7%9a%84%e6%bc%94%e8%bf%9b%e4%b9%8b%e8%b7%af aria-label="第一章：从 GPT-1 到 GPT-3 的演进之路">第一章：从 GPT-1 到 GPT-3 的演进之路</a><ul><li><a href=#11-%e8%af%ad%e8%a8%80%e7%9a%84%e7%bb%9f%e8%ae%a1%e6%9c%ac%e8%b4%a8 aria-label="1.1 语言的统计本质">1.1 语言的统计本质</a></li><li><a href=#12-%e6%a8%a1%e5%9e%8b%e8%a7%84%e6%a8%a1%e7%9a%84%e6%8c%87%e6%95%b0%e7%ba%a7%e5%a2%9e%e9%95%bf aria-label="1.2 模型规模的指数级增长">1.2 模型规模的指数级增长</a></li><li><a href=#13-%e6%b6%8c%e7%8e%b0%e8%83%bd%e5%8a%9b%e9%87%8f%e5%8f%98%e5%bc%95%e5%8f%91%e8%b4%a8%e5%8f%98 aria-label="1.3 涌现能力：量变引发质变">1.3 涌现能力：量变引发质变</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e7%ab%a0transformer-%e6%9e%b6%e6%9e%84%e7%9a%84%e7%b2%be%e9%ab%93 aria-label="第二章：Transformer 架构的精髓">第二章：Transformer 架构的精髓</a><ul><li><a href=#21-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e6%9c%ac%e8%b4%a8 aria-label="2.1 注意力机制的本质">2.1 注意力机制的本质</a></li><li><a href=#22-%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%b9%b6%e8%a1%8c%e5%85%b3%e6%b3%a8%e4%b8%8d%e5%90%8c%e6%96%b9%e9%9d%a2 aria-label="2.2 多头注意力：并行关注不同方面">2.2 多头注意力：并行关注不同方面</a></li><li><a href=#23-%e5%89%8d%e9%a6%88%e7%bd%91%e7%bb%9c%e4%b8%8e%e5%b1%82%e5%bd%92%e4%b8%80%e5%8c%96 aria-label="2.3 前馈网络与层归一化">2.3 前馈网络与层归一化</a></li><li><a href=#24-gpt-3-%e7%9a%84%e5%ae%8c%e6%95%b4%e6%9e%b6%e6%9e%84 aria-label="2.4 GPT-3 的完整架构">2.4 GPT-3 的完整架构</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e7%ab%a0%e8%ae%ad%e7%bb%83-gpt-3%e5%b7%a5%e7%a8%8b%e4%b8%8e%e8%89%ba%e6%9c%af%e7%9a%84%e7%bb%93%e5%90%88 aria-label="第三章：训练 GPT-3——工程与艺术的结合">第三章：训练 GPT-3——工程与艺术的结合</a><ul><li><a href=#31-%e6%95%b0%e6%8d%ae%e4%ba%92%e8%81%94%e7%bd%91%e7%9a%84%e8%af%ad%e8%a8%80 aria-label="3.1 数据：互联网的语言">3.1 数据：互联网的语言</a></li><li><a href=#32-%e8%ae%ad%e7%bb%83%e7%9b%ae%e6%a0%87%e6%9c%80%e5%a4%a7%e4%bc%bc%e7%84%b6%e4%bc%b0%e8%ae%a1 aria-label="3.2 训练目标：最大似然估计">3.2 训练目标：最大似然估计</a></li><li><a href=#33-%e4%bc%98%e5%8c%96%e7%9a%84%e6%8c%91%e6%88%98 aria-label="3.3 优化的挑战">3.3 优化的挑战</a></li><li><a href=#34-%e5%ad%a6%e4%b9%a0%e7%8e%87%e8%b0%83%e5%ba%a6%e4%b8%8e%e4%bc%98%e5%8c%96%e5%99%a8 aria-label="3.4 学习率调度与优化器">3.4 学习率调度与优化器</a></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e7%ab%a0%e5%b0%91%e6%a0%b7%e6%9c%ac%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86 aria-label=第四章：少样本学习的数学原理>第四章：少样本学习的数学原理</a><ul><li><a href=#41-%e5%85%83%e5%ad%a6%e4%b9%a0%e7%9a%84%e8%a7%86%e8%a7%92 aria-label="4.1 元学习的视角">4.1 元学习的视角</a></li><li><a href=#42-%e4%b8%8a%e4%b8%8b%e6%96%87%e5%ad%a6%e4%b9%a0%e4%bd%9c%e4%b8%ba%e9%9a%90%e5%bc%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d aria-label="4.2 上下文学习作为隐式梯度下降">4.2 上下文学习作为隐式梯度下降</a></li><li><a href=#43-%e4%b8%ba%e4%bb%80%e4%b9%88%e6%a8%a1%e5%9e%8b%e8%b6%8a%e5%a4%a7%e5%ad%a6%e5%be%97%e8%b6%8a%e5%bf%ab aria-label="4.3 为什么模型越大，学得越快？">4.3 为什么模型越大，学得越快？</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e7%ab%a0gpt-3-%e7%9a%84%e8%83%bd%e5%8a%9b%e8%be%b9%e7%95%8c%e4%b8%8e%e5%ba%94%e7%94%a8 aria-label="第五章：GPT-3 的能力边界与应用">第五章：GPT-3 的能力边界与应用</a><ul><li><a href=#51-%e4%bb%a4%e4%ba%ba%e6%83%8a%e8%89%b3%e7%9a%84%e8%a1%a8%e7%8e%b0 aria-label="5.1 令人惊艳的表现">5.1 令人惊艳的表现</a></li><li><a href=#52-%e5%b1%80%e9%99%90%e6%80%a7%e4%b8%8e%e9%a3%8e%e9%99%a9 aria-label="5.2 局限性与风险">5.2 局限性与风险</a></li><li><a href=#53-%e5%ba%94%e7%94%a8%e8%8c%83%e5%bc%8f aria-label="5.3 应用范式">5.3 应用范式</a></li></ul></li><li><a href=#%e7%ac%ac%e5%85%ad%e7%ab%a0%e4%bb%8e-gpt-3-%e5%88%b0-gpt-4%e6%9c%aa%e6%9d%a5%e5%b1%95%e6%9c%9b aria-label="第六章：从 GPT-3 到 GPT-4——未来展望">第六章：从 GPT-3 到 GPT-4——未来展望</a><ul><li><a href=#61-%e8%a7%84%e6%a8%a1%e8%bf%98%e8%83%bd%e7%bb%a7%e7%bb%ad%e6%89%a9%e5%a4%a7%e5%90%97 aria-label="6.1 规模还能继续扩大吗？">6.1 规模还能继续扩大吗？</a></li><li><a href=#62-%e6%96%b0%e6%96%b9%e5%90%91%e7%9a%84%e6%8e%a2%e7%b4%a2 aria-label="6.2 新方向的探索">6.2 新方向的探索</a></li><li><a href=#63-%e5%af%b9-ai-%e7%a0%94%e7%a9%b6%e8%8c%83%e5%bc%8f%e7%9a%84%e6%b7%b1%e8%bf%9c%e5%bd%b1%e5%93%8d aria-label="6.3 对 AI 研究范式的深远影响">6.3 对 AI 研究范式的深远影响</a></li></ul></li><li><a href=#%e7%bb%93%e8%af%adagi-%e7%9a%84%e6%9b%99%e5%85%89%e8%bf%98%e6%98%af-hype aria-label="结语：AGI 的曙光还是 hype？">结语：AGI 的曙光还是 hype？</a></li><li><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae aria-label=参考文献>参考文献</a></li></ul></div></details></div><div class=post-content><h2 id=引言从海量数据中学习>引言：从海量数据中学习<a hidden class=anchor aria-hidden=true href=#引言从海量数据中学习>#</a></h2><p>2020 年 6 月，OpenAI 发表了一篇注定载入人工智能史册的论文：《Language Models are Few-Shot Learners》。这篇论文介绍了 GPT-3——一个拥有 1750 亿参数的巨型语言模型。这个数字意味着什么？如果将 GPT-3 的参数全部打印出来，使用标准字体，这些纸张可以从地球堆到月球——再返回地球好几个来回。</p><p>但 GPT-3 的真正革命性之处不在于它的规模，而在于它展现出的<strong>少样本学习能力（Few-Shot Learning）</strong>。在此之前，如果我们想让一个 AI 模型完成翻译任务，需要用成千上万对双语句子"教"它；而 GPT-3 只需要看几个例子，就能理解任务并给出合理的输出。</p><p>这篇文章将带你走进 GPT-3 的世界，理解它背后的数学原理、技术架构，以及它如何改变了我们对人工智能的认知。</p><h2 id=第一章从-gpt-1-到-gpt-3-的演进之路>第一章：从 GPT-1 到 GPT-3 的演进之路<a hidden class=anchor aria-hidden=true href=#第一章从-gpt-1-到-gpt-3-的演进之路>#</a></h2><h3 id=11-语言的统计本质>1.1 语言的统计本质<a hidden class=anchor aria-hidden=true href=#11-语言的统计本质>#</a></h3><p>在深入 GPT-3 之前，让我们先思考一个基本问题：什么是语言模型？</p><p>从数学角度看，语言模型试图回答这样一个问题：给定一段已出现的词序列</p><div class=math>$$\mathbf{x}_{<i} =(x_1, x_2, \ldots, x_{i-1})$$ </div><p>下一个词 $x_i$ 出现的概率是多少？用条件概率表示：</p><div class=math>$$P(x_i \mid \mathbf{x}_{<i}) =P(x_i \mid x_1, x_2, \ldots, x_{i-1})$$ </div><p>整个句子的联合概率可以分解为：</p><div class=math>$$
P(\mathbf{x}) = \prod_{i=1}^{n} P(x_i \mid x_1, \ldots, x_{i-1})
$$</div><p>这就是**自回归语言模型（Autoregressive Language Model）**的核心思想：从左到右，逐个预测下一个词。</p><h3 id=12-模型规模的指数级增长>1.2 模型规模的指数级增长<a hidden class=anchor aria-hidden=true href=#12-模型规模的指数级增长>#</a></h3><p>GPT 系列的发展史，就是一部模型规模不断突破的历史：</p><p><img alt=模型参数规模对比 loading=lazy src=/images/plots/gpt3-scale-comparison.png></p><p><em>图 1：GPT 系列模型参数规模演变。从 GPT-1 的 1.17 亿参数到 GPT-3 的 1750 亿参数，两年内增长了近 1500 倍。</em></p><table><thead><tr><th style=text-align:center>模型</th><th style=text-align:center>发布时间</th><th style=text-align:center>参数量</th><th style=text-align:center>层数</th><th style=text-align:center>注意力头数</th><th style=text-align:center>训练数据量</th></tr></thead><tbody><tr><td style=text-align:center>GPT-1</td><td style=text-align:center>2018.06</td><td style=text-align:center>1.17 亿</td><td style=text-align:center>12</td><td style=text-align:center>12</td><td style=text-align:center>5GB</td></tr><tr><td style=text-align:center>GPT-2</td><td style=text-align:center>2019.02</td><td style=text-align:center>15 亿</td><td style=text-align:center>48</td><td style=text-align:center>16</td><td style=text-align:center>40GB</td></tr><tr><td style=text-align:center>GPT-3</td><td style=text-align:center>2020.06</td><td style=text-align:center>1750 亿</td><td style=text-align:center>96</td><td style=text-align:center>96</td><td style=text-align:center>570GB</td></tr></tbody></table><p>这种规模的增长并非盲目追求"大"。OpenAI 团队在 2020 年初发表的另一篇论文《Scaling Laws for Neural Language Models》中，通过大量实验发现了一个令人惊讶的规律：<strong>语言模型的性能与计算量、模型参数量和数据量之间存在幂律关系</strong>。</p><p>具体来说，测试损失 $L$ 与计算量 $C$ 满足：</p><div class=math>$$L \propto C^{-\alpha}$$</div><p>其中 $\alpha \approx 0.05$ 是一个经验常数。这意味着，投入 10 倍的计算资源，损失大约只会下降 10%——但这 10% 的下降，往往对应着质的飞跃。</p><p><img alt=训练计算量与性能关系 loading=lazy src=/images/plots/gpt3-scaling-law.png></p><p><em>图 2：训练计算量与测试损失的幂律关系。图中标注了 GPT-3 的训练位置（3640 PF-days）。</em></p><h3 id=13-涌现能力量变引发质变>1.3 涌现能力：量变引发质变<a hidden class=anchor aria-hidden=true href=#13-涌现能力量变引发质变>#</a></h3><p>当我们将模型规模推向极致时，一些意想不到的能力开始"涌现"。GPT-3 展现出了以下关键能力：</p><p><strong>零样本学习（Zero-Shot Learning）</strong>：不给任何例子，仅通过任务描述就能完成任务。例如：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>翻译：将以下英文翻译成中文。
</span></span><span class=line><span class=cl>英文：The quick brown fox jumps over the lazy dog.
</span></span><span class=line><span class=cl>中文：
</span></span></code></pre></div><p><strong>单样本学习（One-Shot Learning）</strong>：给一个例子，模型就能模仿：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>翻译示例：
</span></span><span class=line><span class=cl>英文：Hello world → 中文：你好世界
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>英文：Machine learning is fascinating.
</span></span><span class=line><span class=cl>中文：
</span></span></code></pre></div><p><strong>少样本学习（Few-Shot Learning）</strong>：给 10-100 个例子，模型能达到接近微调（fine-tuning）的效果。</p><p><img alt=少样本学习性能曲线 loading=lazy src=/images/plots/gpt3-fewshot-performance.png></p><p><em>图 3：少样本学习性能随模型规模和示例数量的变化。可以看到，随着模型规模增大，少样本学习能力显著提升。</em></p><h2 id=第二章transformer-架构的精髓>第二章：Transformer 架构的精髓<a hidden class=anchor aria-hidden=true href=#第二章transformer-架构的精髓>#</a></h2><h3 id=21-注意力机制的本质>2.1 注意力机制的本质<a hidden class=anchor aria-hidden=true href=#21-注意力机制的本质>#</a></h3><p>GPT-3 的核心架构是 Transformer 的<strong>解码器（Decoder）<strong>部分。要理解它，我们必须先理解</strong>自注意力机制（Self-Attention）</strong>。</p><p>想象你在阅读一句话：&ldquo;猫坐在垫子上，因为它很软。&ldquo;当你读到"它"时，你会自动地将注意力集中在"垫子"上——这就是注意力机制想要模拟的人类认知过程。</p><p>数学上，自注意力可以表示为三个矩阵的运算。给定输入序列的嵌入表示 $\mathbf{X} \in \mathbb{R}^{n \times d}$，我们计算三个投影：</p><div class=math>$$
\mathbf{Q} = \mathbf{X} \mathbf{W}_Q, \quad \mathbf{K} = \mathbf{X} \mathbf{W}_K, \quad \mathbf{V} = \mathbf{X} \mathbf{W}_V
$$</div><p>其中</p><div class=math>$$\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V \in \mathbb{R}^{d \times d_k}$$</div><p>是可学习的参数矩阵，分别称为<strong>查询（Query）</strong>、**键（Key）<strong>和</strong>值（Value）**的投影矩阵。这三个名字来自数据库查询——Query 是你想找的内容，Key 是索引标签，Value 是实际内容。</p><p>注意力分数的计算使用了缩放点积注意力：</p><div class=math>$$
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_k}}\right) \mathbf{V}
$$</div><p>这里的 $\sqrt{d_k}$ 是缩放因子，防止内积过大导致 softmax 梯度消失。这就好比在图书馆找书——如果索引系统太"敏感&rdquo;，相近的关键词会得分接近，难以区分；缩放因子让得分分布更加均匀，让模型更容易"做决定&rdquo;。</p><h3 id=22-多头注意力并行关注不同方面>2.2 多头注意力：并行关注不同方面<a hidden class=anchor aria-hidden=true href=#22-多头注意力并行关注不同方面>#</a></h3><p>GPT-3 使用了<strong>多头注意力（Multi-Head Attention）</strong>，允许模型在不同的"表示子空间"中同时关注信息。这就好比让 96 个专家同时从不同角度分析同一句话——有的关注语法结构，有的关注语义关联，有的关注指代关系。</p><div class=math>$$
\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) \mathbf{W}^O
$$</div><p>其中每个头独立计算：</p><div class=math>$$
\text{head}_i = \text{Attention}(\mathbf{X} \mathbf{W}_i^Q, \mathbf{X} \mathbf{W}_i^K, \mathbf{X} \mathbf{W}_i^V)
$$</div><p>GPT-3 使用了 $h = 96$ 个注意力头，隐藏维度 $d = 12288$，每个头的维度 $d_k = d / h = 128$。这意味着每一层有 96 个"视角"来理解输入，最终将这些视角融合得到全面的表示。</p><h3 id=23-前馈网络与层归一化>2.3 前馈网络与层归一化<a hidden class=anchor aria-hidden=true href=#23-前馈网络与层归一化>#</a></h3><p>每个 Transformer 层还包含一个前馈神经网络（Feed-Forward Network, FFN）：</p><div class=math>$$\text{FFN}(\mathbf{x}) = \max(0, \mathbf{x} \mathbf{W}_1 + \mathbf{b}_1) \mathbf{W}_2 + \mathbf{b}_2$$</div><p>这是一个两层的全连接网络，中间使用 ReLU 激活函数。在 GPT-3 中，FFN 的中间层维度是 $4d = 49152$。</p><p>层归一化（Layer Normalization）则稳定了训练过程：</p><div class=math>$$\text{LayerNorm}(\mathbf{x}) = \gamma \odot \frac{\mathbf{x} - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$$</div><p>其中 $\mu$ 和 $\sigma^2$ 是输入的均值和方差，$\gamma$ 和 $\beta$ 是可学习的缩放和平移参数。这就像是给每一层的输出做"标准化体检"——确保数值分布保持健康，防止训练过程中数值爆炸或消失。</p><h3 id=24-gpt-3-的完整架构>2.4 GPT-3 的完整架构<a hidden class=anchor aria-hidden=true href=#24-gpt-3-的完整架构>#</a></h3><p>将上述组件组合起来，一个 Transformer 解码器层的计算流程如下：</p><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>flowchart TD
Input[输入嵌入] --> LN1[层归一化]
LN1 --> MHA[多头注意力]
MHA --> Add1[残差连接]
Input --> Add1
Add1 --> LN2[层归一化]
LN2 --> FFN[前馈网络]
FFN --> Add2[残差连接]
Add1 --> Add2
Add2 --> Output[输出]
style Input fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff
style LN1 fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style MHA fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff
style Add1 fill:#8E8E93,stroke:#8E8E93,stroke-width:1px,color:#ffffff
style LN2 fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style FFN fill:#AF52DE,stroke:#AF52DE,stroke-width:2px,color:#ffffff
style Add2 fill:#8E8E93,stroke:#8E8E93,stroke-width:1px,color:#ffffff
style Output fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff</div></div><p><strong>图例说明</strong>：</p><ul><li>🔵 蓝色节点：输入/输出</li><li>🟢 绿色节点：归一化操作</li><li>🟠 橙色节点：注意力机制</li><li>🟣 紫色节点：前馈网络</li><li>⚪ 灰色节点：残差连接</li></ul><p>GPT-3 将这样的层堆叠了 96 次，配合 12288 维的隐藏层和 96 个注意力头，构成了这个庞然大物。</p><p><img alt=模型架构对比 loading=lazy src=/images/plots/gpt3-architecture-comparison.png></p><p><em>图 4：主流 Transformer 模型架构对比。气泡大小代表参数量，横轴为层数，纵轴为隐藏层维度。</em></p><h2 id=第三章训练-gpt-3工程与艺术的结合>第三章：训练 GPT-3——工程与艺术的结合<a hidden class=anchor aria-hidden=true href=#第三章训练-gpt-3工程与艺术的结合>#</a></h2><h3 id=31-数据互联网的语言>3.1 数据：互联网的语言<a hidden class=anchor aria-hidden=true href=#31-数据互联网的语言>#</a></h3><p>GPT-3 的训练数据来源于互联网的各个角落，经过精心筛选和过滤：</p><ul><li><strong>Common Crawl</strong>：原始网页数据，经过质量过滤后占 60%</li><li><strong>WebText2</strong>：Reddit 上获得 3 个以上 karma 的网页链接，占 22%</li><li><strong>Books1 和 Books2</strong>：两个互联网图书语料库，占 16%</li><li><strong>Wikipedia</strong>：英文维基百科，占 3%</li></ul><p>总计约 570GB 的文本数据，相当于约 5000 亿个 token（词片段）。</p><h3 id=32-训练目标最大似然估计>3.2 训练目标：最大似然估计<a hidden class=anchor aria-hidden=true href=#32-训练目标最大似然估计>#</a></h3><p>语言模型的训练目标是最大化训练数据的对数似然：</p><div class=math>$$\mathcal{L}(\theta) = \sum_{i=1}^{N} \log P_\theta(x_i \mid x_1, \ldots, x_{i-1})$$</div><p>等价地，最小化交叉熵损失：</p><div class=math>$$\mathcal{L}_{\text{CE}}(\theta) = -\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{|V|} y_{ij} \log \hat{y}_{ij}$$</div><p>其中 $|V|$ 是词汇表大小（GPT-3 使用约 5 万个 token），$y_{ij}$ 是真实标签的 one-hot 编码，$\hat{y}_{ij}$ 是模型预测的概率分布。</p><h3 id=33-优化的挑战>3.3 优化的挑战<a hidden class=anchor aria-hidden=true href=#33-优化的挑战>#</a></h3><p>训练 1750 亿参数的模型，是工程上的巨大挑战：</p><p><strong>模型并行（Model Parallelism）</strong>：单个 GPU 无法容纳整个模型，需要将模型切分到多个设备上。就像一本厚书分成几卷，由不同的人同时阅读不同部分。</p><p><strong>数据并行（Data Parallelism）</strong>：在多个 GPU 上同时处理不同批次的数据。这就像多个学生同时做不同的练习题，然后共享答案。</p><p><strong>梯度累积（Gradient Accumulation）</strong>：在小 batch size 上累积梯度，模拟大 batch 的效果。好比攒够一周的垃圾再一起扔，而不是每次扔一点。</p><p>GPT-3 在 V100 GPU 集群上训练，使用了多种并行策略的组合。整个训练过程消耗了约 3640 PF-days（PetaFLOP-days）的计算量——如果在一个 V100 GPU 上训练，需要 355 年！</p><h3 id=34-学习率调度与优化器>3.4 学习率调度与优化器<a hidden class=anchor aria-hidden=true href=#34-学习率调度与优化器>#</a></h3><p>GPT-3 使用了 Adam 优化器的变体，配合 warm-up 和余弦退火的学习率调度：</p><div class=math>$$\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{t}{T}\pi\right)\right)$$</div><p>其中 $T$ 是总训练步数，$t$ 是当前步数。这种调度方式让学习率平滑下降，有助于收敛到更好的局部最优。你可以把它想象成开车——起步时加速（warm-up），然后平稳行驶，最后缓慢刹车直到停止。这种"温柔"的减速比突然刹车更容易找到好的"停车位置"（局部最优解）。</p><h2 id=第四章少样本学习的数学原理>第四章：少样本学习的数学原理<a hidden class=anchor aria-hidden=true href=#第四章少样本学习的数学原理>#</a></h2><h3 id=41-元学习的视角>4.1 元学习的视角<a hidden class=anchor aria-hidden=true href=#41-元学习的视角>#</a></h3><p>少样本学习可以看作是一种<strong>元学习（Meta-Learning）</strong>，或者说"学习如何学习"。传统的监督学习可以形式化为：</p><div class=math>$$\theta^{\ast} = \arg\min_\theta \mathbb{E}_{(x, y) \sim \mathcal{D}}[\mathcal{L}(f_\theta(x), y)]$$</div><p>而元学习则学习一个先验，使得在面对新任务时只需要少量样本就能快速适应：</p><div class=math>$$\phi^{\ast} = \arg\min_\phi \mathbb{E}_{\mathcal{T} \sim p(\mathcal{T})}[\mathbb{E}_{(x, y) \sim \mathcal{D}_\mathcal{T}}[\mathcal{L}(f_{\theta(\phi, \mathcal{D}_\mathcal{T})}(x), y)]]$$</div><p>GPT-3 的独特之处在于，它没有显式的元学习过程，而是通过海量预训练，将"快速学习的能力"内化到了模型参数中。</p><h3 id=42-上下文学习作为隐式梯度下降>4.2 上下文学习作为隐式梯度下降<a hidden class=anchor aria-hidden=true href=#42-上下文学习作为隐式梯度下降>#</a></h3><p>斯坦福的研究者发现，上下文学习（In-Context Learning）可以看作是一种隐式的梯度下降。考虑一个线性回归的例子：</p><p>给定几个输入-输出对</p><div class=math>$$(x_1, y_1), (x_2, y_2), \ldots, (x_k, y_k)$$</div><p>模型需要预测新输入 $x_{\text{query}}$ 的输出。</p><p>标准的梯度下降更新为：</p><div class=math>$$\mathbf{w}_{t+1} = \mathbf{w}_t - \eta \nabla_{\mathbf{w}} \mathcal{L}(\mathbf{w}_t; \mathbf{X}, \mathbf{y})$$</div><p>而在上下文学习中，Transformer 的注意力机制实际上在执行类似的操作：</p><div class=math>$$\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \mathbf{V} \cdot \text{softmax}\left(\frac{\mathbf{K}^T \mathbf{Q}}{\sqrt{d_k}}\right)$$</div><p>这个计算形式与梯度下降更新有着惊人的相似性——注意力权重可以被理解为"学习率"，而值向量则包含了参数的更新信息。这就像你在做作业时，参考之前的例题——注意力机制决定了你从每个例题中"学多少"，而值向量就是例题的答案。通过这种方式，模型可以在不更新参数的情况下，从上下文中的例子"学习"如何解决问题。</p><p><img alt=上下文学习曲线 loading=lazy src=/images/plots/gpt3-context-learning.png></p><p><em>图 5：上下文长度对模型性能的影响。更长的上下文允许模型看到更多示例，从而提升少样本学习效果。</em></p><h3 id=43-为什么模型越大学得越快>4.3 为什么模型越大，学得越快？<a hidden class=anchor aria-hidden=true href=#43-为什么模型越大学得越快>#</a></h3><p>这一现象与<strong>双 descent 现象</strong>和<strong>隐式正则化</strong>有关。大模型拥有足够的容量来存储大量"原型"任务，同时又通过过参数化获得了良好的泛化能力。</p><p>从信息论角度看，模型的描述长度（Description Length）与训练数据复杂度之间存在权衡：</p><div class=math>$$L(\mathbf{y} \mid \mathbf{x}) \approx -\log P_\theta(\mathbf{y} \mid \mathbf{x}) + \|\theta\|_0$$</div><p>大模型虽然参数量大，但如果训练数据足够多样，它学到的表示可以高度压缩，从而实现良好的泛化。</p><h2 id=第五章gpt-3-的能力边界与应用>第五章：GPT-3 的能力边界与应用<a hidden class=anchor aria-hidden=true href=#第五章gpt-3-的能力边界与应用>#</a></h2><h3 id=51-令人惊艳的表现>5.1 令人惊艳的表现<a hidden class=anchor aria-hidden=true href=#51-令人惊艳的表现>#</a></h3><p>GPT-3 在多个基准测试上刷新了记录：</p><p><strong>语言理解与生成</strong>：</p><ul><li>LAMBADA（完形填空）：86.4% 准确率，超越人类水平的 86.0%</li><li>StoryCloze（故事推理）：87.7% 准确率</li></ul><p><strong>问答与知识</strong>：</p><ul><li>TriviaQA：71.2% 准确率（少样本设置）</li><li>Natural Questions：29.9% 准确率</li></ul><p><strong>翻译</strong>：</p><ul><li>WMT14 英法翻译：39.5 BLEU</li><li>WMT14 法英翻译：40.2 BLEU</li></ul><p><strong>代码生成</strong>：
GPT-3 展现出了惊人的代码理解和生成能力，尽管它的训练数据中代码占比很小。例如，给它一个函数签名和注释，它就能生成完整的实现代码。这启发了后来的 Codex 和 GitHub Copilot。</p><h3 id=52-局限性与风险>5.2 局限性与风险<a hidden class=anchor aria-hidden=true href=#52-局限性与风险>#</a></h3><p>然而，GPT-3 并非万能：</p><p><strong>常识推理的局限</strong>：
尽管 GPT-3 能生成流畅的文本，但在需要深层常识推理的任务上，它仍然表现不佳。例如：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>问：我把一个杯子放在桌子上，然后我把桌子翻过来，杯子在哪里？
</span></span><span class=line><span class=cl>GPT-3 可能会回答：杯子还在桌子上。
</span></span></code></pre></div><p><strong>事实准确性</strong>：
GPT-3 会"一本正经地胡说八道"，生成看似合理但实际上错误的陈述。例如，问它"法国总统是谁"，它可能正确回答"马克龙"；但问"2024年法国总统是谁"，它可能会编造一个名字。这是因为它的目标是生成"合理的文本"，而非"真实的文本"——它更像一个"创意作家"而非"百科全书"。</p><p><strong>偏见与有害内容</strong>：
训练数据中的偏见会被模型学习并放大。研究发现，GPT-3 在涉及性别、种族、宗教等话题时，会生成带有刻板印象的内容。</p><h3 id=53-应用范式>5.3 应用范式<a hidden class=anchor aria-hidden=true href=#53-应用范式>#</a></h3><p>GPT-3 催生了一种新的 AI 应用范式：<strong>提示工程（Prompt Engineering）</strong>。开发者不再需要大量标注数据和模型微调，而是通过精心设计的提示来引导模型的行为。</p><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>flowchart TD
subgraph 传统范式 [传统 ML 范式]
A1[收集数据] --> A2[标注数据]
A2 --> A3[训练模型]
A3 --> A4[微调模型]
A4 --> A5[部署推理]
end
subgraph GPT3 范式 [GPT-3 范式]
B1[设计提示模板] --> B2[提供少量示例]
B2 --> B3[模型推理]
end
A5 --> Result[应用]
B3 --> Result
style A1 fill:#007AFF,stroke:#007AFF,stroke-width:2px,color:#ffffff
style A2 fill:#007AFF,stroke:#007AFF,stroke-width:2px,color:#ffffff
style A3 fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style A4 fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style A5 fill:#AF52DE,stroke:#AF52DE,stroke-width:2px,color:#ffffff
style B1 fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff
style B2 fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff
style B3 fill:#AF52DE,stroke:#AF52DE,stroke-width:2px,color:#ffffff
style Result fill:#34C759,stroke:#34C759,stroke-width:3px,color:#ffffff</div></div><p><strong>图例说明</strong>：</p><ul><li>🔵 蓝色节点：数据准备</li><li>🟢 绿色节点：训练过程</li><li>🟠 橙色节点：提示设计</li><li>🟣 紫色节点：推理部署</li></ul><h2 id=第六章从-gpt-3-到-gpt-4未来展望>第六章：从 GPT-3 到 GPT-4——未来展望<a hidden class=anchor aria-hidden=true href=#第六章从-gpt-3-到-gpt-4未来展望>#</a></h2><h3 id=61-规模还能继续扩大吗>6.1 规模还能继续扩大吗？<a hidden class=anchor aria-hidden=true href=#61-规模还能继续扩大吗>#</a></h3><p>Kaplan 等人的研究暗示，如果我们继续增加模型规模、计算量和数据量，性能还会继续提升。但这一路线面临多重挑战：</p><p><strong>计算成本</strong>：GPT-3 的训练成本估计在数百万美元级别。继续扩大规模意味着指数级增长的投入。</p><p><strong>数据瓶颈</strong>：互联网上的高质量文本数据是有限的。有研究估计，到 2026 年，大语言模型可能会"用尽"所有公开可用的文本数据。</p><p><strong>能耗与环境</strong>：训练巨型模型的碳排放不容忽视。GPT-3 训练产生的碳足迹相当于多辆汽车一生的排放量。</p><h3 id=62-新方向的探索>6.2 新方向的探索<a hidden class=anchor aria-hidden=true href=#62-新方向的探索>#</a></h3><p>研究者们正在探索突破规模限制的新路径：</p><p><strong>混合专家模型（Mixture of Experts, MoE）</strong>：
与其使用一个巨大的稠密模型，不如使用多个小型专家模型，通过一个门控网络动态选择激活哪些专家。这就像医院里有专科医生——心脏病、眼科、骨科各有专家，病人来了先挂号（门控），然后只看相关的专家。</p><div class=math>$$f(\mathbf{x}) = \sum_{i=1}^{N} g_i(\mathbf{x}) \cdot E_i(\mathbf{x})$$</div><p>其中 $g_i$ 是门控函数，$E_i$ 是第 $i$ 个专家网络。这样可以在大幅减少计算量的同时保持甚至提升模型容量。Google 的 Switch Transformer 就采用了这种架构，用 1.6 万亿参数打败了 GPT-3，但计算量只相当于其 1/4。</p><p><strong>检索增强生成（Retrieval-Augmented Generation, RAG）</strong>：
将语言模型与外部知识库结合，在生成时动态检索相关信息，提高事实准确性。这就像考试时允许查阅资料——虽然你理解概念（预训练），但可以查具体事实（检索）。Facebook 的 RAG 模型和 OpenAI 后来在 ChatGPT 中加入的联网功能都是这种思路的体现。</p><p><strong>多模态融合</strong>：
将文本模型与视觉、音频模型结合，构建真正理解世界的多模态智能。GPT-4 就是一个例子——它能理解图片、解读图表、分析梗图，这比纯文本模型更接近人类的多感官认知。</p><h3 id=63-对-ai-研究范式的深远影响>6.3 对 AI 研究范式的深远影响<a hidden class=anchor aria-hidden=true href=#63-对-ai-研究范式的深远影响>#</a></h3><p>GPT-3 的成功标志着 AI 研究范式的几个重要转变：</p><ol><li><strong>从任务特定模型到通用模型</strong>：不再为每个任务训练单独模型</li><li><strong>从监督学习到自监督学习</strong>：利用海量未标注数据</li><li><strong>从模型设计到提示设计</strong>：人类通过自然语言与模型交互</li><li><strong>从学术界主导到工业界主导</strong>：大模型需要巨大的计算资源</li></ol><h2 id=结语agi-的曙光还是-hype>结语：AGI 的曙光还是 hype？<a hidden class=anchor aria-hidden=true href=#结语agi-的曙光还是-hype>#</a></h2><p>GPT-3 的发布引发了关于通用人工智能（AGI）的激烈讨论。它是通往 AGI 的重要里程碑，还是又一个被过度炒作的技术？</p><p>从乐观的角度看，GPT-3 证明了：通过简单的自监督目标（预测下一个词）和足够的规模，模型可以涌现出令人惊讶的能力。这暗示了智能可能比我们想象的更简单、更统一。</p><p>从谨慎的角度看，GPT-3 缺乏真正的理解、意识和意图。它的"智能"只是统计规律的体现，而非对世界的深层认知。</p><p>无论如何，GPT-3 已经改变了自然语言处理的格局。它启发了 ChatGPT、GPT-4 等后续模型，推动了整个 AI 领域的发展。正如论文标题所言，语言模型确实是"少样本学习者"——而这一发现，可能只是 AI 能力冰山的一角。</p><p>当我们回望这段历史，2020 年的那个夏天，或许正是人工智能新时代的开端。</p><hr><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><ol><li><p>Brown, T. B., Mann, B., Ryder, N., et al. (2020). Language Models are Few-Shot Learners. <em>NeurIPS 2020</em>.</p></li><li><p>Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). Attention is All You Need. <em>NeurIPS 2017</em>.</p></li><li><p>Kaplan, J., McCandlish, S., Henighan, T., et al. (2020). Scaling Laws for Neural Language Models. <em>arXiv:2001.08361</em>.</p></li><li><p>Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving Language Understanding by Generative Pre-Training.</p></li><li><p>Radford, A., Wu, J., Child, R., et al. (2019). Language Models are Unsupervised Multitask Learners.</p></li><li><p>Wei, J., Wang, X., Schuurmans, D., et al. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. <em>NeurIPS 2022</em>.</p></li><li><p>Xie, S. M., Raghunathan, A., Liang, P., & Ma, T. (2022). An Explanation of In-context Learning as Implicit Bayesian Inference. <em>ICLR 2022</em>.</p></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a></li><li><a href=https://s-ai-unix.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/>自然语言处理</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%BB%BC%E8%BF%B0/>综述</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-30-word2vec-paper-explained/><span class=title>« Prev</span><br><span>AI 论文解读系列：Word2Vec - 词向量的革命</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-30-ai-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97vision-transformer-%E8%A7%86%E8%A7%89transformer/><span class=title>Next »</span><br><span>AI 论文解读系列：Vision Transformer 视觉Transformer</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：GPT-3——当语言模型学会举一反三 on x" href="https://x.com/intent/tweet/?text=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aGPT-3%e2%80%94%e2%80%94%e5%bd%93%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%ad%a6%e4%bc%9a%e4%b8%be%e4%b8%80%e5%8f%8d%e4%b8%89&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-gpt3-few-shot-learners-paper%2f&amp;hashtags=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%2c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%2c%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%e5%a4%84%e7%90%86%2c%e7%bb%bc%e8%bf%b0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：GPT-3——当语言模型学会举一反三 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-gpt3-few-shot-learners-paper%2f&amp;title=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aGPT-3%e2%80%94%e2%80%94%e5%bd%93%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%ad%a6%e4%bc%9a%e4%b8%be%e4%b8%80%e5%8f%8d%e4%b8%89&amp;summary=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aGPT-3%e2%80%94%e2%80%94%e5%bd%93%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%ad%a6%e4%bc%9a%e4%b8%be%e4%b8%80%e5%8f%8d%e4%b8%89&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-gpt3-few-shot-learners-paper%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：GPT-3——当语言模型学会举一反三 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-gpt3-few-shot-learners-paper%2f&title=AI%20%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb%e7%b3%bb%e5%88%97%ef%bc%9aGPT-3%e2%80%94%e2%80%94%e5%bd%93%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%ad%a6%e4%bc%9a%e4%b8%be%e4%b8%80%e5%8f%8d%e4%b8%89"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI 论文解读系列：GPT-3——当语言模型学会举一反三 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-30-gpt3-few-shot-learners-paper%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>