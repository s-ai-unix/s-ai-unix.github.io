<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>微积分与机器学习：从变化率到神经网络梯度的完整旅程 | s-ai-unix's Blog</title><meta name=keywords content="机器学习,深度学习,微积分,数学,算法"><meta name=description content="深入理解微积分如何驱动现代人工智能：从导数的几何直观到梯度下降的数学原理，从链式法则到反向传播算法，揭示神经网络训练的数学本质。"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-25-calculus-ml-systematic-review/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-25-calculus-ml-systematic-review/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-25-calculus-ml-systematic-review/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="微积分与机器学习：从变化率到神经网络梯度的完整旅程"><meta property="og:description" content="深入理解微积分如何驱动现代人工智能：从导数的几何直观到梯度下降的数学原理，从链式法则到反向传播算法，揭示神经网络训练的数学本质。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-25T19:00:00+08:00"><meta property="article:modified_time" content="2026-01-25T19:00:00+08:00"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="微积分"><meta property="article:tag" content="数学"><meta property="article:tag" content="算法"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/calculus-ml-journey.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/calculus-ml-journey.jpg"><meta name=twitter:title content="微积分与机器学习：从变化率到神经网络梯度的完整旅程"><meta name=twitter:description content="深入理解微积分如何驱动现代人工智能：从导数的几何直观到梯度下降的数学原理，从链式法则到反向传播算法，揭示神经网络训练的数学本质。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"微积分与机器学习：从变化率到神经网络梯度的完整旅程","item":"https://s-ai-unix.github.io/posts/2026-01-25-calculus-ml-systematic-review/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"微积分与机器学习：从变化率到神经网络梯度的完整旅程","name":"微积分与机器学习：从变化率到神经网络梯度的完整旅程","description":"深入理解微积分如何驱动现代人工智能：从导数的几何直观到梯度下降的数学原理，从链式法则到反向传播算法，揭示神经网络训练的数学本质。","keywords":["机器学习","深度学习","微积分","数学","算法"],"articleBody":"引言：为什么需要微积分？ 想象你在山上，想找到最低点。你会怎么做？你会观察脚下的坡度，选择最陡峭的方向迈出一步，然后重复这个过程。这个简单的直觉——沿着负梯度方向走——正是现代人工智能的核心算法。\n从ChatGPT的语言模型到AlphaGo的围棋策略，从图像识别到语音合成，所有这些技术背后都有一个共同的数学基础：微积分。\n微积分研究的是变化。而机器学习本质上是关于优化——通过不断调整参数来减少错误。当我们在高维空间中优化复杂的神经网络时，微积分提供了描述和计算这种变化的精确语言。\n这篇文章将带你深入理解微积分如何驱动现代人工智能。我们不会停留在表面，而是会深入到数学推导的核心，揭示梯度下降、反向传播等算法的数学本质。这是一次从17世纪牛顿和莱布尼茨的发明，到21世纪深度学习革命的完整旅程。\n第一部分：微积分基础理论 1. 导数的本质：从变化率到瞬时变化率 1.1 变化率的直观理解 变化率是人类最早思考的数学问题之一。如果一辆车2小时行驶100公里，平均速度是50公里/小时。但它某一时刻的瞬时速度是多少？\n微积分的答案是：用极限。考虑函数 $f(x)$ 在 $x_0$ 附近的平均变化率： $$ \\frac{f(x_0 + \\Delta x) - f(x_0)}{\\Delta x} $$\n当 $\\Delta x \\to 0$ 时，这个平均变化率的极限就是导数： $$ f^{\\prime}(x_0) = \\lim_{\\Delta x \\to 0} \\frac{f(x_0 + \\Delta x) - f(x_0)}{\\Delta x} $$\n1.2 导数的几何意义 几何直观：导数是切线的斜率。在 $x_0$ 处，曲线 $f(x)$ 可以用直线（切线）逼近： $$ f(x) \\approx f(x_0) + f^{\\prime}(x_0)(x - x_0) $$\n这就是一阶泰勒公式，也是线性化的思想：局部用简单的线性函数逼近复杂的非线性函数。\n严格定义（$\\epsilon-\\delta$ 语言）： $$ \\forall \\epsilon \u003e 0, \\exists \\delta \u003e 0 \\text{ s.t. } |\\Delta x| \u003c \\delta \\implies \\left|\\frac{f(x_0 + \\Delta x) - f(x_0)}{\\Delta x} - f^{\\prime}(x_0)\\right| \u003c \\epsilon $$\n1.3 导数的计算规则 基本法则：\n线性性：$(af + bg)^{\\prime} = af^{\\prime} + bg^{\\prime}$ 乘积法则：$(fg)^{\\prime} = f^{\\prime}g + fg^{\\prime}$ 商法则：$\\left(\\frac{f}{g}\\right)^{\\prime} = \\frac{f^{\\prime}g - fg^{\\prime}}{g^2}$ 链式法则（复合函数）： $$ \\frac{d}{dx}f(g(x)) = f^{\\prime}(g(x)) \\cdot g^{\\prime}(x) $$\n这个看似简单的公式是反向传播算法的数学基础！\n2. 微分与线性化 2.1 微分的几何意义 微分 $dy = f^{\\prime}(x)dx$ 是函数变化的线性近似。在几何上，它是切线纵坐标的变化量。\n关键思想：对于微小的 $\\Delta x$： $$ \\Delta f = f(x + \\Delta x) - f(x) \\approx f^{\\prime}(x)\\Delta x $$\n近似误差是 $\\Delta x$ 的高阶无穷小： $$ \\lim_{\\Delta x \\to 0} \\frac{\\Delta f - f^{\\prime}(x)\\Delta x}{\\Delta x} = 0 $$\n2.2 多元函数的微分 对于多元函数 $f(\\mathbf{x})$，其中 $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)^\\top$，我们需要描述它在所有方向上的变化率。\n偏导数是沿坐标轴方向的变化率： $$ \\frac{\\partial f}{\\partial x_i} = \\lim_{\\Delta x_i \\to 0} \\frac{f(\\mathbf{x} + \\Delta x_i \\mathbf{e}_i) - f(\\mathbf{x})}{\\Delta x_i} $$\n全微分： $$ df = \\sum_{i=1}^n \\frac{\\partial f}{\\partial x_i} dx_i = \\nabla f^\\top d\\mathbf{x} $$\n2.3 梯度：多维的导数 梯度将所有偏导数组合成向量： $$ \\nabla f(\\mathbf{x}) = \\begin{pmatrix} \\dfrac{\\partial f}{\\partial x_1} \\ \\vdots \\ \\dfrac{\\partial f}{\\partial x_n} \\end{pmatrix} $$\n关键性质：梯度指向函数增长最快的方向。因此，负梯度方向是最速下降方向。\n证明（方向导数）： $$ \\frac{\\partial f}{\\partial \\mathbf{u}} = \\nabla f^\\top \\mathbf{u} = \\lVert \\nabla f \\rVert \\lVert \\mathbf{u} \\rVert \\cos\\theta $$\n当 $\\theta = 0$ 时（$\\mathbf{u}$ 与 $\\nabla f$ 同向），方向导数最大。因此 $\\nabla f$ 指向最速上升方向。\n2.4 雅可比矩阵与链式法则 对于向量值函数 $\\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^m$，雅可比矩阵是所有偏导数组成的矩阵： $$ J_{\\mathbf{f}}(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} \u0026 \\cdots \u0026 \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026 \\ddots \u0026 \\vdots \\ \\frac{\\partial f_m}{\\partial x_1} \u0026 \\cdots \u0026 \\frac{\\partial f_m}{\\partial x_n} \\end{pmatrix} $$\n多元链式法则（矩阵形式）： $$ \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} = \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{y}} \\cdot \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} $$\n其中 $\\mathbf{z} = \\mathbf{f}(\\mathbf{y})$ 且 $\\mathbf{y} = \\mathbf{g}(\\mathbf{x})$。\n3. 积分与累积 3.1 从求和到黎曼积分 积分是微分的逆运算，也是累积的工具。从黎曼和开始： $$ \\int_a^b f(x)dx = \\lim_{n \\to \\infty} \\sum_{i=1}^n f(x_i^*)\\Delta x_i $$\n其中 $\\Delta x_i = x_i - x_{i-1}$，$x_i^* \\in [x_{i-1}, x_i]$。\n3.2 微积分基本定理 牛顿-莱布尼茨公式： $$ \\int_a^b f(x)dx = F(b) - F(a) $$\n其中 $F^{\\prime}(x) = f(x)$。这个公式连接了微分和积分，是微积分的核心定理。\n3.3 多重积分与变量替换 二重积分： $$ \\iint_D f(x,y)dxdy = \\int_{y_1}^{y_2} \\int_{x_1(y)}^{x_2(y)} f(x,y)dxdy $$\n变量替换公式（雅可比行列式）： $$ \\iint_{D^*} f(x,y)dxdy = \\iint_D f(x(u,v), y(u,v)) \\left|\\frac{\\partial(x,y)}{\\partial(u,v)}\\right| dudv $$\n其中雅可比行列式： $$ \\frac{\\partial(x,y)}{\\partial(u,v)} = \\det \\begin{pmatrix} \\frac{\\partial x}{\\partial u} \u0026 \\frac{\\partial x}{\\partial v} \\ \\frac{\\partial y}{\\partial u} \u0026 \\frac{\\partial y}{\\partial v} \\end{pmatrix} $$\n4. 级数与逼近 4.1 泰勒展开：用多项式逼近函数 泰勒公式是线性化的自然推广。在 $x_0$ 附近，$f(x)$ 可以用多项式逼近： $$ f(x) = f(x_0) + f^{\\prime}(x_0)(x - x_0) + \\frac{f^{\\prime\\prime}(x_0)}{2!}(x - x_0)^2 + \\cdots + \\frac{f^{(n)}(x_0)}{n!}(x - x_0)^n + R_n(x) $$\n余项形式（拉格朗日型）： $$ R_n(x) = \\frac{f^{(n+1)}(\\xi)}{(n+1)!}(x - x_0)^{n+1} $$\n其中 $\\xi$ 在 $x_0$ 和 $x$ 之间。\n泰勒级数（当 $n \\to \\infty$）： $$ f(x) = \\sum_{n=0}^{\\infty} \\frac{f^{(n)}(x_0)}{n!}(x - x_0)^n $$\n图1：泰勒级数用多项式逼近函数 $e^x$。阶数越高，逼近范围越广，误差越小。\n4.2 多元泰勒展开 对于多元函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$，在 $\\mathbf{x}_0$ 附近： $$ f(\\mathbf{x}_0 + \\Delta \\mathbf{x}) = f(\\mathbf{x}_0) + \\nabla f(\\mathbf{x}_0)^\\top \\Delta \\mathbf{x} + \\frac{1}{2}\\Delta \\mathbf{x}^\\top H(\\mathbf{x}_0) \\Delta \\mathbf{x} + \\mathcal{O}(\\lVert \\Delta \\mathbf{x} \\rVert^3) $$\n其中 $H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$ 是Hessian矩阵（二阶导数矩阵）。\n4.3 在优化中的应用 一阶条件（必要条件）： $$ \\nabla f(\\mathbf{x}^*) = \\mathbf{0} $$\n二阶条件（充分条件，对于凸函数）： $$ H(\\mathbf{x}^*) \\succ 0 \\quad (\\text{正定}) $$\n泰勒展开是理解优化算法（如牛顿法）的数学基础。\n第二部分：机器学习中的微积分 1. 梯度下降法 1.1 算法推导 考虑无约束优化问题： $$ \\min_{\\mathbf{x} \\in \\mathbb{R}^n} f(\\mathbf{x}) $$\n核心思想：在当前点 $\\mathbf{x}_k$，计算梯度 $\\nabla f(\\mathbf{x}k)$，然后沿负梯度方向移动： $$ \\mathbf{x}{k+1} = \\mathbf{x}_k - \\eta \\nabla f(\\mathbf{x}_k) $$\n其中 $\\eta$ 是学习率(learning rate)，控制步长大小。\n1.2 为什么这样走是正确的？ 泰勒展开证明：\n在 $\\mathbf{x}_k$ 附近对 $f$ 做一阶近似： $$ f(\\mathbf{x}_k + \\Delta \\mathbf{x}) \\approx f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^\\top \\Delta \\mathbf{x} $$\n我们要找到 $\\Delta \\mathbf{x}$ 使 $f$ 减小最多。设步长固定：$\\lVert \\Delta \\mathbf{x} \\rVert = \\epsilon$。\n由柯西-施瓦茨不等式： $$ \\nabla f^\\top \\Delta \\mathbf{x} \\geq -\\lVert \\nabla f \\rVert \\cdot \\lVert \\Delta \\mathbf{x} \\rVert = -\\epsilon \\lVert \\nabla f \\rVert $$\n当且仅当 $\\Delta \\mathbf{x}$ 与 $-\\nabla f$ 同向时取等号。因此，负梯度方向是最速下降方向。\n图2：梯度下降在等高线上的优化轨迹。红色箭头显示每次迭代沿负梯度方向移动，最终收敛到最小值（中心点）。\n图3：损失函数曲面上的梯度方向。红色向量显示负梯度方向，指向函数值下降最快的方向。\n1.3 收敛性分析（强凸情况） 假设 $f$ 是 $\\mu$-强凸的，且梯度是 $L$-Lipschitz连续的： $$ \\mu I \\preceq H(\\mathbf{x}) \\preceq LI $$\n收敛率： $$ f(\\mathbf{x}^{(t)}) - f(\\mathbf{x}^) \\leq \\left(1 - \\frac{\\mu}{L}\\right)^{t} [f(\\mathbf{x}^{(0)}) - f(\\mathbf{x}^)] $$\n这是线性收敛（几何收敛）。\n1.4 学习率的选择 学习率 $\\eta$ 太小：收敛慢，需要很多步 学习率 $\\eta$ 太大：可能\"冲过\"最优点，甚至发散\n经验法则：对于 $L$-光滑函数，选择 $\\eta \u003c \\frac{2}{L}$。\n1.5 动量方法 问题：梯度下降在\"峡谷\"形状的损失函数上振荡。\n解决方案：加入动量项，利用历史梯度信息： $$ \\mathbf{v}t = \\beta \\mathbf{v}{t-1} + (1 - \\beta)\\nabla L(\\mathbf{w}t) $$ $$ \\mathbf{w}{t+1} = \\mathbf{w}_t - \\eta \\mathbf{v}_t $$\n动量相当于\"惯性\"，可以帮助算法穿越平坦区域，减少振荡。\n1.6 自适应学习率 AdaGrad：为每个参数使用不同的学习率： $$ \\mathbf{w}{t+1, i} = \\mathbf{w}{t, i} - \\frac{\\eta}{\\sqrt{G_{t, ii} + \\epsilon}} \\nabla L(\\mathbf{w}_t)_i $$\n其中 $G_{t, ii} = \\sum_{j=1}^t (\\nabla L(\\mathbf{w}_j)_i)^2$ 是历史梯度的平方和。\nRMSProp：使用移动平均代替累积求和： $$ G_{t, ii} = \\beta G_{t-1, ii} + (1 - \\beta)(\\nabla L(\\mathbf{w}_t)_i)^2 $$\n图4：不同学习率对梯度下降收敛的影响。绿色线（学习率过小）收敛慢；蓝色线（学习率适中）快速收敛；红色线（学习率过大）振荡；紫色线（动量方法）加速收敛。\n2. 拉格朗日乘数法 2.1 约束优化问题 考虑等式约束优化： $$ \\min f(\\mathbf{x}) \\quad \\text{s.t.} \\quad g(\\mathbf{x}) = 0 $$\n拉格朗日函数： $$ \\mathcal{L}(\\mathbf{x}, \\lambda) = f(\\mathbf{x}) + \\lambda g(\\mathbf{x}) $$\nKKT条件（Karush-Kuhn-Tucker）： $$ \\nabla_{\\mathbf{x}} \\mathcal{L} = \\mathbf{0}, \\quad g(\\mathbf{x}) = 0 $$\n2.2 几何解释 在最优解处，$\\nabla f$ 必须与 $\\nabla g$ 平行（否则可以沿约束曲面移动以降低 $f$）： $$ \\nabla f = -\\lambda \\nabla g $$\n2.3 在SVM中的应用 硬间隔SVM： $$ \\min_{\\mathbf{w}, b} \\frac{1}{2}\\lVert \\mathbf{w} \\rVert^2 \\quad \\text{s.t.} \\quad y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1 $$\n对偶问题： $$ \\max_{\\alpha_i} \\sum_i \\alpha_i - \\frac{1}{2}\\sum_{i,j} \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^\\top \\mathbf{x}_j $$\n3. 信息论中的微积分 3.1 熵的定义与微分 信息熵： $$ H(p) = -\\sum_{i=1}^n p_i \\log p_i $$\n微分熵（连续变量）： $$ h(p) = -\\int p(x)\\log p(x)dx $$\n3.2 交叉熵与KL散度 交叉熵： $$ H(p, q) = -\\sum_i p_i \\log q_i $$\nKL散度（Kullback-Leibler divergence）： $$ D_{KL}(p | q) = \\sum_i p_i \\log \\frac{p_i}{q_i} $$\n性质：$D_{KL}(p | q) \\geq 0$，等号成立当且仅当 $p = q$。\n证明（使用Jensen不等式）： $$ D_{KL}(p | q) = \\mathbb{E}_p\\left[\\log \\frac{p}{q}\\right] = -\\mathbb{E}_p\\left[\\log \\frac{q}{p}\\right] \\geq -\\log \\mathbb{E}_p\\left[\\frac{q}{p}\\right] = -\\log 1 = 0 $$\n3.3 最大熵原理 原理：在所有满足约束的概率分布中，选择熵最大的分布。\n应用：高斯分布是给定方差下熵最大的分布。\n第三部分：深度学习中的微积分 1. 反向传播算法 1.1 神经网络的前向传播 考虑一个简单的两层神经网络： $$ \\mathbf{z}^{(1)} = W^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)} $$ $$ \\mathbf{a}^{(1)} = \\sigma(\\mathbf{z}^{(1)}) $$ $$ \\mathbf{z}^{(2)} = W^{(2)} \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)} $$ $$ \\hat{\\mathbf{y}} = \\sigma(\\mathbf{z}^{(2)}) $$\n损失函数：$L = \\frac{1}{2}\\lVert \\hat{\\mathbf{y}} - \\mathbf{y} \\rVert^2$\n1.2 反向传播的数学推导 我们需要计算损失函数对参数的梯度。\n输出层梯度： $$ \\frac{\\partial L}{\\partial \\mathbf{z}^{(2)}} = (\\hat{\\mathbf{y}} - \\mathbf{y}) \\odot \\sigma^{\\prime}(\\mathbf{z}^{(2)}) $$\n其中 $\\odot$ 是逐元素乘法，$\\sigma^{\\prime}(z) = \\sigma(z)(1 - \\sigma(z))$ 是Sigmoid的导数。\n隐藏层梯度（链式法则）： $$ \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} = \\left[(W^{(2)})^\\top \\frac{\\partial L}{\\partial \\mathbf{z}^{(2)}}\\right] \\odot \\sigma^{\\prime}(\\mathbf{z}^{(1)}) $$\n权重梯度： $$ \\frac{\\partial L}{\\partial W^{(2)}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(2)}} (\\mathbf{a}^{(1)})^\\top $$ $$ \\frac{\\partial L}{\\partial W^{(1)}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} \\mathbf{x}^\\top $$\n这就是反向传播：从输出层反向传播到输入层，使用链式法则计算每一层的梯度。\n1.3 计算图与自动微分 计算图将计算表示为有向无环图(DAG)。每个节点是一个操作，边表示数据流。\n自动微分（Automatic Differentiation）：\n前向模式：从输入到输出计算导数 反向模式：从输出到输入计算导数（反向传播） 优势：精确计算导数（无截断误差），复杂度与输出维度成正比。\n1.4 梯度消失问题 在深层网络中，梯度可能指数级衰减。考虑 $L$ 层线性网络： $$ \\frac{\\partial L}{\\partial \\mathbf{x}} = (W^{(L)})^\\top \\cdots (W^{(1)})^\\top \\frac{\\partial L}{\\partial \\mathbf{y}} $$\n如果权重矩阵的奇异值都小于1，梯度会指数级衰减 → 梯度消失。\n解决方案：\nReLU激活：导数为0或1，不会衰减 残差连接：提供\"梯度高速公路\" 层归一化：规范化激活值分布 2. 激活函数的导数 2.1 Sigmoid函数 定义：$\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n导数：$\\sigma^{\\prime}(z) = \\sigma(z)(1 - \\sigma(z))$\n问题：\n梯度消失：当 $\\lvert z \\rvert$ 很大时，$\\sigma^{\\prime}(z) \\approx 0$ 输出不以零为中心 2.2 Tanh函数 定义：$\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n导数：$\\tanh^{\\prime}(z) = 1 - \\tanh^2(z)$\n优势：输出以零为中心。\n2.3 ReLU函数 定义：$\\text{ReLU}(z) = \\max(0, z)$\n导数：$\\text{ReLU}^{\\prime}(z) = \\begin{cases} 1 \u0026 z \u003e 0 \\ 0 \u0026 z \\leq 0 \\end{cases}$\n优势：\n缓解梯度消失 计算简单 稀疏激活 问题：Dead ReLU（神经元\"死亡\"）。\n图5：常见激活函数及其导数的比较。Sigmoid和Tanh的导数在两端趋于0（梯度消失），ReLU的导数恒为0或1（避免梯度消失）。\n3. 正则化的微积分视角 3.1 L2正则（权重衰减） 目标： $$ \\min_{\\mathbf{w}} L(\\mathbf{w}) + \\frac{\\lambda}{2}\\lVert \\mathbf{w} \\rVert^2 $$\n梯度： $$ \\nabla_{\\mathbf{w}} = \\nabla L(\\mathbf{w}) + \\lambda \\mathbf{w} $$\n几何意义：限制参数空间，防止过拟合。\n3.2 L1正则 目标： $$ \\min_{\\mathbf{w}} L(\\mathbf{w}) + \\lambda \\lVert \\mathbf{w} \\rVert_1 $$\n次梯度： $$ \\partial_{\\mathbf{w}} = \\nabla L(\\mathbf{w}) + \\lambda \\cdot \\text{sign}(\\mathbf{w}) $$\n几何意义：产生稀疏解（特征选择）。\n4. 优化算法的演进 4.1 从SGD到Adam SGD（随机梯度下降）： $$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\nabla L(\\mathbf{w}_t) $$\nMomentum（动量）： $$ \\mathbf{v}t = \\beta \\mathbf{v}{t-1} + (1 - \\beta)\\nabla L(\\mathbf{w}t) $$ $$ \\mathbf{w}{t+1} = \\mathbf{w}_t - \\eta \\mathbf{v}_t $$\nAdam（Adaptive Moment Estimation）：结合动量和自适应学习率： $$ \\mathbf{m}t = \\beta_1 \\mathbf{m}{t-1} + (1 - \\beta_1)\\nabla L(\\mathbf{w}_t) $$ $$ \\mathbf{v}t = \\beta_2 \\mathbf{v}{t-1} + (1 - \\beta_2)(\\nabla L(\\mathbf{w}t))^2 $$ $$ \\mathbf{w}{t+1} = \\mathbf{w}_t - \\frac{\\eta}{\\sqrt{\\hat{\\mathbf{v}}_t + \\epsilon}} \\hat{\\mathbf{m}}_t $$\n其中 $\\hat{\\mathbf{m}}_t = \\frac{\\mathbf{m}_t}{1 - \\beta_1^t}$ 和 $\\hat{\\mathbf{v}}_t = \\frac{\\mathbf{v}_t}{1 - \\beta_2^t}$ 是偏差修正后的估计。\nAdam是现代深度学习的默认优化器。\n4.2 二阶优化：牛顿法 牛顿法使用二阶导数（Hessian矩阵）： $$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - H^{-1} \\nabla L(\\mathbf{w}_t) $$\n其中 $H_{ij} = \\frac{\\partial^2 L}{\\partial w_i \\partial w_j}$。\n优点：二阶收敛（接近最优点时非常快） 缺点：计算Hessian矩阵代价高（$O(d^2)$），可能不正定\nL-BFGS：拟牛顿法，用一阶信息近似Hessian，避免显式计算二阶导数。\n图6：梯度下降与牛顿法的收敛速度比较。红色线（梯度下降）线性收敛，绿色线（牛顿法）二次收敛，快速到达最小值。\n第四部分：高级主题 1. 变分法 1.1 泛函的极值问题 泛函是函数的函数：$J[y] = \\int_{x_1}^{x_2} F(x, y, y^{\\prime})dx$\n变分（Variation）：$\\delta y = \\epsilon \\eta(x)$，其中 $\\eta(x_1) = \\eta(x_2) = 0$\n欧拉-拉格朗日方程： $$ \\frac{\\partial F}{\\partial y} - \\frac{d}{dx}\\left(\\frac{\\partial F}{\\partial y^{\\prime}}\\right) = 0 $$\n推导： $$ \\delta J = \\int \\left(\\frac{\\partial F}{\\partial y}\\delta y + \\frac{\\partial F}{\\partial y^{\\prime}}\\delta y^{\\prime}\\right)dx = \\int \\left(\\frac{\\partial F}{\\partial y} - \\frac{d}{dx}\\frac{\\partial F}{\\partial y^{\\prime}}\\right)\\delta y,dx = 0 $$\n由于 $\\delta y$ 任意，被积函数必须为零。\n1.2 在变分自编码器（VAE）中的应用 ELBO（Evidence Lower Bound）： $$ \\mathcal{L} = \\mathbb{E}{q{\\phi}(z|x)}[\\log p_{\\theta}(x|z)] - D_{KL}(q_{\\phi}(z|x) | p(z)) $$\n通过变分推断优化 $\\phi$ 和 $\\theta$。\n2. 矩阵微积分 2.1 矩阵求导法则 标量对向量： $$ \\frac{\\partial f}{\\partial \\mathbf{x}} = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\ \\vdots \\ \\frac{\\partial f}{\\partial x_n} \\end{pmatrix} $$\n标量对矩阵： $$ \\frac{\\partial f}{\\partial X} = \\begin{pmatrix} \\frac{\\partial f}{\\partial X_{11}} \u0026 \\cdots \\ \\vdots \u0026 \\ddots \\end{pmatrix} $$\n常用公式：\n$\\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{a}^\\top \\mathbf{x} = \\mathbf{a}$ $\\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{x}^\\top A \\mathbf{x} = (A + A^\\top)\\mathbf{x}$ $\\frac{\\partial}{\\partial X} \\text{tr}(AX) = A^\\top$ 2.2 Kronecker乘积 定义： $$ A \\otimes B = \\begin{pmatrix} a_{11}B \u0026 \\cdots \\ \\vdots \u0026 \\ddots \\end{pmatrix} $$\n性质：\n$(A \\otimes B)(C \\otimes D) = (AC) \\otimes (BD)$ $\\text{vec}(ABC) = (C^\\top \\otimes A)\\text{vec}(B)$ 3. 微分几何初步 3.1 流形与切空间 流形：局部像欧几里得空间的拓扑空间。\n切空间 $T_pM$：流形在点 $p$ 处的\"线性化\"。\n切向量：$\\mathbf{v} = \\frac{d}{dt}\\big|_{t=0} \\gamma(t)$，其中 $\\gamma(t)$ 是曲线。\n3.2 黎曼度量 度量张量 $g_p$：定义切空间上的内积： $$ \\langle \\mathbf{u}, \\mathbf{v} \\rangle_p = g_p(\\mathbf{u}, \\mathbf{v}) $$\n测地线：“最短路径\"曲线，满足测地线方程： $$ \\frac{d^2 x^\\mu}{dt^2} + \\Gamma^\\mu_{\\alpha\\beta} \\frac{dx^\\alpha}{dt} \\frac{dx^\\beta}{dt} = 0 $$\n其中 $\\Gamma^\\mu_{\\alpha\\beta}$ 是Christoffel符号。\n3.3 梯度流 在黎曼流形上，梯度下降变为梯度流： $$ \\frac{d\\mathbf{x}}{dt} = -\\text{grad } f(\\mathbf{x}) $$\n其中 $\\text{grad } f$ 由度量定义： $$ g(\\text{grad } f, \\mathbf{v}) = df(\\mathbf{v}) = \\nabla f^\\top \\mathbf{v} $$\n4. 随机微积分 4.1 随机过程的微分 布朗运动 $W_t$：高斯随机过程，满足：\n$W_0 = 0$ 独立增量 $W_t - W_s \\sim \\mathcal{N}(0, t-s)$ 4.2 伊藤积分 定义： $$ I_T = \\int_0^T H_t , dW_t $$\n伊藤公式（链式法则的随机版本）： $$ df(X_t) = f^{\\prime}(X_t)dX_t + \\frac{1}{2}f^{\\prime\\prime}(X_t)d\\langle X \\rangle_t $$\n其中 $d\\langle X \\rangle_t$ 是二次变差。\n4.3 在扩散模型中的应用 SDE（随机微分方程）： $$ d\\mathbf{x} = \\mathbf{f}(\\mathbf{x}, t)dt + g(t)d\\mathbf{w}_t $$\n扩散过程：前向过程逐渐添加噪声，反向过程去噪生成样本。\n第五部分：关键洞察与展望 1. 微积分与几何 微积分本质上是几何：\n导数是切线的斜率 梯度指向最速上升方向 二阶导数描述曲率 积分计算曲线下的面积 理解这些几何直观有助于理解优化算法的行为。\n2. 线性化的重要性 现代人工智能的核心思想是局部线性化：\n神经网络是复杂的非线性函数 但在每个参数点附近，可以用线性函数逼近 通过不断的线性近似和迭代，找到全局最优 泰勒展开是线性化的数学工具： $$ f(\\mathbf{x} + \\Delta \\mathbf{x}) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\top \\Delta \\mathbf{x} + \\frac{1}{2}\\Delta \\mathbf{x}^\\top H \\Delta \\mathbf{x} $$\n3. 链式法则的威力 链式法则使得我们可以计算任意复合函数的导数。神经网络本质上是一个巨大的复合函数，反向传播算法就是链式法则的高效实现。\n现代深度学习框架（PyTorch, TensorFlow）使用自动微分（automatic differentiation）来自动计算梯度，让开发者专注于模型架构而非数学细节。\n4. 优化的艺术 梯度下降看似简单，但有许多改进空间：\n动量：利用历史信息加速收敛 自适应学习率：为每个参数定制步长 二阶方法：利用曲率信息更快收敛 随机性：SGD的噪声有助于跳出局部最优 5. 未来展望 扩散模型的随机微积分：理解SDE的解对改进扩散模型至关重要。\n神经符号AI：结合神经网络和符号推理，需要新的微积分工具。\n优化理论：非凸优化、分布式优化的理论仍在发展中。\n量子机器学习：量子微积分可能带来新的优化方法。\n结语：微积分与AI的未来 从17世纪牛顿和莱布尼茨发明微积分，到21世纪的深度学习革命，微积分一直是描述变化的数学语言。\n在这篇文章中，我们深入探讨了：\n导数与微分：从变化率到梯度，从线性化到链式法则 梯度下降：最优化算法的基础，几何直观与数学严格 反向传播：链式法则的矩阵形式，计算图与自动微分 优化算法：从SGD到Adam，从一阶到二阶方法 高级主题：变分法、矩阵微积分、微分几何、随机微积分 微积分不仅提供了计算梯度的方法，更重要的是，它培养了一种思维方式：用局部变化理解全局行为，用线性逼近处理非线性问题。\n在未来，随着人工智能的发展，微积分将继续发挥核心作用。无论是扩散模型的随机微积分，还是神经符号AI的微积分基础，都需要深厚的微积分功底。\n理解微积分，不仅是掌握一门数学工具，更是培养一种分析问题、解决问题、创新思考的能力。正如伟大的数学家柯西所说：“微积分是人类智慧的结晶。”\n参考文献 Spivak, M. (2008). Calculus On Manifolds (4th ed.). Publish or Perish. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer. Goodfellow, I., Bengio, Y., \u0026 Courville, A. (2016). Deep Learning. MIT Press. Boyd, S., \u0026 Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press. Nielsen, M. A. (2015). Neural Networks and Deep Learning. Determination Press. Rudin, W. (1976). Principles of Mathematical Analysis (3rd ed.). McGraw-Hill. Lee, J. M. (2018). Introduction to Riemannian Manifolds (2nd ed.). Springer. Oksendal, B. (2003). Stochastic Differential Equations (6th ed.). Springer. Nocedal, J., \u0026 Wright, S. J. (2006). Numerical Optimization (2nd ed.). Springer. Petersen, K. B., \u0026 Pedersen, M. S. (2012). The Matrix Cookbook. Technical University of Denmark. ","wordCount":"1716","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/calculus-ml-journey.jpg","datePublished":"2026-01-25T19:00:00+08:00","dateModified":"2026-01-25T19:00:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-25-calculus-ml-systematic-review/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">微积分与机器学习：从变化率到神经网络梯度的完整旅程</h1><div class=post-description>深入理解微积分如何驱动现代人工智能：从导数的几何直观到梯度下降的数学原理，从链式法则到反向传播算法，揭示神经网络训练的数学本质。</div><div class=post-meta><span title='2026-01-25 19:00:00 +0800 CST'>January 25, 2026</span>&nbsp;·&nbsp;<span>9 min</span>&nbsp;·&nbsp;<span>1716 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/calculus-ml-journey.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/calculus-ml-journey.jpg alt=微积分的几何美感></a><figcaption>微积分：描述变化的数学语言</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%be%ae%e7%a7%af%e5%88%86 aria-label=引言：为什么需要微积分？>引言：为什么需要微积分？</a></li><li><a href=#%e7%ac%ac%e4%b8%80%e9%83%a8%e5%88%86%e5%be%ae%e7%a7%af%e5%88%86%e5%9f%ba%e7%a1%80%e7%90%86%e8%ae%ba aria-label=第一部分：微积分基础理论>第一部分：微积分基础理论</a><ul><li><a href=#1-%e5%af%bc%e6%95%b0%e7%9a%84%e6%9c%ac%e8%b4%a8%e4%bb%8e%e5%8f%98%e5%8c%96%e7%8e%87%e5%88%b0%e7%9e%ac%e6%97%b6%e5%8f%98%e5%8c%96%e7%8e%87 aria-label="1. 导数的本质：从变化率到瞬时变化率">1. 导数的本质：从变化率到瞬时变化率</a><ul><li><a href=#11-%e5%8f%98%e5%8c%96%e7%8e%87%e7%9a%84%e7%9b%b4%e8%a7%82%e7%90%86%e8%a7%a3 aria-label="1.1 变化率的直观理解">1.1 变化率的直观理解</a></li><li><a href=#12-%e5%af%bc%e6%95%b0%e7%9a%84%e5%87%a0%e4%bd%95%e6%84%8f%e4%b9%89 aria-label="1.2 导数的几何意义">1.2 导数的几何意义</a></li><li><a href=#13-%e5%af%bc%e6%95%b0%e7%9a%84%e8%ae%a1%e7%ae%97%e8%a7%84%e5%88%99 aria-label="1.3 导数的计算规则">1.3 导数的计算规则</a></li></ul></li><li><a href=#2-%e5%be%ae%e5%88%86%e4%b8%8e%e7%ba%bf%e6%80%a7%e5%8c%96 aria-label="2. 微分与线性化">2. 微分与线性化</a><ul><li><a href=#21-%e5%be%ae%e5%88%86%e7%9a%84%e5%87%a0%e4%bd%95%e6%84%8f%e4%b9%89 aria-label="2.1 微分的几何意义">2.1 微分的几何意义</a></li><li><a href=#22-%e5%a4%9a%e5%85%83%e5%87%bd%e6%95%b0%e7%9a%84%e5%be%ae%e5%88%86 aria-label="2.2 多元函数的微分">2.2 多元函数的微分</a></li><li><a href=#23-%e6%a2%af%e5%ba%a6%e5%a4%9a%e7%bb%b4%e7%9a%84%e5%af%bc%e6%95%b0 aria-label="2.3 梯度：多维的导数">2.3 梯度：多维的导数</a></li><li><a href=#24-%e9%9b%85%e5%8f%af%e6%af%94%e7%9f%a9%e9%98%b5%e4%b8%8e%e9%93%be%e5%bc%8f%e6%b3%95%e5%88%99 aria-label="2.4 雅可比矩阵与链式法则">2.4 雅可比矩阵与链式法则</a></li></ul></li><li><a href=#3-%e7%a7%af%e5%88%86%e4%b8%8e%e7%b4%af%e7%a7%af aria-label="3. 积分与累积">3. 积分与累积</a><ul><li><a href=#31-%e4%bb%8e%e6%b1%82%e5%92%8c%e5%88%b0%e9%bb%8e%e6%9b%bc%e7%a7%af%e5%88%86 aria-label="3.1 从求和到黎曼积分">3.1 从求和到黎曼积分</a></li><li><a href=#32-%e5%be%ae%e7%a7%af%e5%88%86%e5%9f%ba%e6%9c%ac%e5%ae%9a%e7%90%86 aria-label="3.2 微积分基本定理">3.2 微积分基本定理</a></li><li><a href=#33-%e5%a4%9a%e9%87%8d%e7%a7%af%e5%88%86%e4%b8%8e%e5%8f%98%e9%87%8f%e6%9b%bf%e6%8d%a2 aria-label="3.3 多重积分与变量替换">3.3 多重积分与变量替换</a></li></ul></li><li><a href=#4-%e7%ba%a7%e6%95%b0%e4%b8%8e%e9%80%bc%e8%bf%91 aria-label="4. 级数与逼近">4. 级数与逼近</a><ul><li><a href=#41-%e6%b3%b0%e5%8b%92%e5%b1%95%e5%bc%80%e7%94%a8%e5%a4%9a%e9%a1%b9%e5%bc%8f%e9%80%bc%e8%bf%91%e5%87%bd%e6%95%b0 aria-label="4.1 泰勒展开：用多项式逼近函数">4.1 泰勒展开：用多项式逼近函数</a></li><li><a href=#42-%e5%a4%9a%e5%85%83%e6%b3%b0%e5%8b%92%e5%b1%95%e5%bc%80 aria-label="4.2 多元泰勒展开">4.2 多元泰勒展开</a></li><li><a href=#43-%e5%9c%a8%e4%bc%98%e5%8c%96%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8 aria-label="4.3 在优化中的应用">4.3 在优化中的应用</a></li></ul></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e9%83%a8%e5%88%86%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e5%be%ae%e7%a7%af%e5%88%86 aria-label=第二部分：机器学习中的微积分>第二部分：机器学习中的微积分</a><ul><li><a href=#1-%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95 aria-label="1. 梯度下降法">1. 梯度下降法</a><ul><li><a href=#11-%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc aria-label="1.1 算法推导">1.1 算法推导</a></li><li><a href=#12-%e4%b8%ba%e4%bb%80%e4%b9%88%e8%bf%99%e6%a0%b7%e8%b5%b0%e6%98%af%e6%ad%a3%e7%a1%ae%e7%9a%84 aria-label="1.2 为什么这样走是正确的？">1.2 为什么这样走是正确的？</a></li><li><a href=#13-%e6%94%b6%e6%95%9b%e6%80%a7%e5%88%86%e6%9e%90%e5%bc%ba%e5%87%b8%e6%83%85%e5%86%b5 aria-label="1.3 收敛性分析（强凸情况）">1.3 收敛性分析（强凸情况）</a></li><li><a href=#14-%e5%ad%a6%e4%b9%a0%e7%8e%87%e7%9a%84%e9%80%89%e6%8b%a9 aria-label="1.4 学习率的选择">1.4 学习率的选择</a></li><li><a href=#15-%e5%8a%a8%e9%87%8f%e6%96%b9%e6%b3%95 aria-label="1.5 动量方法">1.5 动量方法</a></li><li><a href=#16-%e8%87%aa%e9%80%82%e5%ba%94%e5%ad%a6%e4%b9%a0%e7%8e%87 aria-label="1.6 自适应学习率">1.6 自适应学习率</a></li></ul></li><li><a href=#2-%e6%8b%89%e6%a0%bc%e6%9c%97%e6%97%a5%e4%b9%98%e6%95%b0%e6%b3%95 aria-label="2. 拉格朗日乘数法">2. 拉格朗日乘数法</a><ul><li><a href=#21-%e7%ba%a6%e6%9d%9f%e4%bc%98%e5%8c%96%e9%97%ae%e9%a2%98 aria-label="2.1 约束优化问题">2.1 约束优化问题</a></li><li><a href=#22-%e5%87%a0%e4%bd%95%e8%a7%a3%e9%87%8a aria-label="2.2 几何解释">2.2 几何解释</a></li><li><a href=#23-%e5%9c%a8svm%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8 aria-label="2.3 在SVM中的应用">2.3 在SVM中的应用</a></li></ul></li><li><a href=#3-%e4%bf%a1%e6%81%af%e8%ae%ba%e4%b8%ad%e7%9a%84%e5%be%ae%e7%a7%af%e5%88%86 aria-label="3. 信息论中的微积分">3. 信息论中的微积分</a><ul><li><a href=#31-%e7%86%b5%e7%9a%84%e5%ae%9a%e4%b9%89%e4%b8%8e%e5%be%ae%e5%88%86 aria-label="3.1 熵的定义与微分">3.1 熵的定义与微分</a></li><li><a href=#32-%e4%ba%a4%e5%8f%89%e7%86%b5%e4%b8%8ekl%e6%95%a3%e5%ba%a6 aria-label="3.2 交叉熵与KL散度">3.2 交叉熵与KL散度</a></li><li><a href=#33-%e6%9c%80%e5%a4%a7%e7%86%b5%e5%8e%9f%e7%90%86 aria-label="3.3 最大熵原理">3.3 最大熵原理</a></li></ul></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e9%83%a8%e5%88%86%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e5%be%ae%e7%a7%af%e5%88%86 aria-label=第三部分：深度学习中的微积分>第三部分：深度学习中的微积分</a><ul><li><a href=#1-%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95 aria-label="1. 反向传播算法">1. 反向传播算法</a><ul><li><a href=#11-%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e5%89%8d%e5%90%91%e4%bc%a0%e6%92%ad aria-label="1.1 神经网络的前向传播">1.1 神经网络的前向传播</a></li><li><a href=#12-%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%9a%84%e6%95%b0%e5%ad%a6%e6%8e%a8%e5%af%bc aria-label="1.2 反向传播的数学推导">1.2 反向传播的数学推导</a></li><li><a href=#13-%e8%ae%a1%e7%ae%97%e5%9b%be%e4%b8%8e%e8%87%aa%e5%8a%a8%e5%be%ae%e5%88%86 aria-label="1.3 计算图与自动微分">1.3 计算图与自动微分</a></li><li><a href=#14-%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1%e9%97%ae%e9%a2%98 aria-label="1.4 梯度消失问题">1.4 梯度消失问题</a></li></ul></li><li><a href=#2-%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0%e7%9a%84%e5%af%bc%e6%95%b0 aria-label="2. 激活函数的导数">2. 激活函数的导数</a><ul><li><a href=#21-sigmoid%e5%87%bd%e6%95%b0 aria-label="2.1 Sigmoid函数">2.1 Sigmoid函数</a></li><li><a href=#22-tanh%e5%87%bd%e6%95%b0 aria-label="2.2 Tanh函数">2.2 Tanh函数</a></li><li><a href=#23-relu%e5%87%bd%e6%95%b0 aria-label="2.3 ReLU函数">2.3 ReLU函数</a></li></ul></li><li><a href=#3-%e6%ad%a3%e5%88%99%e5%8c%96%e7%9a%84%e5%be%ae%e7%a7%af%e5%88%86%e8%a7%86%e8%a7%92 aria-label="3. 正则化的微积分视角">3. 正则化的微积分视角</a><ul><li><a href=#31-l2%e6%ad%a3%e5%88%99%e6%9d%83%e9%87%8d%e8%a1%b0%e5%87%8f aria-label="3.1 L2正则（权重衰减）">3.1 L2正则（权重衰减）</a></li><li><a href=#32-l1%e6%ad%a3%e5%88%99 aria-label="3.2 L1正则">3.2 L1正则</a></li></ul></li><li><a href=#4-%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%e7%9a%84%e6%bc%94%e8%bf%9b aria-label="4. 优化算法的演进">4. 优化算法的演进</a><ul><li><a href=#41-%e4%bb%8esgd%e5%88%b0adam aria-label="4.1 从SGD到Adam">4.1 从SGD到Adam</a></li><li><a href=#42-%e4%ba%8c%e9%98%b6%e4%bc%98%e5%8c%96%e7%89%9b%e9%a1%bf%e6%b3%95 aria-label="4.2 二阶优化：牛顿法">4.2 二阶优化：牛顿法</a></li></ul></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e9%83%a8%e5%88%86%e9%ab%98%e7%ba%a7%e4%b8%bb%e9%a2%98 aria-label=第四部分：高级主题>第四部分：高级主题</a><ul><li><a href=#1-%e5%8f%98%e5%88%86%e6%b3%95 aria-label="1. 变分法">1. 变分法</a><ul><li><a href=#11-%e6%b3%9b%e5%87%bd%e7%9a%84%e6%9e%81%e5%80%bc%e9%97%ae%e9%a2%98 aria-label="1.1 泛函的极值问题">1.1 泛函的极值问题</a></li><li><a href=#12-%e5%9c%a8%e5%8f%98%e5%88%86%e8%87%aa%e7%bc%96%e7%a0%81%e5%99%a8vae%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8 aria-label="1.2 在变分自编码器（VAE）中的应用">1.2 在变分自编码器（VAE）中的应用</a></li></ul></li><li><a href=#2-%e7%9f%a9%e9%98%b5%e5%be%ae%e7%a7%af%e5%88%86 aria-label="2. 矩阵微积分">2. 矩阵微积分</a><ul><li><a href=#21-%e7%9f%a9%e9%98%b5%e6%b1%82%e5%af%bc%e6%b3%95%e5%88%99 aria-label="2.1 矩阵求导法则">2.1 矩阵求导法则</a></li><li><a href=#22-kronecker%e4%b9%98%e7%a7%af aria-label="2.2 Kronecker乘积">2.2 Kronecker乘积</a></li></ul></li><li><a href=#3-%e5%be%ae%e5%88%86%e5%87%a0%e4%bd%95%e5%88%9d%e6%ad%a5 aria-label="3. 微分几何初步">3. 微分几何初步</a><ul><li><a href=#31-%e6%b5%81%e5%bd%a2%e4%b8%8e%e5%88%87%e7%a9%ba%e9%97%b4 aria-label="3.1 流形与切空间">3.1 流形与切空间</a></li><li><a href=#32-%e9%bb%8e%e6%9b%bc%e5%ba%a6%e9%87%8f aria-label="3.2 黎曼度量">3.2 黎曼度量</a></li><li><a href=#33-%e6%a2%af%e5%ba%a6%e6%b5%81 aria-label="3.3 梯度流">3.3 梯度流</a></li></ul></li><li><a href=#4-%e9%9a%8f%e6%9c%ba%e5%be%ae%e7%a7%af%e5%88%86 aria-label="4. 随机微积分">4. 随机微积分</a><ul><li><a href=#41-%e9%9a%8f%e6%9c%ba%e8%bf%87%e7%a8%8b%e7%9a%84%e5%be%ae%e5%88%86 aria-label="4.1 随机过程的微分">4.1 随机过程的微分</a></li><li><a href=#42-%e4%bc%8a%e8%97%a4%e7%a7%af%e5%88%86 aria-label="4.2 伊藤积分">4.2 伊藤积分</a></li><li><a href=#43-%e5%9c%a8%e6%89%a9%e6%95%a3%e6%a8%a1%e5%9e%8b%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8 aria-label="4.3 在扩散模型中的应用">4.3 在扩散模型中的应用</a></li></ul></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e9%83%a8%e5%88%86%e5%85%b3%e9%94%ae%e6%b4%9e%e5%af%9f%e4%b8%8e%e5%b1%95%e6%9c%9b aria-label=第五部分：关键洞察与展望>第五部分：关键洞察与展望</a><ul><li><a href=#1-%e5%be%ae%e7%a7%af%e5%88%86%e4%b8%8e%e5%87%a0%e4%bd%95 aria-label="1. 微积分与几何">1. 微积分与几何</a></li><li><a href=#2-%e7%ba%bf%e6%80%a7%e5%8c%96%e7%9a%84%e9%87%8d%e8%a6%81%e6%80%a7 aria-label="2. 线性化的重要性">2. 线性化的重要性</a></li><li><a href=#3-%e9%93%be%e5%bc%8f%e6%b3%95%e5%88%99%e7%9a%84%e5%a8%81%e5%8a%9b aria-label="3. 链式法则的威力">3. 链式法则的威力</a></li><li><a href=#4-%e4%bc%98%e5%8c%96%e7%9a%84%e8%89%ba%e6%9c%af aria-label="4. 优化的艺术">4. 优化的艺术</a></li><li><a href=#5-%e6%9c%aa%e6%9d%a5%e5%b1%95%e6%9c%9b aria-label="5. 未来展望">5. 未来展望</a></li></ul></li><li><a href=#%e7%bb%93%e8%af%ad%e5%be%ae%e7%a7%af%e5%88%86%e4%b8%8eai%e7%9a%84%e6%9c%aa%e6%9d%a5 aria-label=结语：微积分与AI的未来>结语：微积分与AI的未来</a></li><li><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae aria-label=参考文献>参考文献</a></li></ul></div></details></div><div class=post-content><h2 id=引言为什么需要微积分>引言：为什么需要微积分？<a hidden class=anchor aria-hidden=true href=#引言为什么需要微积分>#</a></h2><p>想象你在山上，想找到最低点。你会怎么做？你会观察脚下的坡度，选择最陡峭的方向迈出一步，然后重复这个过程。这个简单的直觉——<strong>沿着负梯度方向走</strong>——正是现代人工智能的核心算法。</p><p>从ChatGPT的语言模型到AlphaGo的围棋策略，从图像识别到语音合成，所有这些技术背后都有一个共同的数学基础：<strong>微积分</strong>。</p><p>微积分研究的是<strong>变化</strong>。而机器学习本质上是关于<strong>优化</strong>——通过不断调整参数来减少错误。当我们在高维空间中优化复杂的神经网络时，微积分提供了描述和计算这种变化的精确语言。</p><p>这篇文章将带你深入理解微积分如何驱动现代人工智能。我们不会停留在表面，而是会深入到数学推导的核心，揭示梯度下降、反向传播等算法的数学本质。这是一次从17世纪牛顿和莱布尼茨的发明，到21世纪深度学习革命的完整旅程。</p><hr><h2 id=第一部分微积分基础理论>第一部分：微积分基础理论<a hidden class=anchor aria-hidden=true href=#第一部分微积分基础理论>#</a></h2><h3 id=1-导数的本质从变化率到瞬时变化率>1. 导数的本质：从变化率到瞬时变化率<a hidden class=anchor aria-hidden=true href=#1-导数的本质从变化率到瞬时变化率>#</a></h3><h4 id=11-变化率的直观理解>1.1 变化率的直观理解<a hidden class=anchor aria-hidden=true href=#11-变化率的直观理解>#</a></h4><p><strong>变化率</strong>是人类最早思考的数学问题之一。如果一辆车2小时行驶100公里，平均速度是50公里/小时。但它某一时刻的<strong>瞬时速度</strong>是多少？</p><p>微积分的答案是：用<strong>极限</strong>。考虑函数 $f(x)$ 在 $x_0$ 附近的平均变化率：
$$
\frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x}
$$</p><p>当 $\Delta x \to 0$ 时，这个平均变化率的极限就是<strong>导数</strong>：
$$
f^{\prime}(x_0) = \lim_{\Delta x \to 0} \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x}
$$</p><h4 id=12-导数的几何意义>1.2 导数的几何意义<a hidden class=anchor aria-hidden=true href=#12-导数的几何意义>#</a></h4><p><strong>几何直观</strong>：导数是切线的斜率。在 $x_0$ 处，曲线 $f(x)$ 可以用直线（切线）逼近：
$$
f(x) \approx f(x_0) + f^{\prime}(x_0)(x - x_0)
$$</p><p>这就是<strong>一阶泰勒公式</strong>，也是<strong>线性化</strong>的思想：局部用简单的线性函数逼近复杂的非线性函数。</p><p><strong>严格定义</strong>（$\epsilon-\delta$ 语言）：
$$
\forall \epsilon > 0, \exists \delta > 0 \text{ s.t. } |\Delta x| &lt; \delta \implies \left|\frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x} - f^{\prime}(x_0)\right| &lt; \epsilon
$$</p><h4 id=13-导数的计算规则>1.3 导数的计算规则<a hidden class=anchor aria-hidden=true href=#13-导数的计算规则>#</a></h4><p><strong>基本法则</strong>：</p><ul><li><strong>线性性</strong>：$(af + bg)^{\prime} = af^{\prime} + bg^{\prime}$</li><li><strong>乘积法则</strong>：$(fg)^{\prime} = f^{\prime}g + fg^{\prime}$</li><li><strong>商法则</strong>：$\left(\frac{f}{g}\right)^{\prime} = \frac{f^{\prime}g - fg^{\prime}}{g^2}$</li></ul><p><strong>链式法则</strong>（复合函数）：
$$
\frac{d}{dx}f(g(x)) = f^{\prime}(g(x)) \cdot g^{\prime}(x)
$$</p><p>这个看似简单的公式是<strong>反向传播算法</strong>的数学基础！</p><h3 id=2-微分与线性化>2. 微分与线性化<a hidden class=anchor aria-hidden=true href=#2-微分与线性化>#</a></h3><h4 id=21-微分的几何意义>2.1 微分的几何意义<a hidden class=anchor aria-hidden=true href=#21-微分的几何意义>#</a></h4><p>微分 $dy = f^{\prime}(x)dx$ 是函数变化的<strong>线性近似</strong>。在几何上，它是切线纵坐标的变化量。</p><p><strong>关键思想</strong>：对于微小的 $\Delta x$：
$$
\Delta f = f(x + \Delta x) - f(x) \approx f^{\prime}(x)\Delta x
$$</p><p>近似误差是 $\Delta x$ 的<strong>高阶无穷小</strong>：
$$
\lim_{\Delta x \to 0} \frac{\Delta f - f^{\prime}(x)\Delta x}{\Delta x} = 0
$$</p><h4 id=22-多元函数的微分>2.2 多元函数的微分<a hidden class=anchor aria-hidden=true href=#22-多元函数的微分>#</a></h4><p>对于多元函数 $f(\mathbf{x})$，其中 $\mathbf{x} = (x_1, x_2, \ldots, x_n)^\top$，我们需要描述它在所有方向上的变化率。</p><p><strong>偏导数</strong>是沿坐标轴方向的变化率：
$$
\frac{\partial f}{\partial x_i} = \lim_{\Delta x_i \to 0} \frac{f(\mathbf{x} + \Delta x_i \mathbf{e}_i) - f(\mathbf{x})}{\Delta x_i}
$$</p><p><strong>全微分</strong>：
$$
df = \sum_{i=1}^n \frac{\partial f}{\partial x_i} dx_i = \nabla f^\top d\mathbf{x}
$$</p><h4 id=23-梯度多维的导数>2.3 梯度：多维的导数<a hidden class=anchor aria-hidden=true href=#23-梯度多维的导数>#</a></h4><p><strong>梯度</strong>将所有偏导数组合成向量：
$$
\nabla f(\mathbf{x}) = \begin{pmatrix} \dfrac{\partial f}{\partial x_1} \ \vdots \ \dfrac{\partial f}{\partial x_n} \end{pmatrix}
$$</p><p><strong>关键性质</strong>：梯度指向函数增长最快的方向。因此，<strong>负梯度方向</strong>是<strong>最速下降方向</strong>。</p><p><strong>证明</strong>（方向导数）：
$$
\frac{\partial f}{\partial \mathbf{u}} = \nabla f^\top \mathbf{u} = \lVert \nabla f \rVert \lVert \mathbf{u} \rVert \cos\theta
$$</p><p>当 $\theta = 0$ 时（$\mathbf{u}$ 与 $\nabla f$ 同向），方向导数最大。因此 $\nabla f$ 指向最速上升方向。</p><h4 id=24-雅可比矩阵与链式法则>2.4 雅可比矩阵与链式法则<a hidden class=anchor aria-hidden=true href=#24-雅可比矩阵与链式法则>#</a></h4><p>对于向量值函数 $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$，<strong>雅可比矩阵</strong>是所有偏导数组成的矩阵：
$$
J_{\mathbf{f}}(\mathbf{x}) = \begin{pmatrix}
\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \
\vdots & \ddots & \vdots \
\frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{pmatrix}
$$</p><p><strong>多元链式法则</strong>（矩阵形式）：
$$
\frac{\partial \mathbf{z}}{\partial \mathbf{x}} = \frac{\partial \mathbf{z}}{\partial \mathbf{y}} \cdot \frac{\partial \mathbf{y}}{\partial \mathbf{x}}
$$</p><p>其中 $\mathbf{z} = \mathbf{f}(\mathbf{y})$ 且 $\mathbf{y} = \mathbf{g}(\mathbf{x})$。</p><h3 id=3-积分与累积>3. 积分与累积<a hidden class=anchor aria-hidden=true href=#3-积分与累积>#</a></h3><h4 id=31-从求和到黎曼积分>3.1 从求和到黎曼积分<a hidden class=anchor aria-hidden=true href=#31-从求和到黎曼积分>#</a></h4><p>积分是微分的逆运算，也是累积的工具。从<strong>黎曼和</strong>开始：
$$
\int_a^b f(x)dx = \lim_{n \to \infty} \sum_{i=1}^n f(x_i^*)\Delta x_i
$$</p><p>其中 $\Delta x_i = x_i - x_{i-1}$，$x_i^* \in [x_{i-1}, x_i]$。</p><h4 id=32-微积分基本定理>3.2 微积分基本定理<a hidden class=anchor aria-hidden=true href=#32-微积分基本定理>#</a></h4><p><strong>牛顿-莱布尼茨公式</strong>：
$$
\int_a^b f(x)dx = F(b) - F(a)
$$</p><p>其中 $F^{\prime}(x) = f(x)$。这个公式连接了微分和积分，是微积分的核心定理。</p><h4 id=33-多重积分与变量替换>3.3 多重积分与变量替换<a hidden class=anchor aria-hidden=true href=#33-多重积分与变量替换>#</a></h4><p><strong>二重积分</strong>：
$$
\iint_D f(x,y)dxdy = \int_{y_1}^{y_2} \int_{x_1(y)}^{x_2(y)} f(x,y)dxdy
$$</p><p><strong>变量替换公式</strong>（雅可比行列式）：
$$
\iint_{D^*} f(x,y)dxdy = \iint_D f(x(u,v), y(u,v)) \left|\frac{\partial(x,y)}{\partial(u,v)}\right| dudv
$$</p><p>其中雅可比行列式：
$$
\frac{\partial(x,y)}{\partial(u,v)} = \det \begin{pmatrix} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{pmatrix}
$$</p><h3 id=4-级数与逼近>4. 级数与逼近<a hidden class=anchor aria-hidden=true href=#4-级数与逼近>#</a></h3><h4 id=41-泰勒展开用多项式逼近函数>4.1 泰勒展开：用多项式逼近函数<a hidden class=anchor aria-hidden=true href=#41-泰勒展开用多项式逼近函数>#</a></h4><p><strong>泰勒公式</strong>是线性化的自然推广。在 $x_0$ 附近，$f(x)$ 可以用多项式逼近：
$$
f(x) = f(x_0) + f^{\prime}(x_0)(x - x_0) + \frac{f^{\prime\prime}(x_0)}{2!}(x - x_0)^2 + \cdots + \frac{f^{(n)}(x_0)}{n!}(x - x_0)^n + R_n(x)
$$</p><p><strong>余项形式</strong>（拉格朗日型）：
$$
R_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!}(x - x_0)^{n+1}
$$</p><p>其中 $\xi$ 在 $x_0$ 和 $x$ 之间。</p><p><strong>泰勒级数</strong>（当 $n \to \infty$）：
$$
f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(x_0)}{n!}(x - x_0)^n
$$</p><p><img alt=泰勒级数逼近 loading=lazy src=/images/math/taylor-series-approximation.png></p><p>图1：泰勒级数用多项式逼近函数 $e^x$。阶数越高，逼近范围越广，误差越小。</p><h4 id=42-多元泰勒展开>4.2 多元泰勒展开<a hidden class=anchor aria-hidden=true href=#42-多元泰勒展开>#</a></h4><p>对于多元函数 $f: \mathbb{R}^n \to \mathbb{R}$，在 $\mathbf{x}_0$ 附近：
$$
f(\mathbf{x}_0 + \Delta \mathbf{x}) = f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^\top \Delta \mathbf{x} + \frac{1}{2}\Delta \mathbf{x}^\top H(\mathbf{x}_0) \Delta \mathbf{x} + \mathcal{O}(\lVert \Delta \mathbf{x} \rVert^3)
$$</p><p>其中 $H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}$ 是<strong>Hessian矩阵</strong>（二阶导数矩阵）。</p><h4 id=43-在优化中的应用>4.3 在优化中的应用<a hidden class=anchor aria-hidden=true href=#43-在优化中的应用>#</a></h4><p><strong>一阶条件</strong>（必要条件）：
$$
\nabla f(\mathbf{x}^*) = \mathbf{0}
$$</p><p><strong>二阶条件</strong>（充分条件，对于凸函数）：
$$
H(\mathbf{x}^*) \succ 0 \quad (\text{正定})
$$</p><p>泰勒展开是理解优化算法（如牛顿法）的数学基础。</p><hr><h2 id=第二部分机器学习中的微积分>第二部分：机器学习中的微积分<a hidden class=anchor aria-hidden=true href=#第二部分机器学习中的微积分>#</a></h2><h3 id=1-梯度下降法>1. 梯度下降法<a hidden class=anchor aria-hidden=true href=#1-梯度下降法>#</a></h3><h4 id=11-算法推导>1.1 算法推导<a hidden class=anchor aria-hidden=true href=#11-算法推导>#</a></h4><p>考虑无约束优化问题：
$$
\min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x})
$$</p><p><strong>核心思想</strong>：在当前点 $\mathbf{x}_k$，计算梯度 $\nabla f(\mathbf{x}<em>k)$，然后沿负梯度方向移动：
$$
\mathbf{x}</em>{k+1} = \mathbf{x}_k - \eta \nabla f(\mathbf{x}_k)
$$</p><p>其中 $\eta$ 是<strong>学习率</strong>(learning rate)，控制步长大小。</p><h4 id=12-为什么这样走是正确的>1.2 为什么这样走是正确的？<a hidden class=anchor aria-hidden=true href=#12-为什么这样走是正确的>#</a></h4><p><strong>泰勒展开证明</strong>：</p><p>在 $\mathbf{x}_k$ 附近对 $f$ 做一阶近似：
$$
f(\mathbf{x}_k + \Delta \mathbf{x}) \approx f(\mathbf{x}_k) + \nabla f(\mathbf{x}_k)^\top \Delta \mathbf{x}
$$</p><p>我们要找到 $\Delta \mathbf{x}$ 使 $f$ 减小最多。设步长固定：$\lVert \Delta \mathbf{x} \rVert = \epsilon$。</p><p>由柯西-施瓦茨不等式：
$$
\nabla f^\top \Delta \mathbf{x} \geq -\lVert \nabla f \rVert \cdot \lVert \Delta \mathbf{x} \rVert = -\epsilon \lVert \nabla f \rVert
$$</p><p>当且仅当 $\Delta \mathbf{x}$ 与 $-\nabla f$ 同向时取等号。因此，<strong>负梯度方向是最速下降方向</strong>。</p><p><img alt=梯度下降轨迹 loading=lazy src=/images/math/gradient-descent-trajectory.png></p><p>图2：梯度下降在等高线上的优化轨迹。红色箭头显示每次迭代沿负梯度方向移动，最终收敛到最小值（中心点）。</p><p><img alt=3D损失函数曲面与梯度方向 loading=lazy src=/images/math/loss-surface-gradient.png></p><p>图3：损失函数曲面上的梯度方向。红色向量显示负梯度方向，指向函数值下降最快的方向。</p><h4 id=13-收敛性分析强凸情况>1.3 收敛性分析（强凸情况）<a hidden class=anchor aria-hidden=true href=#13-收敛性分析强凸情况>#</a></h4><p>假设 $f$ 是 $\mu$-强凸的，且梯度是 $L$-Lipschitz连续的：
$$
\mu I \preceq H(\mathbf{x}) \preceq LI
$$</p><p><strong>收敛率</strong>：
$$
f(\mathbf{x}^{(t)}) - f(\mathbf{x}^<em>) \leq \left(1 - \frac{\mu}{L}\right)^{t} [f(\mathbf{x}^{(0)}) - f(\mathbf{x}^</em>)]
$$</p><p>这是<strong>线性收敛</strong>（几何收敛）。</p><h4 id=14-学习率的选择>1.4 学习率的选择<a hidden class=anchor aria-hidden=true href=#14-学习率的选择>#</a></h4><p>学习率 $\eta$ 太小：收敛慢，需要很多步
学习率 $\eta$ 太大：可能"冲过"最优点，甚至发散</p><p><strong>经验法则</strong>：对于 $L$-光滑函数，选择 $\eta &lt; \frac{2}{L}$。</p><h4 id=15-动量方法>1.5 动量方法<a hidden class=anchor aria-hidden=true href=#15-动量方法>#</a></h4><p><strong>问题</strong>：梯度下降在"峡谷"形状的损失函数上振荡。</p><p><strong>解决方案</strong>：加入动量项，利用历史梯度信息：
$$
\mathbf{v}<em>t = \beta \mathbf{v}</em>{t-1} + (1 - \beta)\nabla L(\mathbf{w}<em>t)
$$
$$
\mathbf{w}</em>{t+1} = \mathbf{w}_t - \eta \mathbf{v}_t
$$</p><p>动量相当于"惯性"，可以帮助算法穿越平坦区域，减少振荡。</p><h4 id=16-自适应学习率>1.6 自适应学习率<a hidden class=anchor aria-hidden=true href=#16-自适应学习率>#</a></h4><p><strong>AdaGrad</strong>：为每个参数使用不同的学习率：
$$
\mathbf{w}<em>{t+1, i} = \mathbf{w}</em>{t, i} - \frac{\eta}{\sqrt{G_{t, ii} + \epsilon}} \nabla L(\mathbf{w}_t)_i
$$</p><p>其中 $G_{t, ii} = \sum_{j=1}^t (\nabla L(\mathbf{w}_j)_i)^2$ 是历史梯度的平方和。</p><p><strong>RMSProp</strong>：使用移动平均代替累积求和：
$$
G_{t, ii} = \beta G_{t-1, ii} + (1 - \beta)(\nabla L(\mathbf{w}_t)_i)^2
$$</p><p><img alt=学习率对收敛的影响 loading=lazy src=/images/math/learning-rate-comparison.png></p><p>图4：不同学习率对梯度下降收敛的影响。绿色线（学习率过小）收敛慢；蓝色线（学习率适中）快速收敛；红色线（学习率过大）振荡；紫色线（动量方法）加速收敛。</p><h3 id=2-拉格朗日乘数法>2. 拉格朗日乘数法<a hidden class=anchor aria-hidden=true href=#2-拉格朗日乘数法>#</a></h3><h4 id=21-约束优化问题>2.1 约束优化问题<a hidden class=anchor aria-hidden=true href=#21-约束优化问题>#</a></h4><p>考虑等式约束优化：
$$
\min f(\mathbf{x}) \quad \text{s.t.} \quad g(\mathbf{x}) = 0
$$</p><p><strong>拉格朗日函数</strong>：
$$
\mathcal{L}(\mathbf{x}, \lambda) = f(\mathbf{x}) + \lambda g(\mathbf{x})
$$</p><p><strong>KKT条件</strong>（Karush-Kuhn-Tucker）：
$$
\nabla_{\mathbf{x}} \mathcal{L} = \mathbf{0}, \quad g(\mathbf{x}) = 0
$$</p><h4 id=22-几何解释>2.2 几何解释<a hidden class=anchor aria-hidden=true href=#22-几何解释>#</a></h4><p>在最优解处，$\nabla f$ 必须与 $\nabla g$ 平行（否则可以沿约束曲面移动以降低 $f$）：
$$
\nabla f = -\lambda \nabla g
$$</p><h4 id=23-在svm中的应用>2.3 在SVM中的应用<a hidden class=anchor aria-hidden=true href=#23-在svm中的应用>#</a></h4><p><strong>硬间隔SVM</strong>：
$$
\min_{\mathbf{w}, b} \frac{1}{2}\lVert \mathbf{w} \rVert^2 \quad \text{s.t.} \quad y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1
$$</p><p>对偶问题：
$$
\max_{\alpha_i} \sum_i \alpha_i - \frac{1}{2}\sum_{i,j} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^\top \mathbf{x}_j
$$</p><h3 id=3-信息论中的微积分>3. 信息论中的微积分<a hidden class=anchor aria-hidden=true href=#3-信息论中的微积分>#</a></h3><h4 id=31-熵的定义与微分>3.1 熵的定义与微分<a hidden class=anchor aria-hidden=true href=#31-熵的定义与微分>#</a></h4><p><strong>信息熵</strong>：
$$
H(p) = -\sum_{i=1}^n p_i \log p_i
$$</p><p><strong>微分熵</strong>（连续变量）：
$$
h(p) = -\int p(x)\log p(x)dx
$$</p><h4 id=32-交叉熵与kl散度>3.2 交叉熵与KL散度<a hidden class=anchor aria-hidden=true href=#32-交叉熵与kl散度>#</a></h4><p><strong>交叉熵</strong>：
$$
H(p, q) = -\sum_i p_i \log q_i
$$</p><p><strong>KL散度</strong>（Kullback-Leibler divergence）：
$$
D_{KL}(p | q) = \sum_i p_i \log \frac{p_i}{q_i}
$$</p><p><strong>性质</strong>：$D_{KL}(p | q) \geq 0$，等号成立当且仅当 $p = q$。</p><p><strong>证明</strong>（使用Jensen不等式）：
$$
D_{KL}(p | q) = \mathbb{E}_p\left[\log \frac{p}{q}\right] = -\mathbb{E}_p\left[\log \frac{q}{p}\right] \geq -\log \mathbb{E}_p\left[\frac{q}{p}\right] = -\log 1 = 0
$$</p><h4 id=33-最大熵原理>3.3 最大熵原理<a hidden class=anchor aria-hidden=true href=#33-最大熵原理>#</a></h4><p><strong>原理</strong>：在所有满足约束的概率分布中，选择熵最大的分布。</p><p><strong>应用</strong>：高斯分布是给定方差下熵最大的分布。</p><hr><h2 id=第三部分深度学习中的微积分>第三部分：深度学习中的微积分<a hidden class=anchor aria-hidden=true href=#第三部分深度学习中的微积分>#</a></h2><h3 id=1-反向传播算法>1. 反向传播算法<a hidden class=anchor aria-hidden=true href=#1-反向传播算法>#</a></h3><h4 id=11-神经网络的前向传播>1.1 神经网络的前向传播<a hidden class=anchor aria-hidden=true href=#11-神经网络的前向传播>#</a></h4><p>考虑一个简单的两层神经网络：
$$
\mathbf{z}^{(1)} = W^{(1)} \mathbf{x} + \mathbf{b}^{(1)}
$$
$$
\mathbf{a}^{(1)} = \sigma(\mathbf{z}^{(1)})
$$
$$
\mathbf{z}^{(2)} = W^{(2)} \mathbf{a}^{(1)} + \mathbf{b}^{(2)}
$$
$$
\hat{\mathbf{y}} = \sigma(\mathbf{z}^{(2)})
$$</p><p>损失函数：$L = \frac{1}{2}\lVert \hat{\mathbf{y}} - \mathbf{y} \rVert^2$</p><h4 id=12-反向传播的数学推导>1.2 反向传播的数学推导<a hidden class=anchor aria-hidden=true href=#12-反向传播的数学推导>#</a></h4><p>我们需要计算损失函数对参数的梯度。</p><p><strong>输出层梯度</strong>：
$$
\frac{\partial L}{\partial \mathbf{z}^{(2)}} = (\hat{\mathbf{y}} - \mathbf{y}) \odot \sigma^{\prime}(\mathbf{z}^{(2)})
$$</p><p>其中 $\odot$ 是逐元素乘法，$\sigma^{\prime}(z) = \sigma(z)(1 - \sigma(z))$ 是Sigmoid的导数。</p><p><strong>隐藏层梯度</strong>（链式法则）：
$$
\frac{\partial L}{\partial \mathbf{z}^{(1)}} = \left[(W^{(2)})^\top \frac{\partial L}{\partial \mathbf{z}^{(2)}}\right] \odot \sigma^{\prime}(\mathbf{z}^{(1)})
$$</p><p><strong>权重梯度</strong>：
$$
\frac{\partial L}{\partial W^{(2)}} = \frac{\partial L}{\partial \mathbf{z}^{(2)}} (\mathbf{a}^{(1)})^\top
$$
$$
\frac{\partial L}{\partial W^{(1)}} = \frac{\partial L}{\partial \mathbf{z}^{(1)}} \mathbf{x}^\top
$$</p><p>这就是<strong>反向传播</strong>：从输出层反向传播到输入层，使用链式法则计算每一层的梯度。</p><h4 id=13-计算图与自动微分>1.3 计算图与自动微分<a hidden class=anchor aria-hidden=true href=#13-计算图与自动微分>#</a></h4><p><strong>计算图</strong>将计算表示为有向无环图(DAG)。每个节点是一个操作，边表示数据流。</p><p><strong>自动微分</strong>（Automatic Differentiation）：</p><ul><li><strong>前向模式</strong>：从输入到输出计算导数</li><li><strong>反向模式</strong>：从输出到输入计算导数（反向传播）</li></ul><p><strong>优势</strong>：精确计算导数（无截断误差），复杂度与输出维度成正比。</p><h4 id=14-梯度消失问题>1.4 梯度消失问题<a hidden class=anchor aria-hidden=true href=#14-梯度消失问题>#</a></h4><p>在深层网络中，梯度可能指数级衰减。考虑 $L$ 层线性网络：
$$
\frac{\partial L}{\partial \mathbf{x}} = (W^{(L)})^\top \cdots (W^{(1)})^\top \frac{\partial L}{\partial \mathbf{y}}
$$</p><p>如果权重矩阵的奇异值都小于1，梯度会指数级衰减 → <strong>梯度消失</strong>。</p><p><strong>解决方案</strong>：</p><ul><li><strong>ReLU激活</strong>：导数为0或1，不会衰减</li><li><strong>残差连接</strong>：提供"梯度高速公路"</li><li><strong>层归一化</strong>：规范化激活值分布</li></ul><h3 id=2-激活函数的导数>2. 激活函数的导数<a hidden class=anchor aria-hidden=true href=#2-激活函数的导数>#</a></h3><h4 id=21-sigmoid函数>2.1 Sigmoid函数<a hidden class=anchor aria-hidden=true href=#21-sigmoid函数>#</a></h4><p><strong>定义</strong>：$\sigma(z) = \frac{1}{1 + e^{-z}}$</p><p><strong>导数</strong>：$\sigma^{\prime}(z) = \sigma(z)(1 - \sigma(z))$</p><p><strong>问题</strong>：</p><ul><li>梯度消失：当 $\lvert z \rvert$ 很大时，$\sigma^{\prime}(z) \approx 0$</li><li>输出不以零为中心</li></ul><h4 id=22-tanh函数>2.2 Tanh函数<a hidden class=anchor aria-hidden=true href=#22-tanh函数>#</a></h4><p><strong>定义</strong>：$\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$</p><p><strong>导数</strong>：$\tanh^{\prime}(z) = 1 - \tanh^2(z)$</p><p><strong>优势</strong>：输出以零为中心。</p><h4 id=23-relu函数>2.3 ReLU函数<a hidden class=anchor aria-hidden=true href=#23-relu函数>#</a></h4><p><strong>定义</strong>：$\text{ReLU}(z) = \max(0, z)$</p><p><strong>导数</strong>：$\text{ReLU}^{\prime}(z) = \begin{cases} 1 & z > 0 \ 0 & z \leq 0 \end{cases}$</p><p><strong>优势</strong>：</p><ul><li>缓解梯度消失</li><li>计算简单</li><li>稀疏激活</li></ul><p><strong>问题</strong>：Dead ReLU（神经元"死亡"）。</p><p><img alt=激活函数比较 loading=lazy src=/images/math/activation-functions.png></p><p>图5：常见激活函数及其导数的比较。Sigmoid和Tanh的导数在两端趋于0（梯度消失），ReLU的导数恒为0或1（避免梯度消失）。</p><h3 id=3-正则化的微积分视角>3. 正则化的微积分视角<a hidden class=anchor aria-hidden=true href=#3-正则化的微积分视角>#</a></h3><h4 id=31-l2正则权重衰减>3.1 L2正则（权重衰减）<a hidden class=anchor aria-hidden=true href=#31-l2正则权重衰减>#</a></h4><p><strong>目标</strong>：
$$
\min_{\mathbf{w}} L(\mathbf{w}) + \frac{\lambda}{2}\lVert \mathbf{w} \rVert^2
$$</p><p><strong>梯度</strong>：
$$
\nabla_{\mathbf{w}} = \nabla L(\mathbf{w}) + \lambda \mathbf{w}
$$</p><p><strong>几何意义</strong>：限制参数空间，防止过拟合。</p><h4 id=32-l1正则>3.2 L1正则<a hidden class=anchor aria-hidden=true href=#32-l1正则>#</a></h4><p><strong>目标</strong>：
$$
\min_{\mathbf{w}} L(\mathbf{w}) + \lambda \lVert \mathbf{w} \rVert_1
$$</p><p><strong>次梯度</strong>：
$$
\partial_{\mathbf{w}} = \nabla L(\mathbf{w}) + \lambda \cdot \text{sign}(\mathbf{w})
$$</p><p><strong>几何意义</strong>：产生稀疏解（特征选择）。</p><h3 id=4-优化算法的演进>4. 优化算法的演进<a hidden class=anchor aria-hidden=true href=#4-优化算法的演进>#</a></h3><h4 id=41-从sgd到adam>4.1 从SGD到Adam<a hidden class=anchor aria-hidden=true href=#41-从sgd到adam>#</a></h4><p><strong>SGD</strong>（随机梯度下降）：
$$
\mathbf{w}_{t+1} = \mathbf{w}_t - \eta \nabla L(\mathbf{w}_t)
$$</p><p><strong>Momentum</strong>（动量）：
$$
\mathbf{v}<em>t = \beta \mathbf{v}</em>{t-1} + (1 - \beta)\nabla L(\mathbf{w}<em>t)
$$
$$
\mathbf{w}</em>{t+1} = \mathbf{w}_t - \eta \mathbf{v}_t
$$</p><p><strong>Adam</strong>（Adaptive Moment Estimation）：结合动量和自适应学习率：
$$
\mathbf{m}<em>t = \beta_1 \mathbf{m}</em>{t-1} + (1 - \beta_1)\nabla L(\mathbf{w}_t)
$$
$$
\mathbf{v}<em>t = \beta_2 \mathbf{v}</em>{t-1} + (1 - \beta_2)(\nabla L(\mathbf{w}<em>t))^2
$$
$$
\mathbf{w}</em>{t+1} = \mathbf{w}_t - \frac{\eta}{\sqrt{\hat{\mathbf{v}}_t + \epsilon}} \hat{\mathbf{m}}_t
$$</p><p>其中 $\hat{\mathbf{m}}_t = \frac{\mathbf{m}_t}{1 - \beta_1^t}$ 和 $\hat{\mathbf{v}}_t = \frac{\mathbf{v}_t}{1 - \beta_2^t}$ 是偏差修正后的估计。</p><p>Adam是现代深度学习的<strong>默认优化器</strong>。</p><h4 id=42-二阶优化牛顿法>4.2 二阶优化：牛顿法<a hidden class=anchor aria-hidden=true href=#42-二阶优化牛顿法>#</a></h4><p><strong>牛顿法</strong>使用二阶导数（Hessian矩阵）：
$$
\mathbf{w}_{t+1} = \mathbf{w}_t - H^{-1} \nabla L(\mathbf{w}_t)
$$</p><p>其中 $H_{ij} = \frac{\partial^2 L}{\partial w_i \partial w_j}$。</p><p><strong>优点</strong>：二阶收敛（接近最优点时非常快）
<strong>缺点</strong>：计算Hessian矩阵代价高（$O(d^2)$），可能不正定</p><p><strong>L-BFGS</strong>：拟牛顿法，用一阶信息近似Hessian，避免显式计算二阶导数。</p><p><img alt="梯度下降 vs 牛顿法" loading=lazy src=/images/math/gd-vs-newton.png></p><p>图6：梯度下降与牛顿法的收敛速度比较。红色线（梯度下降）线性收敛，绿色线（牛顿法）二次收敛，快速到达最小值。</p><hr><h2 id=第四部分高级主题>第四部分：高级主题<a hidden class=anchor aria-hidden=true href=#第四部分高级主题>#</a></h2><h3 id=1-变分法>1. 变分法<a hidden class=anchor aria-hidden=true href=#1-变分法>#</a></h3><h4 id=11-泛函的极值问题>1.1 泛函的极值问题<a hidden class=anchor aria-hidden=true href=#11-泛函的极值问题>#</a></h4><p><strong>泛函</strong>是函数的函数：$J[y] = \int_{x_1}^{x_2} F(x, y, y^{\prime})dx$</p><p><strong>变分</strong>（Variation）：$\delta y = \epsilon \eta(x)$，其中 $\eta(x_1) = \eta(x_2) = 0$</p><p><strong>欧拉-拉格朗日方程</strong>：
$$
\frac{\partial F}{\partial y} - \frac{d}{dx}\left(\frac{\partial F}{\partial y^{\prime}}\right) = 0
$$</p><p><strong>推导</strong>：
$$
\delta J = \int \left(\frac{\partial F}{\partial y}\delta y + \frac{\partial F}{\partial y^{\prime}}\delta y^{\prime}\right)dx = \int \left(\frac{\partial F}{\partial y} - \frac{d}{dx}\frac{\partial F}{\partial y^{\prime}}\right)\delta y,dx = 0
$$</p><p>由于 $\delta y$ 任意，被积函数必须为零。</p><h4 id=12-在变分自编码器vae中的应用>1.2 在变分自编码器（VAE）中的应用<a hidden class=anchor aria-hidden=true href=#12-在变分自编码器vae中的应用>#</a></h4><p><strong>ELBO</strong>（Evidence Lower Bound）：
$$
\mathcal{L} = \mathbb{E}<em>{q</em>{\phi}(z|x)}[\log p_{\theta}(x|z)] - D_{KL}(q_{\phi}(z|x) | p(z))
$$</p><p>通过变分推断优化 $\phi$ 和 $\theta$。</p><h3 id=2-矩阵微积分>2. 矩阵微积分<a hidden class=anchor aria-hidden=true href=#2-矩阵微积分>#</a></h3><h4 id=21-矩阵求导法则>2.1 矩阵求导法则<a hidden class=anchor aria-hidden=true href=#21-矩阵求导法则>#</a></h4><p><strong>标量对向量</strong>：
$$
\frac{\partial f}{\partial \mathbf{x}} = \begin{pmatrix} \frac{\partial f}{\partial x_1} \ \vdots \ \frac{\partial f}{\partial x_n} \end{pmatrix}
$$</p><p><strong>标量对矩阵</strong>：
$$
\frac{\partial f}{\partial X} = \begin{pmatrix} \frac{\partial f}{\partial X_{11}} & \cdots \ \vdots & \ddots \end{pmatrix}
$$</p><p><strong>常用公式</strong>：</p><ul><li>$\frac{\partial}{\partial \mathbf{x}} \mathbf{a}^\top \mathbf{x} = \mathbf{a}$</li><li>$\frac{\partial}{\partial \mathbf{x}} \mathbf{x}^\top A \mathbf{x} = (A + A^\top)\mathbf{x}$</li><li>$\frac{\partial}{\partial X} \text{tr}(AX) = A^\top$</li></ul><h4 id=22-kronecker乘积>2.2 Kronecker乘积<a hidden class=anchor aria-hidden=true href=#22-kronecker乘积>#</a></h4><p><strong>定义</strong>：
$$
A \otimes B = \begin{pmatrix} a_{11}B & \cdots \ \vdots & \ddots \end{pmatrix}
$$</p><p><strong>性质</strong>：</p><ul><li>$(A \otimes B)(C \otimes D) = (AC) \otimes (BD)$</li><li>$\text{vec}(ABC) = (C^\top \otimes A)\text{vec}(B)$</li></ul><h3 id=3-微分几何初步>3. 微分几何初步<a hidden class=anchor aria-hidden=true href=#3-微分几何初步>#</a></h3><h4 id=31-流形与切空间>3.1 流形与切空间<a hidden class=anchor aria-hidden=true href=#31-流形与切空间>#</a></h4><p><strong>流形</strong>：局部像欧几里得空间的拓扑空间。</p><p><strong>切空间</strong> $T_pM$：流形在点 $p$ 处的"线性化"。</p><p><strong>切向量</strong>：$\mathbf{v} = \frac{d}{dt}\big|_{t=0} \gamma(t)$，其中 $\gamma(t)$ 是曲线。</p><h4 id=32-黎曼度量>3.2 黎曼度量<a hidden class=anchor aria-hidden=true href=#32-黎曼度量>#</a></h4><p><strong>度量张量</strong> $g_p$：定义切空间上的内积：
$$
\langle \mathbf{u}, \mathbf{v} \rangle_p = g_p(\mathbf{u}, \mathbf{v})
$$</p><p><strong>测地线</strong>：&ldquo;最短路径"曲线，满足测地线方程：
$$
\frac{d^2 x^\mu}{dt^2} + \Gamma^\mu_{\alpha\beta} \frac{dx^\alpha}{dt} \frac{dx^\beta}{dt} = 0
$$</p><p>其中 $\Gamma^\mu_{\alpha\beta}$ 是<strong>Christoffel符号</strong>。</p><h4 id=33-梯度流>3.3 梯度流<a hidden class=anchor aria-hidden=true href=#33-梯度流>#</a></h4><p>在黎曼流形上，梯度下降变为<strong>梯度流</strong>：
$$
\frac{d\mathbf{x}}{dt} = -\text{grad } f(\mathbf{x})
$$</p><p>其中 $\text{grad } f$ 由度量定义：
$$
g(\text{grad } f, \mathbf{v}) = df(\mathbf{v}) = \nabla f^\top \mathbf{v}
$$</p><h3 id=4-随机微积分>4. 随机微积分<a hidden class=anchor aria-hidden=true href=#4-随机微积分>#</a></h3><h4 id=41-随机过程的微分>4.1 随机过程的微分<a hidden class=anchor aria-hidden=true href=#41-随机过程的微分>#</a></h4><p><strong>布朗运动</strong> $W_t$：高斯随机过程，满足：</p><ul><li>$W_0 = 0$</li><li>独立增量</li><li>$W_t - W_s \sim \mathcal{N}(0, t-s)$</li></ul><h4 id=42-伊藤积分>4.2 伊藤积分<a hidden class=anchor aria-hidden=true href=#42-伊藤积分>#</a></h4><p><strong>定义</strong>：
$$
I_T = \int_0^T H_t , dW_t
$$</p><p><strong>伊藤公式</strong>（链式法则的随机版本）：
$$
df(X_t) = f^{\prime}(X_t)dX_t + \frac{1}{2}f^{\prime\prime}(X_t)d\langle X \rangle_t
$$</p><p>其中 $d\langle X \rangle_t$ 是二次变差。</p><h4 id=43-在扩散模型中的应用>4.3 在扩散模型中的应用<a hidden class=anchor aria-hidden=true href=#43-在扩散模型中的应用>#</a></h4><p><strong>SDE</strong>（随机微分方程）：
$$
d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt + g(t)d\mathbf{w}_t
$$</p><p><strong>扩散过程</strong>：前向过程逐渐添加噪声，反向过程去噪生成样本。</p><hr><h2 id=第五部分关键洞察与展望>第五部分：关键洞察与展望<a hidden class=anchor aria-hidden=true href=#第五部分关键洞察与展望>#</a></h2><h3 id=1-微积分与几何>1. 微积分与几何<a hidden class=anchor aria-hidden=true href=#1-微积分与几何>#</a></h3><p>微积分本质上是<strong>几何</strong>：</p><ul><li>导数是切线的斜率</li><li>梯度指向最速上升方向</li><li>二阶导数描述曲率</li><li>积分计算曲线下的面积</li></ul><p>理解这些几何直观有助于理解优化算法的行为。</p><h3 id=2-线性化的重要性>2. 线性化的重要性<a hidden class=anchor aria-hidden=true href=#2-线性化的重要性>#</a></h3><p>现代人工智能的核心思想是<strong>局部线性化</strong>：</p><ul><li>神经网络是复杂的非线性函数</li><li>但在每个参数点附近，可以用线性函数逼近</li><li>通过不断的线性近似和迭代，找到全局最优</li></ul><p>泰勒展开是线性化的数学工具：
$$
f(\mathbf{x} + \Delta \mathbf{x}) \approx f(\mathbf{x}) + \nabla f(\mathbf{x})^\top \Delta \mathbf{x} + \frac{1}{2}\Delta \mathbf{x}^\top H \Delta \mathbf{x}
$$</p><h3 id=3-链式法则的威力>3. 链式法则的威力<a hidden class=anchor aria-hidden=true href=#3-链式法则的威力>#</a></h3><p>链式法则使得我们可以计算<strong>任意复合函数</strong>的导数。神经网络本质上是一个巨大的复合函数，反向传播算法就是链式法则的高效实现。</p><p>现代深度学习框架（PyTorch, TensorFlow）使用<strong>自动微分</strong>（automatic differentiation）来自动计算梯度，让开发者专注于模型架构而非数学细节。</p><h3 id=4-优化的艺术>4. 优化的艺术<a hidden class=anchor aria-hidden=true href=#4-优化的艺术>#</a></h3><p>梯度下降看似简单，但有许多改进空间：</p><ul><li><strong>动量</strong>：利用历史信息加速收敛</li><li><strong>自适应学习率</strong>：为每个参数定制步长</li><li><strong>二阶方法</strong>：利用曲率信息更快收敛</li><li><strong>随机性</strong>：SGD的噪声有助于跳出局部最优</li></ul><h3 id=5-未来展望>5. 未来展望<a hidden class=anchor aria-hidden=true href=#5-未来展望>#</a></h3><p><strong>扩散模型的随机微积分</strong>：理解SDE的解对改进扩散模型至关重要。</p><p><strong>神经符号AI</strong>：结合神经网络和符号推理，需要新的微积分工具。</p><p><strong>优化理论</strong>：非凸优化、分布式优化的理论仍在发展中。</p><p><strong>量子机器学习</strong>：量子微积分可能带来新的优化方法。</p><hr><h2 id=结语微积分与ai的未来>结语：微积分与AI的未来<a hidden class=anchor aria-hidden=true href=#结语微积分与ai的未来>#</a></h2><p>从17世纪牛顿和莱布尼茨发明微积分，到21世纪的深度学习革命，微积分一直是描述变化的数学语言。</p><p>在这篇文章中，我们深入探讨了：</p><ol><li><strong>导数与微分</strong>：从变化率到梯度，从线性化到链式法则</li><li><strong>梯度下降</strong>：最优化算法的基础，几何直观与数学严格</li><li><strong>反向传播</strong>：链式法则的矩阵形式，计算图与自动微分</li><li><strong>优化算法</strong>：从SGD到Adam，从一阶到二阶方法</li><li><strong>高级主题</strong>：变分法、矩阵微积分、微分几何、随机微积分</li></ol><p>微积分不仅提供了计算梯度的方法，更重要的是，它培养了一种<strong>思维方式</strong>：<strong>用局部变化理解全局行为</strong>，<strong>用线性逼近处理非线性问题</strong>。</p><p>在未来，随着人工智能的发展，微积分将继续发挥核心作用。无论是扩散模型的随机微积分，还是神经符号AI的微积分基础，都需要深厚的微积分功底。</p><p>理解微积分，不仅是掌握一门数学工具，更是培养一种分析问题、解决问题、创新思考的能力。正如伟大的数学家柯西所说：&ldquo;微积分是人类智慧的结晶。&rdquo;</p><hr><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><ol><li>Spivak, M. (2008). <em>Calculus On Manifolds</em> (4th ed.). Publish or Perish.</li><li>Bishop, C. M. (2006). <em>Pattern Recognition and Machine Learning</em>. Springer.</li><li>Goodfellow, I., Bengio, Y., & Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</li><li>Boyd, S., & Vandenberghe, L. (2004). <em>Convex Optimization</em>. Cambridge University Press.</li><li>Nielsen, M. A. (2015). <em>Neural Networks and Deep Learning</em>. Determination Press.</li><li>Rudin, W. (1976). <em>Principles of Mathematical Analysis</em> (3rd ed.). McGraw-Hill.</li><li>Lee, J. M. (2018). <em>Introduction to Riemannian Manifolds</em> (2nd ed.). Springer.</li><li>Oksendal, B. (2003). <em>Stochastic Differential Equations</em> (6th ed.). Springer.</li><li>Nocedal, J., & Wright, S. J. (2006). <em>Numerical Optimization</em> (2nd ed.). Springer.</li><li>Petersen, K. B., & Pedersen, M. S. (2012). <em>The Matrix Cookbook</em>. Technical University of Denmark.</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E5%BE%AE%E7%A7%AF%E5%88%86/>微积分</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%95%B0%E5%AD%A6/>数学</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%AE%97%E6%B3%95/>算法</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-26-connection-differential-geometry/><span class=title>« Prev</span><br><span>微分几何中的联络：一场从直观到严格的数学之旅</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-25-spectral-theorem/><span class=title>Next »</span><br><span>谱定理：线性代数的优雅与机器学习的基石</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 微积分与机器学习：从变化率到神经网络梯度的完整旅程 on x" href="https://x.com/intent/tweet/?text=%e5%be%ae%e7%a7%af%e5%88%86%e4%b8%8e%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%ef%bc%9a%e4%bb%8e%e5%8f%98%e5%8c%96%e7%8e%87%e5%88%b0%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%a2%af%e5%ba%a6%e7%9a%84%e5%ae%8c%e6%95%b4%e6%97%85%e7%a8%8b&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-25-calculus-ml-systematic-review%2f&amp;hashtags=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%2c%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%2c%e5%be%ae%e7%a7%af%e5%88%86%2c%e6%95%b0%e5%ad%a6%2c%e7%ae%97%e6%b3%95"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 微积分与机器学习：从变化率到神经网络梯度的完整旅程 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-25-calculus-ml-systematic-review%2f&amp;title=%e5%be%ae%e7%a7%af%e5%88%86%e4%b8%8e%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%ef%bc%9a%e4%bb%8e%e5%8f%98%e5%8c%96%e7%8e%87%e5%88%b0%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%a2%af%e5%ba%a6%e7%9a%84%e5%ae%8c%e6%95%b4%e6%97%85%e7%a8%8b&amp;summary=%e5%be%ae%e7%a7%af%e5%88%86%e4%b8%8e%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%ef%bc%9a%e4%bb%8e%e5%8f%98%e5%8c%96%e7%8e%87%e5%88%b0%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%a2%af%e5%ba%a6%e7%9a%84%e5%ae%8c%e6%95%b4%e6%97%85%e7%a8%8b&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-25-calculus-ml-systematic-review%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 微积分与机器学习：从变化率到神经网络梯度的完整旅程 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-25-calculus-ml-systematic-review%2f&title=%e5%be%ae%e7%a7%af%e5%88%86%e4%b8%8e%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%ef%bc%9a%e4%bb%8e%e5%8f%98%e5%8c%96%e7%8e%87%e5%88%b0%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%a2%af%e5%ba%a6%e7%9a%84%e5%ae%8c%e6%95%b4%e6%97%85%e7%a8%8b"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 微积分与机器学习：从变化率到神经网络梯度的完整旅程 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-25-calculus-ml-systematic-review%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var n,t=e.innerHTML,s=!1;for(t.indexOf("<em>")!==-1&&(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("</em>")!==-1&&(n=t.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),n=n.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("<em>$")!==-1&&(n=t.replace(/<em>\$/g,"$"),n!==t&&(t=n,s=!0)),t.indexOf("</em>$")!==-1&&(n=t.replace(/<\/em>\$/g,"$"),n!==t&&(t=n,s=!0));t.indexOf("<em>")!==-1&&t.indexOf("</em>")!==-1;){if(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n===t)break;t=n,s=!0}t.indexOf("_{")!==-1&&(n=t.replace(/_{/g,"_{"),n!==t&&(t=n,s=!0)),t.indexOf("}_")!==-1&&(n=t.replace(/}_/g,"}"),n!==t&&(t=n,s=!0)),t.indexOf("\\\\")!==-1&&(n=t.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),n!==t&&(t=n,s=!0)),s&&(e.innerHTML=t)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var n,t=e.innerHTML,s=!1;for(t.indexOf("<em>")!==-1&&(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("</em>")!==-1&&(n=t.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),n=n.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("<em>$")!==-1&&(n=t.replace(/<em>\$/g,"$"),n!==t&&(t=n,s=!0)),t.indexOf("</em>$")!==-1&&(n=t.replace(/<\/em>\$/g,"$"),n!==t&&(t=n,s=!0));t.indexOf("<em>")!==-1&&t.indexOf("</em>")!==-1;){if(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n===t)break;t=n,s=!0}t.indexOf("_{")!==-1&&(n=t.replace(/_{/g,"_{"),n!==t&&(t=n,s=!0)),t.indexOf("}_")!==-1&&(n=t.replace(/}_/g,"}"),n!==t&&(t=n,s=!0)),t.indexOf("\\\\")!==-1&&(n=t.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),n!==t&&(t=n,s=!0)),s&&(e.innerHTML=t)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>