<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>张量：从数学抽象到深度学习核心的系统综述 | s-ai-unix's Blog</title><meta name=keywords content="张量,深度学习,综述,线性代数"><meta name=description content="深入浅出解析张量的数学原理与广泛应用，从张量代数到深度学习，从物理场论到数据分析，完整呈现张量的力量"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-29-tensor-comprehensive-guide/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-29-tensor-comprehensive-guide/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-29-tensor-comprehensive-guide/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="张量：从数学抽象到深度学习核心的系统综述"><meta property="og:description" content="深入浅出解析张量的数学原理与广泛应用，从张量代数到深度学习，从物理场论到数据分析，完整呈现张量的力量"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-29T08:00:00+08:00"><meta property="article:modified_time" content="2026-01-29T08:00:00+08:00"><meta property="article:tag" content="张量"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="综述"><meta property="article:tag" content="线性代数"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/tensor-cover.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/tensor-cover.jpg"><meta name=twitter:title content="张量：从数学抽象到深度学习核心的系统综述"><meta name=twitter:description content="深入浅出解析张量的数学原理与广泛应用，从张量代数到深度学习，从物理场论到数据分析，完整呈现张量的力量"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"张量：从数学抽象到深度学习核心的系统综述","item":"https://s-ai-unix.github.io/posts/2026-01-29-tensor-comprehensive-guide/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"张量：从数学抽象到深度学习核心的系统综述","name":"张量：从数学抽象到深度学习核心的系统综述","description":"深入浅出解析张量的数学原理与广泛应用，从张量代数到深度学习，从物理场论到数据分析，完整呈现张量的力量","keywords":["张量","深度学习","综述","线性代数"],"articleBody":"引言：多维世界的数学语言 想象你正在观察一个正在旋转的陀螺。描述它需要多少参数？\n位置：$3$ 个坐标 $(x, y, z)$ 方向：$3$ 个欧拉角 角速度：$3$ 个分量 转动惯量：$9$ 个数（$3 \\times 3$ 矩阵） 这些量不仅仅是数字的集合，它们有特定的变换规则。当坐标系旋转时，位置和角速度按向量规则变换，而转动惯量则按更复杂的规则变换——这就是张量。\n在物理学中，张量是描述场的通用语言。爱因斯坦的广义相对论用张量写下：\n$$G_{\\mu\\nu} + \\Lambda g_{\\mu\\nu} = \\frac{8\\pi G}{c^4} T_{\\mu\\nu}$$\n在深度学习中，一张 $224 \\times 224$ 的彩色图像是 $224 \\times 224 \\times 3$ 的三阶张量。一批 $32$ 张这样的图像是 $32 \\times 224 \\times 224 \\times 3$ 的四阶张量。\n本文将带你走进张量的世界，从数学定义到物理直觉，从代数运算到现代应用，理解为什么张量成为描述复杂系统的核心工具。\n第一章：张量的本质——超越矩阵的多维数组 1.1 从标量到张量 在数学中，我们熟悉不同维度的对象：\n图 1：张量的维度层级。从0阶标量（单个数字）到1阶向量、2阶矩阵，再到3阶及更高阶张量，维度不断增加。\n*0阶张量：标量\n标量只有一个数值，没有方向：\n$$a = 5, \\quad T = 300\\text{K}, \\quad E = mc^2$$\n标量在坐标变换下不变——无论你从哪个角度看，温度始终是 $300$K。\n*1阶张量：向量\n向量有大小和方向：\n$$\\mathbf{v} = (v_1, v_2, v_3) = v_1 \\mathbf{e}_1 + v_2 \\mathbf{e}_2 + v_3 \\mathbf{e}_3$$\n当坐标系旋转时，向量的分量按特定规则变换：\n$$v’i = \\sum{j=1}^{3} R_{ij} v_j$$\n其中 $R_{ij}$ 是旋转矩阵。\n*2阶张量：矩阵\n矩阵可以看作向量的推广：\n$$\\mathbf{A} = \\begin{pmatrix} a_{11} \u0026 a_{12} \u0026 a_{13} \\ a_{21} \u0026 a_{22} \u0026 a_{23} \\ a_{31} \u0026 a_{32} \u0026 a_{33} \\end{pmatrix}$$\n在坐标变换下，矩阵元素变换为：\n$$a’{ij} = \\sum{k,l} R_{ik} R_{jl} a_{kl}$$\n*3阶及以上张量\n高阶张量有更多指标。一个 $n$ 阶张量有 $n$ 个指标，在 $d$ 维空间中有 $d^n$ 个分量。\n1.2 张量的严格定义 定义：张量是一个多线性映射，它在坐标变换下按特定规则变换。\n具体来说，一个 $(r, s)$ 型张量（$r$ 个逆变指标，$s$ 个协变指标）的变换规则为：\n$$T’^{i_1 \\cdots i_r}{j_1 \\cdots j_s} = \\frac{\\partial x’^{i_1}}{\\partial x^{k_1}} \\cdots \\frac{\\partial x’^{i_r}}{\\partial x^{k_r}} \\frac{\\partial x^{l_1}}{\\partial x’^{j_1}} \\cdots \\frac{\\partial x^{l_s}}{\\partial x’^{j_s}} T^{k_1 \\cdots k_r}{l_1 \\cdots l_s}$$\n这个看似复杂的公式其实捕捉了一个简单思想：张量描述的是独立于坐标系的物理/几何对象。\n1.3 逆变与协变 为什么需要区分逆变和协变？\n考虑速度 $\\mathbf{v}$ 和梯度 $\\nabla f$：\n速度是逆变的：当坐标轴伸长时，速度分量变小（走完相同距离需要更少的\"单位\"） 梯度是协变的：当坐标轴伸长时，梯度分量变大（相同距离上有更多的\"单位\"变化） 数学上，逆变向量用上标 $v^i$，协变向量用下标 $v_i$。\n第二章：张量运算——代数的力量 2.1 基本运算 加法：同阶张量可以逐元素相加\n$$(\\mathbf{A} + \\mathbf{B}){ijk} = A{ijk} + B_{ijk}$$\n数乘：张量可以乘以标量\n$$(c \\mathbf{A}){ijk} = c \\cdot A{ijk}$$\n重要性质：张量的阶在加法和数乘下保持不变。\n2.2 张量积（外积） 张量积是将两个张量组合成更高阶张量的操作。\n图 2：张量积（外积）的可视化。两个向量的外积产生一个矩阵，其中每个元素是相应向量分量的乘积。\n给定两个向量 $\\mathbf{u} \\in \\mathbb{R}^m$ 和 $\\mathbf{v} \\in \\mathbb{R}^n$，它们的外积为：\n$$(\\mathbf{u} \\otimes \\mathbf{v})_{ij} = u_i \\cdot v_j$$\n结果是一个 $m \\times n$ 矩阵。\n例子：\n$$\\mathbf{u} = \\begin{pmatrix} 1 \\ 2 \\ 3 \\end{pmatrix}, \\quad \\mathbf{v} = \\begin{pmatrix} 4 \\ 5 \\end{pmatrix}$$\n$$\\mathbf{u} \\otimes \\mathbf{v} = \\begin{pmatrix} 4 \u0026 5 \\ 8 \u0026 10 \\ 12 \u0026 15 \\end{pmatrix}$$\n2.3 缩并（迹的推广） 缩并是对张量的两个指标求和，降低阶数。\n对于矩阵，缩并就是迹：\n$$\\text{tr}(\\mathbf{A}) = \\sum_{i} A_{ii}$$\n对于高阶张量 $\\mathbf{T}_{ijk}$，缩并第1和第3指标：\n$$S_j = \\sum_{i} T_{iji}$$\n结果是一个向量（阶数从3降到1）。\n2.4 爱因斯坦求和约定 爱因斯坦引入了一个简洁的记号：重复的指标表示求和。\n例如，矩阵乘法：\n$$c_{ij} = \\sum_{k} a_{ik} b_{kj} \\quad \\text{写成} \\quad c_{ij} = a_{ik} b_{kj}$$\n向量内积：\n$$\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i} u_i v_i = u_i v^i$$\n约定规则：\n上标（逆变）和下标（协变）配对时求和 求和指标称为\"哑指标\"，可以任意重命名 结果中不再出现的指标是自由指标 2.5 线性变换的张量视角 图 3：线性变换的可视化。矩阵 $A$ 将向量 $v$ 映射到新的向量 $Av$，同时扭曲了整个空间（网格变形）。\n矩阵作为2阶张量，定义了向量空间之间的线性映射：\n$$\\mathbf{y} = \\mathbf{A} \\mathbf{x}$$\n或指标形式：\n$$y_i = A_{ij} x_j$$\n特征值与特征向量：\n寻找在变换下方向不变的向量：\n$$\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}$$\n这些特殊的向量（特征向量）和对应的缩放因子（特征值）揭示了变换的本质结构。\n第三章：张量在深度学习中的应用 3.1 数据表示的张量形式 深度学习处理的是各种形式的数据，它们都可以用张量表示：\n图 4：深度学习中的张量数据表示。从灰度图像（2D）到RGB图像（3D），批量图像（4D），再到序列数据（2D）。\n灰度图像：$H \\times W$ 的2阶张量\nMNIST：$28 \\times 28$ 医学影像：$512 \\times 512$ 彩色图像：$H \\times W \\times C$ 的3阶张量\n$C = 3$（RGB）或 $C = 4$（RGBA） ImageNet：$224 \\times 224 \\times 3$ 视频：$T \\times H \\times W \\times C$ 的4阶张量\n$T$ 是时间帧数 每秒 $30$ 帧，$10$ 秒视频有 $T = 300$ 批量数据：在Batch维度上堆叠\n一批 $32$ 张RGB图像：$32 \\times 224 \\times 224 \\times 3$ 文本数据：\n词嵌入：$T \\times D$（序列长度 × 嵌入维度） 批量句子：$B \\times T \\times D$ 3.2 神经网络中的张量运算 神经网络的前向传播本质上是张量的层层变换：\n全连接层：\n$$\\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}$$\n或：\n$$y_i = W_{ij} x_j + b_i$$\n卷积层：\n图 5：卷积操作的张量视角。卷积核在输入特征图上滑动，进行局部加权求和，生成输出特征图。\n卷积是张量的局部线性运算。对于输入 $\\mathbf{X}$ 和卷积核 $\\mathbf{K}$：\n$$(\\mathbf{X} * \\mathbf{K}){ij} = \\sum{m} \\sum_{n} X_{i+m, j+n} \\cdot K_{m,n}$$\n批量矩阵乘法：\nTransformer中的注意力机制：\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n这里 $Q, K, V$ 都是3阶张量（$B \\times T \\times D$），运算在Batch维度上并行进行。\n3.3 张量形状与维度操作 深度学习框架（PyTorch、TensorFlow）提供了丰富的张量操作：\nReshape（重塑）：改变张量形状而不改变数据\n$$\\text{reshape}(\\mathbf{X}{2 \\times 3 \\times 4}) = \\mathbf{Y}{6 \\times 4} = \\mathbf{Z}_{24}$$\nTranspose（转置）：交换维度\n$$(\\mathbf{X}^T){ij} = X{ji}$$\n对于高阶张量：\n$$(\\text{permute}(\\mathbf{X}, (0, 2, 1))){ijk} = X{ikj}$$\nBroadcasting（广播）：自动扩展维度进行运算\n$$\\mathbf{X}{3 \\times 1} + \\mathbf{y}{1 \\times 4} = \\mathbf{Z}_{3 \\times 4}$$\n其中 $Z_{ij} = X_{i0} + y_{0j}$\n第四章：张量分解——降维的艺术 4.1 为什么需要张量分解 高阶张量的参数量随阶数指数增长：\n3阶张量 $100 \\times 100 \\times 100$：$10^6$ 个参数 4阶张量 $100 \\times 100 \\times 100 \\times 100$：$10^8$ 个参数 张量分解用更少的参数近似原始张量，实现：\n数据压缩：减少存储需求 去噪：提取主要成分 解释性：发现数据的内在结构 图 6：张量分解的两种主要方法。CP分解将张量表示为秩-1张量的和，Tucker分解使用核心张量和模态矩阵。\n4.2 CP分解 CANDECOMP/PARAFAC (CP) 分解将张量表示为秩-1张量的和：\n$$\\mathbf{X} \\approx \\sum_{r=1}^{R} \\lambda_r \\mathbf{a}_r \\circ \\mathbf{b}_r \\circ \\mathbf{c}_r$$\n其中：\n$\\lambda_r$ 是权重 $\\mathbf{a}_r \\circ \\mathbf{b}_r \\circ \\mathbf{c}_r$ 是向量外积（秩-1张量） $R$ 是秩（rank） 矩阵形式：\n$$X_{ijk} \\approx \\sum_{r=1}^{R} \\lambda_r a_{ir} b_{jr} c_{kr}$$\n*应用：主题建模\n文档-词-时间张量 $\\mathbf{X}_{D \\times W \\times T}$ 的CP分解可以发现有：\n文档主题分布 $\\mathbf{A}$ 主题-词分布 $\\mathbf{B}$ 主题随时间演变 $\\mathbf{C}$ 4.3 Tucker分解 Tucker分解使用一个核心张量和模态矩阵：\n$$\\mathbf{X} \\approx \\mathbf{G} \\times_1 \\mathbf{A} \\times_2 \\mathbf{B} \\times_3 \\mathbf{C}$$\n其中：\n$\\mathbf{G}_{P \\times Q \\times R}$ 是核心张量 $\\mathbf{A}, \\mathbf{B}, \\mathbf{C}$ 是因子矩阵 $\\times_n$ 表示模态-$n$ 乘积 元素形式：\n$$X_{ijk} \\approx \\sum_{p=1}^{P} \\sum_{q=1}^{Q} \\sum_{r=1}^{R} G_{pqr} a_{ip} b_{jq} c_{kr}$$\n与CP分解的关系：当核心张量 $\\mathbf{G}$ 为对角张量时，Tucker分解退化为CP分解。\n4.4 张量网络与量子计算 张量网络是张量分解的可视化表示，在量子物理和机器学习中广泛应用。\n矩阵乘积态 (MPS)：\n将高维张量表示为一维链状结构：\n$$\\mathbf{X}_{i_1 i_2 \\cdots i_N} = \\sum_{\\alpha_1, \\ldots, \\alpha_{N-1}} A^{(1)}_{i_1 \\alpha_1} A^{(2)}_{\\alpha_1 i_2 \\alpha_2} \\cdots A^{(N)}_{\\alpha_{N-1} i_N}$$ 这大大减少了参数数量，从 $d^N$ 降到 $N \\cdot d \\cdot \\chi^2$（$\\chi$ 是键维度）。\n第五章：张量的现代应用 5.1 推荐系统 协同过滤可以用张量建模：\n用户-物品-时间张量 $\\mathbf{X}_{U \\times I \\times T}$\n$X_{uit} = 1$ 如果用户 $u$ 在时间 $t$ 与物品 $i$ 交互 张量分解用于推荐：\n通过CP分解学习用户、物品和时间的低维表示，预测缺失的交互：\n$$\\hat{X}{uit} = \\sum{r=1}^{R} \\lambda_r a_{ur} b_{ir} c_{tr}$$\n这比矩阵分解（仅用户-物品）能捕捉时间动态。\n5.2 计算机视觉 高光谱图像：每个像素有数百个光谱波段\n数据形式：$H \\times W \\times B$ 的3阶张量 张量分解用于去噪和特征提取 视频分析：\n背景-前景分离：将视频张量分解为低秩背景 + 稀疏前景 动作识别：时空特征的张量表示 5.3 自然语言处理 词嵌入的张量表示：\n句子可以表示为 $T \\times D$ 的矩阵（$T$ 个词，每个词 $D$ 维嵌入）。\n文档可以表示为 $D \\times T \\times B$ 的张量（$B$ 个文档）。\nTransformer的自注意力：\n$$\\text{Attention}(Q, K, V){b,t,d} = \\sum{t’} \\text{softmax}\\left(\\frac{Q_{b,t,:} \\cdot K_{b,t’,:}}{\\sqrt{d_k}}\\right)t V{b,t’,d}$$\n这是张量缩并的典型应用。\n5.4 物理学与工程学 应力张量：\n连续介质力学中，应力是2阶张量 $\\sigma_{ij}$，表示单位面积上的力。\n电磁场张量：\n相对论电动力学将电场和磁场统一为4维时空中的2阶反对称张量：\n$$F_{\\mu\\nu} = \\begin{pmatrix} 0 \u0026 -E_x \u0026 -E_y \u0026 -E_z \\ E_x \u0026 0 \u0026 B_z \u0026 -B_y \\ E_y \u0026 -B_z \u0026 0 \u0026 B_x \\ E_z \u0026 B_y \u0026 -B_x \u0026 0 \\end{pmatrix}$$\n黎曼曲率张量：\n描述时空弯曲的4阶张量 $R^\\rho_{\\sigma\\mu\\nu}$，是广义相对论的核心。\n第六章：张量计算框架 6.1 NumPy中的张量 Python的NumPy库是张量计算的基础：\nimport numpy as np # 创建张量 scalar = np.array(5) # 0阶 vector = np.array([1, 2, 3]) # 1阶 matrix = np.array([[1, 2], [3, 4]]) # 2阶 tensor = np.random.rand(3, 4, 5) # 3阶 # 张量运算 C = np.tensordot(A, B, axes=1) # 张量积 D = np.einsum('ijk,jkl-\u003eil', A, B) # 爱因斯坦求和 6.2 PyTorch张量 深度学习框架提供了GPU加速的张量运算：\nimport torch # GPU张量 x = torch.randn(1000, 1000, device='cuda') # 自动微分 x.requires_grad = True y = x ** 2 y.sum().backward() # x.grad 现在包含梯度 6.3 张量分解库 TensorLy：Python张量分解库\nimport tensorly as tl from tensorly.decomposition import parafac, tucker # CP分解 factors = parafac(tensor, rank=5) # Tucker分解 core, factors = tucker(tensor, ranks=[3, 4, 5]) 结语：张量的统一力量 回顾张量的旅程，我们看到：\n数学上，张量是向量和矩阵的自然推广，用统一的框架描述多线性关系。\n物理上，张量提供了坐标系无关的描述，是场论和相对论的语言。\n计算上，张量是现代数据科学的基础——图像、视频、文本都是张量。\n工程上，张量分解为处理高维数据提供了强大工具。\n张量的力量在于统一性：\n标量、向量、矩阵都是张量的特例 张量运算统一了线性代数的各种操作 Einstein求和约定统一了各种缩并规则 正如物理学家John Wheeler所说：\n“物质告诉时空如何弯曲，时空告诉物质如何运动。”\n在这个描述中，物质用应力-能量张量 $T_{\\mu\\nu}$ 表示，时空弯曲用度规张量 $g_{\\mu\\nu}$ 描述——张量语言统一了物质与几何。\n对于深度学习的从业者，理解张量意味着：\n更好的直觉：理解数据的形状和维度操作 更高效的代码：利用张量运算的并行性 更深入的理解：从张量角度理解神经网络 张量不仅是数学抽象，它是描述世界的通用语言——从微观粒子到宏观宇宙，从静态图像到时序数据，从经典物理到量子场论。\n附录：重要公式汇总 张量变换规则 逆变向量： $$v’^i = \\frac{\\partial x’^i}{\\partial x^j} v^j$$\n协变向量： $$v’_i = \\frac{\\partial x^j}{\\partial x’^i} v_j$$\n2阶张量： $$T’^{ij} = \\frac{\\partial x’^i}{\\partial x^k} \\frac{\\partial x’^j}{\\partial x^l} T^{kl}$$\n张量积 $$(\\mathbf{A} \\otimes \\mathbf{B}){i_1 \\cdots i_m j_1 \\cdots j_n} = A{i_1 \\cdots i_m} \\cdot B_{j_1 \\cdots j_n}$$\n缩并 $$C_{j_2 \\cdots j_s}^{i_2 \\cdots i_r} = A_{k j_2 \\cdots j_s}^{k i_2 \\cdots i_r} = \\sum_{k} A_{k j_2 \\cdots j_s}^{k i_2 \\cdots i_r}$$\nCP分解 $$X_{ijk} = \\sum_{r=1}^{R} \\lambda_r a_{ir} b_{jr} c_{kr}$$\nTucker分解 $$X_{ijk} = \\sum_{p,q,r} G_{pqr} a_{ip} b_{jq} c_{kr}$$\n延伸阅读：\nBishop \u0026 Goldberg. Tensor Analysis on Manifolds. Dover, 1980. Kolda \u0026 Bader. “Tensor Decompositions and Applications.” SIAM Review, 2009. Vasilescu \u0026 Terzopoulos. “Multilinear Analysis of Image Ensembles.” CVPR, 2002. Cichocki et al. “Tensor Networks for Dimensionality Reduction.” Foundations and Trends in Machine Learning, 2016. *愿你在张量的多维世界中，发现数据的深层结构。\n","wordCount":"1019","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/tensor-cover.jpg","datePublished":"2026-01-29T08:00:00+08:00","dateModified":"2026-01-29T08:00:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-29-tensor-comprehensive-guide/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">张量：从数学抽象到深度学习核心的系统综述</h1><div class=post-description>深入浅出解析张量的数学原理与广泛应用，从张量代数到深度学习，从物理场论到数据分析，完整呈现张量的力量</div><div class=post-meta><span title='2026-01-29 08:00:00 +0800 CST'>January 29, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>1019 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/tensor-cover.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/tensor-cover.jpg alt=张量与多维数据></a><figcaption>张量：描述世界的多维语言</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e5%a4%9a%e7%bb%b4%e4%b8%96%e7%95%8c%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%ad%e8%a8%80 aria-label=引言：多维世界的数学语言>引言：多维世界的数学语言</a></li><li><a href=#%e7%ac%ac%e4%b8%80%e7%ab%a0%e5%bc%a0%e9%87%8f%e7%9a%84%e6%9c%ac%e8%b4%a8%e8%b6%85%e8%b6%8a%e7%9f%a9%e9%98%b5%e7%9a%84%e5%a4%9a%e7%bb%b4%e6%95%b0%e7%bb%84 aria-label=第一章：张量的本质——超越矩阵的多维数组>第一章：张量的本质——超越矩阵的多维数组</a><ul><li><a href=#11-%e4%bb%8e%e6%a0%87%e9%87%8f%e5%88%b0%e5%bc%a0%e9%87%8f aria-label="1.1 从标量到张量">1.1 从标量到张量</a></li><li><a href=#12-%e5%bc%a0%e9%87%8f%e7%9a%84%e4%b8%a5%e6%a0%bc%e5%ae%9a%e4%b9%89 aria-label="1.2 张量的严格定义">1.2 张量的严格定义</a></li><li><a href=#13-%e9%80%86%e5%8f%98%e4%b8%8e%e5%8d%8f%e5%8f%98 aria-label="1.3 逆变与协变">1.3 逆变与协变</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e7%ab%a0%e5%bc%a0%e9%87%8f%e8%bf%90%e7%ae%97%e4%bb%a3%e6%95%b0%e7%9a%84%e5%8a%9b%e9%87%8f aria-label=第二章：张量运算——代数的力量>第二章：张量运算——代数的力量</a><ul><li><a href=#21-%e5%9f%ba%e6%9c%ac%e8%bf%90%e7%ae%97 aria-label="2.1 基本运算">2.1 基本运算</a></li><li><a href=#22-%e5%bc%a0%e9%87%8f%e7%a7%af%e5%a4%96%e7%a7%af aria-label="2.2 张量积（外积）">2.2 张量积（外积）</a></li><li><a href=#23-%e7%bc%a9%e5%b9%b6%e8%bf%b9%e7%9a%84%e6%8e%a8%e5%b9%bf aria-label="2.3 缩并（迹的推广）">2.3 缩并（迹的推广）</a></li><li><a href=#24-%e7%88%b1%e5%9b%a0%e6%96%af%e5%9d%a6%e6%b1%82%e5%92%8c%e7%ba%a6%e5%ae%9a aria-label="2.4 爱因斯坦求和约定">2.4 爱因斯坦求和约定</a></li><li><a href=#25-%e7%ba%bf%e6%80%a7%e5%8f%98%e6%8d%a2%e7%9a%84%e5%bc%a0%e9%87%8f%e8%a7%86%e8%a7%92 aria-label="2.5 线性变换的张量视角">2.5 线性变换的张量视角</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e7%ab%a0%e5%bc%a0%e9%87%8f%e5%9c%a8%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8 aria-label=第三章：张量在深度学习中的应用>第三章：张量在深度学习中的应用</a><ul><li><a href=#31-%e6%95%b0%e6%8d%ae%e8%a1%a8%e7%a4%ba%e7%9a%84%e5%bc%a0%e9%87%8f%e5%bd%a2%e5%bc%8f aria-label="3.1 数据表示的张量形式">3.1 数据表示的张量形式</a></li><li><a href=#32-%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%ad%e7%9a%84%e5%bc%a0%e9%87%8f%e8%bf%90%e7%ae%97 aria-label="3.2 神经网络中的张量运算">3.2 神经网络中的张量运算</a></li><li><a href=#33-%e5%bc%a0%e9%87%8f%e5%bd%a2%e7%8a%b6%e4%b8%8e%e7%bb%b4%e5%ba%a6%e6%93%8d%e4%bd%9c aria-label="3.3 张量形状与维度操作">3.3 张量形状与维度操作</a></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e7%ab%a0%e5%bc%a0%e9%87%8f%e5%88%86%e8%a7%a3%e9%99%8d%e7%bb%b4%e7%9a%84%e8%89%ba%e6%9c%af aria-label=第四章：张量分解——降维的艺术>第四章：张量分解——降维的艺术</a><ul><li><a href=#41-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%bc%a0%e9%87%8f%e5%88%86%e8%a7%a3 aria-label="4.1 为什么需要张量分解">4.1 为什么需要张量分解</a></li><li><a href=#42-cp%e5%88%86%e8%a7%a3 aria-label="4.2 CP分解">4.2 CP分解</a></li><li><a href=#43-tucker%e5%88%86%e8%a7%a3 aria-label="4.3 Tucker分解">4.3 Tucker分解</a></li><li><a href=#44-%e5%bc%a0%e9%87%8f%e7%bd%91%e7%bb%9c%e4%b8%8e%e9%87%8f%e5%ad%90%e8%ae%a1%e7%ae%97 aria-label="4.4 张量网络与量子计算">4.4 张量网络与量子计算</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e7%ab%a0%e5%bc%a0%e9%87%8f%e7%9a%84%e7%8e%b0%e4%bb%a3%e5%ba%94%e7%94%a8 aria-label=第五章：张量的现代应用>第五章：张量的现代应用</a><ul><li><a href=#51-%e6%8e%a8%e8%8d%90%e7%b3%bb%e7%bb%9f aria-label="5.1 推荐系统">5.1 推荐系统</a></li><li><a href=#52-%e8%ae%a1%e7%ae%97%e6%9c%ba%e8%a7%86%e8%a7%89 aria-label="5.2 计算机视觉">5.2 计算机视觉</a></li><li><a href=#53-%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%e5%a4%84%e7%90%86 aria-label="5.3 自然语言处理">5.3 自然语言处理</a></li><li><a href=#54-%e7%89%a9%e7%90%86%e5%ad%a6%e4%b8%8e%e5%b7%a5%e7%a8%8b%e5%ad%a6 aria-label="5.4 物理学与工程学">5.4 物理学与工程学</a></li></ul></li><li><a href=#%e7%ac%ac%e5%85%ad%e7%ab%a0%e5%bc%a0%e9%87%8f%e8%ae%a1%e7%ae%97%e6%a1%86%e6%9e%b6 aria-label=第六章：张量计算框架>第六章：张量计算框架</a><ul><li><a href=#61-numpy%e4%b8%ad%e7%9a%84%e5%bc%a0%e9%87%8f aria-label="6.1 NumPy中的张量">6.1 NumPy中的张量</a></li><li><a href=#62-pytorch%e5%bc%a0%e9%87%8f aria-label="6.2 PyTorch张量">6.2 PyTorch张量</a></li><li><a href=#63-%e5%bc%a0%e9%87%8f%e5%88%86%e8%a7%a3%e5%ba%93 aria-label="6.3 张量分解库">6.3 张量分解库</a></li></ul></li><li><a href=#%e7%bb%93%e8%af%ad%e5%bc%a0%e9%87%8f%e7%9a%84%e7%bb%9f%e4%b8%80%e5%8a%9b%e9%87%8f aria-label=结语：张量的统一力量>结语：张量的统一力量</a></li><li><a href=#%e9%99%84%e5%bd%95%e9%87%8d%e8%a6%81%e5%85%ac%e5%bc%8f%e6%b1%87%e6%80%bb aria-label=附录：重要公式汇总>附录：重要公式汇总</a><ul><li><a href=#%e5%bc%a0%e9%87%8f%e5%8f%98%e6%8d%a2%e8%a7%84%e5%88%99 aria-label=张量变换规则>张量变换规则</a></li><li><a href=#%e5%bc%a0%e9%87%8f%e7%a7%af aria-label=张量积>张量积</a></li><li><a href=#%e7%bc%a9%e5%b9%b6 aria-label=缩并>缩并</a></li><li><a href=#cp%e5%88%86%e8%a7%a3 aria-label=CP分解>CP分解</a></li><li><a href=#tucker%e5%88%86%e8%a7%a3 aria-label=Tucker分解>Tucker分解</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=引言多维世界的数学语言>引言：多维世界的数学语言<a hidden class=anchor aria-hidden=true href=#引言多维世界的数学语言>#</a></h2><p>想象你正在观察一个正在旋转的陀螺。描述它需要多少参数？</p><ul><li>位置：$3$ 个坐标 $(x, y, z)$</li><li>方向：$3$ 个欧拉角</li><li>角速度：$3$ 个分量</li><li>转动惯量：$9$ 个数（$3 \times 3$ 矩阵）</li></ul><p>这些量不仅仅是数字的集合，它们有特定的<strong>变换规则</strong>。当坐标系旋转时，位置和角速度按向量规则变换，而转动惯量则按更复杂的规则变换——这就是<strong>张量</strong>。</p><p>在物理学中，张量是描述场的通用语言。爱因斯坦的广义相对论用张量写下：</p><p>$$G_{\mu\nu} + \Lambda g_{\mu\nu} = \frac{8\pi G}{c^4} T_{\mu\nu}$$</p><p>在深度学习中，一张 $224 \times 224$ 的彩色图像是 $224 \times 224 \times 3$ 的<strong>三阶张量</strong>。一批 $32$ 张这样的图像是 $32 \times 224 \times 224 \times 3$ 的<strong>四阶张量</strong>。</p><p>本文将带你走进张量的世界，从数学定义到物理直觉，从代数运算到现代应用，理解为什么张量成为描述复杂系统的核心工具。</p><hr><h2 id=第一章张量的本质超越矩阵的多维数组>第一章：张量的本质——超越矩阵的多维数组<a hidden class=anchor aria-hidden=true href=#第一章张量的本质超越矩阵的多维数组>#</a></h2><h3 id=11-从标量到张量>1.1 从标量到张量<a hidden class=anchor aria-hidden=true href=#11-从标量到张量>#</a></h3><p>在数学中，我们熟悉不同维度的对象：</p><p><img alt=张量维度层级 loading=lazy src=/images/plots/tensor-hierarchy.png></p><p>图 1：张量的维度层级。从0阶标量（单个数字）到1阶向量、2阶矩阵，再到3阶及更高阶张量，维度不断增加。</p><p>*<em>0阶张量：标量</em></p><p>标量只有一个数值，没有方向：</p><p>$$a = 5, \quad T = 300\text{K}, \quad E = mc^2$$</p><p>标量在坐标变换下<strong>不变</strong>——无论你从哪个角度看，温度始终是 $300$K。</p><p>*<em>1阶张量：向量</em></p><p>向量有大小和方向：</p><p>$$\mathbf{v} = (v_1, v_2, v_3) = v_1 \mathbf{e}_1 + v_2 \mathbf{e}_2 + v_3 \mathbf{e}_3$$</p><p>当坐标系旋转时，向量的分量按特定规则变换：</p><p>$$v&rsquo;<em>i = \sum</em>{j=1}^{3} R_{ij} v_j$$</p><p>其中 $R_{ij}$ 是旋转矩阵。</p><p>*<em>2阶张量：矩阵</em></p><p>矩阵可以看作向量的推广：</p><p>$$\mathbf{A} = \begin{pmatrix} a_{11} & a_{12} & a_{13} \ a_{21} & a_{22} & a_{23} \ a_{31} & a_{32} & a_{33} \end{pmatrix}$$</p><p>在坐标变换下，矩阵元素变换为：</p><p>$$a&rsquo;<em>{ij} = \sum</em>{k,l} R_{ik} R_{jl} a_{kl}$$</p><p>*<em>3阶及以上张量</em></p><p>高阶张量有更多指标。一个 $n$ 阶张量有 $n$ 个指标，在 $d$ 维空间中有 $d^n$ 个分量。</p><h3 id=12-张量的严格定义>1.2 张量的严格定义<a hidden class=anchor aria-hidden=true href=#12-张量的严格定义>#</a></h3><p><strong>定义</strong>：张量是一个多线性映射，它在坐标变换下按特定规则变换。</p><p>具体来说，一个 $(r, s)$ 型张量（$r$ 个逆变指标，$s$ 个协变指标）的变换规则为：</p><p>$$T&rsquo;^{i_1 \cdots i_r}<em>{j_1 \cdots j_s} = \frac{\partial x&rsquo;^{i_1}}{\partial x^{k_1}} \cdots \frac{\partial x&rsquo;^{i_r}}{\partial x^{k_r}} \frac{\partial x^{l_1}}{\partial x&rsquo;^{j_1}} \cdots \frac{\partial x^{l_s}}{\partial x&rsquo;^{j_s}} T^{k_1 \cdots k_r}</em>{l_1 \cdots l_s}$$</p><p>这个看似复杂的公式其实捕捉了一个简单思想：<strong>张量描述的是独立于坐标系的物理/几何对象</strong>。</p><h3 id=13-逆变与协变>1.3 逆变与协变<a hidden class=anchor aria-hidden=true href=#13-逆变与协变>#</a></h3><p>为什么需要区分逆变和协变？</p><p>考虑速度 $\mathbf{v}$ 和梯度 $\nabla f$：</p><ul><li><strong>速度</strong>是逆变的：当坐标轴伸长时，速度分量变小（走完相同距离需要更少的"单位"）</li><li><strong>梯度</strong>是协变的：当坐标轴伸长时，梯度分量变大（相同距离上有更多的"单位"变化）</li></ul><p>数学上，逆变向量用上标 $v^i$，协变向量用下标 $v_i$。</p><hr><h2 id=第二章张量运算代数的力量>第二章：张量运算——代数的力量<a hidden class=anchor aria-hidden=true href=#第二章张量运算代数的力量>#</a></h2><h3 id=21-基本运算>2.1 基本运算<a hidden class=anchor aria-hidden=true href=#21-基本运算>#</a></h3><p><strong>加法</strong>：同阶张量可以逐元素相加</p><p>$$(\mathbf{A} + \mathbf{B})<em>{ijk} = A</em>{ijk} + B_{ijk}$$</p><p><strong>数乘</strong>：张量可以乘以标量</p><p>$$(c \mathbf{A})<em>{ijk} = c \cdot A</em>{ijk}$$</p><p><strong>重要性质</strong>：张量的阶在加法和数乘下保持不变。</p><h3 id=22-张量积外积>2.2 张量积（外积）<a hidden class=anchor aria-hidden=true href=#22-张量积外积>#</a></h3><p>张量积是将两个张量组合成更高阶张量的操作。</p><p><img alt=张量积 loading=lazy src=/images/plots/tensor-product.png></p><p>图 2：张量积（外积）的可视化。两个向量的外积产生一个矩阵，其中每个元素是相应向量分量的乘积。</p><p>给定两个向量 $\mathbf{u} \in \mathbb{R}^m$ 和 $\mathbf{v} \in \mathbb{R}^n$，它们的外积为：</p><p>$$(\mathbf{u} \otimes \mathbf{v})_{ij} = u_i \cdot v_j$$</p><p>结果是一个 $m \times n$ 矩阵。</p><p><strong>例子</strong>：</p><p>$$\mathbf{u} = \begin{pmatrix} 1 \ 2 \ 3 \end{pmatrix}, \quad \mathbf{v} = \begin{pmatrix} 4 \ 5 \end{pmatrix}$$</p><p>$$\mathbf{u} \otimes \mathbf{v} = \begin{pmatrix} 4 & 5 \ 8 & 10 \ 12 & 15 \end{pmatrix}$$</p><h3 id=23-缩并迹的推广>2.3 缩并（迹的推广）<a hidden class=anchor aria-hidden=true href=#23-缩并迹的推广>#</a></h3><p>缩并是对张量的两个指标求和，降低阶数。</p><p>对于矩阵，缩并就是<strong>迹</strong>：</p><p>$$\text{tr}(\mathbf{A}) = \sum_{i} A_{ii}$$</p><p>对于高阶张量 $\mathbf{T}_{ijk}$，缩并第1和第3指标：</p><p>$$S_j = \sum_{i} T_{iji}$$</p><p>结果是一个向量（阶数从3降到1）。</p><h3 id=24-爱因斯坦求和约定>2.4 爱因斯坦求和约定<a hidden class=anchor aria-hidden=true href=#24-爱因斯坦求和约定>#</a></h3><p>爱因斯坦引入了一个简洁的记号：<strong>重复的指标表示求和</strong>。</p><p>例如，矩阵乘法：</p><p>$$c_{ij} = \sum_{k} a_{ik} b_{kj} \quad \text{写成} \quad c_{ij} = a_{ik} b_{kj}$$</p><p>向量内积：</p><p>$$\mathbf{u} \cdot \mathbf{v} = \sum_{i} u_i v_i = u_i v^i$$</p><p><strong>约定规则</strong>：</p><ul><li>上标（逆变）和下标（协变）配对时求和</li><li>求和指标称为"哑指标"，可以任意重命名</li><li>结果中不再出现的指标是自由指标</li></ul><h3 id=25-线性变换的张量视角>2.5 线性变换的张量视角<a hidden class=anchor aria-hidden=true href=#25-线性变换的张量视角>#</a></h3><p><img alt=张量变换 loading=lazy src=/images/plots/tensor-transformation.png></p><p>图 3：线性变换的可视化。矩阵 $A$ 将向量 $v$ 映射到新的向量 $Av$，同时扭曲了整个空间（网格变形）。</p><p>矩阵作为2阶张量，定义了向量空间之间的线性映射：</p><p>$$\mathbf{y} = \mathbf{A} \mathbf{x}$$</p><p>或指标形式：</p><p>$$y_i = A_{ij} x_j$$</p><p><strong>特征值与特征向量</strong>：</p><p>寻找在变换下方向不变的向量：</p><p>$$\mathbf{A} \mathbf{v} = \lambda \mathbf{v}$$</p><p>这些特殊的向量（特征向量）和对应的缩放因子（特征值）揭示了变换的本质结构。</p><hr><h2 id=第三章张量在深度学习中的应用>第三章：张量在深度学习中的应用<a hidden class=anchor aria-hidden=true href=#第三章张量在深度学习中的应用>#</a></h2><h3 id=31-数据表示的张量形式>3.1 数据表示的张量形式<a hidden class=anchor aria-hidden=true href=#31-数据表示的张量形式>#</a></h3><p>深度学习处理的是各种形式的数据，它们都可以用张量表示：</p><p><img alt=深度学习数据表示 loading=lazy src=/images/plots/tensor-deep-learning.png></p><p>图 4：深度学习中的张量数据表示。从灰度图像（2D）到RGB图像（3D），批量图像（4D），再到序列数据（2D）。</p><p><strong>灰度图像</strong>：$H \times W$ 的2阶张量</p><ul><li>MNIST：$28 \times 28$</li><li>医学影像：$512 \times 512$</li></ul><p><strong>彩色图像</strong>：$H \times W \times C$ 的3阶张量</p><ul><li>$C = 3$（RGB）或 $C = 4$（RGBA）</li><li>ImageNet：$224 \times 224 \times 3$</li></ul><p><strong>视频</strong>：$T \times H \times W \times C$ 的4阶张量</p><ul><li>$T$ 是时间帧数</li><li>每秒 $30$ 帧，$10$ 秒视频有 $T = 300$</li></ul><p><strong>批量数据</strong>：在Batch维度上堆叠</p><ul><li>一批 $32$ 张RGB图像：$32 \times 224 \times 224 \times 3$</li></ul><p><strong>文本数据</strong>：</p><ul><li>词嵌入：$T \times D$（序列长度 × 嵌入维度）</li><li>批量句子：$B \times T \times D$</li></ul><h3 id=32-神经网络中的张量运算>3.2 神经网络中的张量运算<a hidden class=anchor aria-hidden=true href=#32-神经网络中的张量运算>#</a></h3><p>神经网络的前向传播本质上是张量的层层变换：</p><p><strong>全连接层</strong>：</p><p>$$\mathbf{y} = \mathbf{W} \mathbf{x} + \mathbf{b}$$</p><p>或：</p><p>$$y_i = W_{ij} x_j + b_i$$</p><p><strong>卷积层</strong>：</p><p><img alt=卷积操作 loading=lazy src=/images/plots/tensor-convolution.png></p><p>图 5：卷积操作的张量视角。卷积核在输入特征图上滑动，进行局部加权求和，生成输出特征图。</p><p>卷积是张量的<strong>局部线性运算</strong>。对于输入 $\mathbf{X}$ 和卷积核 $\mathbf{K}$：</p><p>$$(\mathbf{X} * \mathbf{K})<em>{ij} = \sum</em>{m} \sum_{n} X_{i+m, j+n} \cdot K_{m,n}$$</p><p><strong>批量矩阵乘法</strong>：</p><p>Transformer中的注意力机制：</p><p>$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$</p><p>这里 $Q, K, V$ 都是3阶张量（$B \times T \times D$），运算在Batch维度上并行进行。</p><h3 id=33-张量形状与维度操作>3.3 张量形状与维度操作<a hidden class=anchor aria-hidden=true href=#33-张量形状与维度操作>#</a></h3><p>深度学习框架（PyTorch、TensorFlow）提供了丰富的张量操作：</p><p><strong>Reshape（重塑）</strong>：改变张量形状而不改变数据</p><p>$$\text{reshape}(\mathbf{X}<em>{2 \times 3 \times 4}) = \mathbf{Y}</em>{6 \times 4} = \mathbf{Z}_{24}$$</p><p><strong>Transpose（转置）</strong>：交换维度</p><p>$$(\mathbf{X}^T)<em>{ij} = X</em>{ji}$$</p><p>对于高阶张量：</p><p>$$(\text{permute}(\mathbf{X}, (0, 2, 1)))<em>{ijk} = X</em>{ikj}$$</p><p><strong>Broadcasting（广播）</strong>：自动扩展维度进行运算</p><p>$$\mathbf{X}<em>{3 \times 1} + \mathbf{y}</em>{1 \times 4} = \mathbf{Z}_{3 \times 4}$$</p><p>其中 $Z_{ij} = X_{i0} + y_{0j}$</p><hr><h2 id=第四章张量分解降维的艺术>第四章：张量分解——降维的艺术<a hidden class=anchor aria-hidden=true href=#第四章张量分解降维的艺术>#</a></h2><h3 id=41-为什么需要张量分解>4.1 为什么需要张量分解<a hidden class=anchor aria-hidden=true href=#41-为什么需要张量分解>#</a></h3><p>高阶张量的参数量随阶数指数增长：</p><ul><li>3阶张量 $100 \times 100 \times 100$：$10^6$ 个参数</li><li>4阶张量 $100 \times 100 \times 100 \times 100$：$10^8$ 个参数</li></ul><p>张量分解用更少的参数近似原始张量，实现：</p><ul><li><strong>数据压缩</strong>：减少存储需求</li><li><strong>去噪</strong>：提取主要成分</li><li><strong>解释性</strong>：发现数据的内在结构</li></ul><p><img alt=张量分解 loading=lazy src=/images/plots/tensor-decomposition.png></p><p>图 6：张量分解的两种主要方法。CP分解将张量表示为秩-1张量的和，Tucker分解使用核心张量和模态矩阵。</p><h3 id=42-cp分解>4.2 CP分解<a hidden class=anchor aria-hidden=true href=#42-cp分解>#</a></h3><p><strong>CANDECOMP/PARAFAC (CP) 分解</strong>将张量表示为秩-1张量的和：</p><p>$$\mathbf{X} \approx \sum_{r=1}^{R} \lambda_r \mathbf{a}_r \circ \mathbf{b}_r \circ \mathbf{c}_r$$</p><p>其中：</p><ul><li>$\lambda_r$ 是权重</li><li>$\mathbf{a}_r \circ \mathbf{b}_r \circ \mathbf{c}_r$ 是向量外积（秩-1张量）</li><li>$R$ 是秩（rank）</li></ul><p><strong>矩阵形式</strong>：</p><p>$$X_{ijk} \approx \sum_{r=1}^{R} \lambda_r a_{ir} b_{jr} c_{kr}$$</p><p>*<em>应用：主题建模</em></p><p>文档-词-时间张量 $\mathbf{X}_{D \times W \times T}$ 的CP分解可以发现有：</p><ul><li>文档主题分布 $\mathbf{A}$</li><li>主题-词分布 $\mathbf{B}$</li><li>主题随时间演变 $\mathbf{C}$</li></ul><h3 id=43-tucker分解>4.3 Tucker分解<a hidden class=anchor aria-hidden=true href=#43-tucker分解>#</a></h3><p><strong>Tucker分解</strong>使用一个核心张量和模态矩阵：</p><p>$$\mathbf{X} \approx \mathbf{G} \times_1 \mathbf{A} \times_2 \mathbf{B} \times_3 \mathbf{C}$$</p><p>其中：</p><ul><li>$\mathbf{G}_{P \times Q \times R}$ 是核心张量</li><li>$\mathbf{A}, \mathbf{B}, \mathbf{C}$ 是因子矩阵</li><li>$\times_n$ 表示模态-$n$ 乘积</li></ul><p><strong>元素形式</strong>：</p><p>$$X_{ijk} \approx \sum_{p=1}^{P} \sum_{q=1}^{Q} \sum_{r=1}^{R} G_{pqr} a_{ip} b_{jq} c_{kr}$$</p><p><strong>与CP分解的关系</strong>：当核心张量 $\mathbf{G}$ 为对角张量时，Tucker分解退化为CP分解。</p><h3 id=44-张量网络与量子计算>4.4 张量网络与量子计算<a hidden class=anchor aria-hidden=true href=#44-张量网络与量子计算>#</a></h3><p><strong>张量网络</strong>是张量分解的可视化表示，在量子物理和机器学习中广泛应用。</p><p><strong>矩阵乘积态 (MPS)</strong>：</p><p>将高维张量表示为一维链状结构：</p><div class=math>$$\mathbf{X}_{i_1 i_2 \cdots i_N} = \sum_{\alpha_1, \ldots, \alpha_{N-1}} A^{(1)}_{i_1 \alpha_1} A^{(2)}_{\alpha_1 i_2 \alpha_2} \cdots A^{(N)}_{\alpha_{N-1} i_N}$$</div><p>这大大减少了参数数量，从 $d^N$ 降到 $N \cdot d \cdot \chi^2$（$\chi$ 是键维度）。</p><hr><h2 id=第五章张量的现代应用>第五章：张量的现代应用<a hidden class=anchor aria-hidden=true href=#第五章张量的现代应用>#</a></h2><h3 id=51-推荐系统>5.1 推荐系统<a hidden class=anchor aria-hidden=true href=#51-推荐系统>#</a></h3><p>协同过滤可以用张量建模：</p><p><strong>用户-物品-时间张量</strong> $\mathbf{X}_{U \times I \times T}$</p><ul><li>$X_{uit} = 1$ 如果用户 $u$ 在时间 $t$ 与物品 $i$ 交互</li></ul><p><strong>张量分解用于推荐</strong>：</p><p>通过CP分解学习用户、物品和时间的低维表示，预测缺失的交互：</p><p>$$\hat{X}<em>{uit} = \sum</em>{r=1}^{R} \lambda_r a_{ur} b_{ir} c_{tr}$$</p><p>这比矩阵分解（仅用户-物品）能捕捉时间动态。</p><h3 id=52-计算机视觉>5.2 计算机视觉<a hidden class=anchor aria-hidden=true href=#52-计算机视觉>#</a></h3><p><strong>高光谱图像</strong>：每个像素有数百个光谱波段</p><ul><li>数据形式：$H \times W \times B$ 的3阶张量</li><li>张量分解用于去噪和特征提取</li></ul><p><strong>视频分析</strong>：</p><ul><li>背景-前景分离：将视频张量分解为低秩背景 + 稀疏前景</li><li>动作识别：时空特征的张量表示</li></ul><h3 id=53-自然语言处理>5.3 自然语言处理<a hidden class=anchor aria-hidden=true href=#53-自然语言处理>#</a></h3><p><strong>词嵌入的张量表示</strong>：</p><p>句子可以表示为 $T \times D$ 的矩阵（$T$ 个词，每个词 $D$ 维嵌入）。</p><p>文档可以表示为 $D \times T \times B$ 的张量（$B$ 个文档）。</p><p><strong>Transformer的自注意力</strong>：</p><p>$$\text{Attention}(Q, K, V)<em>{b,t,d} = \sum</em>{t&rsquo;} \text{softmax}\left(\frac{Q_{b,t,:} \cdot K_{b,t&rsquo;,:}}{\sqrt{d_k}}\right)<em>t V</em>{b,t&rsquo;,d}$$</p><p>这是张量缩并的典型应用。</p><h3 id=54-物理学与工程学>5.4 物理学与工程学<a hidden class=anchor aria-hidden=true href=#54-物理学与工程学>#</a></h3><p><strong>应力张量</strong>：</p><p>连续介质力学中，应力是2阶张量 $\sigma_{ij}$，表示单位面积上的力。</p><p><strong>电磁场张量</strong>：</p><p>相对论电动力学将电场和磁场统一为4维时空中的2阶反对称张量：</p><p>$$F_{\mu\nu} = \begin{pmatrix} 0 & -E_x & -E_y & -E_z \ E_x & 0 & B_z & -B_y \ E_y & -B_z & 0 & B_x \ E_z & B_y & -B_x & 0 \end{pmatrix}$$</p><p><strong>黎曼曲率张量</strong>：</p><p>描述时空弯曲的4阶张量 $R^\rho_{\sigma\mu\nu}$，是广义相对论的核心。</p><hr><h2 id=第六章张量计算框架>第六章：张量计算框架<a hidden class=anchor aria-hidden=true href=#第六章张量计算框架>#</a></h2><h3 id=61-numpy中的张量>6.1 NumPy中的张量<a hidden class=anchor aria-hidden=true href=#61-numpy中的张量>#</a></h3><p>Python的NumPy库是张量计算的基础：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 创建张量</span>
</span></span><span class=line><span class=cl><span class=n>scalar</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>                    <span class=c1># 0阶</span>
</span></span><span class=line><span class=cl><span class=n>vector</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>            <span class=c1># 1阶</span>
</span></span><span class=line><span class=cl><span class=n>matrix</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span> <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>]])</span>     <span class=c1># 2阶</span>
</span></span><span class=line><span class=cl><span class=n>tensor</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span>        <span class=c1># 3阶</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 张量运算</span>
</span></span><span class=line><span class=cl><span class=n>C</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>tensordot</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>B</span><span class=p>,</span> <span class=n>axes</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>          <span class=c1># 张量积</span>
</span></span><span class=line><span class=cl><span class=n>D</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>einsum</span><span class=p>(</span><span class=s1>&#39;ijk,jkl-&gt;il&#39;</span><span class=p>,</span> <span class=n>A</span><span class=p>,</span> <span class=n>B</span><span class=p>)</span>      <span class=c1># 爱因斯坦求和</span>
</span></span></code></pre></div><h3 id=62-pytorch张量>6.2 PyTorch张量<a hidden class=anchor aria-hidden=true href=#62-pytorch张量>#</a></h3><p>深度学习框架提供了GPU加速的张量运算：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># GPU张量</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1000</span><span class=p>,</span> <span class=mi>1000</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;cuda&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 自动微分</span>
</span></span><span class=line><span class=cl><span class=n>x</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span> <span class=o>**</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>y</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># x.grad 现在包含梯度</span>
</span></span></code></pre></div><h3 id=63-张量分解库>6.3 张量分解库<a hidden class=anchor aria-hidden=true href=#63-张量分解库>#</a></h3><p><strong>TensorLy</strong>：Python张量分解库</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorly</span> <span class=k>as</span> <span class=nn>tl</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorly.decomposition</span> <span class=kn>import</span> <span class=n>parafac</span><span class=p>,</span> <span class=n>tucker</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># CP分解</span>
</span></span><span class=line><span class=cl><span class=n>factors</span> <span class=o>=</span> <span class=n>parafac</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>rank</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Tucker分解</span>
</span></span><span class=line><span class=cl><span class=n>core</span><span class=p>,</span> <span class=n>factors</span> <span class=o>=</span> <span class=n>tucker</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>ranks</span><span class=o>=</span><span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>])</span>
</span></span></code></pre></div><hr><h2 id=结语张量的统一力量>结语：张量的统一力量<a hidden class=anchor aria-hidden=true href=#结语张量的统一力量>#</a></h2><p>回顾张量的旅程，我们看到：</p><p><strong>数学上</strong>，张量是向量和矩阵的自然推广，用统一的框架描述多线性关系。</p><p><strong>物理上</strong>，张量提供了坐标系无关的描述，是场论和相对论的语言。</p><p><strong>计算上</strong>，张量是现代数据科学的基础——图像、视频、文本都是张量。</p><p><strong>工程上</strong>，张量分解为处理高维数据提供了强大工具。</p><p>张量的力量在于<strong>统一性</strong>：</p><ul><li>标量、向量、矩阵都是张量的特例</li><li>张量运算统一了线性代数的各种操作</li><li>Einstein求和约定统一了各种缩并规则</li></ul><p>正如物理学家John Wheeler所说：</p><blockquote><p>&ldquo;物质告诉时空如何弯曲，时空告诉物质如何运动。&rdquo;</p></blockquote><p>在这个描述中，物质用应力-能量张量 $T_{\mu\nu}$ 表示，时空弯曲用度规张量 $g_{\mu\nu}$ 描述——张量语言统一了物质与几何。</p><p>对于深度学习的从业者，理解张量意味着：</p><ol><li><strong>更好的直觉</strong>：理解数据的形状和维度操作</li><li><strong>更高效的代码</strong>：利用张量运算的并行性</li><li><strong>更深入的理解</strong>：从张量角度理解神经网络</li></ol><p>张量不仅是数学抽象，它是描述世界的通用语言——从微观粒子到宏观宇宙，从静态图像到时序数据，从经典物理到量子场论。</p><hr><h2 id=附录重要公式汇总>附录：重要公式汇总<a hidden class=anchor aria-hidden=true href=#附录重要公式汇总>#</a></h2><h3 id=张量变换规则>张量变换规则<a hidden class=anchor aria-hidden=true href=#张量变换规则>#</a></h3><p><strong>逆变向量</strong>：
$$v&rsquo;^i = \frac{\partial x&rsquo;^i}{\partial x^j} v^j$$</p><p><strong>协变向量</strong>：
$$v&rsquo;_i = \frac{\partial x^j}{\partial x&rsquo;^i} v_j$$</p><p><strong>2阶张量</strong>：
$$T&rsquo;^{ij} = \frac{\partial x&rsquo;^i}{\partial x^k} \frac{\partial x&rsquo;^j}{\partial x^l} T^{kl}$$</p><h3 id=张量积>张量积<a hidden class=anchor aria-hidden=true href=#张量积>#</a></h3><p>$$(\mathbf{A} \otimes \mathbf{B})<em>{i_1 \cdots i_m j_1 \cdots j_n} = A</em>{i_1 \cdots i_m} \cdot B_{j_1 \cdots j_n}$$</p><h3 id=缩并>缩并<a hidden class=anchor aria-hidden=true href=#缩并>#</a></h3><p>$$C_{j_2 \cdots j_s}^{i_2 \cdots i_r} = A_{k j_2 \cdots j_s}^{k i_2 \cdots i_r} = \sum_{k} A_{k j_2 \cdots j_s}^{k i_2 \cdots i_r}$$</p><h3 id=cp分解>CP分解<a hidden class=anchor aria-hidden=true href=#cp分解>#</a></h3><p>$$X_{ijk} = \sum_{r=1}^{R} \lambda_r a_{ir} b_{jr} c_{kr}$$</p><h3 id=tucker分解>Tucker分解<a hidden class=anchor aria-hidden=true href=#tucker分解>#</a></h3><p>$$X_{ijk} = \sum_{p,q,r} G_{pqr} a_{ip} b_{jq} c_{kr}$$</p><hr><p><strong>延伸阅读</strong>：</p><ul><li>Bishop & Goldberg. <em>Tensor Analysis on Manifolds</em>. Dover, 1980.</li><li>Kolda & Bader. &ldquo;Tensor Decompositions and Applications.&rdquo; <em>SIAM Review</em>, 2009.</li><li>Vasilescu & Terzopoulos. &ldquo;Multilinear Analysis of Image Ensembles.&rdquo; <em>CVPR</em>, 2002.</li><li>Cichocki et al. &ldquo;Tensor Networks for Dimensionality Reduction.&rdquo; <em>Foundations and Trends in Machine Learning</em>, 2016.</li></ul><p>*愿你在张量的多维世界中，发现数据的深层结构。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E5%BC%A0%E9%87%8F/>张量</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%BB%BC%E8%BF%B0/>综述</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/>线性代数</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-29-decision-trees-and-beyond-from-id3-to-modern-gradient-boosting/><span class=title>« Prev</span><br><span>决策树及其衍生算法：从ID3到现代梯度提升</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-29-alexnet-deep-learning-revolution/><span class=title>Next »</span><br><span>AlexNet：开启深度学习革命的里程碑</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 张量：从数学抽象到深度学习核心的系统综述 on x" href="https://x.com/intent/tweet/?text=%e5%bc%a0%e9%87%8f%ef%bc%9a%e4%bb%8e%e6%95%b0%e5%ad%a6%e6%8a%bd%e8%b1%a1%e5%88%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%a0%b8%e5%bf%83%e7%9a%84%e7%b3%bb%e7%bb%9f%e7%bb%bc%e8%bf%b0&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-29-tensor-comprehensive-guide%2f&amp;hashtags=%e5%bc%a0%e9%87%8f%2c%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%2c%e7%bb%bc%e8%bf%b0%2c%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 张量：从数学抽象到深度学习核心的系统综述 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-29-tensor-comprehensive-guide%2f&amp;title=%e5%bc%a0%e9%87%8f%ef%bc%9a%e4%bb%8e%e6%95%b0%e5%ad%a6%e6%8a%bd%e8%b1%a1%e5%88%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%a0%b8%e5%bf%83%e7%9a%84%e7%b3%bb%e7%bb%9f%e7%bb%bc%e8%bf%b0&amp;summary=%e5%bc%a0%e9%87%8f%ef%bc%9a%e4%bb%8e%e6%95%b0%e5%ad%a6%e6%8a%bd%e8%b1%a1%e5%88%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%a0%b8%e5%bf%83%e7%9a%84%e7%b3%bb%e7%bb%9f%e7%bb%bc%e8%bf%b0&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-29-tensor-comprehensive-guide%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 张量：从数学抽象到深度学习核心的系统综述 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-29-tensor-comprehensive-guide%2f&title=%e5%bc%a0%e9%87%8f%ef%bc%9a%e4%bb%8e%e6%95%b0%e5%ad%a6%e6%8a%bd%e8%b1%a1%e5%88%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%a0%b8%e5%bf%83%e7%9a%84%e7%b3%bb%e7%bb%9f%e7%bb%bc%e8%bf%b0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 张量：从数学抽象到深度学习核心的系统综述 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-29-tensor-comprehensive-guide%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>