<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>雅可比矩阵与黑塞矩阵：多变量微积分的双璧 | s-ai-unix's Blog</title><meta name=keywords content="综述,微分几何,机器学习,算法"><meta name=description content="深入探讨雅可比矩阵与黑塞矩阵的数学原理、几何直观和广泛应用，从多元微积分的基础出发，揭示这两个矩阵在多变量分析中的核心地位。"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-28-jacobian-hessian-matrices/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-28-jacobian-hessian-matrices/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-28-jacobian-hessian-matrices/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="雅可比矩阵与黑塞矩阵：多变量微积分的双璧"><meta property="og:description" content="深入探讨雅可比矩阵与黑塞矩阵的数学原理、几何直观和广泛应用，从多元微积分的基础出发，揭示这两个矩阵在多变量分析中的核心地位。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-28T21:54:27+08:00"><meta property="article:modified_time" content="2026-01-28T21:54:27+08:00"><meta property="article:tag" content="综述"><meta property="article:tag" content="微分几何"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="算法"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/jacobian-hessian-cover.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/jacobian-hessian-cover.jpg"><meta name=twitter:title content="雅可比矩阵与黑塞矩阵：多变量微积分的双璧"><meta name=twitter:description content="深入探讨雅可比矩阵与黑塞矩阵的数学原理、几何直观和广泛应用，从多元微积分的基础出发，揭示这两个矩阵在多变量分析中的核心地位。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"雅可比矩阵与黑塞矩阵：多变量微积分的双璧","item":"https://s-ai-unix.github.io/posts/2026-01-28-jacobian-hessian-matrices/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"雅可比矩阵与黑塞矩阵：多变量微积分的双璧","name":"雅可比矩阵与黑塞矩阵：多变量微积分的双璧","description":"深入探讨雅可比矩阵与黑塞矩阵的数学原理、几何直观和广泛应用，从多元微积分的基础出发，揭示这两个矩阵在多变量分析中的核心地位。","keywords":["综述","微分几何","机器学习","算法"],"articleBody":"引言 当我们从单变量微积分迈向多变量微积分时，一个核心问题浮现出来：如何描述多元函数的变化？在单变量情形中，导数 $f’(x)$ 告诉我们函数在某点的瞬时变化率。但当函数 $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ 拥有多个输入和输出时，情况变得复杂起来。\n想象一下，你正在攀登一座山峰。在任何一个位置，你都想知道：\n哪个方向最陡峭？（梯度的方向） 这个陡峭程度在各个方向如何变化？（曲率的描述） 雅可比矩阵和黑塞矩阵正是回答这些问题的数学工具。它们是多变量微积分中的\"双璧\"——一个描述一阶变化（线性近似），一个描述二阶变化（曲率特性）。从牛顿法到神经网络训练，从机器人运动学到广义相对论，这对\"双璧\"无处不在。\n第一章：从一维到多维 1.1 单变量函数的局限性 回顾单变量微积分，函数 $f: \\mathbb{R} \\to \\mathbb{R}$ 在点 $x$ 处的导数定义为：\n$$ f’(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h} $$\n这个定义告诉我们函数在 $x$ 处的瞬时变化率。几何上，它表示函数曲线在该点切线的斜率。\n但当函数有多个输入时，例如 $f(x, y) = x^2 + y^2$，我们可以问：\n沿 $x$ 方向的变化率是多少？ 沿 $y$ 方向的变化率是多少？ 沿任意方向的变化率是多少？ 这就引出了偏导数的概念。\n1.2 偏导数与方向导数 函数 $f(x_1, x_2, \\ldots, x_n)$ 关于 $x_i$ 的偏导数定义为：\n$$ \\frac{\\partial f}{\\partial x_i} = \\lim_{h \\to 0} \\frac{f(x_1, \\ldots, x_i+h, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h} $$\n偏导数告诉我们，当保持其他变量不变时，函数沿某一坐标轴方向的变化率。\n方向导数则更进一步。设 $\\mathbf{u} = (u_1, u_2, \\ldots, u_n)$ 是单位向量，函数 $f$ 在点 $\\mathbf{x}$ 沿方向 $\\mathbf{u}$ 的方向导数为：\n$$ D_{\\mathbf{u}} f(\\mathbf{x}) = \\lim_{h \\to 0} \\frac{f(\\mathbf{x} + h\\mathbf{u}) - f(\\mathbf{x})}{h} = \\sum_{i=1}^{n} \\frac{\\partial f}{\\partial x_i} u_i $$\n这引出了梯度的概念。\n1.3 梯度：最速上升方向 函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 的梯度定义为：\n$$ \\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right)^T $$\n梯度是一个向量，指向函数增长最快的方向，其模长表示增长的速率。\n上图展示了函数 $f(x,y) = x^2 + y^2$ 的梯度向量场。红色箭头表示梯度方向，蓝色等高线表示函数值相等的位置。可以看到，梯度总是垂直于等高线，指向函数值增大的方向。\n第二章：雅可比矩阵——多变量函数的\"导数\" 2.1 从单变量到多变量的推广 现在考虑更一般的情形：向量值函数 $\\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^m$。设输入为 $\\mathbf{x} = (x_1, \\ldots, x_n)^T$，输出为 $\\mathbf{y} = (y_1, \\ldots, y_m)^T$，其中每个 $y_i = f_i(x_1, \\ldots, x_n)$。\n如何描述这个函数在某点 $\\mathbf{x}$ 附近的行为？\n雅可比矩阵（Jacobian Matrix）给出了答案：\n$$ J_{\\mathbf{f}}(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} \u0026 \\frac{\\partial f_1}{\\partial x_2} \u0026 \\cdots \u0026 \\frac{\\partial f_1}{\\partial x_n} \\ \\frac{\\partial f_2}{\\partial x_1} \u0026 \\frac{\\partial f_2}{\\partial x_2} \u0026 \\cdots \u0026 \\frac{\\partial f_2}{\\partial x_n} \\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\ \\frac{\\partial f_m}{\\partial x_1} \u0026 \\frac{\\partial f_m}{\\partial x_2} \u0026 \\cdots \u0026 \\frac{\\partial f_m}{\\partial x_n} \\end{pmatrix} $$\n雅可比矩阵是一个 $m \\times n$ 矩阵，其第 $i$ 行是函数 $f_i$ 的梯度转置，第 $j$ 列是各函数关于 $x_j$ 的偏导数。\n2.2 几何意义：最佳线性近似 雅可比矩阵的核心意义在于：它给出了函数在某点附近的最佳线性近似。\n对于可微函数 $\\mathbf{f}$，在点 $\\mathbf{x}_0$ 附近有：\n$$ \\mathbf{f}(\\mathbf{x}) \\approx \\mathbf{f}(\\mathbf{x}0) + J{\\mathbf{f}}(\\mathbf{x}_0)(\\mathbf{x} - \\mathbf{x}_0) $$\n这正是多元函数的泰勒展开一阶近似。\n上图展示了雅可比矩阵作为线性变换的几何意义。蓝色单位圆经过线性变换后成为红色椭圆。雅可比矩阵的行列式（当 $m=n$ 时）告诉我们：变换对面积（或体积）的伸缩比例。\n2.3 雅可比行列式与变量替换 当 $m = n$ 时，雅可比矩阵是方阵，可以计算其行列式，称为雅可比行列式：\n$$ \\det(J_{\\mathbf{f}}) = \\left| \\frac{\\partial(y_1, \\ldots, y_n)}{\\partial(x_1, \\ldots, x_n)} \\right| $$\n雅可比行列式在多变量积分中扮演关键角色。当我们进行变量替换时：\n$$ \\int_{\\mathbf{f}(D)} g(\\mathbf{y}) , d\\mathbf{y} = \\int_{D} g(\\mathbf{f}(\\mathbf{x})) \\left| \\det(J_{\\mathbf{f}}(\\mathbf{x})) \\right| , d\\mathbf{x} $$\n这解释了为什么极坐标变换 $x = r\\cos\\theta, y = r\\sin\\theta$ 会引入因子 $r$——因为：\n$$ \\det \\begin{pmatrix} \\frac{\\partial x}{\\partial r} \u0026 \\frac{\\partial x}{\\partial \\theta} \\ \\frac{\\partial y}{\\partial r} \u0026 \\frac{\\partial y}{\\partial \\theta} \\end{pmatrix} = \\det \\begin{pmatrix} \\cos\\theta \u0026 -r\\sin\\theta \\ \\sin\\theta \u0026 r\\cos\\theta \\end{pmatrix} = r $$\n2.4 链式法则的矩阵形式 单变量链式法则 $\\frac{d}{dx}f(g(x)) = f’(g(x)) \\cdot g’(x)$ 在多变量情形中如何表达？\n设 $\\mathbf{f}: \\mathbb{R}^m \\to \\mathbb{R}^p$，$\\mathbf{g}: \\mathbb{R}^n \\to \\mathbb{R}^m$，则复合函数 $\\mathbf{f} \\circ \\mathbf{g}: \\mathbb{R}^n \\to \\mathbb{R}^p$ 的雅可比矩阵为：\n$$ J_{\\mathbf{f} \\circ \\mathbf{g}}(\\mathbf{x}) = J_{\\mathbf{f}}(\\mathbf{g}(\\mathbf{x})) \\cdot J_{\\mathbf{g}}(\\mathbf{x}) $$\n这就是链式法则的矩阵形式：复合函数的雅可比等于各函数雅可比的矩阵乘积。\n上图展示了链式法则的几何直观。左图显示内函数 $\\mathbf{g}(t)$ 将参数 $t$ 映射到平面上的螺旋曲线；右图显示复合函数 $(f \\circ \\mathbf{g})(t)$ 的值随参数 $t$ 的变化。\n第三章：黑塞矩阵——曲率的数学描述 3.1 从一阶到二阶 雅可比矩阵告诉我们函数如何变化，但它没有告诉我们变化率本身如何变化。这就好比知道车速，但不知道加速度。\n对于标量函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$，黑塞矩阵（Hessian Matrix）收集了所有的二阶偏导数：\n$$ H_f(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} \u0026 \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \u0026 \\cdots \u0026 \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} \u0026 \\frac{\\partial^2 f}{\\partial x_2^2} \u0026 \\cdots \u0026 \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} \u0026 \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} \u0026 \\cdots \u0026 \\frac{\\partial^2 f}{\\partial x_n^2} \\end{pmatrix} $$\n黑塞矩阵是一个 $n \\times n$ 对称矩阵（当二阶偏导数连续时，根据施瓦茨定理有 $\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}$）。\n3.2 泰勒展开的二阶近似 结合梯度和黑塞矩阵，我们可以写出多元函数的泰勒展开二阶近似：\n$$ f(\\mathbf{x}) \\approx f(\\mathbf{x}_0) + \\nabla f(\\mathbf{x}_0)^T (\\mathbf{x} - \\mathbf{x}_0) + \\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_0)^T H_f(\\mathbf{x}_0) (\\mathbf{x} - \\mathbf{x}_0) $$\n这个近似在优化算法中至关重要——梯度告诉我们向哪个方向走，黑塞矩阵告诉我们应该走多远。\n3.3 黑塞矩阵与曲率 黑塞矩阵最直观的几何意义是描述函数在某点附近的曲率。通过分析黑塞矩阵的特征值，我们可以了解函数在各个方向的弯曲程度。\n设 $H_f(\\mathbf{x})$ 的特征值为 $\\lambda_1, \\lambda_2, \\ldots, \\lambda_n$，对应的特征向量为 $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n$。则：\n若所有 $\\lambda_i \u003e 0$：函数在该点沿所有方向向上凸，是局部极小值 若所有 $\\lambda_i \u003c 0$：函数在该点沿所有方向向下凹，是局部极大值 若有正有负：函数在该点是鞍点 上图展示了函数 $f(x,y) = x^2 - y^2$ 的鞍点结构。左图是三维曲面，右图是等高线。红色线表示沿 $x$ 方向向上凸（$\\lambda_1 \u003e 0$），绿色线表示沿 $y$ 方向向下凹（$\\lambda_2 \u003c 0$）。原点是一个典型的鞍点。\n3.4 高斯曲率与黑塞行列式 在微分几何中，黑塞行列式与高斯曲率密切相关。对于曲面 $z = f(x,y)$，高斯曲率 $K$ 可以用黑塞矩阵表示为：\n$$ K = \\frac{\\det(H_f)}{(1 + |\\nabla f|^2)^2} $$\n当梯度较小时，$K \\approx \\det(H_f) = \\lambda_1 \\lambda_2$。这告诉我们：\n$\\det(H_f) \u003e 0$：椭圆点（局部极值） $\\det(H_f) \u003c 0$：双曲点（鞍点） $\\det(H_f) = 0$：抛物点（柱面或平面） 第四章：雅可比与黑塞的关系 4.1 梯度与雅可比的联系 对于标量函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$，其雅可比矩阵是 $1 \\times n$ 矩阵：\n$$ J_f = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \u0026 \\cdots \u0026 \\frac{\\partial f}{\\partial x_n} \\end{pmatrix} = (\\nabla f)^T $$\n因此，雅可比矩阵就是梯度的转置。\n4.2 黑塞是梯度的雅可比 更有趣的关系是：黑塞矩阵是梯度的雅可比矩阵。\n设 $\\mathbf{g}(\\mathbf{x}) = \\nabla f(\\mathbf{x})$，则：\n$$ J_{\\mathbf{g}} = \\begin{pmatrix} \\frac{\\partial}{\\partial x_1}(\\frac{\\partial f}{\\partial x_1}) \u0026 \\cdots \u0026 \\frac{\\partial}{\\partial x_n}(\\frac{\\partial f}{\\partial x_1}) \\ \\vdots \u0026 \\ddots \u0026 \\vdots \\ \\frac{\\partial}{\\partial x_1}(\\frac{\\partial f}{\\partial x_n}) \u0026 \\cdots \u0026 \\frac{\\partial}{\\partial x_n}(\\frac{\\partial f}{\\partial x_n}) \\end{pmatrix} = H_f $$\n这正是黑塞矩阵的定义！\nflowchart TD subgraph First[\"一阶导数\"] F[\"f(x)\"] --\u003e G[\"∇f = gradient\"] F --\u003e J[\"Jf = Jacobian\"] end subgraph Second[\"二阶导数\"] G --\u003e H[\"Hf = Hessian\"] H --\u003e|\"Hf = J∇f\"| J2[\"Jacobian of gradient\"] end style F fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff style G fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style J fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff style H fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff style J2 fill:#8E8E93,stroke:#8E8E93,stroke-width:1px,color:#ffffff 图例说明：\n🔵 蓝色节点：原函数 🟢 绿色节点：一阶导数（梯度/雅可比） 🟠 橙色节点：二阶导数（黑塞） 4.3 连续性方程与李导数 在更高级的微分几何中，雅可比矩阵和黑塞矩阵的概念可以推广到流形上的李导数和协变导数。雅可比矩阵对应着向量场的推进（pushforward），而黑塞矩阵的推广涉及联络（connection）的概念。\n第五章：应用实例 5.1 优化算法 梯度下降法 梯度下降法只使用一阶信息：\n$$ \\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k) $$\n其中 $\\alpha$ 是学习率。这种方法简单但收敛慢，且容易陷入鞍点。\n牛顿法 牛顿法利用黑塞矩阵的二阶信息：\n$$ \\mathbf{x}_{k+1} = \\mathbf{x}_k - [H_f(\\mathbf{x}_k)]^{-1} \\nabla f(\\mathbf{x}_k) $$\n牛顿法在二次函数上可以一步收敛！\n上图对比了梯度下降（橙色路径，15步）和牛顿法（红色虚线，1步）的优化过程。可以看到，利用黑塞矩阵的二阶信息后，牛顿法直接指向最优解，而梯度下降需要多次迭代。\n拟牛顿法 当变量维度很高时（如深度学习中的数百万参数），计算和存储黑塞矩阵及其逆矩阵是不现实的。拟牛顿法（如BFGS算法）通过迭代近似黑塞矩阵的逆，在计算效率和收敛速度之间取得平衡。\n5.2 机器学习中的损失函数分析 在机器学习中，损失函数的 landscape（景观）决定了训练的难度。\n上图展示了著名的 Rosenbrock 函数（香蕉函数）的优化景观。红色星标表示全局最小值，橙色路径表示优化轨迹。这种函数对优化算法是严峻的考验，因为其狭窄的峡谷形状使得梯度下降容易震荡。\n黑塞矩阵的特征值分析可以告诉我们：\n条件数 $\\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}$：条件数大表示优化景观在某些方向很陡峭，某些方向很平坦，导致梯度下降收敛慢 鞍点密度：高维空间中，鞍点比局部极小值更常见，理解黑塞矩阵有助于设计逃离鞍点的算法 5.3 机器人运动学 在机器人学中，雅可比矩阵描述了关节速度与末端执行器速度之间的关系。\n设 $\\mathbf{\\theta} = (\\theta_1, \\ldots, \\theta_n)^T$ 是关节角度，$\\mathbf{x}$ 是末端执行器的位置，则：\n$$ \\dot{\\mathbf{x}} = J(\\mathbf{\\theta}) \\dot{\\mathbf{\\theta}} $$\n其中 $J$ 是雅可比矩阵。这个关系在：\n逆运动学求解 奇异位形分析 力控制 中都有重要应用。\n5.4 物理与广义相对论 在物理学中，雅可比矩阵和黑塞矩阵的概念以不同形式出现：\n哈密顿力学：雅可比矩阵描述相空间流的演化 广义相对论：克里斯托费尔符号（Christoffel symbols）与黑塞矩阵相关，描述时空的曲率 统计力学：费舍尔信息矩阵是某种意义上的\"黑塞矩阵\" 5.5 计算机视觉与图像配准 在图像配准问题中，我们需要找到最优的几何变换将一幅图像对齐到另一幅。这通常通过最小化某种距离函数来实现，其中雅可比矩阵用于计算变换的局部变形，黑塞矩阵用于加速优化收敛。\n第六章：计算注意事项 6.1 自动微分 在现代机器学习中，手动计算雅可比矩阵和黑塞矩阵是不现实的。自动微分（Automatic Differentiation, AD）技术通过链式法则的计算机实现，可以高效准确地计算任意阶导数。\n深度学习框架（PyTorch、TensorFlow、JAX）都实现了反向传播算法，这实际上就是自动微分的一种高效实现。\n6.2 数值稳定性 实际计算中需要注意：\n有限差分近似：$\\frac{\\partial f}{\\partial x} \\approx \\frac{f(x+h) - f(x)}{h}$，但 $h$ 的选择需要权衡截断误差和舍入误差 黑塞矩阵的稀疏性：在许多实际问题中，黑塞矩阵是稀疏的，利用这一点可以显著降低计算成本 低秩近似：对于大规模问题，可以使用低秩近似来表示黑塞矩阵 6.3 高维挑战 当维度 $n$ 很大时：\n存储黑塞矩阵需要 $O(n^2)$ 空间 计算黑塞矩阵需要 $O(n^2)$ 或更多时间 求逆需要 $O(n^3)$ 时间 这促使了各种近似方法的发展：\n对角近似：只保留黑塞矩阵的对角元素 Kronecker 积近似：在神经网络中，利用网络结构近似黑塞矩阵 随机近似：使用随机采样估计黑塞-向量乘积 结语 雅可比矩阵和黑塞矩阵是多变量微积分的两大支柱。雅可比矩阵描述了函数在某点处的最佳线性近似，是多变量函数\"导数\"的自然推广；黑塞矩阵则描述了函数的二阶特性，告诉我们曲率和变化的加速度。\n这对\"双璧\"不仅在纯数学中地位重要，在机器学习、物理学、工程学等应用领域中更是无处不在。理解它们的几何直观和代数性质，对于深入掌握多变量分析和解决实际问题至关重要。\n从单变量到多变量，从一阶到二阶，数学思维的这种推广揭示了更深层次的结构和联系。正如雅可比矩阵是梯度的自然延伸，黑塞矩阵又是雅可比矩阵的再次应用——这种层层递进的结构，正是数学之美的体现。\n参考文献 Rudin, W. (1976). Principles of Mathematical Analysis. McGraw-Hill. Boyd, S., \u0026 Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press. Nocedal, J., \u0026 Wright, S. (2006). Numerical Optimization. Springer. Goodfellow, I., Bengio, Y., \u0026 Courville, A. (2016). Deep Learning. MIT Press. do Carmo, M. P. (1976). Differential Geometry of Curves and Surfaces. Prentice-Hall. ","wordCount":"879","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/jacobian-hessian-cover.jpg","datePublished":"2026-01-28T21:54:27+08:00","dateModified":"2026-01-28T21:54:27+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-28-jacobian-hessian-matrices/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">雅可比矩阵与黑塞矩阵：多变量微积分的双璧</h1><div class=post-description>深入探讨雅可比矩阵与黑塞矩阵的数学原理、几何直观和广泛应用，从多元微积分的基础出发，揭示这两个矩阵在多变量分析中的核心地位。</div><div class=post-meta><span title='2026-01-28 21:54:27 +0800 CST'>January 28, 2026</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>879 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/jacobian-hessian-cover.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/jacobian-hessian-cover.jpg alt="Jacobian and Hessian Matrices cover image"></a><figcaption>Photo by Unsplash</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80 aria-label=引言>引言</a></li><li><a href=#%e7%ac%ac%e4%b8%80%e7%ab%a0%e4%bb%8e%e4%b8%80%e7%bb%b4%e5%88%b0%e5%a4%9a%e7%bb%b4 aria-label=第一章：从一维到多维>第一章：从一维到多维</a><ul><li><a href=#11-%e5%8d%95%e5%8f%98%e9%87%8f%e5%87%bd%e6%95%b0%e7%9a%84%e5%b1%80%e9%99%90%e6%80%a7 aria-label="1.1 单变量函数的局限性">1.1 单变量函数的局限性</a></li><li><a href=#12-%e5%81%8f%e5%af%bc%e6%95%b0%e4%b8%8e%e6%96%b9%e5%90%91%e5%af%bc%e6%95%b0 aria-label="1.2 偏导数与方向导数">1.2 偏导数与方向导数</a></li><li><a href=#13-%e6%a2%af%e5%ba%a6%e6%9c%80%e9%80%9f%e4%b8%8a%e5%8d%87%e6%96%b9%e5%90%91 aria-label="1.3 梯度：最速上升方向">1.3 梯度：最速上升方向</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e7%ab%a0%e9%9b%85%e5%8f%af%e6%af%94%e7%9f%a9%e9%98%b5%e5%a4%9a%e5%8f%98%e9%87%8f%e5%87%bd%e6%95%b0%e7%9a%84%e5%af%bc%e6%95%b0 aria-label='第二章：雅可比矩阵——多变量函数的"导数"'>第二章：雅可比矩阵——多变量函数的"导数"</a><ul><li><a href=#21-%e4%bb%8e%e5%8d%95%e5%8f%98%e9%87%8f%e5%88%b0%e5%a4%9a%e5%8f%98%e9%87%8f%e7%9a%84%e6%8e%a8%e5%b9%bf aria-label="2.1 从单变量到多变量的推广">2.1 从单变量到多变量的推广</a></li><li><a href=#22-%e5%87%a0%e4%bd%95%e6%84%8f%e4%b9%89%e6%9c%80%e4%bd%b3%e7%ba%bf%e6%80%a7%e8%bf%91%e4%bc%bc aria-label="2.2 几何意义：最佳线性近似">2.2 几何意义：最佳线性近似</a></li><li><a href=#23-%e9%9b%85%e5%8f%af%e6%af%94%e8%a1%8c%e5%88%97%e5%bc%8f%e4%b8%8e%e5%8f%98%e9%87%8f%e6%9b%bf%e6%8d%a2 aria-label="2.3 雅可比行列式与变量替换">2.3 雅可比行列式与变量替换</a></li><li><a href=#24-%e9%93%be%e5%bc%8f%e6%b3%95%e5%88%99%e7%9a%84%e7%9f%a9%e9%98%b5%e5%bd%a2%e5%bc%8f aria-label="2.4 链式法则的矩阵形式">2.4 链式法则的矩阵形式</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e7%ab%a0%e9%bb%91%e5%a1%9e%e7%9f%a9%e9%98%b5%e6%9b%b2%e7%8e%87%e7%9a%84%e6%95%b0%e5%ad%a6%e6%8f%8f%e8%bf%b0 aria-label=第三章：黑塞矩阵——曲率的数学描述>第三章：黑塞矩阵——曲率的数学描述</a><ul><li><a href=#31-%e4%bb%8e%e4%b8%80%e9%98%b6%e5%88%b0%e4%ba%8c%e9%98%b6 aria-label="3.1 从一阶到二阶">3.1 从一阶到二阶</a></li><li><a href=#32-%e6%b3%b0%e5%8b%92%e5%b1%95%e5%bc%80%e7%9a%84%e4%ba%8c%e9%98%b6%e8%bf%91%e4%bc%bc aria-label="3.2 泰勒展开的二阶近似">3.2 泰勒展开的二阶近似</a></li><li><a href=#33-%e9%bb%91%e5%a1%9e%e7%9f%a9%e9%98%b5%e4%b8%8e%e6%9b%b2%e7%8e%87 aria-label="3.3 黑塞矩阵与曲率">3.3 黑塞矩阵与曲率</a></li><li><a href=#34-%e9%ab%98%e6%96%af%e6%9b%b2%e7%8e%87%e4%b8%8e%e9%bb%91%e5%a1%9e%e8%a1%8c%e5%88%97%e5%bc%8f aria-label="3.4 高斯曲率与黑塞行列式">3.4 高斯曲率与黑塞行列式</a></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e7%ab%a0%e9%9b%85%e5%8f%af%e6%af%94%e4%b8%8e%e9%bb%91%e5%a1%9e%e7%9a%84%e5%85%b3%e7%b3%bb aria-label=第四章：雅可比与黑塞的关系>第四章：雅可比与黑塞的关系</a><ul><li><a href=#41-%e6%a2%af%e5%ba%a6%e4%b8%8e%e9%9b%85%e5%8f%af%e6%af%94%e7%9a%84%e8%81%94%e7%b3%bb aria-label="4.1 梯度与雅可比的联系">4.1 梯度与雅可比的联系</a></li><li><a href=#42-%e9%bb%91%e5%a1%9e%e6%98%af%e6%a2%af%e5%ba%a6%e7%9a%84%e9%9b%85%e5%8f%af%e6%af%94 aria-label="4.2 黑塞是梯度的雅可比">4.2 黑塞是梯度的雅可比</a></li><li><a href=#43-%e8%bf%9e%e7%bb%ad%e6%80%a7%e6%96%b9%e7%a8%8b%e4%b8%8e%e6%9d%8e%e5%af%bc%e6%95%b0 aria-label="4.3 连续性方程与李导数">4.3 连续性方程与李导数</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e7%ab%a0%e5%ba%94%e7%94%a8%e5%ae%9e%e4%be%8b aria-label=第五章：应用实例>第五章：应用实例</a><ul><li><a href=#51-%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95 aria-label="5.1 优化算法">5.1 优化算法</a><ul><li><a href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95 aria-label=梯度下降法>梯度下降法</a></li><li><a href=#%e7%89%9b%e9%a1%bf%e6%b3%95 aria-label=牛顿法>牛顿法</a></li><li><a href=#%e6%8b%9f%e7%89%9b%e9%a1%bf%e6%b3%95 aria-label=拟牛顿法>拟牛顿法</a></li></ul></li><li><a href=#52-%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e5%88%86%e6%9e%90 aria-label="5.2 机器学习中的损失函数分析">5.2 机器学习中的损失函数分析</a></li><li><a href=#53-%e6%9c%ba%e5%99%a8%e4%ba%ba%e8%bf%90%e5%8a%a8%e5%ad%a6 aria-label="5.3 机器人运动学">5.3 机器人运动学</a></li><li><a href=#54-%e7%89%a9%e7%90%86%e4%b8%8e%e5%b9%bf%e4%b9%89%e7%9b%b8%e5%af%b9%e8%ae%ba aria-label="5.4 物理与广义相对论">5.4 物理与广义相对论</a></li><li><a href=#55-%e8%ae%a1%e7%ae%97%e6%9c%ba%e8%a7%86%e8%a7%89%e4%b8%8e%e5%9b%be%e5%83%8f%e9%85%8d%e5%87%86 aria-label="5.5 计算机视觉与图像配准">5.5 计算机视觉与图像配准</a></li></ul></li><li><a href=#%e7%ac%ac%e5%85%ad%e7%ab%a0%e8%ae%a1%e7%ae%97%e6%b3%a8%e6%84%8f%e4%ba%8b%e9%a1%b9 aria-label=第六章：计算注意事项>第六章：计算注意事项</a><ul><li><a href=#61-%e8%87%aa%e5%8a%a8%e5%be%ae%e5%88%86 aria-label="6.1 自动微分">6.1 自动微分</a></li><li><a href=#62-%e6%95%b0%e5%80%bc%e7%a8%b3%e5%ae%9a%e6%80%a7 aria-label="6.2 数值稳定性">6.2 数值稳定性</a></li><li><a href=#63-%e9%ab%98%e7%bb%b4%e6%8c%91%e6%88%98 aria-label="6.3 高维挑战">6.3 高维挑战</a></li></ul></li><li><a href=#%e7%bb%93%e8%af%ad aria-label=结语>结语</a></li><li><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae aria-label=参考文献>参考文献</a></li></ul></div></details></div><div class=post-content><h2 id=引言>引言<a hidden class=anchor aria-hidden=true href=#引言>#</a></h2><p>当我们从单变量微积分迈向多变量微积分时，一个核心问题浮现出来：如何描述多元函数的变化？在单变量情形中，导数 $f&rsquo;(x)$ 告诉我们函数在某点的瞬时变化率。但当函数 $f: \mathbb{R}^n \to \mathbb{R}^m$ 拥有多个输入和输出时，情况变得复杂起来。</p><p>想象一下，你正在攀登一座山峰。在任何一个位置，你都想知道：</p><ul><li>哪个方向最陡峭？（梯度的方向）</li><li>这个陡峭程度在各个方向如何变化？（曲率的描述）</li></ul><p>雅可比矩阵和黑塞矩阵正是回答这些问题的数学工具。它们是多变量微积分中的"双璧"——一个描述一阶变化（线性近似），一个描述二阶变化（曲率特性）。从牛顿法到神经网络训练，从机器人运动学到广义相对论，这对"双璧"无处不在。</p><h2 id=第一章从一维到多维>第一章：从一维到多维<a hidden class=anchor aria-hidden=true href=#第一章从一维到多维>#</a></h2><h3 id=11-单变量函数的局限性>1.1 单变量函数的局限性<a hidden class=anchor aria-hidden=true href=#11-单变量函数的局限性>#</a></h3><p>回顾单变量微积分，函数 $f: \mathbb{R} \to \mathbb{R}$ 在点 $x$ 处的导数定义为：</p><p>$$
f&rsquo;(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
$$</p><p>这个定义告诉我们函数在 $x$ 处的瞬时变化率。几何上，它表示函数曲线在该点切线的斜率。</p><p>但当函数有多个输入时，例如 $f(x, y) = x^2 + y^2$，我们可以问：</p><ul><li>沿 $x$ 方向的变化率是多少？</li><li>沿 $y$ 方向的变化率是多少？</li><li>沿任意方向的变化率是多少？</li></ul><p>这就引出了<strong>偏导数</strong>的概念。</p><h3 id=12-偏导数与方向导数>1.2 偏导数与方向导数<a hidden class=anchor aria-hidden=true href=#12-偏导数与方向导数>#</a></h3><p>函数 $f(x_1, x_2, \ldots, x_n)$ 关于 $x_i$ 的偏导数定义为：</p><p>$$
\frac{\partial f}{\partial x_i} = \lim_{h \to 0} \frac{f(x_1, \ldots, x_i+h, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}
$$</p><p>偏导数告诉我们，当保持其他变量不变时，函数沿某一坐标轴方向的变化率。</p><p><strong>方向导数</strong>则更进一步。设 $\mathbf{u} = (u_1, u_2, \ldots, u_n)$ 是单位向量，函数 $f$ 在点 $\mathbf{x}$ 沿方向 $\mathbf{u}$ 的方向导数为：</p><p>$$
D_{\mathbf{u}} f(\mathbf{x}) = \lim_{h \to 0} \frac{f(\mathbf{x} + h\mathbf{u}) - f(\mathbf{x})}{h} = \sum_{i=1}^{n} \frac{\partial f}{\partial x_i} u_i
$$</p><p>这引出了梯度的概念。</p><h3 id=13-梯度最速上升方向>1.3 梯度：最速上升方向<a hidden class=anchor aria-hidden=true href=#13-梯度最速上升方向>#</a></h3><p>函数 $f: \mathbb{R}^n \to \mathbb{R}$ 的<strong>梯度</strong>定义为：</p><p>$$
\nabla f = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right)^T
$$</p><p>梯度是一个向量，指向函数增长最快的方向，其模长表示增长的速率。</p><p><img alt=梯度向量场 loading=lazy src=/images/math/gradient-vector-field.png></p><p>上图展示了函数 $f(x,y) = x^2 + y^2$ 的梯度向量场。红色箭头表示梯度方向，蓝色等高线表示函数值相等的位置。可以看到，梯度总是垂直于等高线，指向函数值增大的方向。</p><h2 id=第二章雅可比矩阵多变量函数的导数>第二章：雅可比矩阵——多变量函数的"导数"<a hidden class=anchor aria-hidden=true href=#第二章雅可比矩阵多变量函数的导数>#</a></h2><h3 id=21-从单变量到多变量的推广>2.1 从单变量到多变量的推广<a hidden class=anchor aria-hidden=true href=#21-从单变量到多变量的推广>#</a></h3><p>现在考虑更一般的情形：向量值函数 $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$。设输入为 $\mathbf{x} = (x_1, \ldots, x_n)^T$，输出为 $\mathbf{y} = (y_1, \ldots, y_m)^T$，其中每个 $y_i = f_i(x_1, \ldots, x_n)$。</p><p>如何描述这个函数在某点 $\mathbf{x}$ 附近的行为？</p><p><strong>雅可比矩阵</strong>（Jacobian Matrix）给出了答案：</p><p>$$
J_{\mathbf{f}}(\mathbf{x}) = \begin{pmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \cdots & \frac{\partial f_1}{\partial x_n} \
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_2}{\partial x_n} \
\vdots & \vdots & \ddots & \vdots \
\frac{\partial f_m}{\partial x_1} & \frac{\partial f_m}{\partial x_2} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{pmatrix}
$$</p><p>雅可比矩阵是一个 $m \times n$ 矩阵，其第 $i$ 行是函数 $f_i$ 的梯度转置，第 $j$ 列是各函数关于 $x_j$ 的偏导数。</p><h3 id=22-几何意义最佳线性近似>2.2 几何意义：最佳线性近似<a hidden class=anchor aria-hidden=true href=#22-几何意义最佳线性近似>#</a></h3><p>雅可比矩阵的核心意义在于：它给出了函数在某点附近的<strong>最佳线性近似</strong>。</p><p>对于可微函数 $\mathbf{f}$，在点 $\mathbf{x}_0$ 附近有：</p><p>$$
\mathbf{f}(\mathbf{x}) \approx \mathbf{f}(\mathbf{x}<em>0) + J</em>{\mathbf{f}}(\mathbf{x}_0)(\mathbf{x} - \mathbf{x}_0)
$$</p><p>这正是多元函数的泰勒展开一阶近似。</p><p><img alt=雅可比线性变换 loading=lazy src=/images/math/jacobian-linear-transform.png></p><p>上图展示了雅可比矩阵作为线性变换的几何意义。蓝色单位圆经过线性变换后成为红色椭圆。雅可比矩阵的行列式（当 $m=n$ 时）告诉我们：变换对面积（或体积）的伸缩比例。</p><h3 id=23-雅可比行列式与变量替换>2.3 雅可比行列式与变量替换<a hidden class=anchor aria-hidden=true href=#23-雅可比行列式与变量替换>#</a></h3><p>当 $m = n$ 时，雅可比矩阵是方阵，可以计算其行列式，称为<strong>雅可比行列式</strong>：</p><p>$$
\det(J_{\mathbf{f}}) = \left| \frac{\partial(y_1, \ldots, y_n)}{\partial(x_1, \ldots, x_n)} \right|
$$</p><p>雅可比行列式在多变量积分中扮演关键角色。当我们进行变量替换时：</p><p>$$
\int_{\mathbf{f}(D)} g(\mathbf{y}) , d\mathbf{y} = \int_{D} g(\mathbf{f}(\mathbf{x})) \left| \det(J_{\mathbf{f}}(\mathbf{x})) \right| , d\mathbf{x}
$$</p><p>这解释了为什么极坐标变换 $x = r\cos\theta, y = r\sin\theta$ 会引入因子 $r$——因为：</p><p>$$
\det \begin{pmatrix} \frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \ \frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta} \end{pmatrix} = \det \begin{pmatrix} \cos\theta & -r\sin\theta \ \sin\theta & r\cos\theta \end{pmatrix} = r
$$</p><h3 id=24-链式法则的矩阵形式>2.4 链式法则的矩阵形式<a hidden class=anchor aria-hidden=true href=#24-链式法则的矩阵形式>#</a></h3><p>单变量链式法则 $\frac{d}{dx}f(g(x)) = f&rsquo;(g(x)) \cdot g&rsquo;(x)$ 在多变量情形中如何表达？</p><p>设 $\mathbf{f}: \mathbb{R}^m \to \mathbb{R}^p$，$\mathbf{g}: \mathbb{R}^n \to \mathbb{R}^m$，则复合函数 $\mathbf{f} \circ \mathbf{g}: \mathbb{R}^n \to \mathbb{R}^p$ 的雅可比矩阵为：</p><p>$$
J_{\mathbf{f} \circ \mathbf{g}}(\mathbf{x}) = J_{\mathbf{f}}(\mathbf{g}(\mathbf{x})) \cdot J_{\mathbf{g}}(\mathbf{x})
$$</p><p>这就是<strong>链式法则的矩阵形式</strong>：复合函数的雅可比等于各函数雅可比的矩阵乘积。</p><p><img alt=链式法则可视化 loading=lazy src=/images/math/jacobian-chain-rule.png></p><p>上图展示了链式法则的几何直观。左图显示内函数 $\mathbf{g}(t)$ 将参数 $t$ 映射到平面上的螺旋曲线；右图显示复合函数 $(f \circ \mathbf{g})(t)$ 的值随参数 $t$ 的变化。</p><h2 id=第三章黑塞矩阵曲率的数学描述>第三章：黑塞矩阵——曲率的数学描述<a hidden class=anchor aria-hidden=true href=#第三章黑塞矩阵曲率的数学描述>#</a></h2><h3 id=31-从一阶到二阶>3.1 从一阶到二阶<a hidden class=anchor aria-hidden=true href=#31-从一阶到二阶>#</a></h3><p>雅可比矩阵告诉我们函数如何变化，但它没有告诉我们变化率本身如何变化。这就好比知道车速，但不知道加速度。</p><p>对于标量函数 $f: \mathbb{R}^n \to \mathbb{R}$，<strong>黑塞矩阵</strong>（Hessian Matrix）收集了所有的二阶偏导数：</p><p>$$
H_f(\mathbf{x}) = \begin{pmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \
\vdots & \vdots & \ddots & \vdots \
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{pmatrix}
$$</p><p>黑塞矩阵是一个 $n \times n$ 对称矩阵（当二阶偏导数连续时，根据施瓦茨定理有 $\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}$）。</p><h3 id=32-泰勒展开的二阶近似>3.2 泰勒展开的二阶近似<a hidden class=anchor aria-hidden=true href=#32-泰勒展开的二阶近似>#</a></h3><p>结合梯度和黑塞矩阵，我们可以写出多元函数的泰勒展开二阶近似：</p><p>$$
f(\mathbf{x}) \approx f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^T (\mathbf{x} - \mathbf{x}_0) + \frac{1}{2}(\mathbf{x} - \mathbf{x}_0)^T H_f(\mathbf{x}_0) (\mathbf{x} - \mathbf{x}_0)
$$</p><p>这个近似在优化算法中至关重要——梯度告诉我们向哪个方向走，黑塞矩阵告诉我们应该走多远。</p><h3 id=33-黑塞矩阵与曲率>3.3 黑塞矩阵与曲率<a hidden class=anchor aria-hidden=true href=#33-黑塞矩阵与曲率>#</a></h3><p>黑塞矩阵最直观的几何意义是描述函数在某点附近的<strong>曲率</strong>。通过分析黑塞矩阵的特征值，我们可以了解函数在各个方向的弯曲程度。</p><p>设 $H_f(\mathbf{x})$ 的特征值为 $\lambda_1, \lambda_2, \ldots, \lambda_n$，对应的特征向量为 $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n$。则：</p><ul><li>若所有 $\lambda_i > 0$：函数在该点沿所有方向向上凸，是局部极小值</li><li>若所有 $\lambda_i &lt; 0$：函数在该点沿所有方向向下凹，是局部极大值</li><li>若有正有负：函数在该点是鞍点</li></ul><p><img alt=黑塞矩阵与曲率 loading=lazy src=/images/math/hessian-curvature.png></p><p>上图展示了函数 $f(x,y) = x^2 - y^2$ 的鞍点结构。左图是三维曲面，右图是等高线。红色线表示沿 $x$ 方向向上凸（$\lambda_1 > 0$），绿色线表示沿 $y$ 方向向下凹（$\lambda_2 &lt; 0$）。原点是一个典型的鞍点。</p><h3 id=34-高斯曲率与黑塞行列式>3.4 高斯曲率与黑塞行列式<a hidden class=anchor aria-hidden=true href=#34-高斯曲率与黑塞行列式>#</a></h3><p>在微分几何中，黑塞行列式与高斯曲率密切相关。对于曲面 $z = f(x,y)$，高斯曲率 $K$ 可以用黑塞矩阵表示为：</p><p>$$
K = \frac{\det(H_f)}{(1 + |\nabla f|^2)^2}
$$</p><p>当梯度较小时，$K \approx \det(H_f) = \lambda_1 \lambda_2$。这告诉我们：</p><ul><li>$\det(H_f) > 0$：椭圆点（局部极值）</li><li>$\det(H_f) &lt; 0$：双曲点（鞍点）</li><li>$\det(H_f) = 0$：抛物点（柱面或平面）</li></ul><h2 id=第四章雅可比与黑塞的关系>第四章：雅可比与黑塞的关系<a hidden class=anchor aria-hidden=true href=#第四章雅可比与黑塞的关系>#</a></h2><h3 id=41-梯度与雅可比的联系>4.1 梯度与雅可比的联系<a hidden class=anchor aria-hidden=true href=#41-梯度与雅可比的联系>#</a></h3><p>对于标量函数 $f: \mathbb{R}^n \to \mathbb{R}$，其雅可比矩阵是 $1 \times n$ 矩阵：</p><p>$$
J_f = \begin{pmatrix} \frac{\partial f}{\partial x_1} & \cdots & \frac{\partial f}{\partial x_n} \end{pmatrix} = (\nabla f)^T
$$</p><p>因此，雅可比矩阵就是梯度的转置。</p><h3 id=42-黑塞是梯度的雅可比>4.2 黑塞是梯度的雅可比<a hidden class=anchor aria-hidden=true href=#42-黑塞是梯度的雅可比>#</a></h3><p>更有趣的关系是：黑塞矩阵是<strong>梯度的雅可比矩阵</strong>。</p><p>设 $\mathbf{g}(\mathbf{x}) = \nabla f(\mathbf{x})$，则：</p><p>$$
J_{\mathbf{g}} = \begin{pmatrix}
\frac{\partial}{\partial x_1}(\frac{\partial f}{\partial x_1}) & \cdots & \frac{\partial}{\partial x_n}(\frac{\partial f}{\partial x_1}) \
\vdots & \ddots & \vdots \
\frac{\partial}{\partial x_1}(\frac{\partial f}{\partial x_n}) & \cdots & \frac{\partial}{\partial x_n}(\frac{\partial f}{\partial x_n})
\end{pmatrix} = H_f
$$</p><p>这正是黑塞矩阵的定义！</p><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>flowchart TD
subgraph First["一阶导数"]
F["f(x)"] --> G["∇f = gradient"]
F --> J["Jf = Jacobian"]
end
subgraph Second["二阶导数"]
G --> H["Hf = Hessian"]
H -->|"Hf = J∇f"| J2["Jacobian of gradient"]
end
style F fill:#007AFF,stroke:#007AFF,stroke-width:3px,color:#ffffff
style G fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style J fill:#34C759,stroke:#34C759,stroke-width:2px,color:#ffffff
style H fill:#FF9500,stroke:#FF9500,stroke-width:2px,color:#ffffff
style J2 fill:#8E8E93,stroke:#8E8E93,stroke-width:1px,color:#ffffff</div></div><p><strong>图例说明</strong>：</p><ul><li>🔵 蓝色节点：原函数</li><li>🟢 绿色节点：一阶导数（梯度/雅可比）</li><li>🟠 橙色节点：二阶导数（黑塞）</li></ul><h3 id=43-连续性方程与李导数>4.3 连续性方程与李导数<a hidden class=anchor aria-hidden=true href=#43-连续性方程与李导数>#</a></h3><p>在更高级的微分几何中，雅可比矩阵和黑塞矩阵的概念可以推广到流形上的李导数和协变导数。雅可比矩阵对应着向量场的推进（pushforward），而黑塞矩阵的推广涉及联络（connection）的概念。</p><h2 id=第五章应用实例>第五章：应用实例<a hidden class=anchor aria-hidden=true href=#第五章应用实例>#</a></h2><h3 id=51-优化算法>5.1 优化算法<a hidden class=anchor aria-hidden=true href=#51-优化算法>#</a></h3><h4 id=梯度下降法>梯度下降法<a hidden class=anchor aria-hidden=true href=#梯度下降法>#</a></h4><p>梯度下降法只使用一阶信息：</p><p>$$
\mathbf{x}_{k+1} = \mathbf{x}_k - \alpha \nabla f(\mathbf{x}_k)
$$</p><p>其中 $\alpha$ 是学习率。这种方法简单但收敛慢，且容易陷入鞍点。</p><h4 id=牛顿法>牛顿法<a hidden class=anchor aria-hidden=true href=#牛顿法>#</a></h4><p>牛顿法利用黑塞矩阵的二阶信息：</p><p>$$
\mathbf{x}_{k+1} = \mathbf{x}_k - [H_f(\mathbf{x}_k)]^{-1} \nabla f(\mathbf{x}_k)
$$</p><p>牛顿法在二次函数上可以一步收敛！</p><p><img alt=牛顿法对比梯度下降 loading=lazy src=/images/math/newton-vs-gradient.png></p><p>上图对比了梯度下降（橙色路径，15步）和牛顿法（红色虚线，1步）的优化过程。可以看到，利用黑塞矩阵的二阶信息后，牛顿法直接指向最优解，而梯度下降需要多次迭代。</p><h4 id=拟牛顿法>拟牛顿法<a hidden class=anchor aria-hidden=true href=#拟牛顿法>#</a></h4><p>当变量维度很高时（如深度学习中的数百万参数），计算和存储黑塞矩阵及其逆矩阵是不现实的。<strong>拟牛顿法</strong>（如BFGS算法）通过迭代近似黑塞矩阵的逆，在计算效率和收敛速度之间取得平衡。</p><h3 id=52-机器学习中的损失函数分析>5.2 机器学习中的损失函数分析<a hidden class=anchor aria-hidden=true href=#52-机器学习中的损失函数分析>#</a></h3><p>在机器学习中，损失函数的 landscape（景观）决定了训练的难度。</p><p><img alt=优化景观 loading=lazy src=/images/math/optimization-landscape.png></p><p>上图展示了著名的 Rosenbrock 函数（香蕉函数）的优化景观。红色星标表示全局最小值，橙色路径表示优化轨迹。这种函数对优化算法是严峻的考验，因为其狭窄的峡谷形状使得梯度下降容易震荡。</p><p>黑塞矩阵的特征值分析可以告诉我们：</p><ul><li><strong>条件数</strong> $\kappa = \frac{\lambda_{\max}}{\lambda_{\min}}$：条件数大表示优化景观在某些方向很陡峭，某些方向很平坦，导致梯度下降收敛慢</li><li><strong>鞍点密度</strong>：高维空间中，鞍点比局部极小值更常见，理解黑塞矩阵有助于设计逃离鞍点的算法</li></ul><h3 id=53-机器人运动学>5.3 机器人运动学<a hidden class=anchor aria-hidden=true href=#53-机器人运动学>#</a></h3><p>在机器人学中，雅可比矩阵描述了关节速度与末端执行器速度之间的关系。</p><p>设 $\mathbf{\theta} = (\theta_1, \ldots, \theta_n)^T$ 是关节角度，$\mathbf{x}$ 是末端执行器的位置，则：</p><p>$$
\dot{\mathbf{x}} = J(\mathbf{\theta}) \dot{\mathbf{\theta}}
$$</p><p>其中 $J$ 是雅可比矩阵。这个关系在：</p><ul><li>逆运动学求解</li><li>奇异位形分析</li><li>力控制</li></ul><p>中都有重要应用。</p><h3 id=54-物理与广义相对论>5.4 物理与广义相对论<a hidden class=anchor aria-hidden=true href=#54-物理与广义相对论>#</a></h3><p>在物理学中，雅可比矩阵和黑塞矩阵的概念以不同形式出现：</p><ul><li><strong>哈密顿力学</strong>：雅可比矩阵描述相空间流的演化</li><li><strong>广义相对论</strong>：克里斯托费尔符号（Christoffel symbols）与黑塞矩阵相关，描述时空的曲率</li><li><strong>统计力学</strong>：费舍尔信息矩阵是某种意义上的"黑塞矩阵"</li></ul><h3 id=55-计算机视觉与图像配准>5.5 计算机视觉与图像配准<a hidden class=anchor aria-hidden=true href=#55-计算机视觉与图像配准>#</a></h3><p>在图像配准问题中，我们需要找到最优的几何变换将一幅图像对齐到另一幅。这通常通过最小化某种距离函数来实现，其中雅可比矩阵用于计算变换的局部变形，黑塞矩阵用于加速优化收敛。</p><h2 id=第六章计算注意事项>第六章：计算注意事项<a hidden class=anchor aria-hidden=true href=#第六章计算注意事项>#</a></h2><h3 id=61-自动微分>6.1 自动微分<a hidden class=anchor aria-hidden=true href=#61-自动微分>#</a></h3><p>在现代机器学习中，手动计算雅可比矩阵和黑塞矩阵是不现实的。<strong>自动微分</strong>（Automatic Differentiation, AD）技术通过链式法则的计算机实现，可以高效准确地计算任意阶导数。</p><p>深度学习框架（PyTorch、TensorFlow、JAX）都实现了反向传播算法，这实际上就是自动微分的一种高效实现。</p><h3 id=62-数值稳定性>6.2 数值稳定性<a hidden class=anchor aria-hidden=true href=#62-数值稳定性>#</a></h3><p>实际计算中需要注意：</p><ul><li><strong>有限差分近似</strong>：$\frac{\partial f}{\partial x} \approx \frac{f(x+h) - f(x)}{h}$，但 $h$ 的选择需要权衡截断误差和舍入误差</li><li><strong>黑塞矩阵的稀疏性</strong>：在许多实际问题中，黑塞矩阵是稀疏的，利用这一点可以显著降低计算成本</li><li><strong>低秩近似</strong>：对于大规模问题，可以使用低秩近似来表示黑塞矩阵</li></ul><h3 id=63-高维挑战>6.3 高维挑战<a hidden class=anchor aria-hidden=true href=#63-高维挑战>#</a></h3><p>当维度 $n$ 很大时：</p><ul><li>存储黑塞矩阵需要 $O(n^2)$ 空间</li><li>计算黑塞矩阵需要 $O(n^2)$ 或更多时间</li><li>求逆需要 $O(n^3)$ 时间</li></ul><p>这促使了各种近似方法的发展：</p><ul><li><strong>对角近似</strong>：只保留黑塞矩阵的对角元素</li><li><strong>Kronecker 积近似</strong>：在神经网络中，利用网络结构近似黑塞矩阵</li><li><strong>随机近似</strong>：使用随机采样估计黑塞-向量乘积</li></ul><h2 id=结语>结语<a hidden class=anchor aria-hidden=true href=#结语>#</a></h2><p>雅可比矩阵和黑塞矩阵是多变量微积分的两大支柱。雅可比矩阵描述了函数在某点处的最佳线性近似，是多变量函数"导数"的自然推广；黑塞矩阵则描述了函数的二阶特性，告诉我们曲率和变化的加速度。</p><p>这对"双璧"不仅在纯数学中地位重要，在机器学习、物理学、工程学等应用领域中更是无处不在。理解它们的几何直观和代数性质，对于深入掌握多变量分析和解决实际问题至关重要。</p><p>从单变量到多变量，从一阶到二阶，数学思维的这种推广揭示了更深层次的结构和联系。正如雅可比矩阵是梯度的自然延伸，黑塞矩阵又是雅可比矩阵的再次应用——这种层层递进的结构，正是数学之美的体现。</p><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><ol><li>Rudin, W. (1976). <em>Principles of Mathematical Analysis</em>. McGraw-Hill.</li><li>Boyd, S., & Vandenberghe, L. (2004). <em>Convex Optimization</em>. Cambridge University Press.</li><li>Nocedal, J., & Wright, S. (2006). <em>Numerical Optimization</em>. Springer.</li><li>Goodfellow, I., Bengio, Y., & Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</li><li>do Carmo, M. P. (1976). <em>Differential Geometry of Curves and Surfaces</em>. Prentice-Hall.</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E7%BB%BC%E8%BF%B0/>综述</a></li><li><a href=https://s-ai-unix.github.io/tags/%E5%BE%AE%E5%88%86%E5%87%A0%E4%BD%95/>微分几何</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%AE%97%E6%B3%95/>算法</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-28-differential-geometry-autonomous-driving/><span class=title>« Prev</span><br><span>弯曲的道路，智能的决策：微分几何如何赋能自动驾驶</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-28-surface-theory-differential-geometry/><span class=title>Next »</span><br><span>曲面论的系统综述：从第一基本型到高斯绝妙定理</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 雅可比矩阵与黑塞矩阵：多变量微积分的双璧 on x" href="https://x.com/intent/tweet/?text=%e9%9b%85%e5%8f%af%e6%af%94%e7%9f%a9%e9%98%b5%e4%b8%8e%e9%bb%91%e5%a1%9e%e7%9f%a9%e9%98%b5%ef%bc%9a%e5%a4%9a%e5%8f%98%e9%87%8f%e5%be%ae%e7%a7%af%e5%88%86%e7%9a%84%e5%8f%8c%e7%92%a7&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-28-jacobian-hessian-matrices%2f&amp;hashtags=%e7%bb%bc%e8%bf%b0%2c%e5%be%ae%e5%88%86%e5%87%a0%e4%bd%95%2c%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%2c%e7%ae%97%e6%b3%95"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 雅可比矩阵与黑塞矩阵：多变量微积分的双璧 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-28-jacobian-hessian-matrices%2f&amp;title=%e9%9b%85%e5%8f%af%e6%af%94%e7%9f%a9%e9%98%b5%e4%b8%8e%e9%bb%91%e5%a1%9e%e7%9f%a9%e9%98%b5%ef%bc%9a%e5%a4%9a%e5%8f%98%e9%87%8f%e5%be%ae%e7%a7%af%e5%88%86%e7%9a%84%e5%8f%8c%e7%92%a7&amp;summary=%e9%9b%85%e5%8f%af%e6%af%94%e7%9f%a9%e9%98%b5%e4%b8%8e%e9%bb%91%e5%a1%9e%e7%9f%a9%e9%98%b5%ef%bc%9a%e5%a4%9a%e5%8f%98%e9%87%8f%e5%be%ae%e7%a7%af%e5%88%86%e7%9a%84%e5%8f%8c%e7%92%a7&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-28-jacobian-hessian-matrices%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 雅可比矩阵与黑塞矩阵：多变量微积分的双璧 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-28-jacobian-hessian-matrices%2f&title=%e9%9b%85%e5%8f%af%e6%af%94%e7%9f%a9%e9%98%b5%e4%b8%8e%e9%bb%91%e5%a1%9e%e7%9f%a9%e9%98%b5%ef%bc%9a%e5%a4%9a%e5%8f%98%e9%87%8f%e5%be%ae%e7%a7%af%e5%88%86%e7%9a%84%e5%8f%8c%e7%92%a7"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 雅可比矩阵与黑塞矩阵：多变量微积分的双璧 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-28-jacobian-hessian-matrices%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>