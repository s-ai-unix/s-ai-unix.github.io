<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>感知机的完整发展历程：从线性分类到深度学习的基石 | s-ai-unix's Blog</title><meta name=keywords content="机器学习,算法,神经网络,综述"><meta name=description content="系统综述感知机的发展历程，从早期的线性分类器到现代深度学习的基础，注重背景和演变过程的介绍，通俗易懂。"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-21-perceptron-development-history/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-21-perceptron-development-history/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-21-perceptron-development-history/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="感知机的完整发展历程：从线性分类到深度学习的基石"><meta property="og:description" content="系统综述感知机的发展历程，从早期的线性分类器到现代深度学习的基础，注重背景和演变过程的介绍，通俗易懂。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-21T08:00:00+08:00"><meta property="article:modified_time" content="2026-01-21T08:00:00+08:00"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="算法"><meta property="article:tag" content="神经网络"><meta property="article:tag" content="综述"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/perceptron-history.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/perceptron-history.jpg"><meta name=twitter:title content="感知机的完整发展历程：从线性分类到深度学习的基石"><meta name=twitter:description content="系统综述感知机的发展历程，从早期的线性分类器到现代深度学习的基础，注重背景和演变过程的介绍，通俗易懂。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"感知机的完整发展历程：从线性分类到深度学习的基石","item":"https://s-ai-unix.github.io/posts/2026-01-21-perceptron-development-history/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"感知机的完整发展历程：从线性分类到深度学习的基石","name":"感知机的完整发展历程：从线性分类到深度学习的基石","description":"系统综述感知机的发展历程，从早期的线性分类器到现代深度学习的基础，注重背景和演变过程的介绍，通俗易懂。","keywords":["机器学习","算法","神经网络","综述"],"articleBody":"引言：人工智能的原点 在人工智能的发展历程中，感知机（Perceptron）是一个具有里程碑意义的概念。它不仅是最早的机器学习算法之一，也是现代深度学习和神经网络的基础。\n感知机的故事开始于 20 世纪中叶，当时计算机科学刚刚萌芽，科学家们开始探索如何让机器具备\"学习\"的能力。\n第一章：感知机的诞生背景 1.1 早期人工智能研究的梦想 20 世纪 40 年代末到 50 年代初，随着计算机的诞生，科学家们开始思考：机器能否像人一样思考和学习？\n图灵测试：1950 年，艾伦·图灵提出了著名的图灵测试，为人工智能的发展奠定了理论基础。 神经网络的早期构想：1943 年，麦卡洛克和皮茨提出了第一个人工神经网络模型，称为麦卡洛克-皮茨神经元。 1.2 罗森布拉特的突破 1957 年，美国心理学家弗兰克·罗森布拉特（Frank Rosenblatt）在康奈尔航空实验室提出了感知机模型。他将感知机描述为\"能够通过经验自动学习的机器\"。\n罗森布拉特的工作受到了神经科学的启发，他试图模拟人类大脑中神经元的工作方式。\n第二章：感知机的核心原理 2.1 感知机的基本结构 感知机是一个简单的线性分类器，它的结构非常简单：\ngraph TD A[输入] --\u003e B[权重] C[偏置] --\u003e D[求和] B --\u003e D D --\u003e E[激活函数] E --\u003e F[输出] style A color:#ffffff,fill:#007AFF,stroke:#007AFF,stroke-width:3px style B color:#ffffff,fill:#34C759,stroke:#34C759,stroke-width:2px style C color:#ffffff,fill:#34C759,stroke:#34C759,stroke-width:2px style D color:#ffffff,fill:#007AFF,stroke:#007AFF,stroke-width:3px style E color:#ffffff,fill:#007AFF,stroke:#007AFF,stroke-width:3px style F color:#ffffff,fill:#007AFF,stroke:#007AFF,stroke-width:3px 2.2 感知机的工作原理 感知机的工作原理可以用以下公式表示：\n$$ y = \\begin{cases} 1, \u0026 \\text{if} ; w \\cdot x + b \\geq 0 \\\\ 0, \u0026 \\text{otherwise} \\end{cases} $$\n其中：\n$x$ 是输入向量 $w$ 是权重向量 $b$ 是偏置 $\\cdot$ 表示点积 2.3 感知机的学习算法 罗森布拉特还提出了感知机的学习算法，通过调整权重和偏置来实现分类：\n初始化权重和偏置 对于每个训练样本，计算输出 根据错误调整权重和偏置 重复步骤 2-3，直到收敛 下图展示了感知机学习AND问题时，权重和偏置随迭代次数的收敛过程：\n图1：感知机学习过程的权重收敛动态。可以看到，经过约4-5轮迭代后，权重 $w_1$、$w_2$ 和偏置 $b$ 都收敛到稳定值，此时感知机能够正确分类所有训练样本。\n第三章：感知机的发展历程 下图展示了从1943年麦卡洛克-皮茨神经元到2012年深度学习爆发的重要里程碑：\n图5：感知机与神经网络发展历程时间线。从1943年的理论雏形，到1957年的感知机诞生，经历了1969年的低谷（AI寒冬），1986年反向传播算法带来复兴，最终在2012年深度学习爆发。\n3.1 早期的成功与热潮 在感知机提出后的几年里，罗森布拉特和他的团队进行了一系列实验，包括使用感知机识别手写数字和简单的图像。\n1960 年，《纽约时报》甚至发表了一篇文章，标题为\"一台电子计算机能够自学：康奈尔大学展示的设备能够识别字母和数字\"。\n3.2 局限性的发现 1969 年，马文·明斯基（Marvin Minsky）和西摩尔·帕普特（Seymour Papert）出版了《感知机》（Perceptrons）一书，指出了感知机的局限性。\n他们证明了感知机无法解决非线性分类问题，最著名的例子是异或（XOR）问题。\n为什么感知机无法解决 XOR 问题？\n首先，让我们理解 XOR 问题的定义。XOR（异或）是一个二元运算，其真值表如下：\n输入 x₁ 输入 x₂ 输出 y 0 0 0 0 1 1 1 0 1 1 1 0 感知机的决策函数是：\n$$ y = \\begin{cases} 1, \u0026 \\text{如果} ; w_1 x_1 + w_2 x_2 + b \\geq 0 \\\\ 0, \u0026 \\text{否则} \\end{cases} $$\n这个决策边界 $w_1 x_1 + w_2 x_2 + b = 0$ 在二维空间中是一条直线。感知机的本质就是寻找一条直线，把不同类别的点分开。\n现在让我们看看为什么无法找到一条直线将 XOR 问题的两类样本正确分开。\n在二维平面上，四个样本点的位置为：\n$(0, 0) \\to$ 类别 0 $(0, 1) \\to$ 类别 1 $(1, 0) \\to$ 类别 1 $(1, 1) \\to$ 类别 0 如图所示，类别 0（蓝色）的点 $(0,0)$ 和 $(1,1)$ 位于主对角线上，类别 1（橙色）的点 $(0,1)$ 和 $(1,0)$ 位于副对角线上。\n从几何上看，要使得一条直线能够正确分类，类别 0 的两个点必须在直线的一侧，类别 1 的两个点必须在直线的另一侧。然而，观察这四个点的位置：\n类别 0 的点 $(0,0)$ 和 $(1,1)$ 位于主对角线上 类别 1 的点 $(0,1)$ 和 $(1,0)$ 位于副对角线上 几何上的直观理解：任意一条直线都会将对角线上的两点分开到两侧，因此无法同时将 $(0,0)$ 和 $(1,1)$ 放在同一侧，同时将 $(0,1)$ 和 $(1,0)$ 放在另一侧。无论你怎么画这条直线，总有一个点会被分错。\n代数上的严格证明：\n为了更严谨地说明这个问题，让我们用代数方法来证明。假设存在权重 $w_1, w_2$ 和偏置 $b$ 能够正确分类 XOR 问题，那么必须满足以下四个不等式：\n对于 $(0,0) \\to 0$：$w_1 \\cdot 0 + w_2 \\cdot 0 + b \u003c 0$，即 $b \u003c 0$ 对于 $(0,1) \\to 1$：$w_1 \\cdot 0 + w_2 \\cdot 1 + b \\geq 0$，即 $w_2 + b \\geq 0$ 对于 $(1,0) \\to 1$：$w_1 \\cdot 1 + w_2 \\cdot 0 + b \\geq 0$，即 $w_1 + b \\geq 0$ 对于 $(1,1) \\to 0$：$w_1 \\cdot 1 + w_2 \\cdot 1 + b \u003c 0$，即 $w_1 + w_2 + b \u003c 0$ 从不等式 (2) 和 (3) 可得：\n$$w_2 \\geq -b \\quad \\text{和} \\quad w_1 \\geq -b$$\n由于 $b \u003c 0$，所以 $-b \u003e 0$，这意味着 $w_1 \u003e 0$ 且 $w_2 \u003e 0$。也就是说，两个权重都必须是正数。\n现在，让我们从不等式 (2) 和 (3) 出发，将两个不等式相加：\n$$w_1 + w_2 \\geq -2b$$\n两边同时加上 $b$：\n$$w_1 + w_2 + b \\geq -2b + b = -b$$\n由于 $b \u003c 0$，所以 $-b \u003e 0$，这意味着：\n$$w_1 + w_2 + b \\geq -b \u003e 0$$\n但是，这与不等式 (4) $w_1 + w_2 + b \u003c 0$ 矛盾！\n因此，不存在任何权重 $w_1, w_2$ 和偏置 $b$ 能够使感知机正确解决 XOR 问题。\n更一般的结论：感知机只能解决线性可分的问题，而 XOR 问题是线性不可分的。这是单层感知机的根本局限性。\n下图直观对比了线性可分的AND问题与线性不可分的XOR问题：\n图2：左图展示AND问题是线性可分的，可以找到一条直线（绿色）将两类样本分开；右图展示XOR问题是线性不可分的，任何直线都无法同时将两个红色点（输出0）分到一侧，两个蓝色点（输出1）分到另一侧。\ngraph TD A[XOR 问题] --\u003e B[线性不可分] B --\u003e C[单层感知机无法解决] C --\u003e D[需要多层感知机] style A color:#ffffff,fill:#FF9500,stroke:#FF9500,stroke-width:2px style B color:#ffffff,fill:#FF9500,stroke:#FF9500,stroke-width:2px style C color:#ffffff,fill:#FF9500,stroke:#FF9500,stroke-width:2px style D color:#ffffff,fill:#FF9500,stroke:#FF9500,stroke-width:2px 3.3 人工智能寒冬 《感知机》一书的出版对人工智能研究产生了深远的影响。许多研究机构和政府机构减少了对神经网络研究的资助，人工智能进入了\"寒冬\"时期。\n3.4 突破与复兴 20 世纪 80 年代，神经网络研究迎来了复兴。但为什么感知机在遭受致命打击后又\"复活\"了呢？关键在于研究者们找到了绕过感知机局限性的方法。\n3.4.1 多层感知机的突破 明斯基和帕普特的证明有一个关键的限制：它只适用于单层感知机。如果我们能够把多个感知机串联起来，形成多层网络，情况会如何呢？\n1986 年，鲁梅尔哈特（Rumelhart）、辛顿（Hinton）和威廉姆斯（Williams）发表了具有里程碑意义的论文，展示了多层感知机（Multi-Layer Perceptron, MLP）可以完美解决 XOR 问题。这个发现为感知机的复兴奠定了基础。\n多层感知机为什么能解决 XOR 问题？\n多层感知机的秘密武器在于隐藏层（Hidden Layer）。通过在输入层和输出层之间插入一层或多层隐藏神经元，我们可以将多条直线（线性边界）组合成复杂的非线性边界。\n一个解决 XOR 问题的典型网络结构是：\n输入层 (2个神经元) $\\to$ 隐藏层 (2个神经元) $\\to$ 输出层 (1个神经元) 让我们用几何直觉来理解这个\"魔法\"是如何发生的：\n第一层感知机画出两条直线，开始对空间进行划分：\n直线 1：$x_1 + x_2 - 0.5 = 0$ 直线 2：$x_1 + x_2 - 1.5 = 0$ 第二层感知机将第一层的输出进行逻辑组合，相当于对两个线性分类区域进行非线性组合。\n从数学上看，多层感知机的决策函数变成了复合函数的形式：\n$$ \\begin{align} f(x) = \\sigma(\u0026w^{(2)}1 \\cdot \\sigma(w^{(1)}{11} x_1 + w^{(1)}_{12} x_2 + b^{(1)}1) \\ \u0026+ w^{(2)}2 \\cdot \\sigma(w^{(1)}{21} x_1 + w^{(1)}{22} x_2 + b^{(1)}_2) + b^{(2)}) \\end{align} $$\n其中 $\\sigma$ 是激活函数（如 Sigmoid 或 ReLU）。这个函数不再是简单的线性函数 $w \\cdot x + b$，而是一个非线性复合函数。\n具体来说，我们可以将 XOR 问题分解为更基本的逻辑运算：\n隐藏层神经元 1：实现 AND 逻辑（$x_1 \\land x_2$） 隐藏层神经元 2：实现 OR 逻辑（$x_1 \\lor x_2$） 输出神经元：实现组合逻辑 XOR 的本质可以表示为：\n$$ x_1 \\oplus x_2 = (x_1 \\lor x_2) \\land \\neg(x_1 \\land x_2) $$\n也就是说，XOR = OR 且不 AND。多层感知机通过组合多个线性分类器（隐藏层神经元），最终实现了这个非线性分类任务。这就是为什么单层感知机做不到的事情，多层感知机可以轻松做到。\n下图展示了多层感知机如何用两条直线解决XOR问题：\n图3：多层感知机的隐藏层用两条直线（橙色和绿色）将输入空间划分为四个区域。通过将区域B和区域C（位于两条直线之间）归为一类，区域A和区域D归为另一类，实现了XOR的非线性分类。\n3.4.2 反向传播算法的突破 你可能会问：既然多层感知机的理论早在 60 年代就存在了，为什么直到 80 年代才真正发挥作用？答案是：训练方法。\n虽然我们知道多层网络可以解决 XOR 问题，但如何找到合适的权重和偏置呢？单层感知机的学习规则对于多层网络不再适用。反向传播算法（Backpropagation）的出现，终于解决了这个难题。\n反向传播算法的核心思想非常优雅：\n前向传播：输入数据通过网络，计算每一层的输出，最终得到预测结果 误差计算：比较预测结果与真实标签，计算误差 反向传播：这是关键！将误差从输出层向输入层反向传播，利用微积分中的链式法则计算每个权重对总误差的贡献（梯度） 权重更新：根据计算出的梯度，使用梯度下降法更新每个权重 链式法则在反向传播中扮演着核心角色。对于一个多层网络，第 $l$ 层的权重 $w^{(l)}$ 对损失函数 $L$ 的梯度可以表示为：\n$$ \\frac{\\partial L}{\\partial w^{(l)}} = \\frac{\\partial L}{\\partial a^{(l)}} \\cdot \\frac{\\partial a^{(l)}}{\\partial z^{(l)}} \\cdot \\frac{\\partial z^{(l)}}{\\partial w^{(l)}} $$\n这个公式告诉我们：要计算某一层权重的梯度，需要将误差从后往前逐层传递，每一层都乘以该层的局部梯度。这种\"分而治之\"的思想，使得训练深层神经网络成为可能。\n3.4.3 理论认识的发展 随着实践的发展，研究者们对感知机的局限性有了更深入的认识：\n万能逼近定理（Universal Approximation Theorem，1989）：Cybenko 和 Hornik 等人独立证明了一个令人惊讶的定理：一个具有至少一个隐藏层的神经网络，只要有足够多的隐藏神经元，就可以以任意精度逼近任何连续函数。这意味着理论上，神经网络可以解决任何复杂的分类问题。\n感知机并没有\"死\"：明斯基和帕普特的批评虽然指出了单层感知机的局限性，但这并不意味着感知机的思想是错误的。单层感知机虽然有限制，但它是多层网络的基本组成单元。现代深度学习的每一层，本质上仍然是感知机的变体——加权求和加非线性激活。\n计算能力的提升：20 世纪 80 年代以后，计算机性能大幅提升，使得训练复杂的神经网络成为可能。理论和实践的结合，终于让感知机迎来了春天。\n3.4.4 突破与复兴的关键技术 总结起来，感知机复兴的关键技术突破包括：\n多层感知机（MLP）：通过堆叠多个感知机构成多层网络，实现非线性分类。这是绕过线性可分限制的直接方法。\n反向传播算法：高效训练深层神经网络的关键算法。没有它，多层网络只是一个理论模型；有了它，多层网络变成了可训练的实用工具。\n新的激活函数：\nSigmoid：早期的选择，将输出映射到 $(0,1)$ 区间，便于概率解释 ReLU（2010s）：$f(x) = max(0, x)$，看似简单的改进，却解决了梯度消失问题，大幅加速了训练过程，成为现代深度学习的默认选择 下图对比了感知机与神经网络中常用的激活函数：\n图4：四种常用激活函数的对比。阶跃函数是原始感知机的激活函数，但不连续不可导；Sigmoid和Tanh是平滑的可导函数，适合梯度下降但存在梯度消失问题；ReLU简单高效，解决了梯度消失问题，成为现代深度学习的默认选择。\n3.5 深度学习的崛起 21 世纪初，随着大数据和 GPU 的普及，深度学习开始崛起：\n卷积神经网络（CNN）：用于图像处理 循环神经网络（RNN）：用于序列数据处理 Transformer：用于自然语言处理 第四章：感知机的现代应用 4.1 图像识别 感知机的原理被应用于图像识别系统，包括：\n手写数字识别 人脸识别 物体检测 4.2 自然语言处理 感知机的概念被扩展到自然语言处理领域：\n文本分类 情感分析 机器翻译 4.3 医疗诊断 感知机和神经网络被用于医疗诊断：\n疾病预测 影像分析 药物发现 第五章：感知机的影响与意义 5.1 理论意义 感知机的提出为机器学习和人工智能的发展奠定了基础：\n它是第一个可以自动学习的算法 它引入了权重调整和监督学习的概念 它为后来的神经网络研究提供了基础 5.2 实践意义 感知机的原理在现代人工智能系统中得到了广泛应用：\n所有的深度学习模型都是基于感知机的扩展 感知机的学习算法是现代优化方法的基础 感知机的思想影响了整个机器学习领域 结语：感知机的未来 感知机从诞生到现在已经有了近 70 年的历史。虽然它本身是一个简单的线性分类器，但它的思想和原理对人工智能的发展产生了深远的影响。\n随着计算能力的不断提高和新算法的不断提出，感知机的原理将继续在人工智能领域发挥重要作用。从简单的线性分类到复杂的深度学习系统，感知机的演变历程展示了人类对智能的不断探索和追求。\n正如罗森布拉特在 1958 年所写的那样：“感知机不仅是一种工具，更是一种思考方式。”\n参考文献：\nRosenblatt, F. (1958). The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. Psychological Review, 65(6), 386-408. Minsky, M., \u0026 Papert, S. (1969). Perceptrons: An Introduction to Computational Geometry. MIT Press. Rumelhart, D. E., Hinton, G. E., \u0026 Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323(6088), 533-536. ","wordCount":"749","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/perceptron-history.jpg","datePublished":"2026-01-21T08:00:00+08:00","dateModified":"2026-01-21T08:00:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-21-perceptron-development-history/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">感知机的完整发展历程：从线性分类到深度学习的基石</h1><div class=post-description>系统综述感知机的发展历程，从早期的线性分类器到现代深度学习的基础，注重背景和演变过程的介绍，通俗易懂。</div><div class=post-meta><span title='2026-01-21 08:00:00 +0800 CST'>January 21, 2026</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>749 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/perceptron-history.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/perceptron-history.jpg alt=感知机发展历程></a><figcaption>感知机：从线性分类到深度学习的基石</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e7%9a%84%e5%8e%9f%e7%82%b9 aria-label=引言：人工智能的原点>引言：人工智能的原点</a></li><li><a href=#%e7%ac%ac%e4%b8%80%e7%ab%a0%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e8%af%9e%e7%94%9f%e8%83%8c%e6%99%af aria-label=第一章：感知机的诞生背景>第一章：感知机的诞生背景</a><ul><li><a href=#11-%e6%97%a9%e6%9c%9f%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e7%a0%94%e7%a9%b6%e7%9a%84%e6%a2%a6%e6%83%b3 aria-label="1.1 早期人工智能研究的梦想">1.1 早期人工智能研究的梦想</a></li><li><a href=#12-%e7%bd%97%e6%a3%ae%e5%b8%83%e6%8b%89%e7%89%b9%e7%9a%84%e7%aa%81%e7%a0%b4 aria-label="1.2 罗森布拉特的突破">1.2 罗森布拉特的突破</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e7%ab%a0%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e6%a0%b8%e5%bf%83%e5%8e%9f%e7%90%86 aria-label=第二章：感知机的核心原理>第二章：感知机的核心原理</a><ul><li><a href=#21-%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e5%9f%ba%e6%9c%ac%e7%bb%93%e6%9e%84 aria-label="2.1 感知机的基本结构">2.1 感知机的基本结构</a></li><li><a href=#22-%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86 aria-label="2.2 感知机的工作原理">2.2 感知机的工作原理</a></li><li><a href=#23-%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95 aria-label="2.3 感知机的学习算法">2.3 感知机的学习算法</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e7%ab%a0%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e5%8f%91%e5%b1%95%e5%8e%86%e7%a8%8b aria-label=第三章：感知机的发展历程>第三章：感知机的发展历程</a><ul><li><a href=#31-%e6%97%a9%e6%9c%9f%e7%9a%84%e6%88%90%e5%8a%9f%e4%b8%8e%e7%83%ad%e6%bd%ae aria-label="3.1 早期的成功与热潮">3.1 早期的成功与热潮</a></li><li><a href=#32-%e5%b1%80%e9%99%90%e6%80%a7%e7%9a%84%e5%8f%91%e7%8e%b0 aria-label="3.2 局限性的发现">3.2 局限性的发现</a></li><li><a href=#33-%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e5%af%92%e5%86%ac aria-label="3.3 人工智能寒冬">3.3 人工智能寒冬</a></li><li><a href=#34-%e7%aa%81%e7%a0%b4%e4%b8%8e%e5%a4%8d%e5%85%b4 aria-label="3.4 突破与复兴">3.4 突破与复兴</a><ul><li><a href=#341-%e5%a4%9a%e5%b1%82%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e7%aa%81%e7%a0%b4 aria-label="3.4.1 多层感知机的突破">3.4.1 多层感知机的突破</a></li><li><a href=#342-%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e7%9a%84%e7%aa%81%e7%a0%b4 aria-label="3.4.2 反向传播算法的突破">3.4.2 反向传播算法的突破</a></li><li><a href=#343-%e7%90%86%e8%ae%ba%e8%ae%a4%e8%af%86%e7%9a%84%e5%8f%91%e5%b1%95 aria-label="3.4.3 理论认识的发展">3.4.3 理论认识的发展</a></li><li><a href=#344-%e7%aa%81%e7%a0%b4%e4%b8%8e%e5%a4%8d%e5%85%b4%e7%9a%84%e5%85%b3%e9%94%ae%e6%8a%80%e6%9c%af aria-label="3.4.4 突破与复兴的关键技术">3.4.4 突破与复兴的关键技术</a></li></ul></li><li><a href=#35-%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%b4%9b%e8%b5%b7 aria-label="3.5 深度学习的崛起">3.5 深度学习的崛起</a></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e7%ab%a0%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e7%8e%b0%e4%bb%a3%e5%ba%94%e7%94%a8 aria-label=第四章：感知机的现代应用>第四章：感知机的现代应用</a><ul><li><a href=#41-%e5%9b%be%e5%83%8f%e8%af%86%e5%88%ab aria-label="4.1 图像识别">4.1 图像识别</a></li><li><a href=#42-%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%e5%a4%84%e7%90%86 aria-label="4.2 自然语言处理">4.2 自然语言处理</a></li><li><a href=#43-%e5%8c%bb%e7%96%97%e8%af%8a%e6%96%ad aria-label="4.3 医疗诊断">4.3 医疗诊断</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e7%ab%a0%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e5%bd%b1%e5%93%8d%e4%b8%8e%e6%84%8f%e4%b9%89 aria-label=第五章：感知机的影响与意义>第五章：感知机的影响与意义</a><ul><li><a href=#51-%e7%90%86%e8%ae%ba%e6%84%8f%e4%b9%89 aria-label="5.1 理论意义">5.1 理论意义</a></li><li><a href=#52-%e5%ae%9e%e8%b7%b5%e6%84%8f%e4%b9%89 aria-label="5.2 实践意义">5.2 实践意义</a></li></ul></li><li><a href=#%e7%bb%93%e8%af%ad%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e6%9c%aa%e6%9d%a5 aria-label=结语：感知机的未来>结语：感知机的未来</a></li></ul></div></details></div><div class=post-content><h2 id=引言人工智能的原点>引言：人工智能的原点<a hidden class=anchor aria-hidden=true href=#引言人工智能的原点>#</a></h2><p>在人工智能的发展历程中，感知机（Perceptron）是一个具有里程碑意义的概念。它不仅是最早的机器学习算法之一，也是现代深度学习和神经网络的基础。</p><p>感知机的故事开始于 20 世纪中叶，当时计算机科学刚刚萌芽，科学家们开始探索如何让机器具备"学习"的能力。</p><h2 id=第一章感知机的诞生背景>第一章：感知机的诞生背景<a hidden class=anchor aria-hidden=true href=#第一章感知机的诞生背景>#</a></h2><h3 id=11-早期人工智能研究的梦想>1.1 早期人工智能研究的梦想<a hidden class=anchor aria-hidden=true href=#11-早期人工智能研究的梦想>#</a></h3><p>20 世纪 40 年代末到 50 年代初，随着计算机的诞生，科学家们开始思考：机器能否像人一样思考和学习？</p><ul><li><strong>图灵测试</strong>：1950 年，艾伦·图灵提出了著名的图灵测试，为人工智能的发展奠定了理论基础。</li><li><strong>神经网络的早期构想</strong>：1943 年，麦卡洛克和皮茨提出了第一个人工神经网络模型，称为麦卡洛克-皮茨神经元。</li></ul><h3 id=12-罗森布拉特的突破>1.2 罗森布拉特的突破<a hidden class=anchor aria-hidden=true href=#12-罗森布拉特的突破>#</a></h3><p>1957 年，美国心理学家弗兰克·罗森布拉特（Frank Rosenblatt）在康奈尔航空实验室提出了感知机模型。他将感知机描述为"能够通过经验自动学习的机器"。</p><p>罗森布拉特的工作受到了神经科学的启发，他试图模拟人类大脑中神经元的工作方式。</p><h2 id=第二章感知机的核心原理>第二章：感知机的核心原理<a hidden class=anchor aria-hidden=true href=#第二章感知机的核心原理>#</a></h2><h3 id=21-感知机的基本结构>2.1 感知机的基本结构<a hidden class=anchor aria-hidden=true href=#21-感知机的基本结构>#</a></h3><p>感知机是一个简单的线性分类器，它的结构非常简单：</p><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>graph TD
A[输入] --> B[权重]
C[偏置] --> D[求和]
B --> D
D --> E[激活函数]
E --> F[输出]
style A color:#ffffff,fill:#007AFF,stroke:#007AFF,stroke-width:3px
style B color:#ffffff,fill:#34C759,stroke:#34C759,stroke-width:2px
style C color:#ffffff,fill:#34C759,stroke:#34C759,stroke-width:2px
style D color:#ffffff,fill:#007AFF,stroke:#007AFF,stroke-width:3px
style E color:#ffffff,fill:#007AFF,stroke:#007AFF,stroke-width:3px
style F color:#ffffff,fill:#007AFF,stroke:#007AFF,stroke-width:3px</div></div><h3 id=22-感知机的工作原理>2.2 感知机的工作原理<a hidden class=anchor aria-hidden=true href=#22-感知机的工作原理>#</a></h3><p>感知机的工作原理可以用以下公式表示：</p><p>$$
y = \begin{cases}
1, & \text{if} ; w \cdot x + b \geq 0 \\
0, & \text{otherwise}
\end{cases}
$$</p><p>其中：</p><ul><li>$x$ 是输入向量</li><li>$w$ 是权重向量</li><li>$b$ 是偏置</li><li>$\cdot$ 表示点积</li></ul><h3 id=23-感知机的学习算法>2.3 感知机的学习算法<a hidden class=anchor aria-hidden=true href=#23-感知机的学习算法>#</a></h3><p>罗森布拉特还提出了感知机的学习算法，通过调整权重和偏置来实现分类：</p><ol><li>初始化权重和偏置</li><li>对于每个训练样本，计算输出</li><li>根据错误调整权重和偏置</li><li>重复步骤 2-3，直到收敛</li></ol><p>下图展示了感知机学习AND问题时，权重和偏置随迭代次数的收敛过程：</p><p><img alt=感知机学习过程 loading=lazy src=/images/plots/perceptron-learning-process.png></p><p>图1：感知机学习过程的权重收敛动态。可以看到，经过约4-5轮迭代后，权重 $w_1$、$w_2$ 和偏置 $b$ 都收敛到稳定值，此时感知机能够正确分类所有训练样本。</p><h2 id=第三章感知机的发展历程>第三章：感知机的发展历程<a hidden class=anchor aria-hidden=true href=#第三章感知机的发展历程>#</a></h2><p>下图展示了从1943年麦卡洛克-皮茨神经元到2012年深度学习爆发的重要里程碑：</p><p><img alt=感知机发展历程 loading=lazy src=/images/plots/perceptron-timeline.png></p><p>图5：感知机与神经网络发展历程时间线。从1943年的理论雏形，到1957年的感知机诞生，经历了1969年的低谷（AI寒冬），1986年反向传播算法带来复兴，最终在2012年深度学习爆发。</p><h3 id=31-早期的成功与热潮>3.1 早期的成功与热潮<a hidden class=anchor aria-hidden=true href=#31-早期的成功与热潮>#</a></h3><p>在感知机提出后的几年里，罗森布拉特和他的团队进行了一系列实验，包括使用感知机识别手写数字和简单的图像。</p><p>1960 年，《纽约时报》甚至发表了一篇文章，标题为"一台电子计算机能够自学：康奈尔大学展示的设备能够识别字母和数字"。</p><h3 id=32-局限性的发现>3.2 局限性的发现<a hidden class=anchor aria-hidden=true href=#32-局限性的发现>#</a></h3><p>1969 年，马文·明斯基（Marvin Minsky）和西摩尔·帕普特（Seymour Papert）出版了《感知机》（Perceptrons）一书，指出了感知机的局限性。</p><p>他们证明了感知机无法解决非线性分类问题，最著名的例子是异或（XOR）问题。</p><p><strong>为什么感知机无法解决 XOR 问题？</strong></p><p>首先，让我们理解 XOR 问题的定义。XOR（异或）是一个二元运算，其真值表如下：</p><table><thead><tr><th style=text-align:center>输入 x₁</th><th style=text-align:center>输入 x₂</th><th style=text-align:center>输出 y</th></tr></thead><tbody><tr><td style=text-align:center>0</td><td style=text-align:center>0</td><td style=text-align:center>0</td></tr><tr><td style=text-align:center>0</td><td style=text-align:center>1</td><td style=text-align:center>1</td></tr><tr><td style=text-align:center>1</td><td style=text-align:center>0</td><td style=text-align:center>1</td></tr><tr><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>0</td></tr></tbody></table><p>感知机的决策函数是：</p><p>$$
y = \begin{cases}
1, & \text{如果} ; w_1 x_1 + w_2 x_2 + b \geq 0 \\
0, & \text{否则}
\end{cases}
$$</p><p>这个决策边界 $w_1 x_1 + w_2 x_2 + b = 0$ 在二维空间中是一条<strong>直线</strong>。感知机的本质就是寻找一条直线，把不同类别的点分开。</p><p>现在让我们看看为什么无法找到一条直线将 XOR 问题的两类样本正确分开。</p><p>在二维平面上，四个样本点的位置为：</p><ul><li>$(0, 0) \to$ 类别 0</li><li>$(0, 1) \to$ 类别 1</li><li>$(1, 0) \to$ 类别 1</li><li>$(1, 1) \to$ 类别 0</li></ul><p><img alt="XOR 问题可视化" loading=lazy src=/images/covers/xor-problem.png></p><p>如图所示，类别 0（蓝色）的点 $(0,0)$ 和 $(1,1)$ 位于主对角线上，类别 1（橙色）的点 $(0,1)$ 和 $(1,0)$ 位于副对角线上。</p><p>从几何上看，要使得一条直线能够正确分类，类别 0 的两个点必须在直线的一侧，类别 1 的两个点必须在直线的另一侧。然而，观察这四个点的位置：</p><ul><li>类别 0 的点 $(0,0)$ 和 $(1,1)$ 位于主对角线上</li><li>类别 1 的点 $(0,1)$ 和 $(1,0)$ 位于副对角线上</li></ul><p><strong>几何上的直观理解</strong>：任意一条直线都会将对角线上的两点分开到两侧，因此无法同时将 $(0,0)$ 和 $(1,1)$ 放在同一侧，同时将 $(0,1)$ 和 $(1,0)$ 放在另一侧。无论你怎么画这条直线，总有一个点会被分错。</p><p><strong>代数上的严格证明</strong>：</p><p>为了更严谨地说明这个问题，让我们用代数方法来证明。假设存在权重 $w_1, w_2$ 和偏置 $b$ 能够正确分类 XOR 问题，那么必须满足以下四个不等式：</p><ol><li>对于 $(0,0) \to 0$：$w_1 \cdot 0 + w_2 \cdot 0 + b &lt; 0$，即 <strong>$b &lt; 0$</strong></li><li>对于 $(0,1) \to 1$：$w_1 \cdot 0 + w_2 \cdot 1 + b \geq 0$，即 <strong>$w_2 + b \geq 0$</strong></li><li>对于 $(1,0) \to 1$：$w_1 \cdot 1 + w_2 \cdot 0 + b \geq 0$，即 <strong>$w_1 + b \geq 0$</strong></li><li>对于 $(1,1) \to 0$：$w_1 \cdot 1 + w_2 \cdot 1 + b &lt; 0$，即 <strong>$w_1 + w_2 + b &lt; 0$</strong></li></ol><p>从不等式 (2) 和 (3) 可得：</p><p>$$w_2 \geq -b \quad \text{和} \quad w_1 \geq -b$$</p><p>由于 $b &lt; 0$，所以 $-b > 0$，这意味着 $w_1 > 0$ 且 $w_2 > 0$。也就是说，两个权重都必须是正数。</p><p>现在，让我们从不等式 (2) 和 (3) 出发，将两个不等式相加：</p><p>$$w_1 + w_2 \geq -2b$$</p><p>两边同时加上 $b$：</p><p>$$w_1 + w_2 + b \geq -2b + b = -b$$</p><p>由于 $b &lt; 0$，所以 $-b > 0$，这意味着：</p><p>$$w_1 + w_2 + b \geq -b > 0$$</p><p>但是，这与不等式 (4) <strong>$w_1 + w_2 + b &lt; 0$</strong> 矛盾！</p><p>因此，不存在任何权重 $w_1, w_2$ 和偏置 $b$ 能够使感知机正确解决 XOR 问题。</p><p><strong>更一般的结论</strong>：感知机只能解决<strong>线性可分</strong>的问题，而 XOR 问题是<strong>线性不可分</strong>的。这是单层感知机的根本局限性。</p><p>下图直观对比了线性可分的AND问题与线性不可分的XOR问题：</p><p><img alt="线性可分 vs 线性不可分" loading=lazy src=/images/plots/perceptron-linear-separability.png></p><p>图2：左图展示AND问题是线性可分的，可以找到一条直线（绿色）将两类样本分开；右图展示XOR问题是线性不可分的，任何直线都无法同时将两个红色点（输出0）分到一侧，两个蓝色点（输出1）分到另一侧。</p><div class=mermaid-wrapper style="background:#fff;padding:2rem 1rem;margin:2rem 0;border-radius:8px;box-shadow:0 2px 12px rgba(0,0,0,8%)"><div class=mermaid>graph TD
A[XOR 问题] --> B[线性不可分]
B --> C[单层感知机无法解决]
C --> D[需要多层感知机]
style A color:#ffffff,fill:#FF9500,stroke:#FF9500,stroke-width:2px
style B color:#ffffff,fill:#FF9500,stroke:#FF9500,stroke-width:2px
style C color:#ffffff,fill:#FF9500,stroke:#FF9500,stroke-width:2px
style D color:#ffffff,fill:#FF9500,stroke:#FF9500,stroke-width:2px</div></div><h3 id=33-人工智能寒冬>3.3 人工智能寒冬<a hidden class=anchor aria-hidden=true href=#33-人工智能寒冬>#</a></h3><p>《感知机》一书的出版对人工智能研究产生了深远的影响。许多研究机构和政府机构减少了对神经网络研究的资助，人工智能进入了"寒冬"时期。</p><h3 id=34-突破与复兴>3.4 突破与复兴<a hidden class=anchor aria-hidden=true href=#34-突破与复兴>#</a></h3><p>20 世纪 80 年代，神经网络研究迎来了复兴。但为什么感知机在遭受致命打击后又"复活"了呢？关键在于研究者们找到了绕过感知机局限性的方法。</p><h4 id=341-多层感知机的突破>3.4.1 多层感知机的突破<a hidden class=anchor aria-hidden=true href=#341-多层感知机的突破>#</a></h4><p>明斯基和帕普特的证明有一个关键的限制：它只适用于<strong>单层</strong>感知机。如果我们能够把多个感知机串联起来，形成多层网络，情况会如何呢？</p><p>1986 年，鲁梅尔哈特（Rumelhart）、辛顿（Hinton）和威廉姆斯（Williams）发表了具有里程碑意义的论文，展示了<strong>多层感知机</strong>（Multi-Layer Perceptron, MLP）可以完美解决 XOR 问题。这个发现为感知机的复兴奠定了基础。</p><p><strong>多层感知机为什么能解决 XOR 问题？</strong></p><p>多层感知机的秘密武器在于<strong>隐藏层</strong>（Hidden Layer）。通过在输入层和输出层之间插入一层或多层隐藏神经元，我们可以将多条直线（线性边界）组合成复杂的非线性边界。</p><p>一个解决 XOR 问题的典型网络结构是：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入层 (2个神经元) $\to$ 隐藏层 (2个神经元) $\to$ 输出层 (1个神经元)
</span></span></code></pre></div><p>让我们用几何直觉来理解这个"魔法"是如何发生的：</p><ol><li><p><strong>第一层感知机</strong>画出两条直线，开始对空间进行划分：</p><ul><li>直线 1：$x_1 + x_2 - 0.5 = 0$</li><li>直线 2：$x_1 + x_2 - 1.5 = 0$</li></ul></li><li><p><strong>第二层感知机</strong>将第一层的输出进行逻辑组合，相当于对两个线性分类区域进行<strong>非线性组合</strong>。</p></li></ol><p>从数学上看，多层感知机的决策函数变成了复合函数的形式：</p><p>$$
\begin{align}
f(x) = \sigma(&amp;w^{(2)}<em>1 \cdot \sigma(w^{(1)}</em>{11} x_1 + w^{(1)}_{12} x_2 + b^{(1)}<em>1) \
&+ w^{(2)}<em>2 \cdot \sigma(w^{(1)}</em>{21} x_1 + w^{(1)}</em>{22} x_2 + b^{(1)}_2) + b^{(2)})
\end{align}
$$</p><p>其中 $\sigma$ 是激活函数（如 Sigmoid 或 ReLU）。这个函数不再是简单的线性函数 $w \cdot x + b$，而是一个<strong>非线性复合函数</strong>。</p><p>具体来说，我们可以将 XOR 问题分解为更基本的逻辑运算：</p><ul><li><strong>隐藏层神经元 1</strong>：实现 AND 逻辑（$x_1 \land x_2$）</li><li><strong>隐藏层神经元 2</strong>：实现 OR 逻辑（$x_1 \lor x_2$）</li><li><strong>输出神经元</strong>：实现组合逻辑</li></ul><p>XOR 的本质可以表示为：</p><p>$$
x_1 \oplus x_2 = (x_1 \lor x_2) \land \neg(x_1 \land x_2)
$$</p><p>也就是说，XOR = OR <strong>且不</strong> AND。多层感知机通过组合多个线性分类器（隐藏层神经元），最终实现了这个非线性分类任务。这就是为什么单层感知机做不到的事情，多层感知机可以轻松做到。</p><p>下图展示了多层感知机如何用两条直线解决XOR问题：</p><p><img alt=多层感知机解决XOR loading=lazy src=/images/plots/perceptron-mlp-xor.png></p><p>图3：多层感知机的隐藏层用两条直线（橙色和绿色）将输入空间划分为四个区域。通过将区域B和区域C（位于两条直线之间）归为一类，区域A和区域D归为另一类，实现了XOR的非线性分类。</p><h4 id=342-反向传播算法的突破>3.4.2 反向传播算法的突破<a hidden class=anchor aria-hidden=true href=#342-反向传播算法的突破>#</a></h4><p>你可能会问：既然多层感知机的理论早在 60 年代就存在了，为什么直到 80 年代才真正发挥作用？答案是：<strong>训练方法</strong>。</p><p>虽然我们知道多层网络可以解决 XOR 问题，但如何找到合适的权重和偏置呢？单层感知机的学习规则对于多层网络不再适用。<strong>反向传播算法</strong>（Backpropagation）的出现，终于解决了这个难题。</p><p>反向传播算法的核心思想非常优雅：</p><ol><li><strong>前向传播</strong>：输入数据通过网络，计算每一层的输出，最终得到预测结果</li><li><strong>误差计算</strong>：比较预测结果与真实标签，计算误差</li><li><strong>反向传播</strong>：这是关键！将误差从输出层向输入层反向传播，利用微积分中的<strong>链式法则</strong>计算每个权重对总误差的贡献（梯度）</li><li><strong>权重更新</strong>：根据计算出的梯度，使用梯度下降法更新每个权重</li></ol><p>链式法则在反向传播中扮演着核心角色。对于一个多层网络，第 $l$ 层的权重 $w^{(l)}$ 对损失函数 $L$ 的梯度可以表示为：</p><p>$$
\frac{\partial L}{\partial w^{(l)}} = \frac{\partial L}{\partial a^{(l)}} \cdot \frac{\partial a^{(l)}}{\partial z^{(l)}} \cdot \frac{\partial z^{(l)}}{\partial w^{(l)}}
$$</p><p>这个公式告诉我们：要计算某一层权重的梯度，需要将误差从后往前逐层传递，每一层都乘以该层的局部梯度。这种"分而治之"的思想，使得训练深层神经网络成为可能。</p><h4 id=343-理论认识的发展>3.4.3 理论认识的发展<a hidden class=anchor aria-hidden=true href=#343-理论认识的发展>#</a></h4><p>随着实践的发展，研究者们对感知机的局限性有了更深入的认识：</p><ol><li><p><strong>万能逼近定理</strong>（Universal Approximation Theorem，1989）：Cybenko 和 Hornik 等人独立证明了一个令人惊讶的定理：一个具有至少一个隐藏层的神经网络，只要有足够多的隐藏神经元，就可以以任意精度逼近任何连续函数。这意味着理论上，神经网络可以解决任何复杂的分类问题。</p></li><li><p><strong>感知机并没有"死"</strong>：明斯基和帕普特的批评虽然指出了单层感知机的局限性，但这并不意味着感知机的思想是错误的。单层感知机虽然有限制，但它是多层网络的基本组成单元。现代深度学习的每一层，本质上仍然是感知机的变体——加权求和加非线性激活。</p></li><li><p><strong>计算能力的提升</strong>：20 世纪 80 年代以后，计算机性能大幅提升，使得训练复杂的神经网络成为可能。理论和实践的结合，终于让感知机迎来了春天。</p></li></ol><h4 id=344-突破与复兴的关键技术>3.4.4 突破与复兴的关键技术<a hidden class=anchor aria-hidden=true href=#344-突破与复兴的关键技术>#</a></h4><p>总结起来，感知机复兴的关键技术突破包括：</p><ul><li><p><strong>多层感知机（MLP）</strong>：通过堆叠多个感知机构成多层网络，实现非线性分类。这是绕过线性可分限制的直接方法。</p></li><li><p><strong>反向传播算法</strong>：高效训练深层神经网络的关键算法。没有它，多层网络只是一个理论模型；有了它，多层网络变成了可训练的实用工具。</p></li><li><p><strong>新的激活函数</strong>：</p><ul><li><strong>Sigmoid</strong>：早期的选择，将输出映射到 $(0,1)$ 区间，便于概率解释</li><li><strong>ReLU（2010s）</strong>：$f(x) = max(0, x)$，看似简单的改进，却解决了梯度消失问题，大幅加速了训练过程，成为现代深度学习的默认选择</li></ul></li></ul><p>下图对比了感知机与神经网络中常用的激活函数：</p><p><img alt=激活函数对比 loading=lazy src=/images/plots/perceptron-activation-functions.png></p><p>图4：四种常用激活函数的对比。阶跃函数是原始感知机的激活函数，但不连续不可导；Sigmoid和Tanh是平滑的可导函数，适合梯度下降但存在梯度消失问题；ReLU简单高效，解决了梯度消失问题，成为现代深度学习的默认选择。</p><h3 id=35-深度学习的崛起>3.5 深度学习的崛起<a hidden class=anchor aria-hidden=true href=#35-深度学习的崛起>#</a></h3><p>21 世纪初，随着大数据和 GPU 的普及，深度学习开始崛起：</p><ul><li><strong>卷积神经网络（CNN）</strong>：用于图像处理</li><li><strong>循环神经网络（RNN）</strong>：用于序列数据处理</li><li><strong>Transformer</strong>：用于自然语言处理</li></ul><h2 id=第四章感知机的现代应用>第四章：感知机的现代应用<a hidden class=anchor aria-hidden=true href=#第四章感知机的现代应用>#</a></h2><h3 id=41-图像识别>4.1 图像识别<a hidden class=anchor aria-hidden=true href=#41-图像识别>#</a></h3><p>感知机的原理被应用于图像识别系统，包括：</p><ul><li>手写数字识别</li><li>人脸识别</li><li>物体检测</li></ul><h3 id=42-自然语言处理>4.2 自然语言处理<a hidden class=anchor aria-hidden=true href=#42-自然语言处理>#</a></h3><p>感知机的概念被扩展到自然语言处理领域：</p><ul><li>文本分类</li><li>情感分析</li><li>机器翻译</li></ul><h3 id=43-医疗诊断>4.3 医疗诊断<a hidden class=anchor aria-hidden=true href=#43-医疗诊断>#</a></h3><p>感知机和神经网络被用于医疗诊断：</p><ul><li>疾病预测</li><li>影像分析</li><li>药物发现</li></ul><h2 id=第五章感知机的影响与意义>第五章：感知机的影响与意义<a hidden class=anchor aria-hidden=true href=#第五章感知机的影响与意义>#</a></h2><h3 id=51-理论意义>5.1 理论意义<a hidden class=anchor aria-hidden=true href=#51-理论意义>#</a></h3><p>感知机的提出为机器学习和人工智能的发展奠定了基础：</p><ul><li>它是第一个可以自动学习的算法</li><li>它引入了权重调整和监督学习的概念</li><li>它为后来的神经网络研究提供了基础</li></ul><h3 id=52-实践意义>5.2 实践意义<a hidden class=anchor aria-hidden=true href=#52-实践意义>#</a></h3><p>感知机的原理在现代人工智能系统中得到了广泛应用：</p><ul><li>所有的深度学习模型都是基于感知机的扩展</li><li>感知机的学习算法是现代优化方法的基础</li><li>感知机的思想影响了整个机器学习领域</li></ul><h2 id=结语感知机的未来>结语：感知机的未来<a hidden class=anchor aria-hidden=true href=#结语感知机的未来>#</a></h2><p>感知机从诞生到现在已经有了近 70 年的历史。虽然它本身是一个简单的线性分类器，但它的思想和原理对人工智能的发展产生了深远的影响。</p><p>随着计算能力的不断提高和新算法的不断提出，感知机的原理将继续在人工智能领域发挥重要作用。从简单的线性分类到复杂的深度学习系统，感知机的演变历程展示了人类对智能的不断探索和追求。</p><p>正如罗森布拉特在 1958 年所写的那样：&ldquo;感知机不仅是一种工具，更是一种思考方式。&rdquo;</p><hr><p><strong>参考文献</strong>：</p><ol><li>Rosenblatt, F. (1958). The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. Psychological Review, 65(6), 386-408.</li><li>Minsky, M., & Papert, S. (1969). Perceptrons: An Introduction to Computational Geometry. MIT Press.</li><li>Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323(6088), 533-536.</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%AE%97%E6%B3%95/>算法</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a></li><li><a href=https://s-ai-unix.github.io/tags/%E7%BB%BC%E8%BF%B0/>综述</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-21-shannon-entropy-comprehensive-guide/><span class=title>« Prev</span><br><span>香农信息熵：不确定性的数学刻度</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-21-from-shell-to-agent/><span class=title>Next »</span><br><span>从 Shell 到 Agent：命令行到自然语言的演进之路</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 感知机的完整发展历程：从线性分类到深度学习的基石 on x" href="https://x.com/intent/tweet/?text=%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e5%ae%8c%e6%95%b4%e5%8f%91%e5%b1%95%e5%8e%86%e7%a8%8b%ef%bc%9a%e4%bb%8e%e7%ba%bf%e6%80%a7%e5%88%86%e7%b1%bb%e5%88%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%9f%ba%e7%9f%b3&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-perceptron-development-history%2f&amp;hashtags=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%2c%e7%ae%97%e6%b3%95%2c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%2c%e7%bb%bc%e8%bf%b0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 感知机的完整发展历程：从线性分类到深度学习的基石 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-perceptron-development-history%2f&amp;title=%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e5%ae%8c%e6%95%b4%e5%8f%91%e5%b1%95%e5%8e%86%e7%a8%8b%ef%bc%9a%e4%bb%8e%e7%ba%bf%e6%80%a7%e5%88%86%e7%b1%bb%e5%88%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%9f%ba%e7%9f%b3&amp;summary=%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e5%ae%8c%e6%95%b4%e5%8f%91%e5%b1%95%e5%8e%86%e7%a8%8b%ef%bc%9a%e4%bb%8e%e7%ba%bf%e6%80%a7%e5%88%86%e7%b1%bb%e5%88%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%9f%ba%e7%9f%b3&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-perceptron-development-history%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 感知机的完整发展历程：从线性分类到深度学习的基石 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-perceptron-development-history%2f&title=%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e5%ae%8c%e6%95%b4%e5%8f%91%e5%b1%95%e5%8e%86%e7%a8%8b%ef%bc%9a%e4%bb%8e%e7%ba%bf%e6%80%a7%e5%88%86%e7%b1%bb%e5%88%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%9f%ba%e7%9f%b3"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 感知机的完整发展历程：从线性分类到深度学习的基石 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-21-perceptron-development-history%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var t,n=e.innerHTML,s=!1;for(n.indexOf("<em>")!==-1&&(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("</em>")!==-1&&(t=n.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),t=t.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),t!==n&&(n=t,s=!0)),n.indexOf("<em>$")!==-1&&(t=n.replace(/<em>\$/g,"$"),t!==n&&(n=t,s=!0)),n.indexOf("</em>$")!==-1&&(t=n.replace(/<\/em>\$/g,"$"),t!==n&&(n=t,s=!0));n.indexOf("<em>")!==-1&&n.indexOf("</em>")!==-1;){if(t=n.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),t=t.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),t===n)break;n=t,s=!0}n.indexOf("_{")!==-1&&(t=n.replace(/_{/g,"_{"),t!==n&&(n=t,s=!0)),n.indexOf("}_")!==-1&&(t=n.replace(/}_/g,"}"),t!==n&&(n=t,s=!0)),n.includes("</em>{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<\/em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),n.includes("<em>{\\mathrm{")&&(t=n.replace(/\\mathbf\{(W|x|y|z)\}<em>\{\\mathrm\{([a-z]+)\}\}/g,"\\mathbf{$1}_{\\mathrm{$2}}"),t=t.replace(/([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t=t.replace(/\\nabla ([LHf])<em>\{\\mathrm\{([a-z]+)\}\}/g,`\\nabla $1_{\\mathrm{$2}}`),t=t.replace(/([A-Z])<em>\{\\mathrm\{([a-z]+)\}\}/g,"$1_{\\mathrm{$2}}"),t!==n&&(n=t,s=!0)),(n.includes("</em>")||n.includes("<em>"))&&(t=n.replace(/(\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$\$[^$]*?)<em>([^$]*?)<\/em>/g,"$1_{$2}"),t=t.replace(/(\$[^$]*?)<em>/g,"$1"),t=t.replace(/<em>([^$]*?\$)/g,"$1"),t=t.replace(/(\$[^$]*?)<\/em>/g,"$1"),t=t.replace(/<\/em>([^$]*?\$)/g,"$1"),t!==n&&(n=t,s=!0)),n.indexOf("\\\\")!==-1&&(t=n.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),t!==n&&(n=t,s=!0)),s&&(e.innerHTML=n)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",packages:{"[+]":["noerrors","noundefined","boldsymbol"]},macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>