<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>流形：从弯曲空间到深度学习与机器人学的漫游 | s-ai-unix's Blog</title><meta name=keywords content="黎曼几何,深度学习,微分几何"><meta name=description content="从高斯和黎曼的几何革命出发，系统讲解流形的概念、历史、数学基础，深入探讨流形在深度学习（流形假设、球面Embedding、双曲空间）和机器人学（SO(3)、四元数、SLAM）中的核心应用，并通过四个实战案例展示流形的强大威力"><meta name=author content="s-ai-unix"><link rel=canonical href=https://s-ai-unix.github.io/posts/2026-01-12-manifold-deep-learning-robotics/><link crossorigin=anonymous href=/assets/css/stylesheet.0833aff7e1cf567ecfb9755701ee95a80535f671e2e1a0a770c9bc20a0a7b5bb.css integrity="sha256-CDOv9+HPVn7PuXVXAe6VqAU19nHi4aCncMm8IKCntbs=" rel="preload stylesheet" as=style><link rel=icon href=https://s-ai-unix.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://s-ai-unix.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://s-ai-unix.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://s-ai-unix.github.io/apple-touch-icon.png><link rel=mask-icon href=https://s-ai-unix.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://s-ai-unix.github.io/posts/2026-01-12-manifold-deep-learning-robotics/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://s-ai-unix.github.io/posts/2026-01-12-manifold-deep-learning-robotics/"><meta property="og:site_name" content="s-ai-unix's Blog"><meta property="og:title" content="流形：从弯曲空间到深度学习与机器人学的漫游"><meta property="og:description" content="从高斯和黎曼的几何革命出发，系统讲解流形的概念、历史、数学基础，深入探讨流形在深度学习（流形假设、球面Embedding、双曲空间）和机器人学（SO(3)、四元数、SLAM）中的核心应用，并通过四个实战案例展示流形的强大威力"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-12T23:10:00+08:00"><meta property="article:modified_time" content="2026-01-12T23:10:00+08:00"><meta property="article:tag" content="黎曼几何"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="微分几何"><meta property="og:image" content="https://s-ai-unix.github.io/images/covers/1620641788421-7a1c342ea42e.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s-ai-unix.github.io/images/covers/1620641788421-7a1c342ea42e.jpg"><meta name=twitter:title content="流形：从弯曲空间到深度学习与机器人学的漫游"><meta name=twitter:description content="从高斯和黎曼的几何革命出发，系统讲解流形的概念、历史、数学基础，深入探讨流形在深度学习（流形假设、球面Embedding、双曲空间）和机器人学（SO(3)、四元数、SLAM）中的核心应用，并通过四个实战案例展示流形的强大威力"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://s-ai-unix.github.io/posts/"},{"@type":"ListItem","position":2,"name":"流形：从弯曲空间到深度学习与机器人学的漫游","item":"https://s-ai-unix.github.io/posts/2026-01-12-manifold-deep-learning-robotics/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"流形：从弯曲空间到深度学习与机器人学的漫游","name":"流形：从弯曲空间到深度学习与机器人学的漫游","description":"从高斯和黎曼的几何革命出发，系统讲解流形的概念、历史、数学基础，深入探讨流形在深度学习（流形假设、球面Embedding、双曲空间）和机器人学（SO(3)、四元数、SLAM）中的核心应用，并通过四个实战案例展示流形的强大威力","keywords":["黎曼几何","深度学习","微分几何"],"articleBody":"引言：当空间开始弯曲 想象一下，你是一只生活在二维平面上的蚂蚁。你可以自由地在平面上行走，测量距离，画出直线和三角形。你所知道的几何——欧几里得几何——似乎是那么完美、那么自洽。\n现在，让我们把这只蚂蚁放到一个巨大的篮球表面。\n蚂蚁会发现什么呢？首先，它会发现\"直线\"不再存在。如果它一直往前走，最终会回到起点——它走的是\"大圆\"，而不是直线。其次，它会发现三角形的内角和不再是180度，而是大于180度。最神奇的是，如果它足够聪明，它可以通过测量距离和角度来发现这个空间的曲率——尽管它从未\"跳出\"过这个二维曲面。\n这就是内蕴几何的魔力，也是流形（Manifold）概念的起点。\n在接下来的篇幅中，我将带你进行一次从19世纪的几何革命到21世纪人工智能的漫游。我们会看到：\n流形的诞生：高斯和黎曼如何改变了我们对空间的理解 流形的数学：为什么流形是\"局部平坦、整体弯曲\"的几何对象 流形在深度学习：从流形假设到球面Embedding 流形在机器人学：从四元数到SLAM 实战案例：四个让你真正理解流形威力的例子 准备好了吗？让我们开始这段跨越时空的数学之旅。\n第一章：几何的危机与重生 1.1 欧几里得的第五公设 公元前300年，亚历山大港的数学家欧几里得写下了《几何原本》——这部奠定了西方科学基础的巨著。欧几里得从五条公设出发，推导出无数深刻的几何定理。其中第五条公设——平行公设——却让数学家们困惑了两千多年。\n平行公设：如果一条直线与两条直线相交，且同侧内角之和小于两个直角，则这两条直线在该侧无限延伸后必定相交。\n这条公设看起来比其他公设复杂得多。数学家们不禁想问：它能否从前四条公设中推导出来？如果可以，那它就不是真正的公设；如果不可以，那是否存在一种\"非欧几里得几何\"，其中平行公设不成立？\n1.2 罗巴切夫斯基的革命 1829年，俄罗斯数学家罗巴切夫斯基（Nikolai Lobachevsky）发表了第一篇非欧几何的论文。他假设过一点可以作多条平行线，由此推导出一套完整的几何体系——双曲几何。\n在双曲几何中：\n三角形的内角和小于180度 相似三角形只有大小完全相同才算相似 不存在矩形，因为四边形的内角和小于360度 罗巴切夫斯基的发现彻底改变了数学家对几何本质的认识：几何不是关于\"真实空间\"的真理，而是关于某种抽象结构的逻辑系统。\n1.3 高斯的绝妙定理 几乎在同一时间，德国数学家高斯也在思考类似的问题。高斯不仅是一个理论家，还是一个实测工作者——他参与了汉诺威的大地测量。在测量中，高斯意识到一个深刻的问题：地球表面的几何能告诉我们什么？\n1827年，高斯发表了绝妙定理（Theorema Egregium）：曲面的高斯曲率是一个内蕴不变量——它完全由曲面自身的几何性质决定，与曲面如何嵌入周围空间无关。\n这个定理的惊人之处在于：曲率不是\"外部\"观察者看到的弯曲，而是曲面\"内部\"几何结构的必然结果。一只生活在曲面上的蚂蚁，通过测量距离和角度，可以计算出它所在空间的曲率——即使它永远无法\"看到\"曲面在三维空间中的弯曲方式。\n高斯的工作开创了内蕴几何的新时代，为流形的诞生奠定了基础。\n1.4 黎曼的推广 1854年，高斯的学生黎曼（Bernhard Riemann）在哥廷根大学发表了著名的就职演讲《论作为几何学基础的假设》。黎曼将高斯的二维曲面理论推广到任意维数，创立了黎曼几何。\n黎曼的核心思想是：几何不在于\"空间是什么\"，而在于\"我们如何测量空间中的距离\"。\n黎曼提出用一个度规张量（Metric Tensor）来描述空间的几何性质。度规告诉我们如何在空间的每一点测量距离和角度。有了度规，我们就可以定义：\n曲线的长度 向量的点积 角度和面积 平行移动 测地线（最直的曲线） 黎曼几何成为了20世纪物理学的基石。1915年，爱因斯坦用黎曼几何描述时空的弯曲，建立了广义相对论。\n第二章：流形的数学定义 2.1 什么是流形？ 在数学中，流形（Manifold）是一个抽象的空间概念。直观地说，流形是一个\"局部看起来像欧几里得空间\"的空间。\n流形的定义：\n一个 $n$ 维流形 $M$ 是一个满足以下条件的拓扑空间：\n局部欧几里得性：对于 $M$ 中的每一点 $p$，存在一个开集 $U \\subseteq M$ 包含 $p$，以及一个从 $U$ 到 $\\mathbb{R}^n$ 的开集的同胚映射（称为坐标图）： $$\\varphi: U \\to \\mathbb{R}^n$$\n相容性：如果两个坐标图 $\\varphi: U \\to \\mathbb{R}^n$ 和 $\\psi: V \\to \\mathbb{R}^n$ 有重叠 $U \\cap V$，那么映射 $\\psi \\circ \\varphi^{-1}$ 和 $\\varphi \\circ \\psi^{-1}$ 是光滑的（$C^\\infty$）。\n第二可数性：流形可以被可数个坐标图覆盖。\n满足条件2的坐标图集合称为图册（Atlas）。如果所有坐标变换都是光滑的，我们称这个流形为光滑流形（Smooth Manifold）。\n2.2 从直观到抽象 让我用几个例子来解释这个抽象定义：\n例子1：圆周\n圆周 $S^1 = {(x, y) \\in \\mathbb{R}^2 \\mid x^2 + y^2 = 1}$ 是一个一维流形。\n为什么？因为圆周的每一点附近都\"看起来像\"一条直线。我们可以用角度 $\\theta$ 作为局部坐标。对于点 $(1, 0)$ 附近，可以用 $\\theta \\in (-\\pi/2, \\pi/2)$；对于点 $(0, 1)$ 附近，可以用 $\\theta \\in (0, \\pi)$，等等。\n例子2：球面\n球面 $S^2 = {(x, y, z) \\in \\mathbb{R}^3 \\mid x^2 + y^2 + z^2 = 1}$ 是一个二维流形。\n我们可以用经度 $\\phi$ 和纬度 $\\theta$ 作为局部坐标。但球面有一个特点：没有任何单个坐标图能覆盖整个球面（南极和北极会导致纬度坐标的奇点）。这就是为什么我们需要多个坐标图——每个坐标图覆盖一部分，然后用坐标变换将它们\"粘\"在一起。\n例子3：环面\n环面（甜甜圈表面）$T^2 = S^1 \\times S^1$ 是一个二维流形。它可以看作是两个圆周的直积。环面是\"紧致\"（compact）流形的经典例子——它是有限的，但没有边界。\n2.3 为什么要用流形？ 你可能会问：为什么要引入这么抽象的概念？直接用 $\\mathbb{R}^n$ 不就行了吗？\n答案是：现实世界中的许多空间不是平坦的 $\\mathbb{R}^n$，而是弯曲的流形。\n考虑以下几个例子：\n旋转：三维空间中的旋转不是 $\\mathbb{R}^3$ 中的向量，而是特殊正交群 $SO(3)$——一个三维流形 姿态：机器人的姿态由旋转矩阵或四元数描述，这些都定义在流形上 图像：所有MNIST数字图像的集合构成一个流形——尽管是高维的，但本质上只有低维结构 蛋白质结构：蛋白质的所有可能构象形成一个流形 流形让我们能够用局部线性化的方法处理全局弯曲的空间。在流形的每一点，我们可以建立局部坐标系（切空间），在切空间中我们可以使用熟悉的线性代数工具。\n2.4 切空间与向量场 在流形上，我们需要一种方法来描述\"方向\"和\"变化\"。这就是切空间（Tangent Space）的概念。\n切空间的定义：\n在流形 $M$ 的点 $p$ 处，切空间 $T_p M$ 是所有经过 $p$ 的曲线的切向量的集合。更抽象地说，切空间是所有在 $p$ 处为零的导数的空间。\n对于 $n$ 维流形，切空间 $T_p M$ 是一个 $n$ 维向量空间。如果 $p$ 的局部坐标是 $(x^1, \\dots, x^n)$，那么切空间的一组基是： $$\\left\\lbrace\\frac{\\partial}{\\partial x^1}, \\dots, \\frac{\\partial}{\\partial x^n}\\right\\rbrace$$\n向量场是流形上的一个函数，它为每一点 $p$ 指定一个切向量 $V(p) \\in T_p M$。向量场可以看作是流形上的\"速度场\"或\"方向场\"。\n2.5 余切空间与张量 与切空间对偶的是余切空间（Cotangent Space）$T^*_p M$。余切空间是切空间的对偶空间，其中的元素是切空间上的线性泛函（线性函数）。\n如果 $\\omega \\in T^*_p M$ 且 $v \\in T_p M$，那么 $\\omega(v)$ 是一个标量。\n更一般地，我们可以定义张量（Tensor）。张量是多线性映射，例如：\n$(0, 1)$ 型张量：余切向量 $(1, 0)$ 型张量：切向量 $(0, 2)$ 型张量：双线性形式（如度规张量） 张量在物理学和工程中无处不在。它们是描述几何和物理量的自然语言。\n第三章：黎曼流形 3.1 度规张量 黎曼流形（Riemannian Manifold）是一个配备了度规张量（Metric Tensor）的光滑流形。度规是一个对称正定的 $(0, 2)$ 型张量：\n$$g_p: T_p M \\times T_p M \\to \\mathbb{R}$$\n度规告诉我们如何在每一点的切空间中测量长度和角度。对于切向量 $u, v \\in T_p M$：\n长度：$|u| = \\sqrt{g_p(u, u)}$ 角度：$\\cos\\theta = \\frac{g_p(u, v)}{|u||v|}$ 在局部坐标 $(x^1, \\dots, x^n)$ 中，度规可以写成矩阵形式：\n$$g = \\begin{pmatrix} g_{11} \u0026 g_{12} \u0026 \\cdots \u0026 g_{1n} \\ g_{21} \u0026 g_{22} \u0026 \\cdots \u0026 g_{2n} \\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\ g_{n1} \u0026 g_{n2} \u0026 \\cdots \u0026 g_{nn} \\end{pmatrix}$$\n其中 $g_{ij} = g\\left(\\frac{\\partial}{\\partial x^i}, \\frac{\\partial}{\\partial x^j}\\right)$。\n3.2 弧长与测地线 有了度规，我们就可以定义曲线的弧长。对于参数化曲线 $\\gamma(t) = (x^1(t), \\dots, x^n(t))$，$t \\in [a, b]$：\n$$\\text{Length}(\\gamma) = \\int_a^b \\sqrt{\\sum_{i,j} g_{ij}(\\gamma(t)) \\frac{dx^i}{dt} \\frac{dx^j}{dt}} , dt$$\n测地线（Geodesic）是曲面上\"最直的曲线\"——它是弧长的极值曲线。测地线满足测地线方程：\n$$\\frac{d^2x^k}{dt^2} + \\sum_{i,j} \\Gamma^k_{ij} \\frac{dx^i}{dt} \\frac{dx^j}{dt} = 0$$\n其中 $\\Gamma^k_{ij}$ 是克里斯托费尔符号（Christoffel Symbol），由度规及其导数计算得出：\n$$\\Gamma^k_{ij} = \\frac{1}{2} g^{kl} \\left( \\frac{\\partial g_{il}}{\\partial x^j} + \\frac{\\partial g_{jl}}{\\partial x^i} - \\frac{\\partial g_{ij}}{\\partial x^l} \\right)$$\n3.3 曲率 曲率是黎曼流形最重要的几何不变量。黎曼曲率张量（Riemann Curvature Tensor）描述了流形的弯曲程度：\n$$R^i_{jkl} = \\frac{\\partial \\Gamma^i_{jl}}{\\partial x^k} - \\frac{\\partial \\Gamma^i_{jk}}{\\partial x^l} + \\Gamma^i_{km}\\Gamma^m_{jl} - \\Gamma^i_{lm}\\Gamma^m_{jk}$$\n曲率张量衡量平行移动与路径的依赖程度。在平坦空间（如 $\\mathbb{R}^n$）中，沿任何闭合路径平行移动一个向量都会回到原来的值；但在弯曲空间中，平行移动的结果与路径有关。\n从黎曼曲率张量，我们可以缩并得到：\n里奇张量（Ricci Tensor）：$R_{jl} = R^i_{jil}$ 标量曲率（Scalar Curvature）：$R = g^{jl} R_{jl}$ 3.4 联络与平行移动 在流形上，我们不能用普通的偏导数来比较不同点的向量——因为没有\"全局坐标系\"。为此，我们引入联络（Connection）的概念。\n协变导数（Covariant Derivative）$\\nabla$ 是一种\"平行移动+微分\"的运算。对于向量场 $V = V^i \\frac{\\partial}{\\partial x^i}$：\n$$\\nabla_j V^i = \\frac{\\partial V^i}{\\partial x^j} + \\Gamma^i_{jk} V^k$$\n协变导数让我们能够在流形上进行\"微分\"——这是微积分在弯曲空间中的推广。\n平行移动是协变导数的积分。一个向量沿曲线 $\\gamma(t)$ 平行移动，如果它的协变导数为零：\n$$\\frac{DV^i}{dt} = \\frac{dV^i}{dt} + \\Gamma^i_{jk} \\frac{dx^j}{dt} V^k = 0$$\n第四章：流形在深度学习中的应用 4.1 流形假设 深度学习成功的关键洞察之一是流形假设（Manifold Hypothesis）：\n高维数据（如图像、文本、声音）实际上分布在低维流形上。\n这个假设是什么意思呢？\n考虑一张 $28 \\times 28$ 像素的MNIST数字图像。从像素的角度看，这个图像是 $\\mathbb{R}^{784}$ 中的一个点。但并非 $\\mathbb{R}^{784}$ 中的所有点都是有效的数字图像。实际上，所有\"看起来像数字\"的图像只占整个空间的很小一部分——它们分布在某个低维流形上。\n类似地：\n人脸图像分布在\"人脸流形\"上 自然语言句子分布在\"语义流形\"上 语音信号分布在\"语音流形\"上 流形学习的目标是从高维数据中恢复这个低维流形的结构。传统的方法包括：\n等度量映射（Isomap）：用图上的最短路径近似测地距离 局部线性嵌入（LLE）：保持局部线性结构 拉普拉斯特征映射（LE）：保持局部邻域关系 4.2 球面Embedding 在深度学习中，我们经常需要将离散对象（如单词、用户、物品）嵌入到连续空间中。传统的嵌入方法使用欧几里得空间 $\\mathbb{R}^d$，但越来越多的研究表明，球面空间 $S^{d-1}$ 可能是更好的选择。\n为什么是球面？\n归一化：球面上的向量自动归一，这对于比较相似度很有用 曲率：球面的正曲率可能更适合某些类型的数据 边界问题：欧几里得空间没有边界，而球面是紧致的 SphereFace 和 FaceNet 等人脸识别方法使用球面嵌入。在球面上，两个向量的内积 $\\langle u, v \\rangle = \\cos\\theta$ 直接与它们之间的角度相关。\n球面上的分布：\n在球面上，我们可以定义各种概率分布，如冯·米塞斯-费舍尔分布（von Mises-Fisher distribution）：\n$$f(x; \\mu, \\kappa) = C_d(\\kappa) \\exp\\left(\\kappa \\mu^\\top x\\right)$$\n其中 $\\mu$ 是均值方向，$\\kappa$ 是集中度参数。这个分布在方向统计中有广泛应用。\n4.3 双曲空间Embedding 如果说球面适合\"封闭\"的数据，那么双曲空间则适合\"树状\"或\"层次化\"的数据。\n双曲空间是曲率为 $-1$ 的空间。在双曲几何中：\n过一点可以作无穷多条平行线 三角形的内角和小于180度 面积和周长增长比欧几里得空间快得多 双曲空间的一个惊人性质是它能自然地表示层次结构。考虑一棵树：从根节点到叶节点的路径长度随深度指数增长。双曲空间的\"指数增长体积\"正好匹配这种结构。\n庞加莱球模型是最常用的双曲空间模型：\n$$\\mathbb{B}^d = {x \\in \\mathbb{R}^d \\mid |x| \u003c 1}$$\n距离函数为：\n$$d(u, v) = \\text{arcosh}\\left(1 + 2\\frac{|u - v|^2}{(1 - |u|^2)(1 - |v|^2)}\\right)$$\n双曲嵌入的应用：\n词嵌入：Nickel和Kiela (2017) 展示了双曲空间能更好地捕捉WordNet的层次结构 图嵌入：双曲空间能自然地表示图的层次结构 少样本学习：双曲空间可能更适合捕捉概念的层次关系 4.4 李群与神经网络 李群（Lie Group）既是群（代数结构）又是流形（几何结构），且群运算在流形上是光滑的。李群在深度学习中有重要应用，特别是在处理对称性和变换时。\n特殊正交群 $SO(3)$ 是三维旋转的群：\n单位元是恒等旋转 群运算是旋转的复合 它是一个三维流形 在机器人学和计算机视觉中，我们需要处理旋转。一个常见的问题是：如何在神经网络中参数化旋转？\n几种常见的方法：\n旋转矩阵：$3 \\times 3$ 正交矩阵，缺点是有冗余（9个参数，只有3个自由度） 欧拉角：三个旋转角，缺点是有万向锁问题 四元数：四个参数，约束 $|\\mathbf{q}| = 1$，这是最常用的方法 轴角：三个参数（旋转轴 $\\mathbf{u}$ 和角度 $\\theta$） 图神经网络中的等变性：\n近年来，等变神经网络（Equivariant Neural Networks）成为一个热门研究方向。这类网络在变换群（如旋转、平移）下保持等变性：\n$$f(g \\cdot x) = g \\cdot f(x)$$\n代表工作包括：\nE(n)等变图神经网络（EGNN） SE(3)-等变Transformer 匹兹堡大学的等变消息传递网络 4.5 黎曼流形优化 深度学习中的优化通常使用随机梯度下降（SGD）及其变体。但当参数位于流形上时，普通的欧几里得优化可能不再适用。\n黎曼流形优化是研究如何在黎曼流形上进行优化的学科。核心思想是：\n使用黎曼梯度代替普通梯度 使用黎曼度量定义学习率 使用黎曼更新来移动点 黎曼梯度是普通梯度关于度规的\"提升\"：\n$$\\text{grad}M f = g^{-1} \\text{grad}{\\mathbb{R}^n} f$$\n黎曼流形SGD（RMSGD）的更新规则为：\n$$x_{t+1} = \\text{Exp}_{x_t}(-\\eta \\cdot \\text{grad}_M f(x_t))$$\n其中 $\\text{Exp}_x(v)$ 是从 $x$ 出发、沿 $v$ 方向的测地线（指数映射）。\n应用场景：\n正交约束优化：当参数矩阵需要保持正交时（如RNN的正交初始化） 低秩矩阵恢复：当参数矩阵需要保持低秩时 概率分布优化：当参数是概率分布时（如KL散度优化） 第五章：流形在机器人学中的应用 5.1 姿态表示：SO(3)与SE(3) 机器人学的核心问题之一是如何表示和操作三维空间中的姿态（position and orientation）。\n位置可以用 $\\mathbb{R}^3$ 中的向量表示，这很简单。\n方向则复杂得多。描述三维旋转的群是：\nSO(3)：特殊正交群，三维旋转，3个自由度 SE(3)：特殊欧几里得群，三维刚体变换（旋转+平移），6个自由度 SO(3)的参数化：\n旋转可以用多种方式参数化：\n旋转矩阵：$R \\in \\mathbb{R}^{3 \\times 3}$，满足 $R^\\top R = I$，$\\det(R) = 1$ 轴角：旋转轴 $\\mathbf{u} \\in S^2$ 和角度 $\\theta \\in \\mathbb{R}$ 欧拉角：三个顺序旋转角（如XYZ, ZYX等），有12种约定 四元数：$\\mathbf{q} = (w, x, y, z) \\in \\mathbb{R}^4$，满足 $|\\mathbf{q}| = 1$ 四元数是SO(3)的最佳参数化：\n四元数相比其他表示有几个优势：\n无奇点：不像欧拉角有万向锁 紧凑：4个参数，而旋转矩阵有9个 高效：旋转的复合只需要四元数乘法 数值稳定：归一化后保持单位范数 四元数的乘法：\n两个四元数 $\\mathbf{q}_1 = (w_1, \\mathbf{v}_1)$ 和 $\\mathbf{q}_2 = (w_2, \\mathbf{v}_2)$ 的乘积为：\n$$\\mathbf{q}_1 \\otimes \\mathbf{q}_2 = (w_1 w_2 - \\mathbf{v}_1 \\cdot \\mathbf{v}_2, w_1 \\mathbf{v}_2 + w_2 \\mathbf{v}_1 + \\mathbf{v}_1 \\times \\mathbf{v}_2)$$\n旋转的复合对应四元数的乘法。\n5.2 运动学与动力学 机器人的运动学描述关节角度与末端执行器位置之间的关系。逆运动学是根据期望末端位置计算所需关节角度的问题。\n对于串联机械臂（如UR5），运动学方程为：\n$$T_{ee} = T_{base} \\cdot A_1(q_1) \\cdot A_2(q_2) \\cdot \\cdots \\cdot A_n(q_n)$$\n其中 $T_{ee}$ 是末端执行器的变换矩阵，$A_i(q_i)$ 是第 $i$ 个关节的变换矩阵，$q_i$ 是关节角度。\n雅可比矩阵描述关节速度与末端速度之间的关系：\n$$\\dot{x} = J(q) \\dot{q}$$\n雅可比矩阵在奇点处会奇异——这是机器人学中的一个核心问题。\n动力学描述关节力矩与运动之间的关系。欧拉-拉格朗日方程是：\n$$M(q) \\ddot{q} + C(q, \\dot{q}) \\dot{q} + G(q) = \\tau$$\n其中 $M(q)$ 是质量矩阵，$C(q, \\dot{q})$ 是科里奥利和离心力项，$G(q)$ 是重力项，$\\tau$ 是关节力矩。\n所有这些方程都在流形（SO(3)或SE(3)）上操作。理解流形的几何对于正确处理机器人问题至关重要。\n5.3 运动规划 机器人运动规划的目标是找到一条从起点到目标的无碰撞路径。\n配置空间（Configuration Space）是机器人所有可能配置的集合。对于 $n$ 自由度的机器人，配置空间是 $\\mathbb{R}^n$（位置关节）或 $T^n$（旋转关节）的子流形。\n传统方法：\n人工势场法：在配置空间中构造势函数，障碍物产生斥力，目标产生引力 快速随机树（RRT）：通过随机采样构建路径树 概率路线图（PRM）：预先构建随机路线图 流形上的运动规划：\n当配置空间是流形时，运动规划变得更加复杂：\n边界处理：流形可能没有边界（如SO(3)），需要考虑循环路径 曲率影响：流形的曲率影响\"最短路径\"（测地线）的形状 拓扑障碍：流形可能有非平凡的拓扑（如环面的\"洞\"），这可能导致路径不存在 SE(3)上的运动规划是一个特别重要的问题。SE(3) 的拓扑是 $\\mathbb{R}^3 \\times SO(3)$，它有一个非平凡的基本群（旋转360度和720度在拓扑上是不同的）。\n5.4 状态估计与SLAM 状态估计是机器人确定自身位置和姿态的问题。当机器人移动时，传感器噪声和运动不确定性会累积，因此需要滤波器或优化来融合多个测量。\n扩展卡尔曼滤波（EKF）是最经典的方法：\n预测步骤：使用运动模型预测状态 更新步骤：使用传感器测量更新状态 在SE(3)上使用EKF需要小心处理：\n状态必须在流形上定义 协方差矩阵需要适当重参数化 雅可比矩阵需要在切空间中计算 同步定位与建图（SLAM）是机器人领域的核心问题。SLAM的目标是同时构建环境地图并定位机器人在其中的位置。\n图优化是现代SLAM的主流方法。问题被建模为一个图：\n节点：机器人的位姿（SE(3)元素）和路标位置（$\\mathbb{R}^3$元素） 边：约束来自运动模型和观测模型 图优化是一个最小二乘问题：\n$$\\min_X \\sum_{(i,j) \\in E} |z_{ij} - h_{ij}(X_i, X_j)|^2_{\\Sigma_{ij}}$$\n其中 $X$ 是所有位姿的集合，$z_{ij}$ 是测量值，$h_{ij}$ 是测量模型，$\\Sigma_{ij}$ 是协方差矩阵。\n流形上的优化需要使用左扰动或右扰动来参数化SE(3)的切空间。\n5.5 强化学习中的流形 机器人强化学习是让机器人通过试错学习控制策略的方法。当状态或动作空间是流形时，我们需要特殊处理。\n状态空间的流形结构：\n姿态：末端执行器的姿态是SE(3)元素 关节配置：旋转关节的角度在 $S^1$ 或 $SO(3)$ 上 接触状态：接触/分离是离散状态 动作空间的流形结构：\n关节速度/力矩：在 $\\mathbb{R}^n$ 上，通常没问题 末端速度：线速度在 $\\mathbb{R}^3$ 上，角速度在 $\\mathfrak{so}(3)$（SO(3)的李代数）上 连续动作：如力/力矩，在 $\\mathbb{R}^6$ 上 策略梯度方法：\n在流形上使用策略梯度方法时，需要：\n正确计算流形上的梯度 使用适当的参数化（如四元数表示旋转） 处理流形的边界或约束 实例：训练机器人开门\n状态：机器人末端的位置（$\\mathbb{R}^3$）、姿态（四元数，$S^3$）、关节角度（$\\mathbb{R}^7$） 动作：关节速度（$\\mathbb{R}^7$） 挑战：四元数的约束 $|\\mathbf{q}| = 1$ 必须在优化中保持 第六章：实战案例 案例一：人脸识别的流形学习 问题背景：\n人脸识别是计算机视觉的核心问题之一。传统方法依赖于手工设计的特征（如HOG、LBP），而深度学习方法则直接从数据中学习特征。\n流形视角：\n所有人脸图像的集合构成一个流形，称为人脸流形。这个流形：\n是高维图像空间（$\\mathbb{R}^{H \\times W \\times 3}$）的低维嵌入 具有非平凡的拓扑结构（可能同胚于某个低维流形） 上面的距离对应于人脸之间的\"视觉相似度\" DeepFace和FaceNet：\nFacebook的DeepFace和Google的FaceNet是两个里程碑式的工作。它们都使用深度卷积神经网络学习人脸嵌入，但关键洞察是：\n中心损失（Center Loss）：让同一个人的所有图像聚集在嵌入空间中的某个中心周围 角度边界损失（Angular Margin Loss）：在球面上增加类别之间的边界 SphereFace 提出了角度边界损失（Angular Margin Loss）：\n$$L_{ang} = -\\log\\left(\\frac{e^{|x| \\cos(m\\theta_{y})}}{e^{|x| \\cos(m\\theta_{y})} + \\sum_{j \\neq y} e^{|x| \\cos\\theta_j}}\\right)$$\n这个损失函数在球面空间上操作，因为 $\\cos\\theta$ 直接对应于球面上两个向量的内积。\n实践要点：\n归一化：将特征向量归一化到单位球面 角度度量：使用角度距离而非欧几里得距离 多尺度：在不同尺度上提取特征，捕捉不同粒度的信息 代码示例：\nimport torch import torch.nn as nn class AngularMarginLoss(nn.Module): def __init__(self, margin=0.5, scale=30): super().__init__() self.margin = margin self.scale = scale def forward(self, logits, labels): # logits: [batch_size, num_classes], 归一化特征的内积 # labels: [batch_size] one_hot = torch.zeros_like(logits) one_hot.scatter_(1, labels.view(-1, 1), 1) # 角度边界 sine = torch.sqrt(1 - logits ** 2) phi = logits - self.margin # 添加角度边界 output = (one_hot * phi + (1 - one_hot) * sine) * self.scale return nn.functional.cross_entropy(output, labels) 案例二：四元数与3D旋转 问题背景：\n在机器人学、计算机图形学和计算机视觉中，精确表示和操作3D旋转是一个核心问题。四元数是解决这个问题的最佳工具之一。\n为什么是四元数？\n考虑以下几种旋转表示的对比：\n表示 参数数量 奇点 数值稳定性 复合效率 旋转矩阵 9 无 中等 低 欧拉角 3 有 低 中等 轴角 4 无 中等 中等 四元数 4 无 高 高 四元数的基础知识：\n一个四元数 $\\mathbf{q} = w + xi + yj + zk$ 可以写成 $\\mathbf{q} = (w, \\mathbf{v})$，其中 $\\mathbf{v} = (x, y, z)$ 是向量部分。\n四元数表示旋转的规则：\n单位四元数：只有单位四元数（$|\\mathbf{q}| = 1$）表示有效的旋转 旋转角度：$\\theta = 2 \\arccos(w)$ 旋转轴：$\\mathbf{u} = \\mathbf{v} / \\sin(\\theta/2)$ 绕任意轴旋转：\n要将一个点 $\\mathbf{p}$ 绕轴 $\\mathbf{u}$ 旋转角度 $\\theta$：\n将点写成纯四元数：$\\mathbf{p} = (0, \\mathbf{p})$ 构造旋转四元数：$\\mathbf{q} = (\\cos(\\theta/2), \\sin(\\theta/2)\\mathbf{u})$ 执行旋转：$\\mathbf{p}’ = \\mathbf{q} \\mathbf{p} \\mathbf{q}^{-1}$ 提取向量部分 四元数的SLERP：\n在动画和路径规划中，我们经常需要在两个旋转之间插值。球面线性插值（SLERP）是专门为四元数设计的插值方法：\n$$\\text{SLERP}(\\mathbf{q}_0, \\mathbf{q}_1, t) = \\frac{\\sin((1-t)\\Omega)}{\\sin\\Omega}\\mathbf{q}_0 + \\frac{\\sin(t\\Omega)}{\\sin\\Omega}\\mathbf{q}_1$$\n其中 $\\Omega$ 是 $\\mathbf{q}_0$ 和 $\\mathbf{q}_1$ 之间的角度，$t \\in [0, 1]$ 是插值参数。\nSLERP保证：\n角速度恒定（如果 $t$ 与时间成正比） 路径是最短测地线 不会产生\"万向锁\"问题 代码示例：\nimport numpy as np def quaternion_to_rotation_matrix(q): \"\"\"将四元数转换为旋转矩阵\"\"\" w, x, y, z = q # 归一化 q = q / np.linalg.norm(q) w, x, y, z = q R = np.array([ [1 - 2*y**2 - 2*z**2, 2*x*y - 2*w*z, 2*x*z + 2*w*y], [2*x*y + 2*w*z, 1 - 2*x**2 - 2*z**2, 2*y*z - 2*w*x], [2*x*z - 2*w*y, 2*y*z + 2*w*x, 1 - 2*x**2 - 2*y**2] ]) return R def slerp(q0, q1, t): \"\"\"球面线性插值\"\"\" # 确保四元数在同一个半球 if np.dot(q0, q1) \u003c 0: q1 = -q1 dot = np.dot(q0, q1) dot = np.clip(dot, -1, 1) # 避免数值误差 omega = np.arccos(dot) sin_omega = np.sin(omega) if sin_omega \u003c 1e-6: return (1-t) * q0 + t * q1 a = np.sin((1-t) * omega) / sin_omega b = np.sin(t * omega) / sin_omega return a * q0 + b * q1 案例三：自动驾驶中的状态估计 问题背景：\n自动驾驶汽车需要精确估计自己的位置和姿态（自车状态）。这包括：\n位置：$(x, y, z)$ 在某个世界坐标系中 姿态：绕三个轴的旋转（横滚、俯仰、偏航） 速度：线速度和角速度 传感器融合：\n自动驾驶使用多种传感器：\nGPS：提供全局位置，但精度有限（几米到几十米） 惯性测量单元（IMU）：提供高频加速度和角速度，但会漂移 轮式里程计：从轮子转速估计位移，但有打滑问题 激光雷达/相机：提供局部环境信息，可以用于匹配 扩展卡尔曼滤波：\nEKF是融合多传感器测量的经典方法。在自动驾驶中，状态通常定义为：\n$$\\mathbf{x} = [x, y, z, \\phi, \\theta, \\psi, v_x, v_y, v_z, \\omega_x, \\omega_y, \\omega_z]^\\top$$\n其中 $\\phi, \\theta, \\psi$ 是欧拉角，$v$ 是线速度，$\\omega$ 是角速度。\n流形上的EKF：\n关键问题是如何处理姿态的流形结构。一种方法是局部切空间滤波：\n在某个参考姿态 $\\bar{R}$ 处建立切空间 在切空间中执行标准的卡尔曼滤波 使用指数映射将结果映射回流形 另一种方法是无迹卡尔曼滤波（UKF），它不需要显式计算雅可比矩阵。\n误差状态卡尔曼滤波：\n在机器人学中，误差状态（Error State）形式更为常见：\n$$\\mathbf{x}{true} = \\mathbf{x}{nom} \\oplus \\delta\\mathbf{x}$$\n其中 $\\mathbf{x}_{nom}$ 是标称状态（在流形上），$\\delta\\mathbf{x}$ 是误差状态（在切空间中）。\n这种形式的优势：\n误差状态是小的，可以用高斯分布近似 避免了大角度旋转的三角函数计算 数值更稳定 代码示例：\nimport numpy as np class ErrorStateEKF: def __init__(self): # 状态维度: 位置(3) + 姿态(3) + 速度(3) + 偏置(6) = 15 self.dim = 15 # 状态: [pos(3), rotvec(3), vel(3), accel_bias(3), gyro_bias(3)] self.state = np.zeros(self.dim) self.cov = np.eye(self.dim) * 0.1 # 噪声参数 self.Q = np.eye(12) * 0.01 # 过程噪声 def predict(self, dt, accel, gyro): \"\"\"预测步骤\"\"\" # 提取状态 pos = self.state[:3] rotvec = self.state[3:6] # 旋转向量 vel = self.state[6:9] accel_bias = self.state[9:12] gyro_bias = self.state[12:15] # 校正测量 accel_corrected = accel - accel_bias gyro_corrected = gyro - gyro_bias # 更新状态 # 位置: pos += vel * dt pos = pos + vel * dt # 速度: vel += accel_corrected * dt (在世界坐标系) # 需要将加速度从体坐标系转换到世界坐标系 C_bn = rotation_vector_to_matrix(rotvec) # 体到世界的旋转 vel = vel + C_bn @ accel_corrected * dt # 姿态: rotvec += gyro_corrected * dt rotvec = rotvec + gyro_corrected * dt # 偏置随机游走 # (简化处理，不更新偏置) # 更新协方差 # ... (雅可比矩阵和卡尔曼增益计算) self.state[:3] = pos self.state[3:6] = rotvec self.state[6:9] = vel def update(self, measurement, H, R): \"\"\"更新步骤\"\"\" # 计算残差 z_pred = H @ self.state y = measurement - z_pred # 卡尔曼增益 S = H @ self.cov @ H.T + R K = self.cov @ H.T @ np.linalg.inv(S) # 更新状态 self.state = self.state + K @ y # 更新协方差 (Joseph form for numerical stability) I_KH = np.eye(self.dim) - K @ H self.cov = I_KH @ self.cov @ I_KH.T + K @ R @ K.T def rotation_vector_to_matrix(rotvec): \"\"\"将旋转向量转换为旋转矩阵\"\"\" theta = np.linalg.norm(rotvec) if theta \u003c 1e-8: return np.eye(3) k = rotvec / theta K = np.array([ [0, -k[2], k[1]], [k[2], 0, -k[0]], [-k[1], k[0], 0] ]) # Rodrigues公式 R = np.eye(3) + np.sin(theta) * K + (1 - np.cos(theta)) * (K @ K) return R 案例四：强化学习中的黎曼流形优化 问题背景：\n在强化学习中，策略梯度方法需要优化高维参数空间。当参数空间具有流形结构时（如正交矩阵、低秩矩阵），黎曼流形优化可以显著提高性能和稳定性。\n实例：正交RNN\n在循环神经网络中，隐藏状态的演化是：\n$$\\mathbf{h}t = \\tanh(W \\mathbf{h}{t-1} + U \\mathbf{x}_t)$$\n如果权重矩阵 $W$ 是正交的，那么梯度会沿着隐藏状态的方向传播而不会消失或爆炸——这解决了RNN的梯度消失问题。\n流形约束：\n$W$ 需要是正交矩阵，即 $W^\\top W = I$。这意味着 $W$ 位于正交群 $O(n)$ 上。\n黎曼梯度下降：\n在正交群上进行优化的步骤：\n黎曼梯度：计算普通梯度 $\\nabla f$，然后投影到切空间 $$\\text{grad}_{O(n)} f = \\nabla f - \\nabla f \\cdot W^\\top W + \\nabla f \\cdot W^\\top W$$（简化） 更准确地说：$\\text{grad} = \\nabla f \\cdot W^\\top W - W^\\top \\cdot \\nabla f \\cdot W$ 的对称部分\n指数映射：从切空间映射回流形 $$\\text{Exp}_W(V) = W \\cdot \\exp(V)$$ 其中 $\\exp$ 是矩阵指数，$V$ 是切空间中的反对称矩阵\n** retraction**：指数映射的近似，计算更高效 $$\\text{Retr}_W(V) = \\text{qf}(W + V)$$ 其中 $\\text{qf}$ 是正交化（QR分解）\n代码示例：\nimport torch import torch.nn as nn import torch.nn.functional as F class OrthogonalRNN(nn.Module): def __init__(self, input_size, hidden_size): super().__init__() self.hidden_size = hidden_size # 权重矩阵，初始化为正交矩阵 self.W = nn.Parameter(torch.randn(hidden_size, hidden_size)) nn.init.orthogonal_(self.W) self.U = nn.Parameter(torch.randn(hidden_size, input_size)) def forward(self, x, h=None): if h is None: h = torch.zeros(x.size(0), self.hidden_size, device=x.device) output = [] for t in range(x.size(1)): h = F.tanh(self.W @ h.unsqueeze(2) + self.U @ x[:, t].unsqueeze(2)) output.append(h.squeeze(2)) return torch.stack(output, dim=1), h def orthogonalize(self): \"\"\"将权重矩阵正交化\"\"\" with torch.no_grad(): W = self.W.data Q, R = torch.linalg.qr(W) # 调整符号以保持与原矩阵一致 d = torch.diag(torch.sign(torch.diag(R))) self.W.data = Q @ d def riemannian_gradient(W, grad): \"\"\"计算正交流形上的黎曼梯度\"\"\" # 梯度投影到切空间 # 对于正交群，切空间是反对称矩阵 # grad_O(n)(f) = grad - grad @ W^T @ W + W^T @ grad @ W 的对称部分 # 简化版本 G = grad - W @ grad.T @ W G = (G + G.T) / 2 # 对称化 return G def riemannian_step(W, grad, lr): \"\"\"黎曼梯度下降一步\"\"\" G = riemannian_gradient(W, grad) # 构建反对称矩阵 K = (G - G.T) / 2 # 使用 retraction 而非指数映射（更高效） # Retr_W(V) = qf(W + V) where qf is QR decomposition with torch.no_grad(): W_new = W - lr * K Q, R = torch.linalg.qr(W_new) d = torch.diag(torch.sign(torch.diag(R))) W.copy_(Q @ d) return W 第七章：现代流形学习的深入探讨 7.1 局部线性嵌入（LLE） 核心思想：\nLLE假设数据在局部是线性的——每个数据点都可以由其邻居的线性组合表示。LLE保持这种局部线性关系，同时将数据嵌入到低维空间。\n算法步骤：\n邻居选择：对于每个点 $x_i$，找到 $k$ 个最近邻 ${x_j}$ 局部重建权值：求解最小二乘问题： $$\\min_{w_{ij}} |x_i - \\sum_j w_{ij} x_j|^2$$ 约束：$\\sum_j w_{ij} = 1$（归一化） 低维嵌入：在低维空间 $y_i$ 中保持相同的权值： $$\\min_{y_i} |y_i - \\sum_j w_{ij} y_j|^2$$ 数学性质：\nLLE的解由稀疏特征值问题给出 保持数据的局部邻域结构 对噪声和异常值敏感 7.2 等度量映射（Isomap） 核心思想：\nIsomap是流形学习的里程碑工作，它用图上的最短路径近似流形上的测地距离，然后使用多维缩放（MDS）将数据嵌入到低维空间。\n算法步骤：\n邻居图构建：连接每个点到其 $k$ 个最近邻 测地距离估计：对于每对点，计算图上的最短路径距离 $d_G(i, j)$ 经典MDS：将距离矩阵嵌入到低维空间 数学性质：\nIsomap理论上可以恢复等距嵌入（如果噪声足够小） 对\"孔洞\"和非均匀采样敏感 计算复杂度较高（需要计算所有对的最短路径） 7.3 拉普拉斯特征映射（LE） 核心思想：\nLE假设相似的点在嵌入空间中应该保持相似。它最小化邻居之间权重的拉普拉斯算子。\n算法步骤：\n邻居图构建：同上 权重计算：$W_{ij} = \\exp(-|x_i - x_j|^2 / t)$ 或二值化 特征分解：求解广义特征值问题： $$L y = \\lambda D y$$ 其中 $L = D - W$ 是拉普拉斯矩阵，$D$ 是度矩阵 数学性质：\nLE与图的拉普拉斯算子的谱相关 保持局部结构，但不保证全局结构 对参数选择（如邻居数 $k$ 和带宽 $t$）敏感 7.4 t-分布随机邻域嵌入（t-SNE） 核心思想：\nt-SNE是可视化高维数据的强大工具。它使用t-分布来测量低维空间中的相似度，解决了\"拥挤问题\"。\n算法步骤：\n高维相似度：使用高斯核： $$p_{j|i} = \\frac{\\exp(-|x_i - x_j|^2 / 2\\sigma^2)}{\\sum_{k \\neq i} \\exp(-|x_i - x_k|^2 / 2\\sigma^2)}$$ 低维相似度：使用t-分布（自由度为1）： $$q_{ij} = \\frac{(1 + |y_i - y_j|^2)^{-1}}{\\sum_{k \\neq l}(1 + |y_k - y_l|^2)^{-1}}$$ KL散度最小化： $$C = KL(P | Q) = \\sum_{i \\neq j} p_{ij} \\log\\frac{p_{ij}}{q_{ij}}$$ 数学性质：\nt-SNE的解通过梯度下降获得 不保持全局结构，只保持局部结构 随机性：多次运行可能产生不同结果 结语：流形的哲学 从几何到智能 回顾我们走过的旅程，从高斯和黎曼的几何革命，到深度学习和机器人学的前沿应用，我们看到了数学概念如何塑造现代科技。\n流形的本质洞察是：复杂系统通常可以分解为局部简单的部分，这些部分通过某种\"粘合\"方式组装成复杂的整体。\n这种洞察无处不在：\n深度学习：神经网络通过叠加简单的非线性变换来学习复杂的函数 机器人学：复杂的机器人运动可以分解为关节空间的简单旋转和平移 计算机视觉：复杂的视觉场景可以分解为局部特征的组合 思想的进化 高斯不会想到他关于曲面曲率的工作会影响到自动驾驶汽车；黎曼不会想到他的几何会成为机器学习的理论基础。\n这就是数学的力量：抽象的概念往往会在意想不到的地方找到应用。\n给读者的话 如果你读到这里，我希望你已经：\n理解了流形的基本概念：从拓扑空间到黎曼流形 看到了流形的应用：从深度学习到机器人学 学到了实战技能：从四元数计算到流形优化 流形是一个庞大的领域，这篇文章只是冰山一角。如果你有兴趣深入学习，我推荐：\n微分几何：Do Carmo的《Differential Geometry of Curves and Surfaces》 黎曼几何：Lee的《Riemannian Manifolds: An Introduction to Curvature》 流形学习：Burges的《Dimension Reduction: A Guided Tour》 李群与机器：Hall的《Lie Groups, Lie Algebras, and Representations》 最后，让我用黎曼1854年演讲的结语来结束这篇文章：\n“几何命题的真实性是不容置疑的，只要它们涉及可感知的关系；但它们的意义是无穷的，如果我们假设这些命题的真实性扩展到可感知范围之外——扩展到无限大的领域……这正是几何与哲学的联系所在。”\n附录：重要公式汇总 流形基础 坐标变换： $$\\frac{\\partial}{\\partial x^i} = \\sum_j \\frac{\\partial \\bar{x}^j}{\\partial x^i} \\frac{\\partial}{\\partial \\bar{x}^j}$$\n黎曼几何 度规张量： $$g_{ij} = \\left\\langle \\frac{\\partial}{\\partial x^i}, \\frac{\\partial}{\\partial x^j} \\right\\rangle$$\n克里斯托费尔符号： $$\\Gamma^k_{ij} = \\frac{1}{2} g^{kl} \\left( \\frac{\\partial g_{il}}{\\partial x^j} + \\frac{\\partial g_{jl}}{\\partial x^i} - \\frac{\\partial g_{ij}}{\\partial x^l} \\right)$$\n测地线方程： $$\\frac{d^2x^k}{dt^2} + \\Gamma^k_{ij} \\frac{dx^i}{dt} \\frac{dx^j}{dt} = 0$$\n黎曼曲率张量： $$R^i_{jkl} = \\partial_k \\Gamma^i_{jl} - \\partial_l \\Gamma^i_{jk} + \\Gamma^i_{km}\\Gamma^m_{jl} - \\Gamma^i_{lm}\\Gamma^m_{jk}$$\n四元数 乘法： $$(w_1, \\mathbf{v}_1) \\otimes (w_2, \\mathbf{v}_2) = (w_1 w_2 - \\mathbf{v}_1 \\cdot \\mathbf{v}_2, w_1 \\mathbf{v}_2 + w_2 \\mathbf{v}_1 + \\mathbf{v}_1 \\times \\mathbf{v}_2)$$\nSLERP： $$\\text{SLERP}(\\mathbf{q}_0, \\mathbf{q}_1, t) = \\frac{\\sin((1-t)\\Omega)}{\\sin\\Omega}\\mathbf{q}_0 + \\frac{\\sin(t\\Omega)}{\\sin\\Omega}\\mathbf{q}_1$$\n流形优化 黎曼梯度： $$\\text{grad}M f = g^{-1} \\text{grad}{\\mathbb{R}^n} f$$\n指数映射： $$\\text{Exp}_p(v) = \\gamma_v(1)$$ 其中 $\\gamma_v$ 是以 $v$ 为初始速度的测地线\n本文旨在为有一定数学基础的读者提供流形的入门导引。更深入的学习建议参考专业教材。\n","wordCount":"2090","inLanguage":"en","image":"https://s-ai-unix.github.io/images/covers/1620641788421-7a1c342ea42e.jpg","datePublished":"2026-01-12T23:10:00+08:00","dateModified":"2026-01-12T23:10:00+08:00","author":{"@type":"Person","name":"s-ai-unix"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://s-ai-unix.github.io/posts/2026-01-12-manifold-deep-learning-robotics/"},"publisher":{"@type":"Organization","name":"s-ai-unix's Blog","logo":{"@type":"ImageObject","url":"https://s-ai-unix.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://s-ai-unix.github.io/css/classical-quote.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://s-ai-unix.github.io/ accesskey=h title="s-ai-unix's Blog (Alt + H)">s-ai-unix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://s-ai-unix.github.io/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://s-ai-unix.github.io/posts/ title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://s-ai-unix.github.io/archives/ title="📁 归档"><span>📁 归档</span></a></li><li><a href=https://s-ai-unix.github.io/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://s-ai-unix.github.io/search/ title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://s-ai-unix.github.io/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://s-ai-unix.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://s-ai-unix.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">流形：从弯曲空间到深度学习与机器人学的漫游</h1><div class=post-description>从高斯和黎曼的几何革命出发，系统讲解流形的概念、历史、数学基础，深入探讨流形在深度学习（流形假设、球面Embedding、双曲空间）和机器人学（SO(3)、四元数、SLAM）中的核心应用，并通过四个实战案例展示流形的强大威力</div><div class=post-meta><span title='2026-01-12 23:10:00 +0800 CST'>January 12, 2026</span>&nbsp;·&nbsp;<span>10 min</span>&nbsp;·&nbsp;<span>2090 words</span>&nbsp;·&nbsp;<span>s-ai-unix</span></div></header><figure class=entry-cover><a href=https://s-ai-unix.github.io/images/covers/1620641788421-7a1c342ea42e.jpg target=_blank rel="noopener noreferrer"><img loading=eager src=https://s-ai-unix.github.io/images/covers/1620641788421-7a1c342ea42e.jpg alt=流形的可视化></a><figcaption>流形：从欧几里得到黎曼几何的跨越</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80%e5%bd%93%e7%a9%ba%e9%97%b4%e5%bc%80%e5%a7%8b%e5%bc%af%e6%9b%b2 aria-label=引言：当空间开始弯曲>引言：当空间开始弯曲</a></li><li><a href=#%e7%ac%ac%e4%b8%80%e7%ab%a0%e5%87%a0%e4%bd%95%e7%9a%84%e5%8d%b1%e6%9c%ba%e4%b8%8e%e9%87%8d%e7%94%9f aria-label=第一章：几何的危机与重生>第一章：几何的危机与重生</a><ul><li><a href=#11-%e6%ac%a7%e5%87%a0%e9%87%8c%e5%be%97%e7%9a%84%e7%ac%ac%e4%ba%94%e5%85%ac%e8%ae%be aria-label="1.1 欧几里得的第五公设">1.1 欧几里得的第五公设</a></li><li><a href=#12-%e7%bd%97%e5%b7%b4%e5%88%87%e5%a4%ab%e6%96%af%e5%9f%ba%e7%9a%84%e9%9d%a9%e5%91%bd aria-label="1.2 罗巴切夫斯基的革命">1.2 罗巴切夫斯基的革命</a></li><li><a href=#13-%e9%ab%98%e6%96%af%e7%9a%84%e7%bb%9d%e5%a6%99%e5%ae%9a%e7%90%86 aria-label="1.3 高斯的绝妙定理">1.3 高斯的绝妙定理</a></li><li><a href=#14-%e9%bb%8e%e6%9b%bc%e7%9a%84%e6%8e%a8%e5%b9%bf aria-label="1.4 黎曼的推广">1.4 黎曼的推广</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e7%ab%a0%e6%b5%81%e5%bd%a2%e7%9a%84%e6%95%b0%e5%ad%a6%e5%ae%9a%e4%b9%89 aria-label=第二章：流形的数学定义>第二章：流形的数学定义</a><ul><li><a href=#21-%e4%bb%80%e4%b9%88%e6%98%af%e6%b5%81%e5%bd%a2 aria-label="2.1 什么是流形？">2.1 什么是流形？</a></li><li><a href=#22-%e4%bb%8e%e7%9b%b4%e8%a7%82%e5%88%b0%e6%8a%bd%e8%b1%a1 aria-label="2.2 从直观到抽象">2.2 从直观到抽象</a></li><li><a href=#23-%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e7%94%a8%e6%b5%81%e5%bd%a2 aria-label="2.3 为什么要用流形？">2.3 为什么要用流形？</a></li><li><a href=#24-%e5%88%87%e7%a9%ba%e9%97%b4%e4%b8%8e%e5%90%91%e9%87%8f%e5%9c%ba aria-label="2.4 切空间与向量场">2.4 切空间与向量场</a></li><li><a href=#25-%e4%bd%99%e5%88%87%e7%a9%ba%e9%97%b4%e4%b8%8e%e5%bc%a0%e9%87%8f aria-label="2.5 余切空间与张量">2.5 余切空间与张量</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e7%ab%a0%e9%bb%8e%e6%9b%bc%e6%b5%81%e5%bd%a2 aria-label=第三章：黎曼流形>第三章：黎曼流形</a><ul><li><a href=#31-%e5%ba%a6%e8%a7%84%e5%bc%a0%e9%87%8f aria-label="3.1 度规张量">3.1 度规张量</a></li><li><a href=#32-%e5%bc%a7%e9%95%bf%e4%b8%8e%e6%b5%8b%e5%9c%b0%e7%ba%bf aria-label="3.2 弧长与测地线">3.2 弧长与测地线</a></li><li><a href=#33-%e6%9b%b2%e7%8e%87 aria-label="3.3 曲率">3.3 曲率</a></li><li><a href=#34-%e8%81%94%e7%bb%9c%e4%b8%8e%e5%b9%b3%e8%a1%8c%e7%a7%bb%e5%8a%a8 aria-label="3.4 联络与平行移动">3.4 联络与平行移动</a></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e7%ab%a0%e6%b5%81%e5%bd%a2%e5%9c%a8%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8 aria-label=第四章：流形在深度学习中的应用>第四章：流形在深度学习中的应用</a><ul><li><a href=#41-%e6%b5%81%e5%bd%a2%e5%81%87%e8%ae%be aria-label="4.1 流形假设">4.1 流形假设</a></li><li><a href=#42-%e7%90%83%e9%9d%a2embedding aria-label="4.2 球面Embedding">4.2 球面Embedding</a></li><li><a href=#43-%e5%8f%8c%e6%9b%b2%e7%a9%ba%e9%97%b4embedding aria-label="4.3 双曲空间Embedding">4.3 双曲空间Embedding</a></li><li><a href=#44-%e6%9d%8e%e7%be%a4%e4%b8%8e%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="4.4 李群与神经网络">4.4 李群与神经网络</a></li><li><a href=#45-%e9%bb%8e%e6%9b%bc%e6%b5%81%e5%bd%a2%e4%bc%98%e5%8c%96 aria-label="4.5 黎曼流形优化">4.5 黎曼流形优化</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e7%ab%a0%e6%b5%81%e5%bd%a2%e5%9c%a8%e6%9c%ba%e5%99%a8%e4%ba%ba%e5%ad%a6%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8 aria-label=第五章：流形在机器人学中的应用>第五章：流形在机器人学中的应用</a><ul><li><a href=#51-%e5%a7%bf%e6%80%81%e8%a1%a8%e7%a4%baso3%e4%b8%8ese3 aria-label="5.1 姿态表示：SO(3)与SE(3)">5.1 姿态表示：SO(3)与SE(3)</a></li><li><a href=#52-%e8%bf%90%e5%8a%a8%e5%ad%a6%e4%b8%8e%e5%8a%a8%e5%8a%9b%e5%ad%a6 aria-label="5.2 运动学与动力学">5.2 运动学与动力学</a></li><li><a href=#53-%e8%bf%90%e5%8a%a8%e8%a7%84%e5%88%92 aria-label="5.3 运动规划">5.3 运动规划</a></li><li><a href=#54-%e7%8a%b6%e6%80%81%e4%bc%b0%e8%ae%a1%e4%b8%8eslam aria-label="5.4 状态估计与SLAM">5.4 状态估计与SLAM</a></li><li><a href=#55-%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e6%b5%81%e5%bd%a2 aria-label="5.5 强化学习中的流形">5.5 强化学习中的流形</a></li></ul></li><li><a href=#%e7%ac%ac%e5%85%ad%e7%ab%a0%e5%ae%9e%e6%88%98%e6%a1%88%e4%be%8b aria-label=第六章：实战案例>第六章：实战案例</a><ul><li><a href=#%e6%a1%88%e4%be%8b%e4%b8%80%e4%ba%ba%e8%84%b8%e8%af%86%e5%88%ab%e7%9a%84%e6%b5%81%e5%bd%a2%e5%ad%a6%e4%b9%a0 aria-label=案例一：人脸识别的流形学习>案例一：人脸识别的流形学习</a></li><li><a href=#%e6%a1%88%e4%be%8b%e4%ba%8c%e5%9b%9b%e5%85%83%e6%95%b0%e4%b8%8e3d%e6%97%8b%e8%bd%ac aria-label=案例二：四元数与3D旋转>案例二：四元数与3D旋转</a></li><li><a href=#%e6%a1%88%e4%be%8b%e4%b8%89%e8%87%aa%e5%8a%a8%e9%a9%be%e9%a9%b6%e4%b8%ad%e7%9a%84%e7%8a%b6%e6%80%81%e4%bc%b0%e8%ae%a1 aria-label=案例三：自动驾驶中的状态估计>案例三：自动驾驶中的状态估计</a></li><li><a href=#%e6%a1%88%e4%be%8b%e5%9b%9b%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e9%bb%8e%e6%9b%bc%e6%b5%81%e5%bd%a2%e4%bc%98%e5%8c%96 aria-label=案例四：强化学习中的黎曼流形优化>案例四：强化学习中的黎曼流形优化</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%83%e7%ab%a0%e7%8e%b0%e4%bb%a3%e6%b5%81%e5%bd%a2%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%b7%b1%e5%85%a5%e6%8e%a2%e8%ae%a8 aria-label=第七章：现代流形学习的深入探讨>第七章：现代流形学习的深入探讨</a><ul><li><a href=#71-%e5%b1%80%e9%83%a8%e7%ba%bf%e6%80%a7%e5%b5%8c%e5%85%a5lle aria-label="7.1 局部线性嵌入（LLE）">7.1 局部线性嵌入（LLE）</a></li><li><a href=#72-%e7%ad%89%e5%ba%a6%e9%87%8f%e6%98%a0%e5%b0%84isomap aria-label="7.2 等度量映射（Isomap）">7.2 等度量映射（Isomap）</a></li><li><a href=#73-%e6%8b%89%e6%99%ae%e6%8b%89%e6%96%af%e7%89%b9%e5%be%81%e6%98%a0%e5%b0%84le aria-label="7.3 拉普拉斯特征映射（LE）">7.3 拉普拉斯特征映射（LE）</a></li><li><a href=#74-t-%e5%88%86%e5%b8%83%e9%9a%8f%e6%9c%ba%e9%82%bb%e5%9f%9f%e5%b5%8c%e5%85%a5t-sne aria-label="7.4 t-分布随机邻域嵌入（t-SNE）">7.4 t-分布随机邻域嵌入（t-SNE）</a></li></ul></li><li><a href=#%e7%bb%93%e8%af%ad%e6%b5%81%e5%bd%a2%e7%9a%84%e5%93%b2%e5%ad%a6 aria-label=结语：流形的哲学>结语：流形的哲学</a><ul><li><a href=#%e4%bb%8e%e5%87%a0%e4%bd%95%e5%88%b0%e6%99%ba%e8%83%bd aria-label=从几何到智能>从几何到智能</a></li><li><a href=#%e6%80%9d%e6%83%b3%e7%9a%84%e8%bf%9b%e5%8c%96 aria-label=思想的进化>思想的进化</a></li><li><a href=#%e7%bb%99%e8%af%bb%e8%80%85%e7%9a%84%e8%af%9d aria-label=给读者的话>给读者的话</a></li></ul></li><li><a href=#%e9%99%84%e5%bd%95%e9%87%8d%e8%a6%81%e5%85%ac%e5%bc%8f%e6%b1%87%e6%80%bb aria-label=附录：重要公式汇总>附录：重要公式汇总</a><ul><li><a href=#%e6%b5%81%e5%bd%a2%e5%9f%ba%e7%a1%80 aria-label=流形基础>流形基础</a></li><li><a href=#%e9%bb%8e%e6%9b%bc%e5%87%a0%e4%bd%95 aria-label=黎曼几何>黎曼几何</a></li><li><a href=#%e5%9b%9b%e5%85%83%e6%95%b0 aria-label=四元数>四元数</a></li><li><a href=#%e6%b5%81%e5%bd%a2%e4%bc%98%e5%8c%96 aria-label=流形优化>流形优化</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=引言当空间开始弯曲>引言：当空间开始弯曲<a hidden class=anchor aria-hidden=true href=#引言当空间开始弯曲>#</a></h2><p>想象一下，你是一只生活在二维平面上的蚂蚁。你可以自由地在平面上行走，测量距离，画出直线和三角形。你所知道的几何——欧几里得几何——似乎是那么完美、那么自洽。</p><p>现在，让我们把这只蚂蚁放到一个巨大的篮球表面。</p><p>蚂蚁会发现什么呢？首先，它会发现"直线"不再存在。如果它一直往前走，最终会回到起点——它走的是"大圆"，而不是直线。其次，它会发现三角形的内角和不再是180度，而是大于180度。最神奇的是，如果它足够聪明，它可以通过测量距离和角度来发现这个空间的曲率——尽管它从未"跳出"过这个二维曲面。</p><p>这就是<strong>内蕴几何</strong>的魔力，也是<strong>流形</strong>（Manifold）概念的起点。</p><p>在接下来的篇幅中，我将带你进行一次从19世纪的几何革命到21世纪人工智能的漫游。我们会看到：</p><ol><li><strong>流形的诞生</strong>：高斯和黎曼如何改变了我们对空间的理解</li><li><strong>流形的数学</strong>：为什么流形是"局部平坦、整体弯曲"的几何对象</li><li><strong>流形在深度学习</strong>：从流形假设到球面Embedding</li><li><strong>流形在机器人学</strong>：从四元数到SLAM</li><li><strong>实战案例</strong>：四个让你真正理解流形威力的例子</li></ol><p>准备好了吗？让我们开始这段跨越时空的数学之旅。</p><hr><h2 id=第一章几何的危机与重生>第一章：几何的危机与重生<a hidden class=anchor aria-hidden=true href=#第一章几何的危机与重生>#</a></h2><h3 id=11-欧几里得的第五公设>1.1 欧几里得的第五公设<a hidden class=anchor aria-hidden=true href=#11-欧几里得的第五公设>#</a></h3><p>公元前300年，亚历山大港的数学家欧几里得写下了《几何原本》——这部奠定了西方科学基础的巨著。欧几里得从五条公设出发，推导出无数深刻的几何定理。其中第五条公设——平行公设——却让数学家们困惑了两千多年。</p><p><strong>平行公设</strong>：如果一条直线与两条直线相交，且同侧内角之和小于两个直角，则这两条直线在该侧无限延伸后必定相交。</p><p>这条公设看起来比其他公设复杂得多。数学家们不禁想问：它能否从前四条公设中推导出来？如果可以，那它就不是真正的公设；如果不可以，那是否存在一种"非欧几里得几何"，其中平行公设不成立？</p><h3 id=12-罗巴切夫斯基的革命>1.2 罗巴切夫斯基的革命<a hidden class=anchor aria-hidden=true href=#12-罗巴切夫斯基的革命>#</a></h3><p>1829年，俄罗斯数学家罗巴切夫斯基（Nikolai Lobachevsky）发表了第一篇非欧几何的论文。他假设过一点可以作多条平行线，由此推导出一套完整的几何体系——双曲几何。</p><p>在双曲几何中：</p><ul><li>三角形的内角和小于180度</li><li>相似三角形只有大小完全相同才算相似</li><li>不存在矩形，因为四边形的内角和小于360度</li></ul><p>罗巴切夫斯基的发现彻底改变了数学家对几何本质的认识：<strong>几何不是关于"真实空间"的真理，而是关于某种抽象结构的逻辑系统</strong>。</p><h3 id=13-高斯的绝妙定理>1.3 高斯的绝妙定理<a hidden class=anchor aria-hidden=true href=#13-高斯的绝妙定理>#</a></h3><p>几乎在同一时间，德国数学家高斯也在思考类似的问题。高斯不仅是一个理论家，还是一个实测工作者——他参与了汉诺威的大地测量。在测量中，高斯意识到一个深刻的问题：<strong>地球表面的几何能告诉我们什么？</strong></p><p>1827年，高斯发表了<strong>绝妙定理</strong>（Theorema Egregium）：<strong>曲面的高斯曲率是一个内蕴不变量</strong>——它完全由曲面自身的几何性质决定，与曲面如何嵌入周围空间无关。</p><p>这个定理的惊人之处在于：曲率不是"外部"观察者看到的弯曲，而是曲面"内部"几何结构的必然结果。一只生活在曲面上的蚂蚁，通过测量距离和角度，可以计算出它所在空间的曲率——即使它永远无法"看到"曲面在三维空间中的弯曲方式。</p><p>高斯的工作开创了<strong>内蕴几何</strong>的新时代，为流形的诞生奠定了基础。</p><h3 id=14-黎曼的推广>1.4 黎曼的推广<a hidden class=anchor aria-hidden=true href=#14-黎曼的推广>#</a></h3><p>1854年，高斯的学生黎曼（Bernhard Riemann）在哥廷根大学发表了著名的就职演讲《论作为几何学基础的假设》。黎曼将高斯的二维曲面理论推广到任意维数，创立了<strong>黎曼几何</strong>。</p><p>黎曼的核心思想是：<strong>几何不在于"空间是什么"，而在于"我们如何测量空间中的距离"</strong>。</p><p>黎曼提出用一个<strong>度规张量</strong>（Metric Tensor）来描述空间的几何性质。度规告诉我们如何在空间的每一点测量距离和角度。有了度规，我们就可以定义：</p><ul><li>曲线的长度</li><li>向量的点积</li><li>角度和面积</li><li>平行移动</li><li>测地线（最直的曲线）</li></ul><p>黎曼几何成为了20世纪物理学的基石。1915年，爱因斯坦用黎曼几何描述时空的弯曲，建立了广义相对论。</p><hr><h2 id=第二章流形的数学定义>第二章：流形的数学定义<a hidden class=anchor aria-hidden=true href=#第二章流形的数学定义>#</a></h2><h3 id=21-什么是流形>2.1 什么是流形？<a hidden class=anchor aria-hidden=true href=#21-什么是流形>#</a></h3><p>在数学中，<strong>流形</strong>（Manifold）是一个抽象的空间概念。直观地说，流形是一个"局部看起来像欧几里得空间"的空间。</p><p><strong>流形的定义</strong>：</p><p>一个 $n$ 维流形 $M$ 是一个满足以下条件的拓扑空间：</p><ol><li><p><strong>局部欧几里得性</strong>：对于 $M$ 中的每一点 $p$，存在一个开集 $U \subseteq M$ 包含 $p$，以及一个从 $U$ 到 $\mathbb{R}^n$ 的开集的同胚映射（称为<strong>坐标图</strong>）：
$$\varphi: U \to \mathbb{R}^n$$</p></li><li><p><strong>相容性</strong>：如果两个坐标图 $\varphi: U \to \mathbb{R}^n$ 和 $\psi: V \to \mathbb{R}^n$ 有重叠 $U \cap V$，那么映射 $\psi \circ \varphi^{-1}$ 和 $\varphi \circ \psi^{-1}$ 是光滑的（$C^\infty$）。</p></li><li><p><strong>第二可数性</strong>：流形可以被可数个坐标图覆盖。</p></li></ol><p>满足条件2的坐标图集合称为<strong>图册</strong>（Atlas）。如果所有坐标变换都是光滑的，我们称这个流形为<strong>光滑流形</strong>（Smooth Manifold）。</p><h3 id=22-从直观到抽象>2.2 从直观到抽象<a hidden class=anchor aria-hidden=true href=#22-从直观到抽象>#</a></h3><p>让我用几个例子来解释这个抽象定义：</p><p><strong>例子1：圆周</strong></p><p>圆周 $S^1 = {(x, y) \in \mathbb{R}^2 \mid x^2 + y^2 = 1}$ 是一个一维流形。</p><p>为什么？因为圆周的每一点附近都"看起来像"一条直线。我们可以用角度 $\theta$ 作为局部坐标。对于点 $(1, 0)$ 附近，可以用 $\theta \in (-\pi/2, \pi/2)$；对于点 $(0, 1)$ 附近，可以用 $\theta \in (0, \pi)$，等等。</p><p><strong>例子2：球面</strong></p><p>球面 $S^2 = {(x, y, z) \in \mathbb{R}^3 \mid x^2 + y^2 + z^2 = 1}$ 是一个二维流形。</p><p>我们可以用经度 $\phi$ 和纬度 $\theta$ 作为局部坐标。但球面有一个特点：没有任何单个坐标图能覆盖整个球面（南极和北极会导致纬度坐标的奇点）。这就是为什么我们需要多个坐标图——每个坐标图覆盖一部分，然后用坐标变换将它们"粘"在一起。</p><p><strong>例子3：环面</strong></p><p>环面（甜甜圈表面）$T^2 = S^1 \times S^1$ 是一个二维流形。它可以看作是两个圆周的直积。环面是"紧致"（compact）流形的经典例子——它是有限的，但没有边界。</p><h3 id=23-为什么要用流形>2.3 为什么要用流形？<a hidden class=anchor aria-hidden=true href=#23-为什么要用流形>#</a></h3><p>你可能会问：为什么要引入这么抽象的概念？直接用 $\mathbb{R}^n$ 不就行了吗？</p><p>答案是：<strong>现实世界中的许多空间不是平坦的 $\mathbb{R}^n$，而是弯曲的流形</strong>。</p><p>考虑以下几个例子：</p><ol><li><strong>旋转</strong>：三维空间中的旋转不是 $\mathbb{R}^3$ 中的向量，而是特殊正交群 $SO(3)$——一个三维流形</li><li><strong>姿态</strong>：机器人的姿态由旋转矩阵或四元数描述，这些都定义在流形上</li><li><strong>图像</strong>：所有MNIST数字图像的集合构成一个流形——尽管是高维的，但本质上只有低维结构</li><li><strong>蛋白质结构</strong>：蛋白质的所有可能构象形成一个流形</li></ol><p>流形让我们能够用<strong>局部线性化</strong>的方法处理<strong>全局弯曲</strong>的空间。在流形的每一点，我们可以建立局部坐标系（切空间），在切空间中我们可以使用熟悉的线性代数工具。</p><h3 id=24-切空间与向量场>2.4 切空间与向量场<a hidden class=anchor aria-hidden=true href=#24-切空间与向量场>#</a></h3><p>在流形上，我们需要一种方法来描述"方向"和"变化"。这就是<strong>切空间</strong>（Tangent Space）的概念。</p><p><strong>切空间的定义</strong>：</p><p>在流形 $M$ 的点 $p$ 处，切空间 $T_p M$ 是所有经过 $p$ 的曲线的切向量的集合。更抽象地说，切空间是所有在 $p$ 处为零的导数的空间。</p><p>对于 $n$ 维流形，切空间 $T_p M$ 是一个 $n$ 维向量空间。如果 $p$ 的局部坐标是 $(x^1, \dots, x^n)$，那么切空间的一组基是：
$$\left\lbrace\frac{\partial}{\partial x^1}, \dots, \frac{\partial}{\partial x^n}\right\rbrace$$</p><p><strong>向量场</strong>是流形上的一个函数，它为每一点 $p$ 指定一个切向量 $V(p) \in T_p M$。向量场可以看作是流形上的"速度场"或"方向场"。</p><h3 id=25-余切空间与张量>2.5 余切空间与张量<a hidden class=anchor aria-hidden=true href=#25-余切空间与张量>#</a></h3><p>与切空间对偶的是<strong>余切空间</strong>（Cotangent Space）$T^*_p M$。余切空间是切空间的<strong>对偶空间</strong>，其中的元素是切空间上的线性泛函（线性函数）。</p><p>如果 $\omega \in T^*_p M$ 且 $v \in T_p M$，那么 $\omega(v)$ 是一个标量。</p><p>更一般地，我们可以定义<strong>张量</strong>（Tensor）。张量是多线性映射，例如：</p><ul><li>$(0, 1)$ 型张量：余切向量</li><li>$(1, 0)$ 型张量：切向量</li><li>$(0, 2)$ 型张量：双线性形式（如度规张量）</li></ul><p>张量在物理学和工程中无处不在。它们是描述几何和物理量的自然语言。</p><hr><h2 id=第三章黎曼流形>第三章：黎曼流形<a hidden class=anchor aria-hidden=true href=#第三章黎曼流形>#</a></h2><h3 id=31-度规张量>3.1 度规张量<a hidden class=anchor aria-hidden=true href=#31-度规张量>#</a></h3><p><strong>黎曼流形</strong>（Riemannian Manifold）是一个配备了<strong>度规张量</strong>（Metric Tensor）的光滑流形。度规是一个对称正定的 $(0, 2)$ 型张量：</p><p>$$g_p: T_p M \times T_p M \to \mathbb{R}$$</p><p>度规告诉我们如何在每一点的切空间中测量长度和角度。对于切向量 $u, v \in T_p M$：</p><ul><li><strong>长度</strong>：$|u| = \sqrt{g_p(u, u)}$</li><li><strong>角度</strong>：$\cos\theta = \frac{g_p(u, v)}{|u||v|}$</li></ul><p>在局部坐标 $(x^1, \dots, x^n)$ 中，度规可以写成矩阵形式：</p><p>$$g = \begin{pmatrix} g_{11} & g_{12} & \cdots & g_{1n} \ g_{21} & g_{22} & \cdots & g_{2n} \ \vdots & \vdots & \ddots & \vdots \ g_{n1} & g_{n2} & \cdots & g_{nn} \end{pmatrix}$$</p><p>其中 $g_{ij} = g\left(\frac{\partial}{\partial x^i}, \frac{\partial}{\partial x^j}\right)$。</p><h3 id=32-弧长与测地线>3.2 弧长与测地线<a hidden class=anchor aria-hidden=true href=#32-弧长与测地线>#</a></h3><p>有了度规，我们就可以定义曲线的<strong>弧长</strong>。对于参数化曲线 $\gamma(t) = (x^1(t), \dots, x^n(t))$，$t \in [a, b]$：</p><p>$$\text{Length}(\gamma) = \int_a^b \sqrt{\sum_{i,j} g_{ij}(\gamma(t)) \frac{dx^i}{dt} \frac{dx^j}{dt}} , dt$$</p><p><strong>测地线</strong>（Geodesic）是曲面上"最直的曲线"——它是弧长的极值曲线。测地线满足<strong>测地线方程</strong>：</p><p>$$\frac{d^2x^k}{dt^2} + \sum_{i,j} \Gamma^k_{ij} \frac{dx^i}{dt} \frac{dx^j}{dt} = 0$$</p><p>其中 $\Gamma^k_{ij}$ 是<strong>克里斯托费尔符号</strong>（Christoffel Symbol），由度规及其导数计算得出：</p><p>$$\Gamma^k_{ij} = \frac{1}{2} g^{kl} \left( \frac{\partial g_{il}}{\partial x^j} + \frac{\partial g_{jl}}{\partial x^i} - \frac{\partial g_{ij}}{\partial x^l} \right)$$</p><h3 id=33-曲率>3.3 曲率<a hidden class=anchor aria-hidden=true href=#33-曲率>#</a></h3><p>曲率是黎曼流形最重要的几何不变量。<strong>黎曼曲率张量</strong>（Riemann Curvature Tensor）描述了流形的弯曲程度：</p><p>$$R^i_{jkl} = \frac{\partial \Gamma^i_{jl}}{\partial x^k} - \frac{\partial \Gamma^i_{jk}}{\partial x^l} + \Gamma^i_{km}\Gamma^m_{jl} - \Gamma^i_{lm}\Gamma^m_{jk}$$</p><p>曲率张量衡量平行移动与路径的依赖程度。在平坦空间（如 $\mathbb{R}^n$）中，沿任何闭合路径平行移动一个向量都会回到原来的值；但在弯曲空间中，平行移动的结果<strong>与路径有关</strong>。</p><p>从黎曼曲率张量，我们可以缩并得到：</p><ul><li><strong>里奇张量</strong>（Ricci Tensor）：$R_{jl} = R^i_{jil}$</li><li><strong>标量曲率</strong>（Scalar Curvature）：$R = g^{jl} R_{jl}$</li></ul><h3 id=34-联络与平行移动>3.4 联络与平行移动<a hidden class=anchor aria-hidden=true href=#34-联络与平行移动>#</a></h3><p>在流形上，我们不能用普通的偏导数来比较不同点的向量——因为没有"全局坐标系"。为此，我们引入<strong>联络</strong>（Connection）的概念。</p><p><strong>协变导数</strong>（Covariant Derivative）$\nabla$ 是一种"平行移动+微分"的运算。对于向量场 $V = V^i \frac{\partial}{\partial x^i}$：</p><p>$$\nabla_j V^i = \frac{\partial V^i}{\partial x^j} + \Gamma^i_{jk} V^k$$</p><p>协变导数让我们能够在流形上进行"微分"——这是微积分在弯曲空间中的推广。</p><p><strong>平行移动</strong>是协变导数的积分。一个向量沿曲线 $\gamma(t)$ 平行移动，如果它的协变导数为零：</p><p>$$\frac{DV^i}{dt} = \frac{dV^i}{dt} + \Gamma^i_{jk} \frac{dx^j}{dt} V^k = 0$$</p><hr><h2 id=第四章流形在深度学习中的应用>第四章：流形在深度学习中的应用<a hidden class=anchor aria-hidden=true href=#第四章流形在深度学习中的应用>#</a></h2><h3 id=41-流形假设>4.1 流形假设<a hidden class=anchor aria-hidden=true href=#41-流形假设>#</a></h3><p>深度学习成功的关键洞察之一是<strong>流形假设</strong>（Manifold Hypothesis）：</p><blockquote><p>高维数据（如图像、文本、声音）实际上分布在低维流形上。</p></blockquote><p>这个假设是什么意思呢？</p><p>考虑一张 $28 \times 28$ 像素的MNIST数字图像。从像素的角度看，这个图像是 $\mathbb{R}^{784}$ 中的一个点。但并非 $\mathbb{R}^{784}$ 中的所有点都是有效的数字图像。实际上，所有"看起来像数字"的图像只占整个空间的很小一部分——它们分布在某个低维流形上。</p><p>类似地：</p><ul><li>人脸图像分布在"人脸流形"上</li><li>自然语言句子分布在"语义流形"上</li><li>语音信号分布在"语音流形"上</li></ul><p><strong>流形学习的</strong>目标是从高维数据中恢复这个低维流形的结构。传统的方法包括：</p><ol><li><strong>等度量映射</strong>（Isomap）：用图上的最短路径近似测地距离</li><li><strong>局部线性嵌入</strong>（LLE）：保持局部线性结构</li><li><strong>拉普拉斯特征映射</strong>（LE）：保持局部邻域关系</li></ol><h3 id=42-球面embedding>4.2 球面Embedding<a hidden class=anchor aria-hidden=true href=#42-球面embedding>#</a></h3><p>在深度学习中，我们经常需要将离散对象（如单词、用户、物品）嵌入到连续空间中。传统的嵌入方法使用欧几里得空间 $\mathbb{R}^d$，但越来越多的研究表明，<strong>球面空间</strong> $S^{d-1}$ 可能是更好的选择。</p><p><strong>为什么是球面？</strong></p><ol><li><strong>归一化</strong>：球面上的向量自动归一，这对于比较相似度很有用</li><li><strong>曲率</strong>：球面的正曲率可能更适合某些类型的数据</li><li><strong>边界问题</strong>：欧几里得空间没有边界，而球面是紧致的</li></ol><p><strong>SphereFace</strong> 和 <strong>FaceNet</strong> 等人脸识别方法使用球面嵌入。在球面上，两个向量的内积 $\langle u, v \rangle = \cos\theta$ 直接与它们之间的角度相关。</p><p><strong>球面上的分布</strong>：</p><p>在球面上，我们可以定义各种概率分布，如<strong>冯·米塞斯-费舍尔分布</strong>（von Mises-Fisher distribution）：</p><p>$$f(x; \mu, \kappa) = C_d(\kappa) \exp\left(\kappa \mu^\top x\right)$$</p><p>其中 $\mu$ 是均值方向，$\kappa$ 是集中度参数。这个分布在方向统计中有广泛应用。</p><h3 id=43-双曲空间embedding>4.3 双曲空间Embedding<a hidden class=anchor aria-hidden=true href=#43-双曲空间embedding>#</a></h3><p>如果说球面适合"封闭"的数据，那么<strong>双曲空间</strong>则适合"树状"或"层次化"的数据。</p><p><strong>双曲空间</strong>是曲率为 $-1$ 的空间。在双曲几何中：</p><ul><li>过一点可以作<strong>无穷多条</strong>平行线</li><li>三角形的内角和小于180度</li><li>面积和周长增长比欧几里得空间快得多</li></ul><p><strong>双曲空间的一个惊人性质是它能自然地表示层次结构</strong>。考虑一棵树：从根节点到叶节点的路径长度随深度指数增长。双曲空间的"指数增长体积"正好匹配这种结构。</p><p><strong>庞加莱球模型</strong>是最常用的双曲空间模型：</p><p>$$\mathbb{B}^d = {x \in \mathbb{R}^d \mid |x| &lt; 1}$$</p><p>距离函数为：</p><p>$$d(u, v) = \text{arcosh}\left(1 + 2\frac{|u - v|^2}{(1 - |u|^2)(1 - |v|^2)}\right)$$</p><p><strong>双曲嵌入的应用</strong>：</p><ol><li><strong>词嵌入</strong>：Nickel和Kiela (2017) 展示了双曲空间能更好地捕捉WordNet的层次结构</li><li><strong>图嵌入</strong>：双曲空间能自然地表示图的层次结构</li><li><strong>少样本学习</strong>：双曲空间可能更适合捕捉概念的层次关系</li></ol><h3 id=44-李群与神经网络>4.4 李群与神经网络<a hidden class=anchor aria-hidden=true href=#44-李群与神经网络>#</a></h3><p><strong>李群</strong>（Lie Group）既是群（代数结构）又是流形（几何结构），且群运算在流形上是光滑的。李群在深度学习中有重要应用，特别是在处理对称性和变换时。</p><p><strong>特殊正交群 $SO(3)$</strong> 是三维旋转的群：</p><ul><li>单位元是恒等旋转</li><li>群运算是旋转的复合</li><li>它是一个三维流形</li></ul><p>在机器人学和计算机视觉中，我们需要处理旋转。一个常见的问题是：<strong>如何在神经网络中参数化旋转？</strong></p><p>几种常见的方法：</p><ol><li><strong>旋转矩阵</strong>：$3 \times 3$ 正交矩阵，缺点是有冗余（9个参数，只有3个自由度）</li><li><strong>欧拉角</strong>：三个旋转角，缺点是有万向锁问题</li><li><strong>四元数</strong>：四个参数，约束 $|\mathbf{q}| = 1$，这是最常用的方法</li><li><strong>轴角</strong>：三个参数（旋转轴 $\mathbf{u}$ 和角度 $\theta$）</li></ol><p><strong>图神经网络中的等变性</strong>：</p><p>近年来，<strong>等变神经网络</strong>（Equivariant Neural Networks）成为一个热门研究方向。这类网络在变换群（如旋转、平移）下保持等变性：</p><p>$$f(g \cdot x) = g \cdot f(x)$$</p><p>代表工作包括：</p><ul><li><strong>E(n)等变图神经网络</strong>（EGNN）</li><li><strong>SE(3)-等变Transformer</strong></li><li><strong>匹兹堡大学的等变消息传递网络</strong></li></ul><h3 id=45-黎曼流形优化>4.5 黎曼流形优化<a hidden class=anchor aria-hidden=true href=#45-黎曼流形优化>#</a></h3><p>深度学习中的优化通常使用<strong>随机梯度下降</strong>（SGD）及其变体。但当参数位于流形上时，普通的欧几里得优化可能不再适用。</p><p><strong>黎曼流形优化</strong>是研究如何在黎曼流形上进行优化的学科。核心思想是：</p><ul><li>使用<strong>黎曼梯度</strong>代替普通梯度</li><li>使用<strong>黎曼度量</strong>定义学习率</li><li>使用<strong>黎曼更新</strong>来移动点</li></ul><p><strong>黎曼梯度</strong>是普通梯度关于度规的"提升"：</p><p>$$\text{grad}<em>M f = g^{-1} \text{grad}</em>{\mathbb{R}^n} f$$</p><p><strong>黎曼流形SGD</strong>（RMSGD）的更新规则为：</p><p>$$x_{t+1} = \text{Exp}_{x_t}(-\eta \cdot \text{grad}_M f(x_t))$$</p><p>其中 $\text{Exp}_x(v)$ 是从 $x$ 出发、沿 $v$ 方向的测地线（指数映射）。</p><p><strong>应用场景</strong>：</p><ol><li><strong>正交约束优化</strong>：当参数矩阵需要保持正交时（如RNN的正交初始化）</li><li><strong>低秩矩阵恢复</strong>：当参数矩阵需要保持低秩时</li><li><strong>概率分布优化</strong>：当参数是概率分布时（如KL散度优化）</li></ol><hr><h2 id=第五章流形在机器人学中的应用>第五章：流形在机器人学中的应用<a hidden class=anchor aria-hidden=true href=#第五章流形在机器人学中的应用>#</a></h2><h3 id=51-姿态表示so3与se3>5.1 姿态表示：SO(3)与SE(3)<a hidden class=anchor aria-hidden=true href=#51-姿态表示so3与se3>#</a></h3><p>机器人学的核心问题之一是如何表示和操作三维空间中的<strong>姿态</strong>（position and orientation）。</p><p><strong>位置</strong>可以用 $\mathbb{R}^3$ 中的向量表示，这很简单。</p><p><strong>方向</strong>则复杂得多。描述三维旋转的群是：</p><ul><li><strong>SO(3)</strong>：特殊正交群，三维旋转，3个自由度</li><li><strong>SE(3)</strong>：特殊欧几里得群，三维刚体变换（旋转+平移），6个自由度</li></ul><p><strong>SO(3)的参数化</strong>：</p><p>旋转可以用多种方式参数化：</p><ol><li><strong>旋转矩阵</strong>：$R \in \mathbb{R}^{3 \times 3}$，满足 $R^\top R = I$，$\det(R) = 1$</li><li><strong>轴角</strong>：旋转轴 $\mathbf{u} \in S^2$ 和角度 $\theta \in \mathbb{R}$</li><li><strong>欧拉角</strong>：三个顺序旋转角（如XYZ, ZYX等），有12种约定</li><li><strong>四元数</strong>：$\mathbf{q} = (w, x, y, z) \in \mathbb{R}^4$，满足 $|\mathbf{q}| = 1$</li></ol><p><strong>四元数是SO(3)的最佳参数化</strong>：</p><p>四元数相比其他表示有几个优势：</p><ul><li><strong>无奇点</strong>：不像欧拉角有万向锁</li><li><strong>紧凑</strong>：4个参数，而旋转矩阵有9个</li><li><strong>高效</strong>：旋转的复合只需要四元数乘法</li><li><strong>数值稳定</strong>：归一化后保持单位范数</li></ul><p><strong>四元数的乘法</strong>：</p><p>两个四元数 $\mathbf{q}_1 = (w_1, \mathbf{v}_1)$ 和 $\mathbf{q}_2 = (w_2, \mathbf{v}_2)$ 的乘积为：</p><p>$$\mathbf{q}_1 \otimes \mathbf{q}_2 = (w_1 w_2 - \mathbf{v}_1 \cdot \mathbf{v}_2, w_1 \mathbf{v}_2 + w_2 \mathbf{v}_1 + \mathbf{v}_1 \times \mathbf{v}_2)$$</p><p>旋转的复合对应四元数的乘法。</p><h3 id=52-运动学与动力学>5.2 运动学与动力学<a hidden class=anchor aria-hidden=true href=#52-运动学与动力学>#</a></h3><p>机器人的<strong>运动学</strong>描述关节角度与末端执行器位置之间的关系。<strong>逆运动学</strong>是根据期望末端位置计算所需关节角度的问题。</p><p>对于串联机械臂（如UR5），运动学方程为：</p><p>$$T_{ee} = T_{base} \cdot A_1(q_1) \cdot A_2(q_2) \cdot \cdots \cdot A_n(q_n)$$</p><p>其中 $T_{ee}$ 是末端执行器的变换矩阵，$A_i(q_i)$ 是第 $i$ 个关节的变换矩阵，$q_i$ 是关节角度。</p><p><strong>雅可比矩阵</strong>描述关节速度与末端速度之间的关系：</p><p>$$\dot{x} = J(q) \dot{q}$$</p><p>雅可比矩阵在奇点处会奇异——这是机器人学中的一个核心问题。</p><p><strong>动力学</strong>描述关节力矩与运动之间的关系。<strong>欧拉-拉格朗日方程</strong>是：</p><p>$$M(q) \ddot{q} + C(q, \dot{q}) \dot{q} + G(q) = \tau$$</p><p>其中 $M(q)$ 是质量矩阵，$C(q, \dot{q})$ 是科里奥利和离心力项，$G(q)$ 是重力项，$\tau$ 是关节力矩。</p><p>所有这些方程都在流形（SO(3)或SE(3)）上操作。理解流形的几何对于正确处理机器人问题至关重要。</p><h3 id=53-运动规划>5.3 运动规划<a hidden class=anchor aria-hidden=true href=#53-运动规划>#</a></h3><p>机器人<strong>运动规划</strong>的目标是找到一条从起点到目标的无碰撞路径。</p><p><strong>配置空间</strong>（Configuration Space）是机器人所有可能配置的集合。对于 $n$ 自由度的机器人，配置空间是 $\mathbb{R}^n$（位置关节）或 $T^n$（旋转关节）的子流形。</p><p><strong>传统方法</strong>：</p><ol><li><strong>人工势场法</strong>：在配置空间中构造势函数，障碍物产生斥力，目标产生引力</li><li><strong>快速随机树</strong>（RRT）：通过随机采样构建路径树</li><li><strong>概率路线图</strong>（PRM）：预先构建随机路线图</li></ol><p><strong>流形上的运动规划</strong>：</p><p>当配置空间是流形时，运动规划变得更加复杂：</p><ol><li><strong>边界处理</strong>：流形可能没有边界（如SO(3)），需要考虑循环路径</li><li><strong>曲率影响</strong>：流形的曲率影响"最短路径"（测地线）的形状</li><li><strong>拓扑障碍</strong>：流形可能有非平凡的拓扑（如环面的"洞"），这可能导致路径不存在</li></ol><p><strong>SE(3)上的运动规划</strong>是一个特别重要的问题。SE(3) 的拓扑是 $\mathbb{R}^3 \times SO(3)$，它有一个非平凡的基本群（旋转360度和720度在拓扑上是不同的）。</p><h3 id=54-状态估计与slam>5.4 状态估计与SLAM<a hidden class=anchor aria-hidden=true href=#54-状态估计与slam>#</a></h3><p><strong>状态估计</strong>是机器人确定自身位置和姿态的问题。当机器人移动时，传感器噪声和运动不确定性会累积，因此需要<strong>滤波器</strong>或<strong>优化</strong>来融合多个测量。</p><p><strong>扩展卡尔曼滤波</strong>（EKF）是最经典的方法：</p><ol><li><strong>预测步骤</strong>：使用运动模型预测状态</li><li><strong>更新步骤</strong>：使用传感器测量更新状态</li></ol><p>在SE(3)上使用EKF需要小心处理：</p><ul><li>状态必须在流形上定义</li><li>协方差矩阵需要适当重参数化</li><li>雅可比矩阵需要在切空间中计算</li></ul><p><strong>同步定位与建图</strong>（SLAM）是机器人领域的核心问题。SLAM的目标是同时构建环境地图并定位机器人在其中的位置。</p><p><strong>图优化</strong>是现代SLAM的主流方法。问题被建模为一个图：</p><ul><li><strong>节点</strong>：机器人的位姿（SE(3)元素）和路标位置（$\mathbb{R}^3$元素）</li><li><strong>边</strong>：约束来自运动模型和观测模型</li></ul><p>图优化是一个最小二乘问题：</p><p>$$\min_X \sum_{(i,j) \in E} |z_{ij} - h_{ij}(X_i, X_j)|^2_{\Sigma_{ij}}$$</p><p>其中 $X$ 是所有位姿的集合，$z_{ij}$ 是测量值，$h_{ij}$ 是测量模型，$\Sigma_{ij}$ 是协方差矩阵。</p><p><strong>流形上的优化</strong>需要使用<strong>左扰动</strong>或<strong>右扰动</strong>来参数化SE(3)的切空间。</p><h3 id=55-强化学习中的流形>5.5 强化学习中的流形<a hidden class=anchor aria-hidden=true href=#55-强化学习中的流形>#</a></h3><p>机器人强化学习是让机器人通过试错学习控制策略的方法。当状态或动作空间是流形时，我们需要特殊处理。</p><p><strong>状态空间的流形结构</strong>：</p><ol><li><strong>姿态</strong>：末端执行器的姿态是SE(3)元素</li><li><strong>关节配置</strong>：旋转关节的角度在 $S^1$ 或 $SO(3)$ 上</li><li><strong>接触状态</strong>：接触/分离是离散状态</li></ol><p><strong>动作空间的流形结构</strong>：</p><ol><li><strong>关节速度/力矩</strong>：在 $\mathbb{R}^n$ 上，通常没问题</li><li><strong>末端速度</strong>：线速度在 $\mathbb{R}^3$ 上，角速度在 $\mathfrak{so}(3)$（SO(3)的李代数）上</li><li><strong>连续动作</strong>：如力/力矩，在 $\mathbb{R}^6$ 上</li></ol><p><strong>策略梯度方法</strong>：</p><p>在流形上使用策略梯度方法时，需要：</p><ul><li>正确计算流形上的梯度</li><li>使用适当的参数化（如四元数表示旋转）</li><li>处理流形的边界或约束</li></ul><p><strong>实例</strong>：训练机器人开门</p><ul><li><strong>状态</strong>：机器人末端的位置（$\mathbb{R}^3$）、姿态（四元数，$S^3$）、关节角度（$\mathbb{R}^7$）</li><li><strong>动作</strong>：关节速度（$\mathbb{R}^7$）</li><li><strong>挑战</strong>：四元数的约束 $|\mathbf{q}| = 1$ 必须在优化中保持</li></ul><hr><h2 id=第六章实战案例>第六章：实战案例<a hidden class=anchor aria-hidden=true href=#第六章实战案例>#</a></h2><h3 id=案例一人脸识别的流形学习>案例一：人脸识别的流形学习<a hidden class=anchor aria-hidden=true href=#案例一人脸识别的流形学习>#</a></h3><p><strong>问题背景</strong>：</p><p>人脸识别是计算机视觉的核心问题之一。传统方法依赖于手工设计的特征（如HOG、LBP），而深度学习方法则直接从数据中学习特征。</p><p><strong>流形视角</strong>：</p><p>所有人脸图像的集合构成一个流形，称为<strong>人脸流形</strong>。这个流形：</p><ul><li>是高维图像空间（$\mathbb{R}^{H \times W \times 3}$）的低维嵌入</li><li>具有非平凡的拓扑结构（可能同胚于某个低维流形）</li><li>上面的距离对应于人脸之间的"视觉相似度"</li></ul><p><strong>DeepFace和FaceNet</strong>：</p><p>Facebook的DeepFace和Google的FaceNet是两个里程碑式的工作。它们都使用<strong>深度卷积神经网络</strong>学习人脸嵌入，但关键洞察是：</p><ol><li><strong>中心损失</strong>（Center Loss）：让同一个人的所有图像聚集在嵌入空间中的某个中心周围</li><li><strong>角度边界损失</strong>（Angular Margin Loss）：在球面上增加类别之间的边界</li></ol><p><strong>SphereFace</strong> 提出了<strong>角度边界损失</strong>（Angular Margin Loss）：</p><p>$$L_{ang} = -\log\left(\frac{e^{|x| \cos(m\theta_{y})}}{e^{|x| \cos(m\theta_{y})} + \sum_{j \neq y} e^{|x| \cos\theta_j}}\right)$$</p><p>这个损失函数在<strong>球面空间</strong>上操作，因为 $\cos\theta$ 直接对应于球面上两个向量的内积。</p><p><strong>实践要点</strong>：</p><ol><li><strong>归一化</strong>：将特征向量归一化到单位球面</li><li><strong>角度度量</strong>：使用角度距离而非欧几里得距离</li><li><strong>多尺度</strong>：在不同尺度上提取特征，捕捉不同粒度的信息</li></ol><p><strong>代码示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>AngularMarginLoss</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>margin</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>scale</span><span class=o>=</span><span class=mi>30</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>margin</span> <span class=o>=</span> <span class=n>margin</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>scale</span> <span class=o>=</span> <span class=n>scale</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># logits: [batch_size, num_classes], 归一化特征的内积</span>
</span></span><span class=line><span class=cl>        <span class=c1># labels: [batch_size]</span>
</span></span><span class=line><span class=cl>        <span class=n>one_hot</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>logits</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>one_hot</span><span class=o>.</span><span class=n>scatter_</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>labels</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># 角度边界</span>
</span></span><span class=line><span class=cl>        <span class=n>sine</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>logits</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>phi</span> <span class=o>=</span> <span class=n>logits</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>margin</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># 添加角度边界</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=p>(</span><span class=n>one_hot</span> <span class=o>*</span> <span class=n>phi</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>one_hot</span><span class=p>)</span> <span class=o>*</span> <span class=n>sine</span><span class=p>)</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>scale</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=案例二四元数与3d旋转>案例二：四元数与3D旋转<a hidden class=anchor aria-hidden=true href=#案例二四元数与3d旋转>#</a></h3><p><strong>问题背景</strong>：</p><p>在机器人学、计算机图形学和计算机视觉中，精确表示和操作3D旋转是一个核心问题。四元数是解决这个问题的最佳工具之一。</p><p><strong>为什么是四元数？</strong></p><p>考虑以下几种旋转表示的对比：</p><table><thead><tr><th>表示</th><th>参数数量</th><th>奇点</th><th>数值稳定性</th><th>复合效率</th></tr></thead><tbody><tr><td>旋转矩阵</td><td>9</td><td>无</td><td>中等</td><td>低</td></tr><tr><td>欧拉角</td><td>3</td><td>有</td><td>低</td><td>中等</td></tr><tr><td>轴角</td><td>4</td><td>无</td><td>中等</td><td>中等</td></tr><tr><td>四元数</td><td>4</td><td>无</td><td>高</td><td>高</td></tr></tbody></table><p><strong>四元数的基础知识</strong>：</p><p>一个四元数 $\mathbf{q} = w + xi + yj + zk$ 可以写成 $\mathbf{q} = (w, \mathbf{v})$，其中 $\mathbf{v} = (x, y, z)$ 是向量部分。</p><p>四元数表示旋转的规则：</p><ul><li><strong>单位四元数</strong>：只有单位四元数（$|\mathbf{q}| = 1$）表示有效的旋转</li><li><strong>旋转角度</strong>：$\theta = 2 \arccos(w)$</li><li><strong>旋转轴</strong>：$\mathbf{u} = \mathbf{v} / \sin(\theta/2)$</li></ul><p><strong>绕任意轴旋转</strong>：</p><p>要将一个点 $\mathbf{p}$ 绕轴 $\mathbf{u}$ 旋转角度 $\theta$：</p><ol><li>将点写成纯四元数：$\mathbf{p} = (0, \mathbf{p})$</li><li>构造旋转四元数：$\mathbf{q} = (\cos(\theta/2), \sin(\theta/2)\mathbf{u})$</li><li>执行旋转：$\mathbf{p}&rsquo; = \mathbf{q} \mathbf{p} \mathbf{q}^{-1}$</li><li>提取向量部分</li></ol><p><strong>四元数的SLERP</strong>：</p><p>在动画和路径规划中，我们经常需要在两个旋转之间插值。<strong>球面线性插值</strong>（SLERP）是专门为四元数设计的插值方法：</p><p>$$\text{SLERP}(\mathbf{q}_0, \mathbf{q}_1, t) = \frac{\sin((1-t)\Omega)}{\sin\Omega}\mathbf{q}_0 + \frac{\sin(t\Omega)}{\sin\Omega}\mathbf{q}_1$$</p><p>其中 $\Omega$ 是 $\mathbf{q}_0$ 和 $\mathbf{q}_1$ 之间的角度，$t \in [0, 1]$ 是插值参数。</p><p>SLERP保证：</p><ul><li>角速度恒定（如果 $t$ 与时间成正比）</li><li>路径是最短测地线</li><li>不会产生"万向锁"问题</li></ul><p><strong>代码示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>quaternion_to_rotation_matrix</span><span class=p>(</span><span class=n>q</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;将四元数转换为旋转矩阵&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>w</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>z</span> <span class=o>=</span> <span class=n>q</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 归一化</span>
</span></span><span class=line><span class=cl>    <span class=n>q</span> <span class=o>=</span> <span class=n>q</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>q</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>w</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>z</span> <span class=o>=</span> <span class=n>q</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>R</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=mi>1</span> <span class=o>-</span> <span class=mi>2</span><span class=o>*</span><span class=n>y</span><span class=o>**</span><span class=mi>2</span> <span class=o>-</span> <span class=mi>2</span><span class=o>*</span><span class=n>z</span><span class=o>**</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=o>*</span><span class=n>x</span><span class=o>*</span><span class=n>y</span> <span class=o>-</span> <span class=mi>2</span><span class=o>*</span><span class=n>w</span><span class=o>*</span><span class=n>z</span><span class=p>,</span> <span class=mi>2</span><span class=o>*</span><span class=n>x</span><span class=o>*</span><span class=n>z</span> <span class=o>+</span> <span class=mi>2</span><span class=o>*</span><span class=n>w</span><span class=o>*</span><span class=n>y</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=mi>2</span><span class=o>*</span><span class=n>x</span><span class=o>*</span><span class=n>y</span> <span class=o>+</span> <span class=mi>2</span><span class=o>*</span><span class=n>w</span><span class=o>*</span><span class=n>z</span><span class=p>,</span> <span class=mi>1</span> <span class=o>-</span> <span class=mi>2</span><span class=o>*</span><span class=n>x</span><span class=o>**</span><span class=mi>2</span> <span class=o>-</span> <span class=mi>2</span><span class=o>*</span><span class=n>z</span><span class=o>**</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=o>*</span><span class=n>y</span><span class=o>*</span><span class=n>z</span> <span class=o>-</span> <span class=mi>2</span><span class=o>*</span><span class=n>w</span><span class=o>*</span><span class=n>x</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=mi>2</span><span class=o>*</span><span class=n>x</span><span class=o>*</span><span class=n>z</span> <span class=o>-</span> <span class=mi>2</span><span class=o>*</span><span class=n>w</span><span class=o>*</span><span class=n>y</span><span class=p>,</span> <span class=mi>2</span><span class=o>*</span><span class=n>y</span><span class=o>*</span><span class=n>z</span> <span class=o>+</span> <span class=mi>2</span><span class=o>*</span><span class=n>w</span><span class=o>*</span><span class=n>x</span><span class=p>,</span> <span class=mi>1</span> <span class=o>-</span> <span class=mi>2</span><span class=o>*</span><span class=n>x</span><span class=o>**</span><span class=mi>2</span> <span class=o>-</span> <span class=mi>2</span><span class=o>*</span><span class=n>y</span><span class=o>**</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>R</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>slerp</span><span class=p>(</span><span class=n>q0</span><span class=p>,</span> <span class=n>q1</span><span class=p>,</span> <span class=n>t</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;球面线性插值&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 确保四元数在同一个半球</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>q0</span><span class=p>,</span> <span class=n>q1</span><span class=p>)</span> <span class=o>&lt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>q1</span> <span class=o>=</span> <span class=o>-</span><span class=n>q1</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>dot</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>q0</span><span class=p>,</span> <span class=n>q1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>dot</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=n>dot</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>  <span class=c1># 避免数值误差</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>omega</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arccos</span><span class=p>(</span><span class=n>dot</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>sin_omega</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>omega</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>sin_omega</span> <span class=o>&lt;</span> <span class=mf>1e-6</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>t</span><span class=p>)</span> <span class=o>*</span> <span class=n>q0</span> <span class=o>+</span> <span class=n>t</span> <span class=o>*</span> <span class=n>q1</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>a</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>((</span><span class=mi>1</span><span class=o>-</span><span class=n>t</span><span class=p>)</span> <span class=o>*</span> <span class=n>omega</span><span class=p>)</span> <span class=o>/</span> <span class=n>sin_omega</span>
</span></span><span class=line><span class=cl>    <span class=n>b</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>t</span> <span class=o>*</span> <span class=n>omega</span><span class=p>)</span> <span class=o>/</span> <span class=n>sin_omega</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>a</span> <span class=o>*</span> <span class=n>q0</span> <span class=o>+</span> <span class=n>b</span> <span class=o>*</span> <span class=n>q1</span>
</span></span></code></pre></div><h3 id=案例三自动驾驶中的状态估计>案例三：自动驾驶中的状态估计<a hidden class=anchor aria-hidden=true href=#案例三自动驾驶中的状态估计>#</a></h3><p><strong>问题背景</strong>：</p><p>自动驾驶汽车需要精确估计自己的位置和姿态（<strong>自车状态</strong>）。这包括：</p><ul><li><strong>位置</strong>：$(x, y, z)$ 在某个世界坐标系中</li><li><strong>姿态</strong>：绕三个轴的旋转（横滚、俯仰、偏航）</li><li><strong>速度</strong>：线速度和角速度</li></ul><p><strong>传感器融合</strong>：</p><p>自动驾驶使用多种传感器：</p><ul><li><strong>GPS</strong>：提供全局位置，但精度有限（几米到几十米）</li><li><strong>惯性测量单元</strong>（IMU）：提供高频加速度和角速度，但会漂移</li><li><strong>轮式里程计</strong>：从轮子转速估计位移，但有打滑问题</li><li><strong>激光雷达/相机</strong>：提供局部环境信息，可以用于匹配</li></ul><p><strong>扩展卡尔曼滤波</strong>：</p><p>EKF是融合多传感器测量的经典方法。在自动驾驶中，状态通常定义为：</p><p>$$\mathbf{x} = [x, y, z, \phi, \theta, \psi, v_x, v_y, v_z, \omega_x, \omega_y, \omega_z]^\top$$</p><p>其中 $\phi, \theta, \psi$ 是欧拉角，$v$ 是线速度，$\omega$ 是角速度。</p><p><strong>流形上的EKF</strong>：</p><p>关键问题是如何处理姿态的流形结构。一种方法是<strong>局部切空间滤波</strong>：</p><ol><li>在某个参考姿态 $\bar{R}$ 处建立切空间</li><li>在切空间中执行标准的卡尔曼滤波</li><li>使用指数映射将结果映射回流形</li></ol><p>另一种方法是<strong>无迹卡尔曼滤波</strong>（UKF），它不需要显式计算雅可比矩阵。</p><p><strong>误差状态卡尔曼滤波</strong>：</p><p>在机器人学中，<strong>误差状态</strong>（Error State）形式更为常见：</p><p>$$\mathbf{x}<em>{true} = \mathbf{x}</em>{nom} \oplus \delta\mathbf{x}$$</p><p>其中 $\mathbf{x}_{nom}$ 是标称状态（在流形上），$\delta\mathbf{x}$ 是误差状态（在切空间中）。</p><p>这种形式的优势：</p><ul><li>误差状态是小的，可以用高斯分布近似</li><li>避免了大角度旋转的三角函数计算</li><li>数值更稳定</li></ul><p><strong>代码示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ErrorStateEKF</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 状态维度: 位置(3) + 姿态(3) + 速度(3) + 偏置(6) = 15</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dim</span> <span class=o>=</span> <span class=mi>15</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># 状态: [pos(3), rotvec(3), vel(3), accel_bias(3), gyro_bias(3)]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>state</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>cov</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>eye</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>dim</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># 噪声参数</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>Q</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>eye</span><span class=p>(</span><span class=mi>12</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.01</span>  <span class=c1># 过程噪声</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dt</span><span class=p>,</span> <span class=n>accel</span><span class=p>,</span> <span class=n>gyro</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;预测步骤&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 提取状态</span>
</span></span><span class=line><span class=cl>        <span class=n>pos</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>[:</span><span class=mi>3</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>rotvec</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>[</span><span class=mi>3</span><span class=p>:</span><span class=mi>6</span><span class=p>]</span>  <span class=c1># 旋转向量</span>
</span></span><span class=line><span class=cl>        <span class=n>vel</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>[</span><span class=mi>6</span><span class=p>:</span><span class=mi>9</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>accel_bias</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>[</span><span class=mi>9</span><span class=p>:</span><span class=mi>12</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>gyro_bias</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>[</span><span class=mi>12</span><span class=p>:</span><span class=mi>15</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># 校正测量</span>
</span></span><span class=line><span class=cl>        <span class=n>accel_corrected</span> <span class=o>=</span> <span class=n>accel</span> <span class=o>-</span> <span class=n>accel_bias</span>
</span></span><span class=line><span class=cl>        <span class=n>gyro_corrected</span> <span class=o>=</span> <span class=n>gyro</span> <span class=o>-</span> <span class=n>gyro_bias</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># 更新状态</span>
</span></span><span class=line><span class=cl>        <span class=c1># 位置: pos += vel * dt</span>
</span></span><span class=line><span class=cl>        <span class=n>pos</span> <span class=o>=</span> <span class=n>pos</span> <span class=o>+</span> <span class=n>vel</span> <span class=o>*</span> <span class=n>dt</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># 速度: vel += accel_corrected * dt (在世界坐标系)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 需要将加速度从体坐标系转换到世界坐标系</span>
</span></span><span class=line><span class=cl>        <span class=n>C_bn</span> <span class=o>=</span> <span class=n>rotation_vector_to_matrix</span><span class=p>(</span><span class=n>rotvec</span><span class=p>)</span>  <span class=c1># 体到世界的旋转</span>
</span></span><span class=line><span class=cl>        <span class=n>vel</span> <span class=o>=</span> <span class=n>vel</span> <span class=o>+</span> <span class=n>C_bn</span> <span class=o>@</span> <span class=n>accel_corrected</span> <span class=o>*</span> <span class=n>dt</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># 姿态: rotvec += gyro_corrected * dt</span>
</span></span><span class=line><span class=cl>        <span class=n>rotvec</span> <span class=o>=</span> <span class=n>rotvec</span> <span class=o>+</span> <span class=n>gyro_corrected</span> <span class=o>*</span> <span class=n>dt</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># 偏置随机游走</span>
</span></span><span class=line><span class=cl>        <span class=c1># (简化处理，不更新偏置)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># 更新协方差</span>
</span></span><span class=line><span class=cl>        <span class=c1># ... (雅可比矩阵和卡尔曼增益计算)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>[:</span><span class=mi>3</span><span class=p>]</span> <span class=o>=</span> <span class=n>pos</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>[</span><span class=mi>3</span><span class=p>:</span><span class=mi>6</span><span class=p>]</span> <span class=o>=</span> <span class=n>rotvec</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>[</span><span class=mi>6</span><span class=p>:</span><span class=mi>9</span><span class=p>]</span> <span class=o>=</span> <span class=n>vel</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>update</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>measurement</span><span class=p>,</span> <span class=n>H</span><span class=p>,</span> <span class=n>R</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;更新步骤&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 计算残差</span>
</span></span><span class=line><span class=cl>        <span class=n>z_pred</span> <span class=o>=</span> <span class=n>H</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>state</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=n>measurement</span> <span class=o>-</span> <span class=n>z_pred</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># 卡尔曼增益</span>
</span></span><span class=line><span class=cl>        <span class=n>S</span> <span class=o>=</span> <span class=n>H</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>cov</span> <span class=o>@</span> <span class=n>H</span><span class=o>.</span><span class=n>T</span> <span class=o>+</span> <span class=n>R</span>
</span></span><span class=line><span class=cl>        <span class=n>K</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>cov</span> <span class=o>@</span> <span class=n>H</span><span class=o>.</span><span class=n>T</span> <span class=o>@</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>inv</span><span class=p>(</span><span class=n>S</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># 更新状态</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>state</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>state</span> <span class=o>+</span> <span class=n>K</span> <span class=o>@</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># 更新协方差 (Joseph form for numerical stability)</span>
</span></span><span class=line><span class=cl>        <span class=n>I_KH</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>eye</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>dim</span><span class=p>)</span> <span class=o>-</span> <span class=n>K</span> <span class=o>@</span> <span class=n>H</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>cov</span> <span class=o>=</span> <span class=n>I_KH</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>cov</span> <span class=o>@</span> <span class=n>I_KH</span><span class=o>.</span><span class=n>T</span> <span class=o>+</span> <span class=n>K</span> <span class=o>@</span> <span class=n>R</span> <span class=o>@</span> <span class=n>K</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>rotation_vector_to_matrix</span><span class=p>(</span><span class=n>rotvec</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;将旋转向量转换为旋转矩阵&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>theta</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>rotvec</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>theta</span> <span class=o>&lt;</span> <span class=mf>1e-8</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>eye</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>k</span> <span class=o>=</span> <span class=n>rotvec</span> <span class=o>/</span> <span class=n>theta</span>
</span></span><span class=line><span class=cl>    <span class=n>K</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=n>k</span><span class=p>[</span><span class=mi>2</span><span class=p>],</span> <span class=n>k</span><span class=p>[</span><span class=mi>1</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=n>k</span><span class=p>[</span><span class=mi>2</span><span class=p>],</span> <span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=n>k</span><span class=p>[</span><span class=mi>0</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=o>-</span><span class=n>k</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>k</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=p>])</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># Rodrigues公式</span>
</span></span><span class=line><span class=cl>    <span class=n>R</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>eye</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>theta</span><span class=p>)</span> <span class=o>*</span> <span class=n>K</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>theta</span><span class=p>))</span> <span class=o>*</span> <span class=p>(</span><span class=n>K</span> <span class=o>@</span> <span class=n>K</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>R</span>
</span></span></code></pre></div><h3 id=案例四强化学习中的黎曼流形优化>案例四：强化学习中的黎曼流形优化<a hidden class=anchor aria-hidden=true href=#案例四强化学习中的黎曼流形优化>#</a></h3><p><strong>问题背景</strong>：</p><p>在强化学习中，策略梯度方法需要优化高维参数空间。当参数空间具有流形结构时（如正交矩阵、低秩矩阵），黎曼流形优化可以显著提高性能和稳定性。</p><p><strong>实例：正交RNN</strong></p><p>在循环神经网络中，隐藏状态的演化是：</p><p>$$\mathbf{h}<em>t = \tanh(W \mathbf{h}</em>{t-1} + U \mathbf{x}_t)$$</p><p>如果权重矩阵 $W$ 是正交的，那么梯度会沿着隐藏状态的方向传播而不会消失或爆炸——这解决了RNN的梯度消失问题。</p><p><strong>流形约束</strong>：</p><p>$W$ 需要是正交矩阵，即 $W^\top W = I$。这意味着 $W$ 位于<strong>正交群</strong> $O(n)$ 上。</p><p><strong>黎曼梯度下降</strong>：</p><p>在正交群上进行优化的步骤：</p><ol><li><p><strong>黎曼梯度</strong>：计算普通梯度 $\nabla f$，然后投影到切空间
$$\text{grad}_{O(n)} f = \nabla f - \nabla f \cdot W^\top W + \nabla f \cdot W^\top W$$（简化）
更准确地说：$\text{grad} = \nabla f \cdot W^\top W - W^\top \cdot \nabla f \cdot W$ 的对称部分</p></li><li><p><strong>指数映射</strong>：从切空间映射回流形
$$\text{Exp}_W(V) = W \cdot \exp(V)$$
其中 $\exp$ 是矩阵指数，$V$ 是切空间中的反对称矩阵</p></li><li><p>** retraction**：指数映射的近似，计算更高效
$$\text{Retr}_W(V) = \text{qf}(W + V)$$
其中 $\text{qf}$ 是正交化（QR分解）</p></li></ol><p><strong>代码示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>OrthogonalRNN</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span> <span class=o>=</span> <span class=n>hidden_size</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># 权重矩阵，初始化为正交矩阵</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>orthogonal_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>W</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>U</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>input_size</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>h</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>h</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>h</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>x</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>            <span class=n>h</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>W</span> <span class=o>@</span> <span class=n>h</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>U</span> <span class=o>@</span> <span class=n>x</span><span class=p>[:,</span> <span class=n>t</span><span class=p>]</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>output</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>h</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span> <span class=n>h</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>orthogonalize</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;将权重矩阵正交化&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>W</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W</span><span class=o>.</span><span class=n>data</span>
</span></span><span class=line><span class=cl>            <span class=n>Q</span><span class=p>,</span> <span class=n>R</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>qr</span><span class=p>(</span><span class=n>W</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># 调整符号以保持与原矩阵一致</span>
</span></span><span class=line><span class=cl>            <span class=n>d</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>diag</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>sign</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>diag</span><span class=p>(</span><span class=n>R</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>W</span><span class=o>.</span><span class=n>data</span> <span class=o>=</span> <span class=n>Q</span> <span class=o>@</span> <span class=n>d</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>riemannian_gradient</span><span class=p>(</span><span class=n>W</span><span class=p>,</span> <span class=n>grad</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;计算正交流形上的黎曼梯度&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 梯度投影到切空间</span>
</span></span><span class=line><span class=cl>    <span class=c1># 对于正交群，切空间是反对称矩阵</span>
</span></span><span class=line><span class=cl>    <span class=c1># grad_O(n)(f) = grad - grad @ W^T @ W + W^T @ grad @ W 的对称部分</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 简化版本</span>
</span></span><span class=line><span class=cl>    <span class=n>G</span> <span class=o>=</span> <span class=n>grad</span> <span class=o>-</span> <span class=n>W</span> <span class=o>@</span> <span class=n>grad</span><span class=o>.</span><span class=n>T</span> <span class=o>@</span> <span class=n>W</span>
</span></span><span class=line><span class=cl>    <span class=n>G</span> <span class=o>=</span> <span class=p>(</span><span class=n>G</span> <span class=o>+</span> <span class=n>G</span><span class=o>.</span><span class=n>T</span><span class=p>)</span> <span class=o>/</span> <span class=mi>2</span>  <span class=c1># 对称化</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>G</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>riemannian_step</span><span class=p>(</span><span class=n>W</span><span class=p>,</span> <span class=n>grad</span><span class=p>,</span> <span class=n>lr</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;黎曼梯度下降一步&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>G</span> <span class=o>=</span> <span class=n>riemannian_gradient</span><span class=p>(</span><span class=n>W</span><span class=p>,</span> <span class=n>grad</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 构建反对称矩阵</span>
</span></span><span class=line><span class=cl>    <span class=n>K</span> <span class=o>=</span> <span class=p>(</span><span class=n>G</span> <span class=o>-</span> <span class=n>G</span><span class=o>.</span><span class=n>T</span><span class=p>)</span> <span class=o>/</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 使用 retraction 而非指数映射（更高效）</span>
</span></span><span class=line><span class=cl>    <span class=c1># Retr_W(V) = qf(W + V) where qf is QR decomposition</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>W_new</span> <span class=o>=</span> <span class=n>W</span> <span class=o>-</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>K</span>
</span></span><span class=line><span class=cl>        <span class=n>Q</span><span class=p>,</span> <span class=n>R</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>qr</span><span class=p>(</span><span class=n>W_new</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>d</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>diag</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>sign</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>diag</span><span class=p>(</span><span class=n>R</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=n>W</span><span class=o>.</span><span class=n>copy_</span><span class=p>(</span><span class=n>Q</span> <span class=o>@</span> <span class=n>d</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>W</span>
</span></span></code></pre></div><hr><h2 id=第七章现代流形学习的深入探讨>第七章：现代流形学习的深入探讨<a hidden class=anchor aria-hidden=true href=#第七章现代流形学习的深入探讨>#</a></h2><h3 id=71-局部线性嵌入lle>7.1 局部线性嵌入（LLE）<a hidden class=anchor aria-hidden=true href=#71-局部线性嵌入lle>#</a></h3><p><strong>核心思想</strong>：</p><p>LLE假设数据在局部是线性的——每个数据点都可以由其邻居的线性组合表示。LLE保持这种局部线性关系，同时将数据嵌入到低维空间。</p><p><strong>算法步骤</strong>：</p><ol><li><strong>邻居选择</strong>：对于每个点 $x_i$，找到 $k$ 个最近邻 ${x_j}$</li><li><strong>局部重建权值</strong>：求解最小二乘问题：
$$\min_{w_{ij}} |x_i - \sum_j w_{ij} x_j|^2$$
约束：$\sum_j w_{ij} = 1$（归一化）</li><li><strong>低维嵌入</strong>：在低维空间 $y_i$ 中保持相同的权值：
$$\min_{y_i} |y_i - \sum_j w_{ij} y_j|^2$$</li></ol><p><strong>数学性质</strong>：</p><ul><li>LLE的解由稀疏特征值问题给出</li><li>保持数据的局部邻域结构</li><li>对噪声和异常值敏感</li></ul><h3 id=72-等度量映射isomap>7.2 等度量映射（Isomap）<a hidden class=anchor aria-hidden=true href=#72-等度量映射isomap>#</a></h3><p><strong>核心思想</strong>：</p><p>Isomap是流形学习的里程碑工作，它用图上的最短路径近似流形上的测地距离，然后使用多维缩放（MDS）将数据嵌入到低维空间。</p><p><strong>算法步骤</strong>：</p><ol><li><strong>邻居图构建</strong>：连接每个点到其 $k$ 个最近邻</li><li><strong>测地距离估计</strong>：对于每对点，计算图上的最短路径距离 $d_G(i, j)$</li><li><strong>经典MDS</strong>：将距离矩阵嵌入到低维空间</li></ol><p><strong>数学性质</strong>：</p><ul><li>Isomap理论上可以恢复等距嵌入（如果噪声足够小）</li><li>对"孔洞"和非均匀采样敏感</li><li>计算复杂度较高（需要计算所有对的最短路径）</li></ul><h3 id=73-拉普拉斯特征映射le>7.3 拉普拉斯特征映射（LE）<a hidden class=anchor aria-hidden=true href=#73-拉普拉斯特征映射le>#</a></h3><p><strong>核心思想</strong>：</p><p>LE假设相似的点在嵌入空间中应该保持相似。它最小化邻居之间权重的拉普拉斯算子。</p><p><strong>算法步骤</strong>：</p><ol><li><strong>邻居图构建</strong>：同上</li><li><strong>权重计算</strong>：$W_{ij} = \exp(-|x_i - x_j|^2 / t)$ 或二值化</li><li><strong>特征分解</strong>：求解广义特征值问题：
$$L y = \lambda D y$$
其中 $L = D - W$ 是拉普拉斯矩阵，$D$ 是度矩阵</li></ol><p><strong>数学性质</strong>：</p><ul><li>LE与图的拉普拉斯算子的谱相关</li><li>保持局部结构，但不保证全局结构</li><li>对参数选择（如邻居数 $k$ 和带宽 $t$）敏感</li></ul><h3 id=74-t-分布随机邻域嵌入t-sne>7.4 t-分布随机邻域嵌入（t-SNE）<a hidden class=anchor aria-hidden=true href=#74-t-分布随机邻域嵌入t-sne>#</a></h3><p><strong>核心思想</strong>：</p><p>t-SNE是可视化高维数据的强大工具。它使用t-分布来测量低维空间中的相似度，解决了"拥挤问题"。</p><p><strong>算法步骤</strong>：</p><ol><li><strong>高维相似度</strong>：使用高斯核：
$$p_{j|i} = \frac{\exp(-|x_i - x_j|^2 / 2\sigma^2)}{\sum_{k \neq i} \exp(-|x_i - x_k|^2 / 2\sigma^2)}$$</li><li><strong>低维相似度</strong>：使用t-分布（自由度为1）：
$$q_{ij} = \frac{(1 + |y_i - y_j|^2)^{-1}}{\sum_{k \neq l}(1 + |y_k - y_l|^2)^{-1}}$$</li><li><strong>KL散度最小化</strong>：
$$C = KL(P | Q) = \sum_{i \neq j} p_{ij} \log\frac{p_{ij}}{q_{ij}}$$</li></ol><p><strong>数学性质</strong>：</p><ul><li>t-SNE的解通过梯度下降获得</li><li>不保持全局结构，只保持局部结构</li><li>随机性：多次运行可能产生不同结果</li></ul><hr><h2 id=结语流形的哲学>结语：流形的哲学<a hidden class=anchor aria-hidden=true href=#结语流形的哲学>#</a></h2><h3 id=从几何到智能>从几何到智能<a hidden class=anchor aria-hidden=true href=#从几何到智能>#</a></h3><p>回顾我们走过的旅程，从高斯和黎曼的几何革命，到深度学习和机器人学的前沿应用，我们看到了数学概念如何塑造现代科技。</p><p><strong>流形的本质洞察</strong>是：<strong>复杂系统通常可以分解为局部简单的部分，这些部分通过某种"粘合"方式组装成复杂的整体</strong>。</p><p>这种洞察无处不在：</p><ol><li><strong>深度学习</strong>：神经网络通过叠加简单的非线性变换来学习复杂的函数</li><li><strong>机器人学</strong>：复杂的机器人运动可以分解为关节空间的简单旋转和平移</li><li><strong>计算机视觉</strong>：复杂的视觉场景可以分解为局部特征的组合</li></ol><h3 id=思想的进化>思想的进化<a hidden class=anchor aria-hidden=true href=#思想的进化>#</a></h3><p>高斯不会想到他关于曲面曲率的工作会影响到自动驾驶汽车；黎曼不会想到他的几何会成为机器学习的理论基础。</p><p>这就是数学的力量：<strong>抽象的概念往往会在意想不到的地方找到应用</strong>。</p><h3 id=给读者的话>给读者的话<a hidden class=anchor aria-hidden=true href=#给读者的话>#</a></h3><p>如果你读到这里，我希望你已经：</p><ol><li><strong>理解了流形的基本概念</strong>：从拓扑空间到黎曼流形</li><li><strong>看到了流形的应用</strong>：从深度学习到机器人学</li><li><strong>学到了实战技能</strong>：从四元数计算到流形优化</li></ol><p>流形是一个庞大的领域，这篇文章只是冰山一角。如果你有兴趣深入学习，我推荐：</p><ol><li><strong>微分几何</strong>：Do Carmo的《Differential Geometry of Curves and Surfaces》</li><li><strong>黎曼几何</strong>：Lee的《Riemannian Manifolds: An Introduction to Curvature》</li><li><strong>流形学习</strong>：Burges的《Dimension Reduction: A Guided Tour》</li><li><strong>李群与机器</strong>：Hall的《Lie Groups, Lie Algebras, and Representations》</li></ol><p>最后，让我用黎曼1854年演讲的结语来结束这篇文章：</p><blockquote><p>&ldquo;几何命题的真实性是不容置疑的，只要它们涉及可感知的关系；但它们的意义是无穷的，如果我们假设这些命题的真实性扩展到可感知范围之外——扩展到无限大的领域&mldr;&mldr;这正是几何与哲学的联系所在。&rdquo;</p></blockquote><hr><h2 id=附录重要公式汇总>附录：重要公式汇总<a hidden class=anchor aria-hidden=true href=#附录重要公式汇总>#</a></h2><h3 id=流形基础>流形基础<a hidden class=anchor aria-hidden=true href=#流形基础>#</a></h3><p><strong>坐标变换</strong>：
$$\frac{\partial}{\partial x^i} = \sum_j \frac{\partial \bar{x}^j}{\partial x^i} \frac{\partial}{\partial \bar{x}^j}$$</p><h3 id=黎曼几何>黎曼几何<a hidden class=anchor aria-hidden=true href=#黎曼几何>#</a></h3><p><strong>度规张量</strong>：
$$g_{ij} = \left\langle \frac{\partial}{\partial x^i}, \frac{\partial}{\partial x^j} \right\rangle$$</p><p><strong>克里斯托费尔符号</strong>：
$$\Gamma^k_{ij} = \frac{1}{2} g^{kl} \left( \frac{\partial g_{il}}{\partial x^j} + \frac{\partial g_{jl}}{\partial x^i} - \frac{\partial g_{ij}}{\partial x^l} \right)$$</p><p><strong>测地线方程</strong>：
$$\frac{d^2x^k}{dt^2} + \Gamma^k_{ij} \frac{dx^i}{dt} \frac{dx^j}{dt} = 0$$</p><p><strong>黎曼曲率张量</strong>：
$$R^i_{jkl} = \partial_k \Gamma^i_{jl} - \partial_l \Gamma^i_{jk} + \Gamma^i_{km}\Gamma^m_{jl} - \Gamma^i_{lm}\Gamma^m_{jk}$$</p><h3 id=四元数>四元数<a hidden class=anchor aria-hidden=true href=#四元数>#</a></h3><p><strong>乘法</strong>：
$$(w_1, \mathbf{v}_1) \otimes (w_2, \mathbf{v}_2) = (w_1 w_2 - \mathbf{v}_1 \cdot \mathbf{v}_2, w_1 \mathbf{v}_2 + w_2 \mathbf{v}_1 + \mathbf{v}_1 \times \mathbf{v}_2)$$</p><p><strong>SLERP</strong>：
$$\text{SLERP}(\mathbf{q}_0, \mathbf{q}_1, t) = \frac{\sin((1-t)\Omega)}{\sin\Omega}\mathbf{q}_0 + \frac{\sin(t\Omega)}{\sin\Omega}\mathbf{q}_1$$</p><h3 id=流形优化>流形优化<a hidden class=anchor aria-hidden=true href=#流形优化>#</a></h3><p><strong>黎曼梯度</strong>：
$$\text{grad}<em>M f = g^{-1} \text{grad}</em>{\mathbb{R}^n} f$$</p><p><strong>指数映射</strong>：
$$\text{Exp}_p(v) = \gamma_v(1)$$
其中 $\gamma_v$ 是以 $v$ 为初始速度的测地线</p><hr><p><em>本文旨在为有一定数学基础的读者提供流形的入门导引。更深入的学习建议参考专业教材。</em></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://s-ai-unix.github.io/tags/%E9%BB%8E%E6%9B%BC%E5%87%A0%E4%BD%95/>黎曼几何</a></li><li><a href=https://s-ai-unix.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li><li><a href=https://s-ai-unix.github.io/tags/%E5%BE%AE%E5%88%86%E5%87%A0%E4%BD%95/>微分几何</a></li></ul><nav class=paginav><a class=prev href=https://s-ai-unix.github.io/posts/2026-01-13-iso26262-1-vocabulary/><span class=title>« Prev</span><br><span>ISO 26262-1 词汇：功能安全标准的语言基础</span>
</a><a class=next href=https://s-ai-unix.github.io/posts/2026-01-12-gauss-theorema-egregium/><span class=title>Next »</span><br><span>高斯绝妙定理：弯曲时空的内禀几何</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 流形：从弯曲空间到深度学习与机器人学的漫游 on x" href="https://x.com/intent/tweet/?text=%e6%b5%81%e5%bd%a2%ef%bc%9a%e4%bb%8e%e5%bc%af%e6%9b%b2%e7%a9%ba%e9%97%b4%e5%88%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%8e%e6%9c%ba%e5%99%a8%e4%ba%ba%e5%ad%a6%e7%9a%84%e6%bc%ab%e6%b8%b8&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-12-manifold-deep-learning-robotics%2f&amp;hashtags=%e9%bb%8e%e6%9b%bc%e5%87%a0%e4%bd%95%2c%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%2c%e5%be%ae%e5%88%86%e5%87%a0%e4%bd%95"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 流形：从弯曲空间到深度学习与机器人学的漫游 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-12-manifold-deep-learning-robotics%2f&amp;title=%e6%b5%81%e5%bd%a2%ef%bc%9a%e4%bb%8e%e5%bc%af%e6%9b%b2%e7%a9%ba%e9%97%b4%e5%88%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%8e%e6%9c%ba%e5%99%a8%e4%ba%ba%e5%ad%a6%e7%9a%84%e6%bc%ab%e6%b8%b8&amp;summary=%e6%b5%81%e5%bd%a2%ef%bc%9a%e4%bb%8e%e5%bc%af%e6%9b%b2%e7%a9%ba%e9%97%b4%e5%88%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%8e%e6%9c%ba%e5%99%a8%e4%ba%ba%e5%ad%a6%e7%9a%84%e6%bc%ab%e6%b8%b8&amp;source=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-12-manifold-deep-learning-robotics%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 流形：从弯曲空间到深度学习与机器人学的漫游 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-12-manifold-deep-learning-robotics%2f&title=%e6%b5%81%e5%bd%a2%ef%bc%9a%e4%bb%8e%e5%bc%af%e6%9b%b2%e7%a9%ba%e9%97%b4%e5%88%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%8e%e6%9c%ba%e5%99%a8%e4%ba%ba%e5%ad%a6%e7%9a%84%e6%bc%ab%e6%b8%b8"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 流形：从弯曲空间到深度学习与机器人学的漫游 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fs-ai-unix.github.io%2fposts%2f2026-01-12-manifold-deep-learning-robotics%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=s-ai-unix/blog data-repo-id=R_kgDOQ3Njaw data-category=General data-category-id=DIC_kwDOQ3Nja84C0yve data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://s-ai-unix.github.io/>s-ai-unix's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var n,t=e.innerHTML,s=!1;for(t.indexOf("<em>")!==-1&&(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("</em>")!==-1&&(n=t.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),n=n.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("<em>$")!==-1&&(n=t.replace(/<em>\$/g,"$"),n!==t&&(t=n,s=!0)),t.indexOf("</em>$")!==-1&&(n=t.replace(/<\/em>\$/g,"$"),n!==t&&(t=n,s=!0));t.indexOf("<em>")!==-1&&t.indexOf("</em>")!==-1;){if(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n===t)break;t=n,s=!0}t.indexOf("_{")!==-1&&(n=t.replace(/_{/g,"_{"),n!==t&&(t=n,s=!0)),t.indexOf("}_")!==-1&&(n=t.replace(/}_/g,"}"),n!==t&&(t=n,s=!0)),t.indexOf("\\\\")!==-1&&(n=t.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),n!==t&&(t=n,s=!0)),s&&(e.innerHTML=t)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){"use strict";function e(){var e=document.querySelectorAll("p, li, td, div, span, h1, h2, h3, h4, h5, h6, ol, ul");e.forEach(function(e){if(e.classList.contains("classical-quote-container")||e.classList.contains("quote-text")||e.closest(".classical-quote-container"))return;var n,t=e.innerHTML,s=!1;for(t.indexOf("<em>")!==-1&&(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("</em>")!==-1&&(n=t.replace(/\$([^$]*?)<\/em>([^$]*?)\$/g,function(e,t,n){return"$"+t+"_{"+n+"$"}),n=n.replace(/\$\$([^$]*?)<\/em>([^$]*?)\$\$/g,function(e,t,n){return"$$"+t+"_{"+n+"$$"}),n!==t&&(t=n,s=!0)),t.indexOf("<em>$")!==-1&&(n=t.replace(/<em>\$/g,"$"),n!==t&&(t=n,s=!0)),t.indexOf("</em>$")!==-1&&(n=t.replace(/<\/em>\$/g,"$"),n!==t&&(t=n,s=!0));t.indexOf("<em>")!==-1&&t.indexOf("</em>")!==-1;){if(n=t.replace(/\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$/g,function(e,t,n,s){return/^[\d.,]+$/.test((t+n+s).replace(/[.,]/g,""))?e:"$"+t+"_"+n+s+"$"}),n=n.replace(/\$\$([^$]*?)<em>([^<]+?)<\/em>([^$]*?)\$\$/g,function(e,t,n,s){return"$$"+t+"_"+n+s+"$$"}),n===t)break;t=n,s=!0}t.indexOf("_{")!==-1&&(n=t.replace(/_{/g,"_{"),n!==t&&(t=n,s=!0)),t.indexOf("}_")!==-1&&(n=t.replace(/}_/g,"}"),n!==t&&(t=n,s=!0)),t.indexOf("\\\\")!==-1&&(n=t.replace(/(\$\{1,2\})([^$]*?)(\$\{1,2\})/g,function(e,t,n,s){for(;n.indexOf("\\\\")!==-1;)n=n.replace(/\\\\/g,"\\");return t+n+s}),n!==t&&(t=n,s=!0)),s&&(e.innerHTML=t)})}window.MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,tags:"ams",macros:{oiint:"\\mathop{∯}",oiiint:"\\mathop{∰}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:function(){return e(),MathJax.startup.defaultPageReady().then(function(){console.log("MathJax formulas rendered successfully")})}}}})()</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/css/mermaid-custom.css><script src=/js/mermaid-converter.js></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';

  
  setTimeout(() => {
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#e3f2fd',
        primaryTextColor: '#1565c0',
        primaryBorderColor: '#2196f3',
        lineColor: '#42a5f5',
        secondaryColor: '#f3e5f5',
        tertiaryColor: '#fff9c4',
        background: '#ffffff',
        mainBkg: '#ffffff',
        nodeBorder: '#2196f3',
        clusterBkg: '#ffffff',
        clusterBorder: '#e0e0e0',
        titleColor: '#1565c0',
        edgeLabelBackground: '#fafafa',
      },
      securityLevel: 'loose',
    });
  }, 100);
</script><script src=/js/toc.js></script></body></html>